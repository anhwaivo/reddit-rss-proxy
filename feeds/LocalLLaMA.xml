<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-02T12:49:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mfor6n</id>
    <title>RAG or prompt engineering</title>
    <updated>2025-08-02T11:57:33+00:00</updated>
    <author>
      <name>/u/SignatureHuman8057</name>
      <uri>https://old.reddit.com/user/SignatureHuman8057</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt; &lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?&lt;/p&gt; &lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?&lt;/p&gt; &lt;p&gt;So both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignatureHuman8057"&gt; /u/SignatureHuman8057 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T11:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mffjjj</id>
    <title>EasyWhisperUI – GPU accelerated Open Source Whisper UI for Windows &amp; macOS now with Live Transcriptions!</title>
    <updated>2025-08-02T02:43:45+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, it’s been a while but I’m happy to announce another major update for my app &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;, now with &lt;strong&gt;live transcriptions&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;It features full cross-platform GPU acceleration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; on Windows (Intel, AMD, or NVIDIA)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Metal&lt;/strong&gt; on macOS (Apple silicon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New features!&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GPU-accelerated Live Transcriptions&lt;/strong&gt; • Transcribe speech in real time using your default mic (user request)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Cleanup&lt;/strong&gt; • Automatically removes repeated segments from live transcriptions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open in Notepad Checkbox&lt;/strong&gt; • New option to disable automatic opening in Notepad after transcription (user request)&lt;/li&gt; &lt;li&gt;Various bug fixes and code improvements.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Other key features&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Batch File Processing&lt;/strong&gt; • Drag &amp;amp; drop multiple files — EasyWhisperUI will queue and transcribe them automatically (user request)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU-Only Toggle&lt;/strong&gt; • Option to disable GPU acceleration and run fully on CPU (user request)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modern UI&lt;/strong&gt; • Acrylic background on Windows, clean layout and spacing improvements&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;macOS Support&lt;/strong&gt; • EasyWhisperUI works on macOS thanks to a community contribution&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installer Included&lt;/strong&gt; • Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There are a lot more features — check out the GitHub for more info:&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have any suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T02:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8la7</id>
    <title>Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good</title>
    <updated>2025-08-01T21:19:07+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt; &lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepeqh</id>
    <title>The OpenAI Open weight model might be 120B</title>
    <updated>2025-08-01T06:47:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt; &lt;img alt="The OpenAI Open weight model might be 120B" src="https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg" title="The OpenAI Open weight model might be 120B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt; &lt;p&gt;So as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mepeqh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T06:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1meu3jn</id>
    <title>Gemini 2.5 Deep Think mode benchmarks!</title>
    <updated>2025-08-01T11:36:06+00:00</updated>
    <author>
      <name>/u/Beautiful-Essay1945</name>
      <uri>https://old.reddit.com/user/Beautiful-Essay1945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt; &lt;img alt="Gemini 2.5 Deep Think mode benchmarks!" src="https://preview.redd.it/8wnv6pme9egf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=557a01b879fc1bdbf0cc88dc3a91d0b4a7b1c10c" title="Gemini 2.5 Deep Think mode benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful-Essay1945"&gt; /u/Beautiful-Essay1945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8wnv6pme9egf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T11:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfn2xf</id>
    <title>Saidia: Offline-First AI Assistant for Educators in low-connectivity regions</title>
    <updated>2025-08-02T10:16:15+00:00</updated>
    <author>
      <name>/u/dokasto_</name>
      <uri>https://old.reddit.com/user/dokasto_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt; &lt;p&gt;Built using Electron, Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dokasto/Saidia"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dokasto_"&gt; /u/dokasto_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf3nw4</id>
    <title>I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape</title>
    <updated>2025-08-01T18:05:28+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt; &lt;img alt="I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape" src="https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg" title="I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt; &lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt; &lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt; &lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt; &lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt; &lt;p&gt;You're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt; &lt;p&gt;That's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt; &lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers. You're overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt; &lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt; &lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt; &lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns. Make sure you're on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt; &lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/c2/leaderboard"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/c2/explorer"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/the-crypt-keeper/reasonscape"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:05:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfda7s</id>
    <title>Horizon Beta - new openai open source model?</title>
    <updated>2025-08-02T00:49:51+00:00</updated>
    <author>
      <name>/u/popsumbong</name>
      <uri>https://old.reddit.com/user/popsumbong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/"&gt; &lt;img alt="Horizon Beta - new openai open source model?" src="https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4" title="Horizon Beta - new openai open source model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/popsumbong"&gt; /u/popsumbong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/openrouter/horizon-beta"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T00:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0qlf</id>
    <title>Qwen3-235B-A22B-2507 is the top open weights model on lmarena</title>
    <updated>2025-08-01T16:14:40+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmarena_ai/status/1951308670375174457"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfariy</id>
    <title>All local Roo Code and qwen3 coder 30B Q8</title>
    <updated>2025-08-01T22:51:12+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"&gt; &lt;img alt="All local Roo Code and qwen3 coder 30B Q8" src="https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931537c4a1cfa56a0f6a573590137de43c243511" title="All local Roo Code and qwen3 coder 30B Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Roo Code with &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mostlygeek/vibecities"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt; &lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's my llama-swap config: &lt;/p&gt; &lt;p&gt;``` macros: &amp;quot;qwen3-coder-server&amp;quot;: | /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0 --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05 --jinja --swa-full&lt;/p&gt; &lt;p&gt;models: &amp;quot;Q3-30B-CODER-3090&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot; description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot; filters: # enforce recommended params for model strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot; cmd: | ${qwen3-coder-server} --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf --ctx-size 184320 # rebalance layers/context a bit better across dual GPUs --tensor-split 46,54 ```&lt;/p&gt; &lt;p&gt;Roo code MCP settings: &lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;mcpServers&amp;quot;: { &amp;quot;vibecities&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;, &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;, &amp;quot;headers&amp;quot;: { &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot; }, &amp;quot;alwaysAllow&amp;quot;: [ &amp;quot;page_list&amp;quot;, &amp;quot;page_set&amp;quot;, &amp;quot;page_get&amp;quot; ], &amp;quot;disabled&amp;quot;: false } } } &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g5aj1csfjhgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfk3y2</id>
    <title>MetaStoneTec/XBai-o4</title>
    <updated>2025-08-02T07:00:21+00:00</updated>
    <author>
      <name>/u/ljosif</name>
      <uri>https://old.reddit.com/user/ljosif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried &lt;a href="https://huggingface.co/MetaStoneTec/XBai-o4"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt; &lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance &lt;/p&gt; &lt;p&gt;Have not tried it myself, downloading atm from &lt;a href="https://huggingface.co/mradermacher/XBai-o4-GGUF"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljosif"&gt; /u/ljosif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfnq2r</id>
    <title>Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090</title>
    <updated>2025-08-02T10:57:41+00:00</updated>
    <author>
      <name>/u/kargafe</name>
      <uri>https://old.reddit.com/user/kargafe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt; &lt;img alt="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" src="https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f" title="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt; &lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. I used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison. I'm also planning to run some fine-tuning benchmarks later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kargafe"&gt; /u/kargafe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erib4a6t7lgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgwyu</id>
    <title>Horizon Alpha vs Horizon Beta</title>
    <updated>2025-08-02T03:55:12+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/"&gt; &lt;img alt="Horizon Alpha vs Horizon Beta" src="https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9f64a0239d8ac07dfc6361bbf8a90a0b8fa7592" title="Horizon Alpha vs Horizon Beta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beta seems really solid from early testing, not a magnitude better than what SOTA's offer but still impressive&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dg8cy7ia4jgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T03:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfldxj</id>
    <title>Small LLM in german</title>
    <updated>2025-08-02T08:22:20+00:00</updated>
    <author>
      <name>/u/Ghulaschsuppe</name>
      <uri>https://old.reddit.com/user/Ghulaschsuppe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ghulaschsuppe"&gt; /u/Ghulaschsuppe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T08:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfjn88</id>
    <title>TTS Model Comparisons: My Personal Rankings (So far) of TTS Models</title>
    <updated>2025-08-02T06:32:11+00:00</updated>
    <author>
      <name>/u/iKontact</name>
      <uri>https://old.reddit.com/user/iKontact</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt; &lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt; &lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKontact"&gt; /u/iKontact &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T06:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfn7pv</id>
    <title>Tool calling is now supported on World's first Intermediate Reasoning model</title>
    <updated>2025-08-02T10:24:52+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt; &lt;p&gt;Updated Model link:- &lt;a href="https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br /&gt; API and Chat page :- &lt;a href="https://helpingai.co"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfbw8a</id>
    <title>DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls</title>
    <updated>2025-08-01T23:43:00+00:00</updated>
    <author>
      <name>/u/JAlbrethsen</name>
      <uri>https://old.reddit.com/user/JAlbrethsen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"&gt; &lt;img alt="DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls" src="https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24b8b3213823bb044c73076e1852d1957545a17f" title="DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JAlbrethsen"&gt; /u/JAlbrethsen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T23:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfeazc</id>
    <title>Cerebras Pro Coder Deceptive Limits</title>
    <updated>2025-08-02T01:41:01+00:00</updated>
    <author>
      <name>/u/snipsthekittycat</name>
      <uri>https://old.reddit.com/user/snipsthekittycat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt; &lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snipsthekittycat"&gt; /u/snipsthekittycat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T01:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8pdo</id>
    <title>China report the finetune deepseek scientific model 40.44% on HLE</title>
    <updated>2025-08-01T21:23:37+00:00</updated>
    <author>
      <name>/u/Afraid_Hall_2971</name>
      <uri>https://old.reddit.com/user/Afraid_Hall_2971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt; &lt;img alt="China report the finetune deepseek scientific model 40.44% on HLE" src="https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19933808cef3cec6dce268be3e9d5d269f435579" title="China report the finetune deepseek scientific model 40.44% on HLE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hg：&lt;a href="https://huggingface.co/ScienceOne-AI/S1-Base-671B"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Afraid_Hall_2971"&gt; /u/Afraid_Hall_2971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rnyzqia76hgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf92r1</id>
    <title>MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs</title>
    <updated>2025-08-01T21:38:58+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt; &lt;img alt="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" src="https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg" title="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt; &lt;h1&gt;Autonomous Research Mode&lt;/h1&gt; &lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt; &lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt; &lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt; &lt;h1&gt;Document Management&lt;/h1&gt; &lt;p&gt;The application is built around a document management system.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mf92r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf3tm9</id>
    <title>The “Leaked” 120 B OpenAI Model is not Trained in FP4</title>
    <updated>2025-08-01T18:11:35+00:00</updated>
    <author>
      <name>/u/badbutt21</name>
      <uri>https://old.reddit.com/user/badbutt21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt; &lt;img alt="The “Leaked” 120 B OpenAI Model is not Trained in FP4" src="https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06" title="The “Leaked” 120 B OpenAI Model is not Trained in FP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badbutt21"&gt; /u/badbutt21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g1yk8r6b8ggf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfll39</id>
    <title>AI models are picking up hidden habits from each other | IBM</title>
    <updated>2025-08-02T08:35:31+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"&gt; &lt;img alt="AI models are picking up hidden habits from each other | IBM" src="https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46e4049e81a052bc62430cfe7e667f62662c693b" title="AI models are picking up hidden habits from each other | IBM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ibm.com/think/news/ai-models-subliminal-learning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T08:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfaigh</id>
    <title>We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)</title>
    <updated>2025-08-01T22:40:00+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Organization&lt;/th&gt; &lt;th align="left"&gt;HuggingFace Link&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Modality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;dots.ocr&lt;/td&gt; &lt;td align="left"&gt;REDnote Hilab&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5-Air&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;480B-A35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Base&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intern S1&lt;/td&gt; &lt;td align="left"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;241B-A22B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;49B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 7B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 14B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 32B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;step3&lt;/td&gt; &lt;td align="left"&gt;StepFun&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/stepfun-ai/step3"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;321B-A38B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B-A0.6B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X Instruct-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X PPO-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Small 24B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Mini 3B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B Base&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ling lite-1.5 2506&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-lite-1.5-2506"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ming Lite Omni-1.5&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;20.3B&lt;/td&gt; &lt;td align="left"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 32B 0727&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 4B 0729&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 8B&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;command a vision 07-2025&lt;/td&gt; &lt;td align="left"&gt;Cohere&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-vision-07-2025"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;112B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT V1 40B&lt;/td&gt; &lt;td align="left"&gt;Kwaipilot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;40B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 2B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;671B-A37B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-405B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;405B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;109B-A17B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-70B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 4.0 VL Light&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-4.0-VL-Light"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 3.1&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-3.1"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmOCR 7B 0725&lt;/td&gt; &lt;td align="left"&gt;AllenAI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5v 3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 7B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-7B"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 21B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-21B"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 70B preview SFT&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt; &lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt; &lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;[EDIT] &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyone could publish a new model—with some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:40:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfitwb</id>
    <title>Skywork MindLink 32B/72B</title>
    <updated>2025-08-02T05:41:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"&gt; &lt;img alt="Skywork MindLink 32B/72B" src="https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7" title="Skywork MindLink 32B/72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/MindLink-32B-0801"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/MindLink-72B-0801"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/im7w319dnjgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T05:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgj0g</id>
    <title>all I need....</title>
    <updated>2025-08-02T03:34:51+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt; &lt;img alt="all I need...." src="https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd" title="all I need...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggc3dzhr0jgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T03:34:51+00:00</published>
  </entry>
</feed>
