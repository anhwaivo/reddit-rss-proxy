<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-17T22:49:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ldojsu</id>
    <title>What's your favorite desktop client?</title>
    <updated>2025-06-17T14:26:59+00:00</updated>
    <author>
      <name>/u/tuananh_org</name>
      <uri>https://old.reddit.com/user/tuananh_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I forgot to mention Linux. Prefer one with MCP support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuananh_org"&gt; /u/tuananh_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T14:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lduxn0</id>
    <title>🧠 New Paper Alert: Curriculum Learning Boosts LLM Training Efficiency!</title>
    <updated>2025-06-17T18:31:12+00:00</updated>
    <author>
      <name>/u/Ok-Cut-3551</name>
      <uri>https://old.reddit.com/user/Ok-Cut-3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🧠 &lt;strong&gt;New Paper Alert: Curriculum Learning Boosts LLM Training Efficiency&lt;/strong&gt;&lt;br /&gt; 📄 &lt;a href="https://arxiv.org/abs/2506.11300"&gt;Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔥 Over &lt;strong&gt;200+ pretraining runs&lt;/strong&gt; analyzed in this large-scale study exploring &lt;strong&gt;Curriculum Learning (CL)&lt;/strong&gt; as an alternative to random data sampling. The paper shows how &lt;strong&gt;organizing training data from easy to hard&lt;/strong&gt; (instead of shuffling everything) can lead to faster convergence and better final performance.&lt;/p&gt; &lt;h1&gt;🧩 Key Takeaways:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Evaluated &lt;strong&gt;3 curriculum strategies&lt;/strong&gt;: → &lt;em&gt;Vanilla CL&lt;/em&gt; (strict easy-to-hard) → &lt;em&gt;Pacing-based sampling&lt;/em&gt; (gradual mixing) → &lt;em&gt;Interleaved curricula&lt;/em&gt; (injecting harder examples early)&lt;/li&gt; &lt;li&gt;Tested &lt;strong&gt;6 difficulty metrics&lt;/strong&gt; to rank training data.&lt;/li&gt; &lt;li&gt;CL warm-up improved performance by &lt;strong&gt;up to 3.5%&lt;/strong&gt; compared to random sampling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This work is one of the &lt;strong&gt;most comprehensive investigations of curriculum strategies for LLMs pretraining&lt;/strong&gt; to date, and the insights are actionable even for smaller-scale local training.&lt;/p&gt; &lt;p&gt;🔗 Full preprint: &lt;a href="https://arxiv.org/abs/2506.11300"&gt;https://arxiv.org/abs/2506.11300&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Cut-3551"&gt; /u/Ok-Cut-3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lduxn0/new_paper_alert_curriculum_learning_boosts_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lduxn0/new_paper_alert_curriculum_learning_boosts_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lduxn0/new_paper_alert_curriculum_learning_boosts_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T18:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld116d</id>
    <title>MiniMax latest open-sourcing LLM, MiniMax-M1 — setting new standards in long-context reasoning,m</title>
    <updated>2025-06-16T18:42:52+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt; &lt;img alt="MiniMax latest open-sourcing LLM, MiniMax-M1 — setting new standards in long-context reasoning,m" src="https://external-preview.redd.it/NmY1emg2N3kzYzdmMYrLLSKpxq16_nlRw_xdAcAPTlqNhk8r4UDdsUawD6kP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3275f690016b299979a56d72371c6133b5aa21d3" title="MiniMax latest open-sourcing LLM, MiniMax-M1 — setting new standards in long-context reasoning,m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The coding demo in video is so amazing!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;World’s longest context window: 1M-token input, 80k-token output&lt;/li&gt; &lt;li&gt;State-of-the-art agentic use among open-source models&lt;/li&gt; &lt;li&gt;&lt;p&gt;RL at unmatched efficiency: trained with just $534,700&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40k: ​&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-40k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;80k: ​&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-80k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Space: ​&lt;a href="https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1"&gt;https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1"&gt;https://github.com/MiniMax-AI/MiniMax-M1&lt;/a&gt; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tech Report: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf"&gt;https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Apache 2.0 license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t859utey3c7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1le0b5t</id>
    <title>Which search engine to use with Open WebUI</title>
    <updated>2025-06-17T22:04:19+00:00</updated>
    <author>
      <name>/u/MengerianMango</name>
      <uri>https://old.reddit.com/user/MengerianMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get away from being tied to chatgpt. I tried DDG first, but they rate limit so hard. I'm now using brave pro ai, but it doesn't seem like it reliably returns useful context. I've tried asking for the weather tomorrow in my city, fail. Tried asking a simple query &amp;quot;For 64 bit vectorizable operations, should I expect Ryzen 9950x or RTX 6000 Blackwell to outperform?&amp;quot;, fail -- even failed with follow up simplified question &amp;quot;can you just compare the FLOPS&amp;quot;, it can't even get 2 numbers to make a table. Super disappointing. It's not the model. I've tried with local models and I even connected gpt-4.1. Seems like no matter the quality of the model or the quality of the search terms, results are garbage. This shouldn't be hard. ChatGPT (ie their web interface) handles it trivially.&lt;/p&gt; &lt;p&gt;So I'm here to ask what you guys are using and having some success with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MengerianMango"&gt; /u/MengerianMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0b5t/which_search_engine_to_use_with_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0b5t/which_search_engine_to_use_with_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le0b5t/which_search_engine_to_use_with_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T22:04:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldayo0</id>
    <title>Deepseek r1 0528 ties opus for #1 rank on webdev</title>
    <updated>2025-06-17T01:45:49+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;685 B params. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1934650635657367671"&gt;https://x.com/lmarena_ai/status/1934650635657367671&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T01:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lddrfu</id>
    <title>Quartet - a new algorithm for training LLMs in native FP4 on 5090s</title>
    <updated>2025-06-17T04:08:30+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this paper while looking to see if training LLMs on Blackwell's new FP4 hardware was possible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/papers/2505.14669"&gt;Quartet: Native FP4 Training Can Be Optimal for Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and the associated code, with kernels you can use for your own training:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/IST-DASLab/Quartet"&gt;https://github.com/IST-DASLab/Quartet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to these researchers, training in FP4 is now a reasonable, and in many cases optimal, alternative to higher precision training!&lt;/p&gt; &lt;p&gt;DeepSeek was trained in FP8, which was cutting edge at the time. I can't wait to see the new frontiers FP4 unlocks.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;I just tried to install it to start experimenting. Even though their README states &amp;quot;Kernels are 'Coming soon...'&amp;quot;, they created the python library for consumers to use a &lt;a href="https://github.com/IST-DASLab/Quartet/pull/3"&gt;couple weeks ago in a PR called &amp;quot;Kernels&amp;quot;&lt;/a&gt;, and included them in the initial release.&lt;/p&gt; &lt;p&gt;It seems that the actual cuda kernels are contained in a python package called &lt;code&gt;qutlass&lt;/code&gt;, however, and that does not appear to be published anywhere yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T04:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjq1m</id>
    <title>I love the inference performances of QWEN3-30B-A3B but how do you use it in real world use case ? What prompts are you using ? What is your workflow ? How is it useful for you ?</title>
    <updated>2025-06-17T10:34:34+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys I successful run on my old laptop QWEN3-30B-A3B-Q4-UD with 32K token window&lt;/p&gt; &lt;p&gt;I wanted to know how you use in real world use case this model.&lt;/p&gt; &lt;p&gt;And what are you best prompts for this specific model&lt;/p&gt; &lt;p&gt;Feel free to share your journey with me I need inspiration&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjd5t</id>
    <title>Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons</title>
    <updated>2025-06-17T10:12:14+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2506.01963"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjd5t/breaking_quadratic_barriers_a_nonattention_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjd5t/breaking_quadratic_barriers_a_nonattention_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld66t0</id>
    <title>Fortune 500s Are Burning Millions on LLM APIs. Why Not Build Their Own?</title>
    <updated>2025-06-16T22:04:50+00:00</updated>
    <author>
      <name>/u/Neat-Knowledge5642</name>
      <uri>https://old.reddit.com/user/Neat-Knowledge5642</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You’re at a Fortune 500 company, spending millions annually on LLM APIs (OpenAI, Google, etc). Yet you’re limited by IP concerns, data control, and vendor constraints.&lt;/p&gt; &lt;p&gt;At what point does it make sense to build your own LLM in-house?&lt;/p&gt; &lt;p&gt;I work at a company behind one of the major LLMs, and the amount enterprises pay us is wild. Why aren’t more of them building their own models? Is it talent? Infra complexity? Risk aversion?&lt;/p&gt; &lt;p&gt;Curious where this logic breaks.&lt;/p&gt; &lt;p&gt;Edit: What about an acquisition? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat-Knowledge5642"&gt; /u/Neat-Knowledge5642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T22:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldl4ii</id>
    <title>Latent Attention for Small Language Models</title>
    <updated>2025-06-17T11:53:39+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt; &lt;img alt="Latent Attention for Small Language Models" src="https://b.thumbs.redditmedia.com/mwEsle7ZgYWmGBZGHOvoYZ1KFhy1Sk-MrECs6_POs7M.jpg" title="Latent Attention for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h4pmsjrt7h7f1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1406dd1c4fe6260378cd828114ffaf2f1724b600"&gt;https://preview.redd.it/h4pmsjrt7h7f1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1406dd1c4fe6260378cd828114ffaf2f1724b600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to paper: &lt;a href="https://arxiv.org/pdf/2506.09342"&gt;https://arxiv.org/pdf/2506.09342&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1) We trained 30M parameter Generative Pre-trained Transformer (GPT) models on 100,000 synthetic stories and benchmarked three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE).&lt;/p&gt; &lt;p&gt;(2) It led to a beautiful study in which we showed that MLA outperforms MHA: 45% memory reduction and 1.4 times inference speedup with minimal quality loss. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;This shows 2 things:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;(1) Small Language Models (SLMs) can become increasingly powerful when integrated with Multi-Head Latent Attention (MLA). &lt;/p&gt; &lt;p&gt;(2) All industries and startups building SLMs should replace MHA with MLA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T11:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldokl7</id>
    <title>Best frontend for vllm?</title>
    <updated>2025-06-17T14:27:51+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to optimise my inferences. &lt;/p&gt; &lt;p&gt;I use LM studio for an easy inference of llama.cpp but was wondering if there is a gui for more optimised inference. &lt;/p&gt; &lt;p&gt;Also is there anther gui for llama.cpp that lets you tweak inference settings a bit more? Like expert offloading etc? &lt;/p&gt; &lt;p&gt;Thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T14:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldu04l</id>
    <title>SAGA Update: Now with Autonomous Knowledge Graph Healing &amp; A More Robust Core!</title>
    <updated>2025-06-17T17:56:02+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, everyone!&lt;/p&gt; &lt;p&gt;A few weeks ago, I shared a major update to SAGA (Semantic And Graph-enhanced Authoring), my autonomous novel generation project. The response was incredible, and since then, I've been focused on making the system not just more capable, but smarter, more maintainable, and more professional. I'm thrilled to share the next evolution of SAGA and its NANA engine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Refresher: What is SAGA?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA is an open-source project designed to write entire novels. It uses a team of specialized AI agents for planning, drafting, evaluation, and revision. The magic comes from its &amp;quot;long-term memory&amp;quot;—a Neo4j graph database—that tracks characters, world-building, and plot, allowing SAGA to maintain coherence over tens of thousands of words.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New &amp;amp; Improved? This is a Big One!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This update moves SAGA from a clever pipeline to a truly intelligent, self-maintaining system.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Autonomous Knowledge Graph Maintenance &amp;amp; Healing!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; The &lt;code&gt;KGMaintainerAgent&lt;/code&gt; is no longer just an updater; it's now a &lt;strong&gt;healer&lt;/strong&gt;. Periodically (every &lt;code&gt;KG_HEALING_INTERVAL&lt;/code&gt; chapters), it runs a maintenance cycle to: &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Resolve Duplicate Entities:&lt;/strong&gt; Finds similarly named characters or items (e.g., &amp;quot;The Sunstone&amp;quot; and &amp;quot;Sunstone&amp;quot;) and uses an LLM to decide if they should be merged in the graph.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Enrich &amp;quot;Thin&amp;quot; Nodes:&lt;/strong&gt; Identifies stub entities (like a character mentioned in a relationship but never described) and uses an LLM to generate a plausible description based on context.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Run Consistency Checks:&lt;/strong&gt; Actively looks for contradictions in the graph, like a character having both &amp;quot;Brave&amp;quot; and &amp;quot;Cowardly&amp;quot; traits, or a character performing actions after they were marked as dead.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;From Markdown to Validated YAML for User Input:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Initial setup is now driven by a much more robust &lt;code&gt;user_story_elements.yaml&lt;/code&gt; file.&lt;/li&gt; &lt;li&gt; This input is validated against Pydantic models, making it far more reliable and structured than the previous Markdown parser. The &lt;code&gt;[Fill-in]&lt;/code&gt; placeholder system is still fully supported.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Professional Data Access Layer:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; This is a huge architectural improvement. All direct Neo4j queries have been moved out of the agents and into a dedicated &lt;code&gt;data_access&lt;/code&gt; package (&lt;code&gt;character_queries&lt;/code&gt;, &lt;code&gt;world_queries&lt;/code&gt;, etc.).&lt;/li&gt; &lt;li&gt; This makes the system much cleaner, easier to maintain, and separates the &amp;quot;how&amp;quot; of data storage from the &amp;quot;what&amp;quot; of agent logic.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Formalized KG Schema &amp;amp; Smarter Patching:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; The Knowledge Graph schema (all node labels and relationship types) is now formally defined in &lt;code&gt;kg_constants.py&lt;/code&gt;.&lt;/li&gt; &lt;li&gt; The revision logic is now smarter, with the patch-generation LLM able to suggest an explicit &lt;strong&gt;deletion&lt;/strong&gt; of a text segment by returning an empty string, allowing for more nuanced revisions than just replacement.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Smarter Planning &amp;amp; Decoupled Finalization:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; The &lt;code&gt;PlannerAgent&lt;/code&gt; now generates more sophisticated scene plans that include &amp;quot;directorial&amp;quot; cues like &lt;code&gt;scene_type&lt;/code&gt; (&amp;quot;ACTION&amp;quot;, &amp;quot;DIALOGUE&amp;quot;), &lt;code&gt;pacing&lt;/code&gt;, and &lt;code&gt;character_arc_focus&lt;/code&gt;.&lt;/li&gt; &lt;li&gt; A new &lt;code&gt;FinalizeAgent&lt;/code&gt; cleanly handles all end-of-chapter tasks (summarizing, KG extraction, saving), making the main orchestration loop much cleaner.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Upgraded Configuration System:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Configuration is now managed by Pydantic's &lt;code&gt;BaseSettings&lt;/code&gt; in &lt;code&gt;config.py&lt;/code&gt;, allowing for easy and clean overrides from a &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Core Architecture: Now More Robust&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The agentic pipeline is still the heart of SAGA, but it's now more refined:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;strong&gt;Initial Setup:&lt;/strong&gt; Parses &lt;code&gt;user_story_elements.yaml&lt;/code&gt; or generates initial story elements, then performs a full sync to Neo4j.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Chapter Loop:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Plan:&lt;/strong&gt; &lt;code&gt;PlannerAgent&lt;/code&gt; details scenes with directorial focus.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Context:&lt;/strong&gt; Hybrid semantic &amp;amp; KG context is built.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Draft:&lt;/strong&gt; &lt;code&gt;DraftingAgent&lt;/code&gt; writes the chapter.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Evaluate:&lt;/strong&gt; &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; &amp;amp; &lt;code&gt;WorldContinuityAgent&lt;/code&gt; scrutinize the draft.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Revise:&lt;/strong&gt; &lt;code&gt;revision_logic&lt;/code&gt; applies targeted patches (including deletions) or performs a full rewrite.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Finalize:&lt;/strong&gt; The new &lt;code&gt;FinalizeAgent&lt;/code&gt; takes over, using the &lt;code&gt;KGMaintainerAgent&lt;/code&gt; to extract knowledge, summarize, and save everything to Neo4j.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Heal (Periodic):&lt;/strong&gt; The &lt;code&gt;KGMaintainerAgent&lt;/code&gt; runs its new maintenance cycle to improve the graph's health and consistency.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why This Matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;These changes are about building a system that can &lt;em&gt;truly scale&lt;/em&gt;. An autonomous writer that can create a 50-chapter novel needs a way to self-correct its own &amp;quot;memory&amp;quot; and understanding. The KG healing, robust data layer, and improved configuration are all foundational pieces for that long-term goal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance is Still Strong:&lt;/strong&gt; Using local GGUF models (Qwen3 14B for narration/planning, smaller Qwen3s for other tasks), SAGA still generates: * &lt;strong&gt;3 chapters&lt;/strong&gt; (each ~13,000+ tokens of narrative) * In approximately &lt;strong&gt;11 minutes&lt;/strong&gt; * This includes all planning, evaluation, KG updates, and now the potential for KG healing cycles.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Lanerra/saga/blob/master/SAGA-KG-Ch18.png"&gt;Knowledge Graph at 18 chapters&lt;/a&gt; &lt;code&gt;plaintext Novel: The Edge of Knowing Current Chapter: 18 Current Step: Run Finished Tokens Generated (this run): 180,961 Requests/Min: 257.91 Elapsed Time: 01:15:55 &lt;/code&gt; &lt;strong&gt;Check it out &amp;amp; Get Involved:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt; (The README has been completely rewritten to reflect the new architecture!)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Setup:&lt;/strong&gt; You'll need Python, Ollama (for embeddings), an OpenAI-API compatible LLM server, and Neo4j (a &lt;code&gt;docker-compose.yml&lt;/code&gt; is provided).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Resetting:&lt;/strong&gt; To start fresh, &lt;code&gt;docker-compose down -v&lt;/code&gt; is the cleanest way to wipe the Neo4j volume.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm incredibly excited about these updates. SAGA feels less like a script and more like a true, learning system now. I'd love for you to pull the latest version, try it out, and see what sagas NANA can spin up for you with its newly enhanced intelligence.&lt;/p&gt; &lt;p&gt;As always, feedback, ideas, and issues are welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldu04l/saga_update_now_with_autonomous_knowledge_graph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldu04l/saga_update_now_with_autonomous_knowledge_graph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldu04l/saga_update_now_with_autonomous_knowledge_graph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T17:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldfipl</id>
    <title>It seems as if the more you learn about AI, the less you trust it</title>
    <updated>2025-06-17T05:54:21+00:00</updated>
    <author>
      <name>/u/RhubarbSimilar1683</name>
      <uri>https://old.reddit.com/user/RhubarbSimilar1683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is kind of a rant so sorry if not everything has to do with the title, For example, when the blog post on vibe coding was released on February 2025, I was surprised to see the writer talking about using it mostly for disposable projects and not for stuff that will go to production since that is what everyone seems to be using it for. That blog post was written by an OpenAI employee. Then Geoffrey Hinton and Yann LeCun occasionally talk about how AI can be dangerous if misused or how LLMs are not that useful currently because they don't really reason at an architectural level yet you see tons of people without the same level of education on AI selling snake oil based on LLMs. You then see people talking about how LLMs completely replace programmers even though senior programmers point out they seem to make subtle bugs all the time that people often can't find nor fix because they didn't learn programming since they thought it was obsolete.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RhubarbSimilar1683"&gt; /u/RhubarbSimilar1683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T05:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldisw8</id>
    <title>nvidia/AceReason-Nemotron-1.1-7B · Hugging Face</title>
    <updated>2025-06-17T09:35:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"&gt; &lt;img alt="nvidia/AceReason-Nemotron-1.1-7B · Hugging Face" src="https://external-preview.redd.it/W0uW5ur2PESMsYX8R36VZEsECrEgC1Wcshp3sPMT3JY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e3d6301fc6618942fb7fc855b933ef80b8bc864" title="nvidia/AceReason-Nemotron-1.1-7B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T09:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1le0mpb</id>
    <title>Llama.cpp is much faster! Any changes made recently?</title>
    <updated>2025-06-17T22:17:52+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've ditched Ollama for about 3 months now, and been on a journey testing multiple wrappers. KoboldCPP coupled with llama swap has been good but I experienced so many hang ups (I leave my PC running 24/7 to serve AI requests), and waking up almost daily and Kobold (or in combination with AMD drivers) would not work. I had to reset llama swap or reboot the PC for it work again.&lt;/p&gt; &lt;p&gt;That said, I tried llama.cpp a few weeks ago and it wasn't smooth with Vulkan (likely some changes that was reverted back). Tried it again yesterday, and the inference speed is 20% faster on average across multiple model types and sizes.&lt;/p&gt; &lt;p&gt;Specifically for Vulkan, I didn't see anything major in the release notes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1le0mpb/llamacpp_is_much_faster_any_changes_made_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T22:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldhej3</id>
    <title>Who is ACTUALLY running local or open source model daily and mainly?</title>
    <updated>2025-06-17T07:59:50+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I've started to notice a lot of folk on here comment that they're using Claude or GPT, so: &lt;/p&gt; &lt;p&gt;Out of curiosity,&lt;br /&gt; - who is using local or open source models as their daily driver for any task: code, writing , agents?&lt;br /&gt; - what's you setup, are you serving remotely, sharing with friends, using local inference?&lt;br /&gt; - what kind if apps are you using? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T07:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldvosh</id>
    <title>Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp</title>
    <updated>2025-06-17T19:00:08+00:00</updated>
    <author>
      <name>/u/sipjca</name>
      <uri>https://old.reddit.com/user/sipjca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"&gt; &lt;img alt="Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp" src="https://external-preview.redd.it/bDzCT4ZXzfyUunJH2t6KbXRCZTeW_8YRhybKom8weVk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49821e08e57000fbfa143c29ef88ad4252597b67" title="Handy - a simple, open-source offline speech-to-text app written in Rust using whisper.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I built a simple, offline speech-to-text app after breaking my finger - now open sourcing it&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Made a cross-platform speech-to-text app using whisper.cpp that runs completely offline. Press shortcut, speak, get text pasted anywhere. It's rough around the edges but works well and is designed to be easily modified/extended - including adding LLM calls after transcription.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I broke my finger a while back and suddenly couldn't type properly. Tried existing speech-to-text solutions but they were either subscription-based, cloud-dependent, or I couldn't modify them to work exactly how I needed for coding and daily computer use.&lt;/p&gt; &lt;p&gt;So I built Handy - intentionally simple speech-to-text that runs entirely on your machine using whisper.cpp (Whisper Small model). No accounts, no subscriptions, no data leaving your computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Press keyboard shortcut → speak → press again (or use push-to-talk)&lt;/li&gt; &lt;li&gt;Transcribes with whisper.cpp and pastes directly into whatever app you're using&lt;/li&gt; &lt;li&gt;Works across Windows, macOS, Linux&lt;/li&gt; &lt;li&gt;GPU accelerated where available&lt;/li&gt; &lt;li&gt;Completely offline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's literally it. No fancy UI, no feature creep, just reliable local speech-to-text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I'm sharing this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was my first Rust project and there are definitely rough edges, but the core functionality works well. More importantly, I designed it to be easily forkable and extensible because that's what I was looking for when I started this journey.&lt;/p&gt; &lt;p&gt;The codebase is intentionally simple - you can understand the whole thing in an afternoon. If you want to add LLM integration (calling an LLM after transcription to rewrite/enhance the text), custom post-processing, or whatever else, the foundation is there and it's straightforward to extend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm hoping it might be useful for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;People who want reliable offline speech-to-text without subscriptions&lt;/li&gt; &lt;li&gt;Developers who want to experiment with voice computing interfaces&lt;/li&gt; &lt;li&gt;Anyone who prefers tools they can actually modify instead of being stuck with someone else's feature decisions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Project Reality&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are known bugs and architectural decisions that could be better. I'm documenting issues openly because I'd rather have people know what they're getting into. This isn't trying to compete with polished commercial solutions - it's trying to be the most hackable and modifiable foundation for people who want to build their own thing.&lt;/p&gt; &lt;p&gt;If you're looking for something perfect out of the box, this probably isn't it. If you're looking for something you can understand, modify, and make your own, it might be exactly what you need.&lt;/p&gt; &lt;p&gt;Would love feedback from anyone who tries it out, especially if you run into issues or see ways to make the codebase cleaner and more accessible for others to build on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sipjca"&gt; /u/sipjca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://handy.computer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldvosh/handy_a_simple_opensource_offline_speechtotext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T19:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldv6jb</id>
    <title>Newly Released MiniMax-M1 80B vs Claude Opus 4</title>
    <updated>2025-06-17T18:40:54+00:00</updated>
    <author>
      <name>/u/Just_Lingonberry_352</name>
      <uri>https://old.reddit.com/user/Just_Lingonberry_352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"&gt; &lt;img alt="Newly Released MiniMax-M1 80B vs Claude Opus 4" src="https://preview.redd.it/gwxrxooh8j7f1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3152d1553ef91f7e6cd9a02b26203647e9fb1e5" title="Newly Released MiniMax-M1 80B vs Claude Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lingonberry_352"&gt; /u/Just_Lingonberry_352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gwxrxooh8j7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv6jb/newly_released_minimaxm1_80b_vs_claude_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T18:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldr3ln</id>
    <title>Google launches Gemini 2.5 Flash Lite (API only)</title>
    <updated>2025-06-17T16:05:43+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"&gt; &lt;img alt="Google launches Gemini 2.5 Flash Lite (API only)" src="https://preview.redd.it/93ekds1ugi7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2557d0a1e69c350eb9aa80a0342ae2a8213d06" title="Google launches Gemini 2.5 Flash Lite (API only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://console.cloud.google.com/vertex-ai/studio/"&gt;https://console.cloud.google.com/vertex-ai/studio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pricing not yet announced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/93ekds1ugi7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T16:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldi5rs</id>
    <title>There are no plans for a Qwen3-72B</title>
    <updated>2025-06-17T08:52:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"&gt; &lt;img alt="There are no plans for a Qwen3-72B" src="https://preview.redd.it/wwq0gc8bbg7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58bc7167b2a1d6c339112ec6468ce00e1eff9e6f" title="There are no plans for a Qwen3-72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwq0gc8bbg7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T08:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjyhf</id>
    <title>Completed Local LLM Rig</title>
    <updated>2025-06-17T10:48:48+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt; &lt;img alt="Completed Local LLM Rig" src="https://external-preview.redd.it/HJkY2jxSg_GtjUMbmHI4EEBqY3YefZ9gwrvbaXuZONc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5994778697f45b5780284b90c07b85daa01d2e" title="Completed Local LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So proud it's finally done!&lt;/p&gt; &lt;p&gt;GPU: 4 x RTX 3090 CPU: TR 3945wx 12c RAM: 256GB DDR4@3200MT/s SSD: PNY 3040 2TB MB: Asrock Creator WRX80 PSU: Seasonic Prime 2200W RAD: Heatkiller MoRa 420 Case: Silverstone RV-02&lt;/p&gt; &lt;p&gt;Was a long held dream to fit 4 x 3090 in an ATX form factor, all in my good old Silverstone Raven from 2011. An absolute classic. GPU temps at 57C.&lt;/p&gt; &lt;p&gt;Now waiting for the Fractal 180mm LED fans to put into the bottom. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ldjyhf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldqroi</id>
    <title>A free goldmine of tutorials for the components you need to create production-level agents</title>
    <updated>2025-06-17T15:53:14+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I’ve just launched a free resource with 25 detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt; &lt;p&gt;The response so far has been incredible! (the repo got nearly 500 stars in just 8 hours from launch) This is part of my broader effort to create high-quality open source educational material. I already have over 100 code tutorials on GitHub with nearly 40,000 stars.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The link is in the first comment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The content is organized into these categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Orchestration&lt;/li&gt; &lt;li&gt;Tool integration&lt;/li&gt; &lt;li&gt;Observability&lt;/li&gt; &lt;li&gt;Deployment&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt; &lt;li&gt;Agent Frameworks&lt;/li&gt; &lt;li&gt;Model Customization&lt;/li&gt; &lt;li&gt;Multi-agent Coordination&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Evaluation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T15:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldv0hk</id>
    <title>:grab popcorn: OpenAI weighs “nuclear option” of antitrust complaint against Microsoft</title>
    <updated>2025-06-17T18:34:19+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"&gt; &lt;img alt=":grab popcorn: OpenAI weighs “nuclear option” of antitrust complaint against Microsoft" src="https://external-preview.redd.it/o-39gdKiRmg7xCtqAV9Kzd__IIP_fxUuIZpgEMOTxUU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c859f5b46d753fef35eae0d18284d780a193b411" title=":grab popcorn: OpenAI weighs “nuclear option” of antitrust complaint against Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/ai/2025/06/openai-weighs-nuclear-option-of-antitrust-complaint-against-microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldv0hk/grab_popcorn_openai_weighs_nuclear_option_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T18:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldxuk1</id>
    <title>The Gemini 2.5 models are sparse mixture-of-experts (MoE)</title>
    <updated>2025-06-17T20:24:15+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt; &lt;img alt="The Gemini 2.5 models are sparse mixture-of-experts (MoE)" src="https://b.thumbs.redditmedia.com/HQc2i0Um5x7pBOoOn-gVnTMmIXgB-srLNatUzMp3veI.jpg" title="The Gemini 2.5 models are sparse mixture-of-experts (MoE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the &lt;a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf"&gt;model report&lt;/a&gt;. It should be a surprise to noone, but it's good to see this being spelled out. We barely ever learn anything about the architecture of closed models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhyrdk2dqj7f1.png?width=1056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca3d89968dc6bf950d030bbab25243aeb7623cf4"&gt;https://preview.redd.it/zhyrdk2dqj7f1.png?width=1056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca3d89968dc6bf950d030bbab25243aeb7623cf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I am still hoping for a Gemma-3N report...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldxuk1/the_gemini_25_models_are_sparse_mixtureofexperts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T20:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldsez0</id>
    <title>Gemini 2.5 Pro and Flash are stable in AI Studio</title>
    <updated>2025-06-17T16:56:00+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"&gt; &lt;img alt="Gemini 2.5 Pro and Flash are stable in AI Studio" src="https://preview.redd.it/ng7glnbmpi7f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19eafc0ab72b6a67d86e4a94088f6a7857bf8cfb" title="Gemini 2.5 Pro and Flash are stable in AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's also a new Gemini 2.5 flash preview model at the bottom there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ng7glnbmpi7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T16:56:00+00:00</published>
  </entry>
</feed>
