<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-19T16:49:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k2ysh5</id>
    <title>Can any local models make these studio Ghibli style images?</title>
    <updated>2025-04-19T15:26:15+00:00</updated>
    <author>
      <name>/u/OnceMoreOntoTheBrie</name>
      <uri>https://old.reddit.com/user/OnceMoreOntoTheBrie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be a lot of fun if they could.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OnceMoreOntoTheBrie"&gt; /u/OnceMoreOntoTheBrie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ysh5/can_any_local_models_make_these_studio_ghibli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ysh5/can_any_local_models_make_these_studio_ghibli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ysh5/can_any_local_models_make_these_studio_ghibli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T15:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2o007</id>
    <title>Is there a formula or rule of thumb about the effect of increasing context size on tok/sec speed? Does it *linearly* slow down, or *exponentially* or ...?</title>
    <updated>2025-04-19T04:20:08+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, is there a way to estimate how much VRAM is needed to run a model with P parameters, quantized at Q bits per parameter, with context length C?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T04:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zn6o</id>
    <title>SGLang vs vLLM</title>
    <updated>2025-04-19T16:03:44+00:00</updated>
    <author>
      <name>/u/diptanuc</name>
      <uri>https://old.reddit.com/user/diptanuc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone here use SGLang in production? I am trying to understand where SGLang shines. We adopted vLLM in our company(Tensorlake), and it works well at any load when we use it for offline inference within functions.&lt;/p&gt; &lt;p&gt;I would imagine the main difference in performance would come from RadixAttention vs PagedAttention?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diptanuc"&gt; /u/diptanuc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2wt76</id>
    <title>Open source alternatives to Llamaparse?</title>
    <updated>2025-04-19T13:55:30+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for something similar to Llamaparse that will optimize documents for llm consumption but open source and can be integrated into a custom application. I plan to use it with both local and public models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wt76/open_source_alternatives_to_llamaparse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wt76/open_source_alternatives_to_llamaparse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wt76/open_source_alternatives_to_llamaparse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T13:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250fu</id>
    <title>Gemma 3 QAT launch with MLX, llama.cpp, Ollama, LM Studio, and Hugging Face</title>
    <updated>2025-04-18T13:31:34+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Some weeks ago we released GGUFs corresponding to the QAT checkpoints of Gemma 3. Thanks to QAT, the model is able to preserve similar quality as &lt;code&gt;bfloat16&lt;/code&gt; while significantly reducing the memory requirements to load the model. That is, QAT is an additional fine-tuning that makes the model more rigorous to quantization.&lt;/p&gt; &lt;p&gt;As we only released the GGUFs, we got feedback that it would be great to have the unquantized QAT-based checkpoints to allow people to quantize for their own tools. So...we did it! Today we're releasing the unquantized QAT-based checkpoints. The models preserve quality better than naive quantization. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also collaborated with Prince (from MLX), llama.cpp, Ollama, LM Studio, and Hugging Face to make sure you can use the models in all your favorite tools!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post : &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unquantized checkpoints: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; (try ollama run gemma3:12b-it-qat)&lt;/li&gt; &lt;li&gt;LM Studio: &lt;a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"&gt;https://lmstudio.ai/model/gemma-3-12b-it-qat&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX: &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"&gt;https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2paa5</id>
    <title>Is Gemma3-12B-QAT bad?</title>
    <updated>2025-04-19T05:43:16+00:00</updated>
    <author>
      <name>/u/FbF_</name>
      <uri>https://old.reddit.com/user/FbF_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying it out compared to the Bartowski's Q4_K_M version and it seems noticeably worse. It just tends to be more repetitive and summarize the prompt uncritically. It's not clear to me if they compared the final QAT model with the non-quantized BF16 version in their proclamation of having a better quantization. Has anyone else had the same experience or done more in-depth analyses on the difference in output with the non-quantized model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FbF_"&gt; /u/FbF_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k27fz2</id>
    <title>I created an interactive tool to visualize *every* attention weight matrix within GPT-2!</title>
    <updated>2025-04-18T15:18:17+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt; &lt;img alt="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" src="https://external-preview.redd.it/YW45M2FibXYwbXZlMWaepLM_4Oin4KjR_zAxiUwp5NOaLzCHkxa3urw0ZqL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60742f26e3a407482898d8e82f2a5d6e8f6ee5f" title="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgo9qamv0mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k29oe2</id>
    <title>QAT is slowly becoming mainstream now?</title>
    <updated>2025-04-18T16:52:07+00:00</updated>
    <author>
      <name>/u/__amberluz__</name>
      <uri>https://old.reddit.com/user/__amberluz__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released a QAT optimized Gemma 3 - 27 billion parameter model. The quantization aware training claims to recover close to 97% of the accuracy loss that happens during the quantization. Do you think this is slowly becoming the norm? Will non-quantized safetensors slowly become obsolete?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__amberluz__"&gt; /u/__amberluz__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250r6</id>
    <title>New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality.</title>
    <updated>2025-04-18T13:32:01+00:00</updated>
    <author>
      <name>/u/Sea_Sympathy_495</name>
      <uri>https://old.reddit.com/user/Sea_Sympathy_495</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt; &lt;img alt="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." src="https://external-preview.redd.it/5lq32BTIzHqmPYcHvNrCp8JMhag9gsSSkR3cQgoYZBU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed6a861b423ef5ef481e863b5c6947b3cef14c0c" title="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Sympathy_495"&gt; /u/Sea_Sympathy_495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/?linkId=14034718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2chcw</id>
    <title>Gemma 27B QAT works surprisingly well at Q2_K</title>
    <updated>2025-04-18T18:49:11+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test how well QAT models do at a lower quant size so I grabbed the smallest quant currently out for it, Q2_K at 10.5 GB. &lt;a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use my models mostly for my Japanese indie game, so following instructions, custom formatting and if it can roleplay or not is what I look for in models. My tests were all done in Japanese, which many models already have issues with at Q4 so I mostly use Q5. In my testing there were no grammatical errors, no random English or Chinese characters. It was able to roleplay in a custom format where I split the spoken words, the actions and the thoughts of the character into different brackets like ()&amp;lt;&amp;gt;「」without any issues. I also asked it basic questions about celebrities, and historical events, it got names and basic information right but dates were all wrong. My tests were done in Ollama with the standard Gemma3 settings.&lt;/p&gt; &lt;p&gt;Overall I am really impressed by the performance of the model especially for being a 27B at Q2. In theory running a 70B model at Q2 would fit into a single 24GB GPU so this technology is very interesting and could allow us to fit even larger models into our cards. After testing it I am really excited for more QAT models to come out in the future.&lt;/p&gt; &lt;p&gt;Have you guys tried running them at smaller quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T18:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2li9f</id>
    <title>Speed testing Llama 4 Maverick with various hardware configs</title>
    <updated>2025-04-19T01:56:23+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured I would share some speed tests of Llama 4 Maverick with my various hardware setups.&lt;br /&gt; Wish we had VLLM quants, guessing the 3090's would be 2x faster vs llama.cpp.&lt;/p&gt; &lt;p&gt;llama.cpp 10x P40's - Q3.5 full offload&lt;br /&gt; &lt;strong&gt;15 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;162 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama.cpp on 16x 3090's - Q4.5 full offload&lt;br /&gt; &lt;strong&gt;36 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;781 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers on 1x 3090 + 16 core DDR4 Epyc - Q4.5&lt;br /&gt; &lt;strong&gt;29 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;129 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers really shines with these tiny active param MOE's.&lt;/p&gt; &lt;p&gt;EDIT:&lt;br /&gt; Not my numbers but the M3 ultra can do:&lt;br /&gt; 47 T/s gen&lt;br /&gt; 332 T/s prompt&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2v8di</id>
    <title>Are there actually uncensored writing models out there ? (Reka Flash)</title>
    <updated>2025-04-19T12:33:02+00:00</updated>
    <author>
      <name>/u/Mochila-Mochila</name>
      <uri>https://old.reddit.com/user/Mochila-Mochila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I downloaded &lt;em&gt;Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF&lt;/em&gt; and ran it in LMStudio. Works pretty nicely, according to the few trials I did.&lt;/p&gt; &lt;p&gt;However, I soon hit a roadblock :&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I’m sorry, but I can’t assist with this request. The scenario you’ve described involves serious ethical concerns, including non-consensual acts, power imbalances, and harmful stereotypes that conflict with principles of respect, safety, and equality. Writing explicit content that normalizes or glorifies such dynamics would violate ethical guidelines and contribute to harm.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, nah, fuck that shit. If I'm going local, it's precisely to avoid this sort of garbage non-answer.&lt;/p&gt; &lt;p&gt;So I'm wondering if there are actually uncensored models readily available for use, or if I'm SOL and would need to train my own (tough luck).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit :&lt;/strong&gt; been trying &lt;em&gt;Qwen-qwq-32B&lt;/em&gt; and it's much better. This is why we need a multipolar world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mochila-Mochila"&gt; /u/Mochila-Mochila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T12:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28ulo</id>
    <title>Time to step up the /local reasoning game</title>
    <updated>2025-04-18T16:17:11+00:00</updated>
    <author>
      <name>/u/vornamemitd</name>
      <uri>https://old.reddit.com/user/vornamemitd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt; &lt;img alt="Time to step up the /local reasoning game" src="https://preview.redd.it/wtibm8c3cmve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f80a0bad3e3f79619d29663e49d519eaa7898d" title="Time to step up the /local reasoning game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest OAI models tucked away behind intrusive &amp;quot;ID verification&amp;quot;....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vornamemitd"&gt; /u/vornamemitd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wtibm8c3cmve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2u8h4</id>
    <title>Why is the QAT version not smaller on ollama for me?</title>
    <updated>2025-04-19T11:34:35+00:00</updated>
    <author>
      <name>/u/apocalypsedg</name>
      <uri>https://old.reddit.com/user/apocalypsedg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;[ggtdd@endeavour ~]$ ollama run gemma3:27b&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello world&lt;/code&gt; &lt;br /&gt; &lt;code&gt;Hello to you too! 👋 ^C&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama ps&lt;/code&gt;&lt;br /&gt; &lt;code&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/code&gt; &lt;br /&gt; &lt;code&gt;gemma3:27b a418f5838eaf 21 GB 10%/90% CPU/GPU 4 minutes from now&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama run gemma3:27b-it-qat&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello world&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Hello to you too!^C&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama ps&lt;/code&gt;&lt;br /&gt; &lt;code&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/code&gt; &lt;br /&gt; &lt;code&gt;gemma3:27b-it-qat 29eb0b9aeda3 22 GB 14%/86% CPU/GPU 4 minutes from now&lt;/code&gt; &lt;/p&gt; &lt;p&gt;The original actually takes up less space. What am I doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apocalypsedg"&gt; /u/apocalypsedg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T11:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2wj2s</id>
    <title>How much VRAM for 10 millions context tokens with Llama 4 ?</title>
    <updated>2025-04-19T13:41:38+00:00</updated>
    <author>
      <name>/u/kokoshkatheking</name>
      <uri>https://old.reddit.com/user/kokoshkatheking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I hypothetically want to use the 10 millions input context token that Llama 4 scout supports, how much memory would be needed to run that ? I try to find the answer myself but did not find any real world usage report. In my experience KV cache requirements scale very fast … I expect memory requirements for such a use case to be something like hundreds on VRAM. I would love to be wrong here :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kokoshkatheking"&gt; /u/kokoshkatheking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T13:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zw3l</id>
    <title>Llama 4 after inferencing bug fixes aftermath</title>
    <updated>2025-04-19T16:14:47+00:00</updated>
    <author>
      <name>/u/MutedSwimming3347</name>
      <uri>https://old.reddit.com/user/MutedSwimming3347</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A collection of results after fixing inferencing bugs&lt;/p&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/humanitys_last_exam"&gt;https://scale.com/leaderboard/humanitys_last_exam&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/singularity/s/amRrK1io0g"&gt;https://www.reddit.com/r/singularity/s/amRrK1io0g&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/ivqHiGGeRb"&gt;https://www.reddit.com/r/LocalLLaMA/s/ivqHiGGeRb&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Which providers host the correct implementation? What are your experiences? &lt;/p&gt; &lt;p&gt;Is openrouter the right place to go? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutedSwimming3347"&gt; /u/MutedSwimming3347 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zqdq</id>
    <title>ubergarm/gemma-3-27b-it-qat-GGUF</title>
    <updated>2025-04-19T16:07:45+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt; &lt;img alt="ubergarm/gemma-3-27b-it-qat-GGUF" src="https://external-preview.redd.it/zP1F1IPzW39T2A62RLwjUYufUjCjW6qaeKGMiNx3iNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fca46c2f6538aed2b4f6365368f26987fd027b43" title="ubergarm/gemma-3-27b-it-qat-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just quantized two GGUFs that beat google's 4bit GGUF in perplexity comparisons!&lt;/p&gt; &lt;p&gt;They only run on &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork which provides new SotA quantizationsof google's recently updated Quantization Aware Training (QAT) 4bit full model.&lt;/p&gt; &lt;p&gt;32k context in 24GB VRAM or as little as 12GB VRAM offloading just KV Cache and attention layers with repacked CPU optimized tensors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/gemma-3-27b-it-qat-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2qrqq</id>
    <title>Amoral Gemma 3 - QAT</title>
    <updated>2025-04-19T07:25:15+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt; &lt;img alt="Amoral Gemma 3 - QAT" src="https://preview.redd.it/zvrccxdusqve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2786d393d1943e56077477a8167c9ea8a34db8e1" title="Amoral Gemma 3 - QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The same old Amoral Gemma 3, just with the QAT at q4. Refer to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;my first post&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-1B-v2-qat"&gt;[1B] &lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-v2-qat"&gt;[4B]&lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2-qat"&gt;[12B]&lt;/a&gt; [27B - coming soon]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvrccxdusqve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T07:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ov6b</id>
    <title>How are NSFW LLMs trained/fine-tuned?</title>
    <updated>2025-04-19T05:15:43+00:00</updated>
    <author>
      <name>/u/GeneTangerine</name>
      <uri>https://old.reddit.com/user/GeneTangerine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does someone know? Generally LLMs are censored, do you guys have any resources?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeneTangerine"&gt; /u/GeneTangerine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2spil</id>
    <title>RTX 5080 is about a 3090 but with less VRAM :(</title>
    <updated>2025-04-19T09:48:49+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the 5080 to my bench list&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I know the models are old but I need to be able to compare them to the old benches I cannot rerun them all for now.&lt;/p&gt; &lt;p&gt;&lt;del&gt;The 5080 has performance on par with a 3090 (but 16gb of VRAM are a bummer), if only it had 24gb of VRAM would have been a interesting alternative.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;I want to the test the 5070Ti too but currently the ollama container doesn't seems to start on any of the 5070ti available on vast (I wasted about 1$ and 2 hours worth of my time in attempts)&lt;/del&gt;&lt;/p&gt; &lt;p&gt;EDIT: &lt;/p&gt; &lt;p&gt;I was able to test the 5070ti 16gb and it got performance on par with the 4090!!!&lt;/p&gt; &lt;p&gt;So I had to rerun the 5080 (TWICE with two different instances) and I got new values that are a little higher than the 5070TI but not that much (about 5% more). &lt;/p&gt; &lt;p&gt;I don't know what issue the first instance had (older drivers maybe?) &lt;/p&gt; &lt;p&gt;I've update the bench with the new data&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T09:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2kl84</id>
    <title>gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params).</title>
    <updated>2025-04-19T01:06:33+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt; &lt;img alt="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." src="https://preview.redd.it/2mx3qffqxove1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fccae6bd7191507d797de642e31420fffe50ff03" title="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mx3qffqxove1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ycef</id>
    <title>I've built a lightweight hallucination detector for RAG pipelines – open source, fast, runs up to 4K tokens</title>
    <updated>2025-04-19T15:06:53+00:00</updated>
    <author>
      <name>/u/henzy123</name>
      <uri>https://old.reddit.com/user/henzy123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hallucinations are still one of the biggest headaches in RAG pipelines, especially in tricky domains (medical, legal, etc). Most detection methods either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Has context window limitations&lt;/strong&gt;, particularly in encoder-only models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Has high inference costs&lt;/strong&gt; from LLM-based hallucination detectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we've put together &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;&lt;strong&gt;LettuceDetect&lt;/strong&gt;&lt;/a&gt; — an open-source, encoder-based framework that flags hallucinated spans in LLM-generated answers. No LLM required, runs faster, and integrates easily into any RAG setup.&lt;/p&gt; &lt;h1&gt;🥬 Quick highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token-level detection&lt;/strong&gt; → tells you exactly which parts of the answer aren't backed by your retrieved context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-context ready&lt;/strong&gt; → built on ModernBERT, handles up to 4K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accurate &amp;amp; efficient&lt;/strong&gt; → hits 79.22% F1 on the RAGTruth benchmark, competitive with fine-tuned LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MIT licensed&lt;/strong&gt; → comes with Python packages, pretrained models, Hugging Face demo&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;https://github.com/KRLabsOrg/LettuceDetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog: &lt;a href="https://huggingface.co/blog/adaamko/lettucedetect"&gt;https://huggingface.co/blog/adaamko/lettucedetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Preprint: &lt;a href="https://arxiv.org/abs/2502.17125"&gt;https://arxiv.org/abs/2502.17125&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo + models: &lt;a href="https://huggingface.co/KRLabsOrg"&gt;https://huggingface.co/KRLabsOrg&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what you think here — especially if you're doing local RAG, hallucination eval, or trying to keep things lightweight. Also working on real-time detection (not just post-gen), so open to ideas/collabs there too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henzy123"&gt; /u/henzy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T15:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2uztr</id>
    <title>Llama 4 is actually goat</title>
    <updated>2025-04-19T12:20:00+00:00</updated>
    <author>
      <name>/u/Remote_Cap_</name>
      <uri>https://old.reddit.com/user/Remote_Cap_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVME&lt;/p&gt; &lt;p&gt;Some old 6 core i5&lt;/p&gt; &lt;p&gt;64gb ram&lt;/p&gt; &lt;p&gt;LLaMa.C++ &amp;amp; mmap&lt;/p&gt; &lt;p&gt;Unsloth dynamic quants&lt;/p&gt; &lt;p&gt;Runs Scout at 2.5 tokens/s Runs Maverick at 2 tokens/s&lt;/p&gt; &lt;p&gt;2x that with GPU offload &amp;amp; --override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt; &lt;p&gt;200 dollar junk and now feeling the big leagues. From 24b to 400b in an architecture update and 100K+ context fits now?&lt;/p&gt; &lt;p&gt;Huge upgrade for me for free, goat imo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote_Cap_"&gt; /u/Remote_Cap_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T12:20:00+00:00</published>
  </entry>
</feed>
