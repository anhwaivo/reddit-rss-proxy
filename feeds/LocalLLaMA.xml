<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-26T22:05:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ixtxbw</id>
    <title>😂😂 someone made a "touch grass" app with a vLLM, you gotta go and actually touch grass to unlock your phone</title>
    <updated>2025-02-25T12:33:46+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt; &lt;img alt="😂😂 someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" src="https://b.thumbs.redditmedia.com/5hq40VPLgBMcOH3vwQ7e1MxMGeAfqIgssUMVtLMafsg.jpg" title="😂😂 someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixtxbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyx9tr</id>
    <title>Kioxia AiSAQ SSD-backed RAG Open Sourced</title>
    <updated>2025-02-26T20:23:52+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;News: &lt;a href="https://www.servethehome.com/kioxia-aisaq-ssd-backed-rag-open-sourced/"&gt;https://www.servethehome.com/kioxia-aisaq-ssd-backed-rag-open-sourced/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/kioxiaamerica/aisaq-diskann"&gt;https://github.com/kioxiaamerica/aisaq-diskann&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ArXiv paper: &lt;a href="https://arxiv.org/abs/2404.06004"&gt;https://arxiv.org/abs/2404.06004&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;AiSAQ (All-in-Storage ANNS with Product Quantization) is a DRAM-free method for approximate nearest neighbor search (ANNS). This code forked off from &lt;a href="https://github.com/Microsoft/DiskANN"&gt;code for Microsoft DiskANN&lt;/a&gt; algorithm. Currently we support only simple index search described in &lt;a href="https://arxiv.org/abs/2404.06004"&gt;our arXiv paper&lt;/a&gt;.&amp;quot;&lt;/p&gt; &lt;p&gt;I hope this helps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx9tr/kioxia_aisaq_ssdbacked_rag_open_sourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx9tr/kioxia_aisaq_ssdbacked_rag_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx9tr/kioxia_aisaq_ssdbacked_rag_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T20:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyv2o9</id>
    <title>LMArena Releases Prompt-to-Leaderboard</title>
    <updated>2025-02-26T18:53:14+00:00</updated>
    <author>
      <name>/u/omegaswepon</name>
      <uri>https://old.reddit.com/user/omegaswepon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tweet: &lt;a href="https://x.com/lmarena_ai/status/1894767009977811256"&gt;https://x.com/lmarena_ai/status/1894767009977811256&lt;/a&gt; github: &lt;a href="https://github.com/lmarena/p2l"&gt;https://github.com/lmarena/p2l&lt;/a&gt; models: &lt;a href="https://huggingface.co/collections/lmarena-ai/prompt-to-leaderboard-67bcf7ddf6022ef3cfd260cc"&gt;https://huggingface.co/collections/lmarena-ai/prompt-to-leaderboard-67bcf7ddf6022ef3cfd260cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It takes in a prompt and outputs a full leaderboard conditioned on that exact prompt. Also acts as a router. &lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omegaswepon"&gt; /u/omegaswepon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyv2o9/lmarena_releases_prompttoleaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyv2o9/lmarena_releases_prompttoleaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyv2o9/lmarena_releases_prompttoleaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy22ux</id>
    <title>Gemma 3 27b just dropped (Gemini API models list)</title>
    <updated>2025-02-25T18:31:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt; &lt;img alt="Gemma 3 27b just dropped (Gemini API models list)" src="https://preview.redd.it/y2nlshypwble1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16e52e66ab49c258198ed2169eecedba2241176d" title="Gemma 3 27b just dropped (Gemini API models list)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2nlshypwble1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyz3qa</id>
    <title>Fine tuning on an embedding model</title>
    <updated>2025-02-26T21:40:26+00:00</updated>
    <author>
      <name>/u/dabrox02</name>
      <uri>https://old.reddit.com/user/dabrox02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm recently trying to do a fine tuning project on an embedding model to recommend books. I understand that it must be an embedding model for retrieving and ranking books. The dataset I built consists of 4 columns [title, authors, categories, description] with approximately 200k books.&lt;/p&gt; &lt;p&gt;I'm a newbie at this so I don't really know what kind of loss function I should use. I've tried to format the dataset in triplets but I get the following error: &amp;quot;IterableDataset is not defined.&amp;quot; I'm using the sentence-transformers package.&lt;/p&gt; &lt;p&gt;If you know of a resource that explains how to do something similar or an easier-to-use package, I'd really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabrox02"&gt; /u/dabrox02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyz3qa/fine_tuning_on_an_embedding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyz3qa/fine_tuning_on_an_embedding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyz3qa/fine_tuning_on_an_embedding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T21:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybgj2</id>
    <title>TinyR1-32B-Preview (surpassing official R1 distill 32B performance)</title>
    <updated>2025-02-26T01:14:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt; &lt;img alt="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" src="https://external-preview.redd.it/n9Pibq5ap97rYKS_QfhVUwq5U1l5cN9jQ5aOHTyyDyg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659b372ed30298182f662bdd30b00d3b42381833" title="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyn408</id>
    <title>Dual EPYC CPU build...avoiding the bottleneck</title>
    <updated>2025-02-26T13:08:11+00:00</updated>
    <author>
      <name>/u/Dry_Parfait2606</name>
      <uri>https://old.reddit.com/user/Dry_Parfait2606</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm figuring out if I can make a dual 7002 run without having a cpu-to-cpu bottleneck...&lt;/p&gt; &lt;p&gt;Its a 1-2TB ram build, so I'm just trying to get very cheap ram and being able to run the bigger models like 405b &amp;amp; 700B...at &amp;lt;1TB/s speeds of course.&lt;/p&gt; &lt;p&gt;I've read something about NUMA nodes but I have no idea where to begin with to actually resolve the bottleneck of a dual cpu.. Can someone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Parfait2606"&gt; /u/Dry_Parfait2606 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T13:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7k6b</id>
    <title>Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider</title>
    <updated>2025-02-25T22:17:32+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt; &lt;img alt="Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" src="https://external-preview.redd.it/LHkWl_VkJgCRA11Syl07lcXlc5oC-0ZjNEgwTTmbGnM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=309552b833882287b79a1c8f0cb6385eb21a5228" title="Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-gaming-gpus-modded-with-2x-vram-for-ai-workloads"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iymmj7</id>
    <title>I built a Linux shell with Ollama integration and natural language commands in Rust</title>
    <updated>2025-02-26T12:42:02+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"&gt; &lt;img alt="I built a Linux shell with Ollama integration and natural language commands in Rust" src="https://external-preview.redd.it/okC7TfMdMSOgHLRE90xJF9n5RQbj7e5VdAzGJOsXCBQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eade031b76e801fa92e08d5614383b6f840d8032" title="I built a Linux shell with Ollama integration and natural language commands in Rust" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/phildougherty/llmsh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T12:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyx72j</id>
    <title>Claude 3.7 Sonnet and Grok 3 appear on WebDev Arena</title>
    <updated>2025-02-26T20:20:33+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx72j/claude_37_sonnet_and_grok_3_appear_on_webdev_arena/"&gt; &lt;img alt="Claude 3.7 Sonnet and Grok 3 appear on WebDev Arena" src="https://preview.redd.it/7bhinoc3ljle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d5ce1377a9bb56312aedcc788e5e38b46df4a8c" title="Claude 3.7 Sonnet and Grok 3 appear on WebDev Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7bhinoc3ljle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx72j/claude_37_sonnet_and_grok_3_appear_on_webdev_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyx72j/claude_37_sonnet_and_grok_3_appear_on_webdev_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T20:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy6rid</id>
    <title>Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together</title>
    <updated>2025-02-25T21:44:22+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt; &lt;img alt="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" src="https://b.thumbs.redditmedia.com/p-QWrsvBVwjKiJeQsp_Oo8L1SYP2Wqt_48i9b9Kgics.jpg" title="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy6rid"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T21:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7e4x</id>
    <title>RTX 4090 48GB</title>
    <updated>2025-02-25T22:10:28+00:00</updated>
    <author>
      <name>/u/xg357</name>
      <uri>https://old.reddit.com/user/xg357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt; &lt;img alt="RTX 4090 48GB" src="https://b.thumbs.redditmedia.com/i7dCpIN6G-2UgPHgeJaiSWC47liBasG9PHIrLoSF1kw.jpg" title="RTX 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got one of these legendary 4090 with 48gb of ram from eBay. I am from Canada. &lt;/p&gt; &lt;p&gt;What do you want me to test? And any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xg357"&gt; /u/xg357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy7e4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iywf6n</id>
    <title>Gemma 2 2B: Small in Size, Giant in Multilingual Performance</title>
    <updated>2025-02-26T19:48:26+00:00</updated>
    <author>
      <name>/u/thecalmgreen</name>
      <uri>https://old.reddit.com/user/thecalmgreen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just like many of you, I’m really excited about the new member of the Gemma family—especially the smaller models.&lt;/p&gt; &lt;p&gt;I’d like to highlight how impressive the Gemma 2 2B is: a true milestone. For a long time, it was difficult to find truly multilingual models capable of fluently mastering languages beyond English, even among large-scale systems. In contrast, the Gemma 2 9B was one of the first to demonstrate real proficiency in my language, making it a genuinely useful tool for me.&lt;/p&gt; &lt;p&gt;What the Gemma 2 2B achieves is astonishing. In terms of multilingual performance, it even surpasses massive models like the Llama 3 400B—at least in my native language and others I’ve tested. I’m amazed that with just 2 billion parameters, it has reached this level of performance. I still wonder how this was possible.&lt;/p&gt; &lt;p&gt;My admiration for the Gemma 2 2B goes beyond its performance: it also stems from the recent trend of &amp;quot;normalizing&amp;quot; large models as if they were small, something common in companies like Mistral. Calling a 24B model “small” shows a disconnect from the reality of users who rely on open-source models that are not colossal and need to run on home hardware.&lt;/p&gt; &lt;p&gt;I hope that with the launch of Gemma 3, Google doesn’t adopt this misguided narrative. Beyond models in the 27/32B range, I hope we see significant advancements in smaller systems, in the 2 to 10B range.&lt;/p&gt; &lt;p&gt;In my opinion, simply increasing the model size with each generation is not, by itself, a meaningful technical breakthrough—just as expanding the context length in &amp;quot;thinking&amp;quot; models doesn’t automatically guarantee better answers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecalmgreen"&gt; /u/thecalmgreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywf6n/gemma_2_2b_small_in_size_giant_in_multilingual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywf6n/gemma_2_2b_small_in_size_giant_in_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iywf6n/gemma_2_2b_small_in_size_giant_in_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T19:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybcnl</id>
    <title>DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix</title>
    <updated>2025-02-26T01:09:10+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt; &lt;img alt="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" src="https://external-preview.redd.it/qnXEqoF7LuYpZmlhfqFWCAFt6nE0AU8_d2ok4KoHKZ0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abee123819338d8d068f68944ed6953760e40e9e" title="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in DeepSeek-V3&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DeepGEMM"&gt;https://github.com/deepseek-ai/DeepGEMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9"&gt;https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyuz01</id>
    <title>Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO</title>
    <updated>2025-02-26T18:49:01+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt; &lt;img alt="Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO" src="https://external-preview.redd.it/kwC-Tr7ndw31AC4xEyGoQV3L0FpYDQr7n0II8xbY0xU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d97f72696e1b21ced4b55a5892668a3fdd40d66a" title="Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! We created this mini quickstart tutorial so once completed, you'll be able to transform any open LLM like Llama to have chain-of-thought reasoning by using &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You'll learn about Reward Functions, explanations behind GRPO, dataset prep, usecases and more! Hopefully it's helpful for you all! 😃&lt;/p&gt; &lt;p&gt;Full Guide (with pics): &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/"&gt;https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These instructions are for our Google Colab &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;notebooks&lt;/a&gt;. If you are installing Unsloth locally, you can also copy our notebooks inside your favorite code editor.&lt;/p&gt; &lt;p&gt;&lt;em&gt;The GRPO notebooks we are using:&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;&lt;em&gt;Llama 3.1 (8B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb)&lt;em&gt;,&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B"&gt;&lt;em&gt;Phi-4 (14B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb) &lt;em&gt;and&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B"&gt;&lt;em&gt;Qwen2.5 (3B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#1. Install Unsloth&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our Colab notebook, click Runtime &amp;gt; Run all. We'd highly recommend you checking out our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;Fine-tuning Guide&lt;/a&gt; before getting started. If installing locally, ensure you have the correct &lt;a href="https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements"&gt;requirements&lt;/a&gt; and use pip install unsloth&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/26dgnth9tgle1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61a748deef81e5a771bc2420947bfc67104d8956"&gt;https://preview.redd.it/26dgnth9tgle1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61a748deef81e5a771bc2420947bfc67104d8956&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#2. Learn about GRPO &amp;amp; Reward Functions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Before we get started, it is recommended to learn more about GRPO, reward functions and how they work. Read more about them including &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips"&gt;tips &amp;amp; tricks&lt;/a&gt;&lt;a href="/o/HpyELzcNe0topgVLGCZY/s/xhOjnexMCB3dmuQFQ2Zq/%7E/changes/218/basics/reasoning-grpo-and-rl#basics-tips"&gt; here&lt;/a&gt;. You will also need enough VRAM. In general, model parameters = amount of VRAM you will need. In Colab, we are using their free 16GB VRAM GPUs which can train any model up to 16B in parameters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#3. Configure desired settings&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected optimal settings for the best results for you already and you can change the model to whichever you want listed in our &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;supported models&lt;/a&gt;. Would not recommend changing other settings if you're a beginner.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mh114uw0ugle1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c895c2b016a88d86d3a3c2138e2929ab3b927f53"&gt;https://preview.redd.it/mh114uw0ugle1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c895c2b016a88d86d3a3c2138e2929ab3b927f53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#4. Select your dataset&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected OpenAI's GSM8K dataset already but you could change it to your own or any public one on Hugging Face. You can read more about &lt;a href="/o/HpyELzcNe0topgVLGCZY/s/xhOjnexMCB3dmuQFQ2Zq/%7E/changes/218/basics/datasets-101"&gt;datasets here&lt;/a&gt;. Your dataset should still have at least 2 columns for question and answer pairs. However the answer must not reveal the reasoning behind how it derived the answer from the question. See below for an example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pgrd3xamtgle1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4630f2d5aad304f8bebaec1d8e2acea877ac4c8f"&gt;https://preview.redd.it/pgrd3xamtgle1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4630f2d5aad304f8bebaec1d8e2acea877ac4c8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#5. Reward Functions/Verifier&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier"&gt;Reward Functions/Verifiers&lt;/a&gt; lets us know if the model is doing well or not according to the dataset you have provided. Each generation run will be assessed on how it performs to the score of the average of the rest of generations. You can create your own reward functions however we have already pre-selected them for you with &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions"&gt;Will's GSM8K&lt;/a&gt; reward functions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2oadoawotgle1.png?width=2284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab31dedc2e4de01176b42606f06be9a0228c67e"&gt;https://preview.redd.it/2oadoawotgle1.png?width=2284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab31dedc2e4de01176b42606f06be9a0228c67e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this, we have 5 different ways which we can reward each generation. You can also input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, set a rule: &amp;quot;If the answer sounds too robotic, deduct 3 points.&amp;quot; This helps refine outputs based on quality criteria. See examples of what they can look like &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example Reward Function for an Email Automation Task:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Question: Inbound email&lt;/li&gt; &lt;li&gt;Answer: Outbound email&lt;/li&gt; &lt;li&gt;Reward Functions: &lt;ul&gt; &lt;li&gt;If the answer contains a required keyword → +1&lt;/li&gt; &lt;li&gt;If the answer exactly matches the ideal response → +1&lt;/li&gt; &lt;li&gt;If the response is too long → -1&lt;/li&gt; &lt;li&gt;If the recipient's name is included → +1&lt;/li&gt; &lt;li&gt;If a signature block (phone, email, address) is present → +1&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;#6. Train your model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected hyperparameters for the most optimal results however you could change them. Read all about &lt;a href="https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"&gt;parameters here&lt;/a&gt;. You should see the reward increase overtime. We would recommend you train for at least 300 steps which may take 30 mins however, for optimal results, you should train for longer.&lt;/p&gt; &lt;p&gt;You will also see sample answers which allows you to see how the model is learning. Some may have steps, XML tags, attempts etc. and the idea is as trains it's going to get better and better because it's going to get scored higher and higher until we get the outputs we desire with long reasoning chains of answers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bckurqkutgle1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bd09c09ee7c146b88502cbf627a9878e0c2c6ca"&gt;https://preview.redd.it/bckurqkutgle1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bd09c09ee7c146b88502cbf627a9878e0c2c6ca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;And that's it - really hope you guys enjoyed it and please leave us any feedback!! :)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2t7c</id>
    <title>Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990</title>
    <updated>2025-02-25T19:01:07+00:00</updated>
    <author>
      <name>/u/sobe3249</name>
      <uri>https://old.reddit.com/user/sobe3249</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt; &lt;img alt="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" src="https://preview.redd.it/erki80wv1cle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ada9d2780ffd78b32f14450c762f69f014324845" title="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobe3249"&gt; /u/sobe3249 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erki80wv1cle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T19:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyun6z</id>
    <title>Using DeepSeek R1 for RAG: Do's and Don'ts</title>
    <updated>2025-02-26T18:35:26+00:00</updated>
    <author>
      <name>/u/z_yang</name>
      <uri>https://old.reddit.com/user/z_yang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyun6z/using_deepseek_r1_for_rag_dos_and_donts/"&gt; &lt;img alt="Using DeepSeek R1 for RAG: Do's and Don'ts" src="https://external-preview.redd.it/J90gfCQ3-_Bk8Pfww7GrFrRSHQGIwwS4i9k5y0iFllc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477a8035901a08fe25624317f127d5642d0c94b6" title="Using DeepSeek R1 for RAG: Do's and Don'ts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_yang"&gt; /u/z_yang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.skypilot.co/deepseek-rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyun6z/using_deepseek_r1_for_rag_dos_and_donts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyun6z/using_deepseek_r1_for_rag_dos_and_donts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyo8ul</id>
    <title>What's the best machine I can get for local LLM's with a $25k budget?</title>
    <updated>2025-02-26T14:04:35+00:00</updated>
    <author>
      <name>/u/NootropicDiary</name>
      <uri>https://old.reddit.com/user/NootropicDiary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This rig would be purely for running local LLM's and sending the data back and forth to my mac desktop (which I'll be upgrading to the new mac pro which should be dropping later this year and will be a beast in itself).&lt;/p&gt; &lt;p&gt;I do a lot of coding and I love the idea of a blistering fast reasoning model that doesn't require anything being sent over the external network + I reckon within the next year there's going to be some insane optimizations and distillations.&lt;/p&gt; &lt;p&gt;Budget can potentially take another $5/$10K on top if necessary.&lt;/p&gt; &lt;p&gt;Anyway, please advise!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NootropicDiary"&gt; /u/NootropicDiary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T14:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyuy62</id>
    <title>Is Qwen2.5 Coder 32b still considered a good model for coding?</title>
    <updated>2025-02-26T18:48:03+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that we have DeepSeek and the new Claud Sonnet 3.7, do you think the Qwen model is still doing okay, especially when you consider its size compared to the others?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuy62/is_qwen25_coder_32b_still_considered_a_good_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuy62/is_qwen25_coder_32b_still_considered_a_good_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuy62/is_qwen25_coder_32b_still_considered_a_good_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iywa97</id>
    <title>Kokoro TTS app</title>
    <updated>2025-02-26T19:42:35+00:00</updated>
    <author>
      <name>/u/PureRely</name>
      <uri>https://old.reddit.com/user/PureRely</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt; &lt;img alt="Kokoro TTS app" src="https://a.thumbs.redditmedia.com/eO9wAZe_63WifM9NEq_9zI_dgiPIUnvjywpY4LL6cw4.jpg" title="Kokoro TTS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a &lt;strong&gt;Kokoro TTS&lt;/strong&gt; app for personal use. Is this something you think others would like?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8snb5h68ejle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c913925e66dbdf41861a629458d647da3784bf5f"&gt;https://preview.redd.it/8snb5h68ejle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c913925e66dbdf41861a629458d647da3784bf5f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PureRely"&gt; /u/PureRely &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T19:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iym55d</id>
    <title>Is the Framework Desktop Overhyped for Running LLMs?</title>
    <updated>2025-02-26T12:14:12+00:00</updated>
    <author>
      <name>/u/roworu</name>
      <uri>https://old.reddit.com/user/roworu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly don't understand hype about that new Framework Desktop. From what I saw, the bandwidth for them would become a bottleneck for all LLMs you could theoretically put in these 128GB. So what is the point then? Yes, the pricing per VRAM DB is better than Apple's, but the generation speed is like 6 t/s at absolute best? Why would anyone want these for running LLMs? Isn't M-based devices would be better for that purpose?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roworu"&gt; /u/roworu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T12:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyfvhb</id>
    <title>Perplexity is forking Chrome</title>
    <updated>2025-02-26T05:05:09+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt; &lt;img alt="Perplexity is forking Chrome" src="https://preview.redd.it/ubxe59mr1fle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f3bdf014e84bea19fddef06263361ac5e64ab3" title="Perplexity is forking Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ubxe59mr1fle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T05:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyxdoj</id>
    <title>Wan2.1 Video Model Native Support in ComfyUI!</title>
    <updated>2025-02-26T20:28:27+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"&gt; &lt;img alt="Wan2.1 Video Model Native Support in ComfyUI!" src="https://external-preview.redd.it/YzcwdWNidGltamxlMTtYNTK5bBV-Sy4X4Od7toYxpb_33iOS8jpvVSU_0FTg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41079618205739c415b15bd7117833b093e236eb" title="Wan2.1 Video Model Native Support in ComfyUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ComfyUI announced native support for Wan 2.1. Blog post with workflow can be found here: &lt;a href="https://blog.comfy.org/p/wan21-video-model-native-support"&gt;https://blog.comfy.org/p/wan21-video-model-native-support&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ebt3wzuimjle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T20:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iylebm</id>
    <title>Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily</title>
    <updated>2025-02-26T11:28:21+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"&gt; &lt;img alt="Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily" src="https://preview.redd.it/cgapkix5ygle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff465a64ccfdf7ec55be9d23c4cf95d51107da4a" title="Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgapkix5ygle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T11:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyudod</id>
    <title>IBM launches Granite 3.2</title>
    <updated>2025-02-26T18:24:44+00:00</updated>
    <author>
      <name>/u/twavisdegwet</name>
      <uri>https://old.reddit.com/user/twavisdegwet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"&gt; &lt;img alt="IBM launches Granite 3.2" src="https://external-preview.redd.it/XzGfI5bXa9bZ2rE7qDhqEboSxpsg13nLvY3bLGQqSVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a08f8956935d376bfc4a36bb05ac5bfa5fdb87d" title="IBM launches Granite 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/twavisdegwet"&gt; /u/twavisdegwet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision?lnk=hpls2us"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:24:44+00:00</published>
  </entry>
</feed>
