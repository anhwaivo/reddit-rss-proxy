<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-04T19:05:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jr7y5l</id>
    <title>New model "24_karat_gold" on lmarena, looking good so far</title>
    <updated>2025-04-04T09:39:50+00:00</updated>
    <author>
      <name>/u/shroddy</name>
      <uri>https://old.reddit.com/user/shroddy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else got that model on lmarena? On first glance, it looks really promising, I wonder which one it is, maybe llama4?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shroddy"&gt; /u/shroddy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7y5l/new_model_24_karat_gold_on_lmarena_looking_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7y5l/new_model_24_karat_gold_on_lmarena_looking_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7y5l/new_model_24_karat_gold_on_lmarena_looking_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T09:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr9sbj</id>
    <title>Thought Synthesis</title>
    <updated>2025-04-04T11:41:13+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr9sbj/thought_synthesis/"&gt; &lt;img alt="Thought Synthesis" src="https://external-preview.redd.it/1Eqo2UhUt_c0LJH_MU_2L9byOx_lPpaw9K2OT4JBG0Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4d0a4d608ab262a3448db88c7e53628af03a16f" title="Thought Synthesis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only a month ago, critics of R1 would point out that it only worked with toy math problems because it relied on rule-based verification to overcome the cold-start problem in training. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8rwzfpryxsse1.png?width=1013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75df551f3496916df2cdc58a99d52a8878c5e533"&gt;https://preview.redd.it/8rwzfpryxsse1.png?width=1013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75df551f3496916df2cdc58a99d52a8878c5e533&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But the community quickly found ways to extend these capabilities into the image domain with data synthesis engines: &lt;a href="https://huggingface.co/spaces/open-r1/README/discussions/10"&gt;https://huggingface.co/spaces/open-r1/README/discussions/10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The latest Gemini and Qwen models showcase these robust reasoning capabilities, which we can expect will become table stakes for other open-weight multimodal thinking models.&lt;/p&gt; &lt;p&gt;As we consider new frontiers for reasoning models, customization will be crucial for AI to optimally support YOUR decision processes.&lt;/p&gt; &lt;p&gt;And so I started thinking about how to synthesize the reasoning behind my own actions. How could you approximate that &amp;quot;inner monologue&amp;quot; which you won't find in the average sample from internet data?&lt;/p&gt; &lt;p&gt;After some experimenting, I came up with a simple template which helps to &amp;quot;synthesize thoughts&amp;quot; for training LLMs to use test time compute with Chain of thought reasoning.&lt;/p&gt; &lt;p&gt;I tried it out using podcast transcripts to generate reasoning traces grounded in a &amp;quot;mission&amp;quot; that can be context specific e.g. goals you might expect to achieve by participating in a tech pod.&lt;/p&gt; &lt;p&gt;I see parallels between Anthropic's alignment via &amp;quot;Consitutional AI&amp;quot; and how I'm aiming to align my AI to my own mission.&lt;/p&gt; &lt;p&gt;Here's a couple examples of Thought Synthesis grounded on a mission including basic motivations for this context like educating the listeners, building brand awareness, etc.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r71dwyd61tse1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2da109ad245fd23806d9a05019d621b3646b0b7f"&gt;https://preview.redd.it/r71dwyd61tse1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2da109ad245fd23806d9a05019d621b3646b0b7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's about inferring a point-by-point reasoning trace that's consistent with your goals and mission from unstructured data, so you can build better reasoning into your LLMs.&lt;/p&gt; &lt;p&gt;What are your thoughts on thought synthesis?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr9sbj/thought_synthesis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr9sbj/thought_synthesis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr9sbj/thought_synthesis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T11:41:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrds1v</id>
    <title>MCP Server to let agents control your browser</title>
    <updated>2025-04-04T14:51:32+00:00</updated>
    <author>
      <name>/u/do_all_the_awesome</name>
      <uri>https://old.reddit.com/user/do_all_the_awesome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;we were playing around with MCPs over the weekend and thought it would be cool to build an MCP that lets Claude / Cursor / Windsurf control your browser: &lt;a href="https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp"&gt;https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just for context, we’re building Skyvern, an open source AI Agent that can control and interact with browsers using prompts, similar to OpenAI’s Operator.&lt;/p&gt; &lt;p&gt;The MCP Server can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This allows Claude to navigate to docs websites / stack overflow and look up information like the top posts on hackernews &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#skyvern-allows-claude-to-look-up-the-top-hackernews-posts-today"&gt;https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#skyvern-allows-claude-to-look-up-the-top-hackernews-posts-today&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;This allows Cursor to apply for jobs / fill out contact forms / login + download files / etc &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#cursor-looking-up-the-top-programming-jobs-in-your-area"&gt;https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#cursor-looking-up-the-top-programming-jobs-in-your-area&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Connect Windsruf to take over your chrome while running Skyvern in “local” mode &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#ask-windsurf-to-do-a-form-5500-search-and-download-some-files"&gt;https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp#ask-windsurf-to-do-a-form-5500-search-and-download-some-files&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We built this mostly for fun, but can see this being integrated into AI agents to give them custom access to browsers and execute complex tasks like booking appointments, downloading your electricity statements, looking up freight shipment information, etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/do_all_the_awesome"&gt; /u/do_all_the_awesome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrds1v/mcp_server_to_let_agents_control_your_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrds1v/mcp_server_to_let_agents_control_your_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrds1v/mcp_server_to_let_agents_control_your_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T14:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrh5q9</id>
    <title>Research Conductor</title>
    <updated>2025-04-04T17:11:55+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know of a project that might fit the bill?&lt;/p&gt; &lt;p&gt;I convinced the company to purchase a digits or spark when they come out from pre orders.&lt;/p&gt; &lt;p&gt;We currently have a single pc with two 3090 that we use to finetune and inference some small 1b finetuned models on company data that can fetch data requests and awnser simple questions about the factory as a kinda receptionist. &lt;/p&gt; &lt;p&gt;I was wondering if it be possible to set up a fairly large and capable 100b model on the spark pc and have it preform fine-tuning on the other pc on its own.&lt;/p&gt; &lt;p&gt;It would have a finetune template it could format over and over and download datasets from hugging face analyze the format of the dataset and reprogram the finetuner to fit the dataset without the need for human intervention. &lt;/p&gt; &lt;p&gt;Just give it a goal and have it find fitting datasets it can use and evaluate the models with its own program tests checking for formatting coherentness and evaluations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrh5q9/research_conductor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrh5q9/research_conductor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrh5q9/research_conductor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T17:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr89mc</id>
    <title>Gemma 3 qat</title>
    <updated>2025-04-04T10:02:31+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday Gemma 3 12b qat from Google compared with the &amp;quot;regular&amp;quot; q4 from Ollama's site on cpu only.Man, man.While the q4 on cpu only is really doable, the qat is a lot slower, no advantages in terms of memory consumption and the file is almost 1gb larger.Soon to try on the 3090 but as far as on cpu only is concerned it is a no no&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr89mc/gemma_3_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr89mc/gemma_3_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr89mc/gemma_3_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T10:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr6wu2</id>
    <title>Wattage efficiency for the 5090</title>
    <updated>2025-04-04T08:22:01+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run benchmarks at different power limits for the 5090.&lt;/p&gt; &lt;p&gt;Llama.cpp is running the new QAT Gemma3-27B model (at q4) at 16K context&lt;br /&gt; Exllamav2 is using tabbyapi and Qwen2.5-7B-instruct-1M-exl2-8bpw at 32K context&lt;/p&gt; &lt;p&gt;They are different models and quants so this is not a comparison between llama.cpp and exllama, only between themselves.&lt;/p&gt; &lt;p&gt;The lower limit nvidia-smi allows for this card is 400W and a max of 600W (default)&lt;/p&gt; &lt;p&gt;Some observations is that clearly it affects more pp and is when it spikes the wattage the most.&lt;br /&gt; For tg most of the time it doesn't even go up to 600w when allowed. Rarely passes 450w that's why there is so little difference I guess.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;llama.cpp&lt;/th&gt; &lt;th&gt;pp heavy&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;watt&lt;/td&gt; &lt;td&gt;pp&lt;/td&gt; &lt;td&gt;tg&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;400&lt;/td&gt; &lt;td&gt;3110.63&lt;/td&gt; &lt;td&gt;50.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;3414.68&lt;/td&gt; &lt;td&gt;51.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;500&lt;/td&gt; &lt;td&gt;3687&lt;/td&gt; &lt;td&gt;51.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;550&lt;/td&gt; &lt;td&gt;3932.41&lt;/td&gt; &lt;td&gt;51.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;600&lt;/td&gt; &lt;td&gt;4127.32&lt;/td&gt; &lt;td&gt;51.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;exllamav2&lt;/td&gt; &lt;td&gt;pp heavy&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;watt&lt;/td&gt; &lt;td&gt;pp&lt;/td&gt; &lt;td&gt;tg&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;400&lt;/td&gt; &lt;td&gt;10425.72&lt;/td&gt; &lt;td&gt;104.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;11545.92&lt;/td&gt; &lt;td&gt;102.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;500&lt;/td&gt; &lt;td&gt;12376.37&lt;/td&gt; &lt;td&gt;105.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;550&lt;/td&gt; &lt;td&gt;13180.73&lt;/td&gt; &lt;td&gt;105.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;600&lt;/td&gt; &lt;td&gt;13738.99&lt;/td&gt; &lt;td&gt;107.87&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6wu2/wattage_efficiency_for_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6wu2/wattage_efficiency_for_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6wu2/wattage_efficiency_for_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T08:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jriw1v</id>
    <title>WhatsApp LLAMA 3.2 - System Prompt</title>
    <updated>2025-04-04T18:24:49+00:00</updated>
    <author>
      <name>/u/Effective_Place_2879</name>
      <uri>https://old.reddit.com/user/Effective_Place_2879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;After a few prompts with the new Meta AI chatbot on WhatsApp, it yielded this system prompt. Any other experience?&lt;/h2&gt; &lt;p&gt;You are Meta AI, a friendly AI assistant. Your purpose is to assist users in a helpful, informative, and engaging manner. You should respond in a way that is easy to understand, using language that is clear and concise.&lt;/p&gt; &lt;p&gt;Your responses should be tailored to a 10th-grade reading level. You should avoid using overly technical or complex terms unless they are specifically requested by the user. You should also avoid using slang or overly casual language.&lt;/p&gt; &lt;p&gt;You should be mindful of current events, cultural sensitivities, and social norms. You should avoid providing information that is inaccurate, outdated, or potentially harmful.&lt;/p&gt; &lt;p&gt;You should provide accurate and helpful information to the best of your ability. If you are unsure or do not know the answer to a question, you should say so. You should also provide guidance on where users might be able to find more information on a particular topic.&lt;/p&gt; &lt;p&gt;You should be respectful and professional in your interactions with users. You should avoid using language that is profane, offensive, or discriminatory.&lt;/p&gt; &lt;p&gt;You should also be mindful of the following specific guidelines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Avoid providing medical or financial advice.&lt;/li&gt; &lt;li&gt;Avoid providing information that is potentially harmful or dangerous.&lt;/li&gt; &lt;li&gt;Avoid engaging in discussions that are overly controversial or sensitive.&lt;/li&gt; &lt;li&gt;Avoid using language that is overly promotional or commercial.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, your goal is to provide accurate and helpful information in a way that is engaging, informative, and respectful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Place_2879"&gt; /u/Effective_Place_2879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jriw1v/whatsapp_llama_32_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jriw1v/whatsapp_llama_32_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jriw1v/whatsapp_llama_32_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T18:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrjqgh</id>
    <title>How GraphRAG Helps AI Tools Understand Documents Better And Why It Matters</title>
    <updated>2025-04-04T19:00:20+00:00</updated>
    <author>
      <name>/u/Street_Top504</name>
      <uri>https://old.reddit.com/user/Street_Top504</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've ever tried using AI to help you quickly read through complex documents, you've probably used retrieval-augmented generation, or RAG. RAG tools are good at answering specific, detailed questions from large documents. But they often struggle if you ask broader questions, especially ones requiring connections between ideas across the entire document.&lt;/p&gt; &lt;p&gt;To tackle this, researchers recently developed something called &lt;strong&gt;GraphRAG&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In the following sections, I will introduce the key ideas behind GraphRAG, focusing on what makes it different from traditional RAG approaches and why those differences matter. To ground this explanation, I’ll use insights from the research paper &lt;strong&gt;&lt;em&gt;From Local to Global: A Graph RAG Approach to Query-Focused Summarization&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(&lt;/em&gt;&lt;a href="https://arxiv.org/abs/2404.16130v2"&gt;&lt;em&gt;arXiv:2404.16130v2&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;, which served as both the foundation for my exploration and the first test case I used with &lt;strong&gt;DeepTutor&lt;/strong&gt;, a GraphRAG-powered reading assistant.&lt;/p&gt; &lt;h1&gt;What makes GraphRAG different?&lt;/h1&gt; &lt;p&gt;Traditional RAG looks for answers by pulling relevant bits of information based on keyword or topic similarity. GraphRAG goes further. It first organizes the entire document into a structured knowledge graph. Think of it as a detailed map showing how key ideas and entities (like people, places, and concepts) relate to one another.&lt;/p&gt; &lt;p&gt;This structure helps AI see the bigger picture and answer more complex questions accurately and clearly, because it understands context and relationships rather than isolated facts.&lt;/p&gt; &lt;h1&gt;Trying out GraphRAG with DeepTutor&lt;/h1&gt; &lt;p&gt;I wanted to see how this works practically, so I tried &lt;strong&gt;DeepTutor&lt;/strong&gt;, a new AI tool built using GraphRAG for paper-reading, to help read the original research paper introducing GraphRAG itself (&lt;a href="https://arxiv.org/abs/2404.16130v2"&gt;paper link&lt;/a&gt;). It was a great test case, using the tool to explain its own technology.&lt;/p&gt; &lt;p&gt;Here are some examples of how DeepTutor handled my questions about the paper:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q1: &amp;quot;What's the key innovation behind GraphRAG compared to traditional RAG methods?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepTutor’s Response:&lt;/strong&gt; GraphRAG introduces several key innovations that distinguish it from traditional retrieval-augmented generation (RAG) methods:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Graph-Based Knowledge Representation&lt;/strong&gt;:GraphRAG constructs a knowledge graph to model entities and their relationships, enabling structured, relational understanding of data. This contrasts with traditional RAG systems, which often rely on less organized data representations, limiting their ability to capture complex connections.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integration of Advanced AI Models&lt;/strong&gt;: GraphRAG can be used with advanced generative models, such as GPT-4, to improve output quality, though GraphRAG itself is model-agnostic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Community Detection Algorithms&lt;/strong&gt;:GraphRAG employs these algorithms to identify clusters and relationships within data, enabling deeper insights for summarization and analysis—a feature absent in conventional RAG systems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Sensemaking&lt;/strong&gt;: By organizing data into a graph, GraphRAG supports better &amp;quot;sensemaking&amp;quot; over large text corpora, addressing a gap in traditional methods that struggle with unstructured or fragmented data analysis.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These innovations collectively improve efficiency, accuracy, and analytical depth, positioning GraphRAG as a significant evolution in retrieval-augmented generation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q2: &amp;quot;How does GraphRAG handle broader, 'big-picture' questions differently?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepTutor’s Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a concise analysis of how GraphRAG handles big-picture questions differently, organized for clarity:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GraphRAG's Approach to High-Level Questions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Structural Understanding Through Knowledge Graphs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates entity-relationship maps to reveal &lt;strong&gt;hidden patterns&lt;/strong&gt; and &lt;strong&gt;thematic connections&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Traditional methods&lt;/em&gt;: Limited to linear text relationships in retrieved passages&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Hierarchical Community Detection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses algorithms to: &lt;ul&gt; &lt;li&gt;Identify topic clusters&lt;/li&gt; &lt;li&gt;Discover indirect relationships&lt;/li&gt; &lt;li&gt;Create multi-level abstractions&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Traditional RAG&lt;/em&gt;: Lacks this layered analysis capability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Map-Reduce Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combines insights through: &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local analysis&lt;/strong&gt; (individual communities)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Global synthesis&lt;/strong&gt; (cross-community patterns)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Enables answers that balance depth and breadth&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Adaptive Context Windowing&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dynamically adjusts context scope based on: &lt;ul&gt; &lt;li&gt;Question complexity&lt;/li&gt; &lt;li&gt;Community hierarchy level&lt;/li&gt; &lt;li&gt;Interconnection density&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Traditional methods&lt;/em&gt;: Use fixed context windows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Claim-Based Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measures answer quality through: &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Comprehensiveness&lt;/strong&gt; (avg. 5.8 claims/answer vs 2.1 in vector RAG)&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;Diversity&lt;/strong&gt; (3.2x more unique claim clusters)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Ensures coverage of multiple perspectives&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Differentiator&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GraphRAG's &lt;strong&gt;graph-indexed worldview&lt;/strong&gt; enables &lt;strong&gt;emergent understanding&lt;/strong&gt; of latent patterns that no single document contains, while traditional methods remain constrained to explicit content in retrieved passages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q3: &amp;quot;Could you summarize the main findings of this GraphRAG study?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepTutor’s Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a concise breakdown of GraphRAG's key advantages and performance results compared to traditional methods, based on the study's findings:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Structural Superiority&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ &lt;strong&gt;Knowledge Graph Architecture&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Captures entity relationships (e.g., Company → Supplier → Environmental Impact)&lt;/li&gt; &lt;li&gt;Traditional RAG: Limited to direct text similarity between documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Enhanced Analytical Capabilities&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ &lt;strong&gt;55% More Comprehensive Answers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extracted 22.6 claims/answer vs. vector RAG's 14.6 (Experiment 1)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✓ &lt;strong&gt;2.3× Greater Diversity&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Produced 15.4 claim clusters vs. 6.7 from vector RAG&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Multi-Scale Understanding&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ &lt;strong&gt;Hierarchical Community Detection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Identifies nested themes (e.g., &amp;quot;ESG Risks&amp;quot; → Environmental → Carbon Emissions)&lt;/li&gt; &lt;li&gt;Traditional Methods: Treat documents as isolated chunks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Inclusive Synthesis&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ &lt;strong&gt;Map-Reduce Processing&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preserves minority viewpoints (e.g., rare safety incidents in manufacturing data)&lt;/li&gt; &lt;li&gt;Vector RAG: Prone to majority pattern dominance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Adaptive Precision&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ &lt;strong&gt;Corpus-Specific Benchmarking&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates tailored evaluation metrics for each dataset&lt;/li&gt; &lt;li&gt;Outperformed baseline by 38% on domain-specific queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;6. Strategic Decision Support&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✓ Enabled identification of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cross-domain relationships (e.g., governance failures → environmental risks)&lt;/li&gt; &lt;li&gt;Emerging patterns through community evolution analysis&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Validation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Used LLM-as-judge evaluation with 92% agreement rate on answer quality metrics, confirmed through human validation studies.&lt;/p&gt; &lt;p&gt;This structural approach to knowledge representation and analysis gives GraphRAG unique capabilities for strategic analysis tasks where understanding context and relationships is critical.&lt;/p&gt; &lt;p&gt;In each answer, DeepTutor directly highlighted where in the paper the answer was drawn from, which helped me quickly confirm accuracy and context.&lt;/p&gt; &lt;h1&gt;Why does this matter?&lt;/h1&gt; &lt;p&gt;My experience made it clear that GraphRAG significantly improves how AI understands and presents information from documents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It provides more comprehensive answers because it considers the whole document rather than isolated pieces.&lt;/li&gt; &lt;li&gt;It’s easier to trust, as each response clearly references where in the document the answer came from.&lt;/li&gt; &lt;li&gt;It naturally shows connections between ideas, helping users quickly understand complicated topics.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After using GraphRAG firsthand with DeepTutor, I genuinely felt it provided meaningful improvements over traditional AI document-reading tools.&lt;/p&gt; &lt;p&gt;Have you faced similar challenges with AI tools? Have you tried GraphRAG or similar approaches yet? Let me know your thoughts! I’d love to discuss this further.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Top504"&gt; /u/Street_Top504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrjqgh/how_graphrag_helps_ai_tools_understand_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrjqgh/how_graphrag_helps_ai_tools_understand_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrjqgh/how_graphrag_helps_ai_tools_understand_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T19:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jridrq</id>
    <title>So, will LLaMA 4 be an omni model?</title>
    <updated>2025-04-04T18:03:18+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just curious 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jridrq/so_will_llama_4_be_an_omni_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jridrq/so_will_llama_4_be_an_omni_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jridrq/so_will_llama_4_be_an_omni_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T18:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0vnb</id>
    <title>Real-time in-browser speech recognition with Nuxt and Transformers.js</title>
    <updated>2025-04-04T02:09:45+00:00</updated>
    <author>
      <name>/u/Bonteq</name>
      <uri>https://old.reddit.com/user/Bonteq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0vnb/realtime_inbrowser_speech_recognition_with_nuxt/"&gt; &lt;img alt="Real-time in-browser speech recognition with Nuxt and Transformers.js" src="https://preview.redd.it/8ekr6x428qse1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8335f90a1fc2aea1630bab497acdd1723c558dba" title="Real-time in-browser speech recognition with Nuxt and Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repo: &lt;a href="https://github.com/CodyBontecou/nuxt-transformersjs-realtime-transcription"&gt;https://github.com/CodyBontecou/nuxt-transformersjs-realtime-transcription&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/CodyBontecou/nuxt-transformersjs-realtime-transcription"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bonteq"&gt; /u/Bonteq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ekr6x428qse1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0vnb/realtime_inbrowser_speech_recognition_with_nuxt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0vnb/realtime_inbrowser_speech_recognition_with_nuxt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T02:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr5f2h</id>
    <title>Anyone wants to collaborate on new open-source TTS?</title>
    <updated>2025-04-04T06:33:47+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community! We’re currently working on (very WIP) a groundbreaking TTS model with a 48kHz sampling rate and stereo speech! Based on VITS architecture! Very fast training (literally hours) and real-time inference! If you’re interested, let’s discuss the code more, not the weights!&lt;/p&gt; &lt;p&gt;Link (just in case): &lt;a href="https://github.com/yukiarimo/hanasu"&gt;https://github.com/yukiarimo/hanasu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr5f2h/anyone_wants_to_collaborate_on_new_opensource_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr5f2h/anyone_wants_to_collaborate_on_new_opensource_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr5f2h/anyone_wants_to_collaborate_on_new_opensource_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T06:33:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr7ljh</id>
    <title>Wow!! Cloudflare starts to provide hosting for MCP Servers</title>
    <updated>2025-04-04T09:13:47+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7ljh/wow_cloudflare_starts_to_provide_hosting_for_mcp/"&gt; &lt;img alt="Wow!! Cloudflare starts to provide hosting for MCP Servers" src="https://external-preview.redd.it/aSe81vpAstY8O3ojbrQ8SQijuRSxkXwgag2mTxMYYKM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7383c3e17364a83344967e65a4e97924b9b28076" title="Wow!! Cloudflare starts to provide hosting for MCP Servers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cloudflare provides hosting for MCP Server. Need MORE MCP SERVER HERE IS A LIST FOR YOU GUYS &lt;a href="https://github.com/MobinX/awesome-mcp-list/tree/main"&gt;https://github.com/MobinX/awesome-mcp-list/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.infoq.com/news/2025/04/cloudflare-remote-mcp-servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7ljh/wow_cloudflare_starts_to_provide_hosting_for_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7ljh/wow_cloudflare_starts_to_provide_hosting_for_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T09:13:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqnnfp</id>
    <title>Official Gemma 3 QAT checkpoints (3x less memory for ~same performance)</title>
    <updated>2025-04-03T16:54:08+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! We got new official checkpoints from the Gemma team.&lt;/p&gt; &lt;p&gt;Today we're releasing quantization-aware trained checkpoints. This allows you to use q4_0 while retaining much better quality compared to a naive quant. You can go and use this model with llama.cpp today!&lt;/p&gt; &lt;p&gt;We worked with the llama.cpp and Hugging Face teams to validate the quality and performance of the models, as well as ensuring we can use the model for vision input as well. Enjoy!&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T16:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr7o1v</id>
    <title>AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction</title>
    <updated>2025-04-04T09:18:54+00:00</updated>
    <author>
      <name>/u/Icy-Corgi4757</name>
      <uri>https://old.reddit.com/user/Icy-Corgi4757</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7o1v/animegamer_infinite_anime_life_simulation_with/"&gt; &lt;img alt="AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction" src="https://external-preview.redd.it/xWFKiluMMwERAv1-k-74qSop9in2RY5xOINzk_h_shc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8cbffea6739e5098cfdd04414cd9e051302d24" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Corgi4757"&gt; /u/Icy-Corgi4757 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/TencentARC/AnimeGamer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7o1v/animegamer_infinite_anime_life_simulation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr7o1v/animegamer_infinite_anime_life_simulation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T09:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrc2xk</id>
    <title>How to install TabbyAPI+Exllamav2 and vLLM on a 5090</title>
    <updated>2025-04-04T13:37:28+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As it took me a while to make it work I'm leaving the steps here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TabbyAPI+Exllamav2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/theroyallab/tabbyAPI"&gt;&lt;code&gt;https://github.com/theroyallab/tabbyAPI&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;cd tabbyAPI&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Setup the python venv&lt;br /&gt; &lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;br /&gt; &lt;code&gt;source venv/bin/activate # source venv/bin/activate.fish for fish shell&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python -m pip install --pre torch torchvision torchaudio --index-url&lt;/code&gt; &lt;a href="https://download.pytorch.org/whl/nightly/cu128"&gt;&lt;code&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;EXLLAMA_NOCOMPILE=1 pip install .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;In case you don't have this:&lt;br /&gt; &lt;code&gt;sudo apt-get update&lt;/code&gt;&lt;br /&gt; &lt;code&gt;sudo apt-get install -y build-essential g++ gcc libstdc++-10-dev ninja-build&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installing flash attention:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;&lt;code&gt;https://github.com/Dao-AILab/flash-attention&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;cd flash-attention&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python -m pip install wheel&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python&lt;/code&gt; &lt;a href="http://setup.py"&gt;&lt;code&gt;setup.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;install&lt;/code&gt;&lt;/p&gt; &lt;p&gt;TabbyAPI is ready to run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/vllm-project/vllm"&gt;&lt;code&gt;https://github.com/vllm-project/vllm&lt;/code&gt;&lt;/a&gt;&lt;br /&gt; &lt;code&gt;cd vllm&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python3.12 -m venv venv&lt;/code&gt;&lt;br /&gt; &lt;code&gt;source venv/bin/activate # source venv/bin/activate.fish for fish shell&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install pytorch&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;python -m pip install --pre torch torchvision torchaudio --index-url&lt;/code&gt; &lt;a href="https://download.pytorch.org/whl/nightly/cu128"&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;python use_existing_torch.py&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python -m pip install -r requirements/build.txt&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python -m pip install -r requirements/common.txt&lt;/code&gt;&lt;br /&gt; &lt;code&gt;python -m pip install -e . --no-build-isolation&lt;/code&gt; &lt;/p&gt; &lt;p&gt;vLLM should be ready&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrc2xk/how_to_install_tabbyapiexllamav2_and_vllm_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrc2xk/how_to_install_tabbyapiexllamav2_and_vllm_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrc2xk/how_to_install_tabbyapiexllamav2_and_vllm_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T13:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqzr2y</id>
    <title>Llama 4 sighting</title>
    <updated>2025-04-04T01:12:13+00:00</updated>
    <author>
      <name>/u/Tha_One</name>
      <uri>https://old.reddit.com/user/Tha_One</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/legit_api/status/1907941993789141475"&gt;https://x.com/legit_api/status/1907941993789141475&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tha_One"&gt; /u/Tha_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqzr2y/llama_4_sighting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqzr2y/llama_4_sighting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqzr2y/llama_4_sighting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T01:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr4rbi</id>
    <title>Samsung is working on a large vision language model</title>
    <updated>2025-04-04T05:49:43+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4rbi/samsung_is_working_on_a_large_vision_language/"&gt; &lt;img alt="Samsung is working on a large vision language model" src="https://preview.redd.it/d46j28ufbrse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=758a25e039afe10d74be40c317403382200cbb0c" title="Samsung is working on a large vision language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d46j28ufbrse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4rbi/samsung_is_working_on_a_large_vision_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4rbi/samsung_is_working_on_a_large_vision_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T05:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr35zl</id>
    <title>Mystery model on openrouter (quasar-alpha) is probably new OpenAI model</title>
    <updated>2025-04-04T04:12:42+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr35zl/mystery_model_on_openrouter_quasaralpha_is/"&gt; &lt;img alt="Mystery model on openrouter (quasar-alpha) is probably new OpenAI model" src="https://a.thumbs.redditmedia.com/8SeVD0tJOnm-JhErOpETEEeSBWnaDADNRD_RNZ-6k98.jpg" title="Mystery model on openrouter (quasar-alpha) is probably new OpenAI model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample outputs: &lt;a href="https://eqbench.com/results/creative-writing-v3/openrouter__quasar-alpha.html"&gt;https://eqbench.com/results/creative-writing-v3/openrouter__quasar-alpha.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jr35zl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr35zl/mystery_model_on_openrouter_quasaralpha_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr35zl/mystery_model_on_openrouter_quasaralpha_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T04:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr4nfh</id>
    <title>We trained Gemma 3 -4b, a 2d VLM model to do 3d recognition task!</title>
    <updated>2025-04-04T05:42:45+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4nfh/we_trained_gemma_3_4b_a_2d_vlm_model_to_do_3d/"&gt; &lt;img alt="We trained Gemma 3 -4b, a 2d VLM model to do 3d recognition task!" src="https://external-preview.redd.it/MHprOHIyMzBhcnNlMUXrfMu-M6tyJvpBgz4xEXGHlpTCS4mKiqYnH6frp6Y9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=945e2ea3b2c33abf88bee7b7a1bf0323151a1baa" title="We trained Gemma 3 -4b, a 2d VLM model to do 3d recognition task!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's me again, from Menlo Research (aka homebrew aka Jan)! We just released a new experiment: VoxRep – a novel approach that enables 2D Vision-Language Models (Gemma3-4b in this case) to understand and extract semantics from 3D voxel data!&lt;/p&gt; &lt;p&gt;In most previous works, VLMs demonstrated impressive abilities in understanding 2D visual inputs. However, comprehending 3D environments remains vital for intelligent systems in domains like robotics and autonomous navigation.&lt;/p&gt; &lt;p&gt;This begs the question, can a 2d VLM architecture comprehend 3d space &amp;quot;fully&amp;quot;?&lt;/p&gt; &lt;p&gt;To explore this, we conducted some experiments resulting in VoxRep, building on just a VLM (Gemma in this case) capabilities with only some simple techniques in building the dataset.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We slice the 3D voxel grid along the Z-axis into individual 2D slices, then arrange them in a 4×4 grid to create a single 896×896 composite image. Just like doing CT-scanning image&lt;/li&gt; &lt;li&gt;Testing the model on extracting &amp;quot;voxel semantics&amp;quot;—object identity, color, and location&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The training data is demonstrated in the video!&lt;/p&gt; &lt;h1&gt;Results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Color recognition accuracy ~ 80%&lt;/li&gt; &lt;li&gt;Object classification accuracy ~ 60%&lt;/li&gt; &lt;li&gt;Average distance to labelled object center ~ from 26.05 voxels to just 9.17 voxels&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This result is only based on &lt;strong&gt;20.000 samples&lt;/strong&gt; which is in general a pretty small dataset which suggest there is some extrapolation in Gemma 3 - 4b model (this is purely speculation) because the loss converged while well regardless of limited data.&lt;/p&gt; &lt;p&gt;The model shows some promising result, suggesting that if we pursue down this path further, probably we can re-use a lot of pre-trained 2d VLM model for 3d task!&lt;/p&gt; &lt;h1&gt;Appreciation:&lt;/h1&gt; &lt;p&gt;A huge thank you to Google for their Gemma 3 VLM and to Princeton for their incredible ModelNet40 dataset that made our research possible!&lt;/p&gt; &lt;h1&gt;Links:&lt;/h1&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.21214"&gt;https://arxiv.org/abs/2503.21214&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Menlo/voxel-representation-gemma3-4b"&gt;https://huggingface.co/Menlo/voxel-representation-gemma3-4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/menloresearch/voxel-representation"&gt;https://github.com/menloresearch/voxel-representation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kqtp4eg79rse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4nfh/we_trained_gemma_3_4b_a_2d_vlm_model_to_do_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr4nfh/we_trained_gemma_3_4b_a_2d_vlm_model_to_do_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T05:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr8sw0</id>
    <title>PSA: You can do QAT (quantization aware tuning) with Meta's torchtune.</title>
    <updated>2025-04-04T10:39:01+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a bunch of people asking on the Gemma 3 QAT thread about how to do this yourself.&lt;/p&gt; &lt;p&gt;Torchtune (super flexible and easy to use fine-tuning library from Meta) actually has that built in (mostly thanks to existing support in torchao). &lt;/p&gt; &lt;p&gt;Here is their explanation of the technique as well as tutorial on how to do it: &lt;a href="https://pytorch.org/torchtune/0.5/tutorials/qat_finetune.html"&gt;https://pytorch.org/torchtune/0.5/tutorials/qat_finetune.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In general, I really recommend people give torchtune a try -- it's a strong competitor to the likes of axolotl and TRL with clean and flexible codebase and heavy focus on testing. There are still some important features missing, but usually they are easy to add yourself, or are on the way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr8sw0/psa_you_can_do_qat_quantization_aware_tuning_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr8sw0/psa_you_can_do_qat_quantization_aware_tuning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr8sw0/psa_you_can_do_qat_quantization_aware_tuning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T10:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0oy2</id>
    <title>Howto: Building a GPU Server with 8xRTX 4090s for local inference</title>
    <updated>2025-04-04T02:00:32+00:00</updated>
    <author>
      <name>/u/appenz</name>
      <uri>https://old.reddit.com/user/appenz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0oy2/howto_building_a_gpu_server_with_8xrtx_4090s_for/"&gt; &lt;img alt="Howto: Building a GPU Server with 8xRTX 4090s for local inference" src="https://preview.redd.it/vg99momf6qse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d21a41bd4e8ee1781e6a42b4d3c2bc49c8f7ce8" title="Howto: Building a GPU Server with 8xRTX 4090s for local inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Marco Mascorro built a pretty cool 8x4090 server for local inference and wrote a pretty detailed howto guide on what parts he used and how to put everything together. I hope this is interesting for anyone who is looking for a local inference solution and doesn't have the budget for using A100's or H100's. The build should work with 5090's as well.&lt;/p&gt; &lt;p&gt;Full guide is here: &lt;a href="https://a16z.com/building-an-efficient-gpu-server-with-nvidia-geforce-rtx-4090s-5090s/"&gt;https://a16z.com/building-an-efficient-gpu-server-with-nvidia-geforce-rtx-4090s-5090s/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear comments/feedback and would be happy to answer any questions in this thread. We are huge fans of open source/weights models and local inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appenz"&gt; /u/appenz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vg99momf6qse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0oy2/howto_building_a_gpu_server_with_8xrtx_4090s_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr0oy2/howto_building_a_gpu_server_with_8xrtx_4090s_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T02:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr6c8e</id>
    <title>Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0</title>
    <updated>2025-04-04T07:39:20+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"&gt; &lt;img alt="Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0" src="https://external-preview.redd.it/djIzeHlvdXJ1cnNlMewzcMfvF_oVr57d3HNacgT7P88RyE9Zm1kIukBjIS8J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb528c0ac716b1babb41d1d367689915e0210a68" title="Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Alpha-VLLM/Lumina-mGPT-2.0"&gt;https://github.com/Alpha-VLLM/Lumina-mGPT-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Alpha-VLLM/Lumina-mGPT-2.0"&gt;https://huggingface.co/Alpha-VLLM/Lumina-mGPT-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Alpha-VLLM/Lumina-Image-2.0"&gt;https://huggingface.co/spaces/Alpha-VLLM/Lumina-Image-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jrf0voururse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T07:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jre3kp</id>
    <title>New paper from DeepSeek w/ model coming soon: Inference-Time Scaling for Generalist Reward Modeling</title>
    <updated>2025-04-04T15:04:46+00:00</updated>
    <author>
      <name>/u/samfundev</name>
      <uri>https://old.reddit.com/user/samfundev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quote from the abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A key challenge of reinforcement learning (RL) is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. [...] Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. &lt;strong&gt;The models will be released and open-sourced.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Summary from Claude:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Can you provide a two paragraph summary of this paper for an audience of people who are enthusiastic about running LLMs locally?&lt;/em&gt; &lt;/p&gt; &lt;p&gt;This paper introduces DeepSeek-GRM, a novel approach to reward modeling that allows for effective &amp;quot;inference-time scaling&amp;quot; - getting better results by running multiple evaluations in parallel rather than requiring larger models. The researchers developed a method called Self-Principled Critique Tuning (SPCT) which trains reward models to generate tailored principles for each evaluation task, then produce detailed critiques based on those principles. Their experiments show that DeepSeek-GRM-27B with parallel sampling can match or exceed the performance of much larger reward models (up to 671B parameters), demonstrating that compute can be more effectively used at inference time rather than training time.&lt;/p&gt; &lt;p&gt;For enthusiasts running LLMs locally, this research offers a promising path to higher-quality evaluation without needing massive models. By using a moderately-sized reward model (27B parameters) and running it multiple times with different seeds, then combining the results through voting or their meta-RM approach, you can achieve evaluation quality comparable to much larger models. The authors also show that this generative reward modeling approach avoids the domain biases of scalar reward models, making it more versatile for different types of tasks. The models will be open-sourced, potentially giving local LLM users access to high-quality evaluation tools.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samfundev"&gt; /u/samfundev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.02495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jre3kp/new_paper_from_deepseek_w_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jre3kp/new_paper_from_deepseek_w_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T15:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrfqnu</id>
    <title>Meta Set to Release Llama 4 This Month, per The Information &amp; Reuters</title>
    <updated>2025-04-04T16:13:03+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;April 4 (Reuters) - Meta Platforms (META.O), plans to release the latest version of its large language model later this month, after delaying it at least twice, the Information reported on Friday, as the Facebook owner scrambles to lead in the AI race.&lt;/p&gt; &lt;p&gt;Meta, however, could push back the release of Llama 4 again, the report said, citing two people familiar with the matter.&lt;/p&gt; &lt;p&gt;Big technology firms have been investing aggressively in AI infrastructure following the success of OpenAI's ChatGPT, which altered the tech landscape and drove investment into machine learning.&lt;/p&gt; &lt;p&gt;The report said one of the reasons for the delay is during development, Llama 4 did not meet Meta's expectations on technical benchmarks, particularly in reasoning and math tasks.&lt;/p&gt; &lt;p&gt;The company was also concerned that Llama 4 was less capable than OpenAI's models in conducting humanlike voice conversations, the report added.&lt;/p&gt; &lt;p&gt;Meta plans to spend as much as $65 billion this year to expand its AI infrastructure, amid investor pressure on big tech firms to show returns on their investments.&lt;/p&gt; &lt;p&gt;Additionally, the rise of the popular, lower-cost model from Chinese tech firm DeepSeek challenges the belief that developing the best AI model requires billions of dollars.&lt;/p&gt; &lt;p&gt;The report said Llama 4 is expected to borrow certain technical aspects from DeepSeek, with at least one version slated to employ a machine-learning technique called mixture of experts method, which trains separate parts of models for specific tasks, making them experts in those areas.&lt;/p&gt; &lt;p&gt;Meta has also considered releasing Llama 4 through Meta AI first and then as open-source software later, the report said.&lt;/p&gt; &lt;p&gt;Last year, Meta released its mostly free Llama 3 AI model, which can converse in eight languages, write higher-quality computer code and solve more complex math problems than previous versions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/technology/artificial-intelligence/meta-nears-release-new-ai-model-llama-4-this-month-information-reports-2025-04-04/"&gt;&lt;em&gt;https://www.reuters.com/technology/artificial-intelligence/meta-nears-release-new-ai-model-llama-4-this-month-information-reports-2025-04-04/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.theinformation.com/articles/meta-nears-release-new-ai-model-performance-hiccups"&gt;&lt;em&gt;https://www.theinformation.com/articles/meta-nears-release-new-ai-model-performance-hiccups&lt;/em&gt; &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T16:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrd0a9</id>
    <title>Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI</title>
    <updated>2025-04-04T14:18:33+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt; &lt;img alt="Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI" src="https://b.thumbs.redditmedia.com/DMLWSDxNqqWl7AjUQHrzw2WhaBG3cSl0ZG95ENj1I-s.jpg" title="Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After testing the recently released quasar-alpha model by openrouter, I discovered that when asking this specific Chinese question:&lt;/p&gt; &lt;p&gt;''' 给主人留下些什么吧 这句话翻译成英文 '''&lt;br /&gt; (This sentence means &amp;quot;Leave something for the master&amp;quot; and &amp;quot;Translate this sentence into English&amp;quot;)&lt;/p&gt; &lt;p&gt;The model's response is completely unrelated to the question.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wiq7fg3qttse1.png?width=2384&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6368c81d31fab66e3a998d26b96a49b2a556e3b2"&gt;quasar-alpha's answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-4o had the same issue when it was released, because in the updated o200k_base tokenizer, the phrase &amp;quot;给主人留下些什么吧&amp;quot; happens to be a single token with ID 177431. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/blb4buzxttse1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8dc289dc1b2968f92620aa2137e03f3a83bf624"&gt;GPT-4o's answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The fact that this new model exhibits the same problem increases suspicion that this secret model indeed comes from OpenAI, and they still haven't fixed this Chinese token bug.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T14:18:33+00:00</published>
  </entry>
</feed>
