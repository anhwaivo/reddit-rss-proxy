<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-08T18:24:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikjhn7</id>
    <title>Why are many SWEs salty about LLM use for coding?</title>
    <updated>2025-02-08T09:20:38+00:00</updated>
    <author>
      <name>/u/delicate_rabbit</name>
      <uri>https://old.reddit.com/user/delicate_rabbit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am SWE, and I'm using LLM on daily basis. It helps immensely. If I give it correct prompts/context it will spit out the methods/logic I need. It will generate complex SQL queries (if I need them) etc, etc. It will explain concepts I am not familiar with. It will even break down complex problems into digestable chunks where I can then form a whole picture of what I wanna do.&lt;/p&gt; &lt;p&gt;If I am unsure about the syntax/how I'd write some code, or hell even if I straight up don't know how to do it, it will give me the result or at least the direction. However I always, always check if it makes sense. I just don't blindly copy whatever it spits out. If it doesn't work, I fine tune it so it does.&lt;/p&gt; &lt;p&gt;So I am not sure why are so many shitting on it? &lt;/p&gt; &lt;p&gt;&amp;quot;You will forget how to do it yourself !&amp;quot; &lt;/p&gt; &lt;p&gt;Sure, the pure syntax/coding skills might get rustier, but if you can rely on it, evaluate the suggestion, so what? To me it is somewhat akin to saying: &amp;quot;your will forget how to create fire with 2 rocks because you are using the lighter!&amp;quot; If I understand what the end result should be does it matter that I used the lighter and know what fire does?&lt;/p&gt; &lt;p&gt;&amp;quot;AI gives me intern level results!&amp;quot;&lt;/p&gt; &lt;p&gt;Have you tried giving it a detailed prompt and context instead of a vague 5 word sentence before getting mad?&lt;/p&gt; &lt;p&gt;At the end of the day it's just a tool right? If you're getting the result, why does it matter how you got there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/delicate_rabbit"&gt; /u/delicate_rabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iksy75</id>
    <title>In Eleven Labs I can record a real voice performance and convert that performance into a different voice (and accent), is this possible locally yet?</title>
    <updated>2025-02-08T17:45:49+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Eleven Labs is the only service I'm paying for, mainly because of the feature I described in my title, is there an offline local alternative that is able to do this?&lt;/p&gt; &lt;p&gt;So far, I'm able to clone any voice I want, but I can't transfer a real performance. &lt;/p&gt; &lt;p&gt;Is this possible yet locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikt9an</id>
    <title>I Built lfind: A Natural Language File Finder Using LLMs</title>
    <updated>2025-02-08T17:58:53+00:00</updated>
    <author>
      <name>/u/Mahrkeenerh1</name>
      <uri>https://old.reddit.com/user/Mahrkeenerh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt; &lt;img alt="I Built lfind: A Natural Language File Finder Using LLMs" src="https://preview.redd.it/rwb26a0yeyhe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=653ea170e4f0049c8cfde86a3d70e9eb14484f48" title="I Built lfind: A Natural Language File Finder Using LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mahrkeenerh1"&gt; /u/Mahrkeenerh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwb26a0yeyhe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikrbhw</id>
    <title>Photonics. 30x efficiency?</title>
    <updated>2025-02-08T16:37:30+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please cost less than a car... PCIe card:&lt;br /&gt; &lt;a href="https://qant.com/photonic-computing/"&gt;https://qant.com/photonic-computing/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Apparently Nvidia and TSMC have created a photonics chip as well:&lt;br /&gt; &lt;a href="https://wccftech.com/nvidia-tsmc-develop-advanced-silicon-photonic-chip-prototype-says-report/"&gt;https://wccftech.com/nvidia-tsmc-develop-advanced-silicon-photonic-chip-prototype-says-report/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikrbhw/photonics_30x_efficiency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikrbhw/photonics_30x_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikrbhw/photonics_30x_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikprg7</id>
    <title>Trouble with running llama.cpp with Deepseek-R1 on 4x NVME raid0.</title>
    <updated>2025-02-08T15:29:24+00:00</updated>
    <author>
      <name>/u/U_A_beringianus</name>
      <uri>https://old.reddit.com/user/U_A_beringianus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to get some speed benefit out of running llama.cpp with the model (Deepseek-R1, 671B, Q2) on a 4x nvme raid0 in comparison to a single nvme. But running it from raid yields a much, much lower inference speed than running it from a single disk.&lt;br /&gt; The raid0, with 16 PCIe (4.0) lanes in total, yields 25GB/s (with negligible CPU usage) when benchmarked with fio (for sequential reads in 1MB chunks), the single nvme yields 7GB/s.&lt;br /&gt; With the model mem-mapped from the single disk, I get 1.2t/s (no GPU offload), with roughly 40%-50% of CPU usage by llama.cpp, so it seems I/O is the bottleneck in this case. But with the model mem-mapped from the raid I get merely &amp;lt;0.1 t/s, tens of seconds per token, with the CPU fully utilized.&lt;br /&gt; My first wild guess here is that llama.cpp does very small, discontinuous, random reads, which causes a lot of CPU overhead, when reading from a software raid.&lt;br /&gt; I tested/tried the following things also: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Filesystem doesn't matter, tried ext4, btrfs, f2fs on the raid. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;md-raid (set up with mdadm) vs. btrfs-raid0 did not make a difference. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;In an attempt to reduce CPU overhead I used only 2 instead of 4 nvmes for raid0 -&amp;gt; no improvement &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Put swap on the raid array, and invoked llama.cpp with --no-mmap, to force the majority of the model into that swap: 0.5-0.7 t/s, so while better than mem-mapping from the raid, still slower than mem-mapping from a single disk. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;dissolved the raid, and put the parts of split gguf (4 pieces), onto a separate Filesystem/nvme each: Expectedly, the same speed as from a single nvme (1.2 t/s), since llama.cpp doesn't seem to read the parts in parallel. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;With raid0, tinkered with various stripe sizes and block sizes, always making sure they are well aligned: Negligible differences in speed.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So is there any way to get some use for llama.cpp out of those 4 NVMEs, with 16 direct-to-cpu PCIe lanes to them? I'd be happy if I could get llama.cpp inference to be at least a tiny bit faster with those than running simply from a single device.&lt;br /&gt; With simply writing/reading huge files, I get incredibly high speeds out of that array.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U_A_beringianus"&gt; /u/U_A_beringianus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikprg7/trouble_with_running_llamacpp_with_deepseekr1_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikprg7/trouble_with_running_llamacpp_with_deepseekr1_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikprg7/trouble_with_running_llamacpp_with_deepseekr1_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikpuwn</id>
    <title>A TTS model with specific entonation?</title>
    <updated>2025-02-08T15:33:37+00:00</updated>
    <author>
      <name>/u/RodrigoDNGT</name>
      <uri>https://old.reddit.com/user/RodrigoDNGT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been searching for TTS models that can specify the entonation voice desired (happy, sad, and many others like whispers). I found the F5 TTS model finetuned with brazilian Portuguese language (it's the language what I want to generate the audios) but even using audios with the desired entonation, the model still uses the phrase context to give emotion or entonation or don't give any emotion at all.&lt;/p&gt; &lt;p&gt;I was wondering if I finetune this model (or the other one: XTTS v2) with a specific dataset that has the entonation desired and finetune this model, it will generate audios only with this entonation?&lt;/p&gt; &lt;p&gt;Do you think it's possible? I mean, if I finetune a model only with angry audios, the model will generate only angry audios? or this just not gonna work and still generate audios by the phrase context? I'm questioning this before any dataset preparation and starting the fine-tuning. &lt;/p&gt; &lt;p&gt;Someone already did this test? &lt;/p&gt; &lt;p&gt;My final plan with this question is to finetune multiple models, each one with specific entonation. Then, when generates an audio, first the algorithm will select an TTS model finetuned by the entonation chosed, then, the chosed model will generate the audios. So, I will have more control with the entonations if this idea work &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RodrigoDNGT"&gt; /u/RodrigoDNGT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikpuwn/a_tts_model_with_specific_entonation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikpuwn/a_tts_model_with_specific_entonation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikpuwn/a_tts_model_with_specific_entonation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iknxmq</id>
    <title>What are the major improvements from 2017 that lead to current SOTA LLM?</title>
    <updated>2025-02-08T14:03:20+00:00</updated>
    <author>
      <name>/u/Doug_Fripon</name>
      <uri>https://old.reddit.com/user/Doug_Fripon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to update my knowledge on transformers architecture since the foundational Attention is all you need paper from 2017. I'm struggling to find (or generate) a synthetic trustful resource that provides a high-level picture of the major improvements to the SOTA since then.&lt;/p&gt; &lt;p&gt;Can we identify the major LLM architectural evolutions from the last few years? I suggest we don't cover the multimodal topics unless directly applicable to LLM. &lt;/p&gt; &lt;p&gt;For example, the RoPE paper from 2021 &lt;a href="https://arxiv.org/pdf/2104.09864"&gt;https://arxiv.org/pdf/2104.09864&lt;/a&gt; that introduces rotary position embeddings seems a major update that reduces the dependency of explicit position encoding into the embeddings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doug_Fripon"&gt; /u/Doug_Fripon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqsal</id>
    <title>Which models do you run locally?</title>
    <updated>2025-02-08T16:14:15+00:00</updated>
    <author>
      <name>/u/santhosh1993</name>
      <uri>https://old.reddit.com/user/santhosh1993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, if you are using a specific model heavily? which factors stood out for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/santhosh1993"&gt; /u/santhosh1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikosn5</id>
    <title>What are the best models for code autocomplete (like cursor autocomplete)?</title>
    <updated>2025-02-08T14:46:04+00:00</updated>
    <author>
      <name>/u/vniversvs_</name>
      <uri>https://old.reddit.com/user/vniversvs_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;that's it. i decided to use my small GPU to host not a full coding assistant, but rather a good autocomplete, and invest the money i'd have spent on a huge GPU to pay for APIs.&lt;/p&gt; &lt;p&gt;but then which model to choose? i'm trying currently qwen 1.5B, heard some good things about startcoder 3B. what is your experience? are there really good autocomplete-specialized models out there? like many here, i'm looking for that cursor experience but in a cheaper way. I think the largest my GPU would be able to handle is something around 5B unquantized, maybe 14B with reasonable quantization. &lt;/p&gt; &lt;p&gt;also, are there benchmarks for this particular task? i've seen some benchmarks but haven't found their actual results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vniversvs_"&gt; /u/vniversvs_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikosn5/what_are_the_best_models_for_code_autocomplete/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikosn5/what_are_the_best_models_for_code_autocomplete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikosn5/what_are_the_best_models_for_code_autocomplete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxdue</id>
    <title>Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser.</title>
    <updated>2025-02-07T15:20:49+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt; &lt;img alt="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." src="https://external-preview.redd.it/eXpiZzdyaDVpcWhlMePeQo88FDwgFQaiUAHhHRFDa4M37cixJTBs9Mic6GzX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3934e49e8a9ed14426802e792f696c88e956d015" title="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5b2t6sh5iqhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikmv9x</id>
    <title>Podcasts with TinyLlama and Kokoro on iOS</title>
    <updated>2025-02-08T13:07:14+00:00</updated>
    <author>
      <name>/u/derjanni</name>
      <uri>https://old.reddit.com/user/derjanni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Llama friends,&lt;/p&gt; &lt;p&gt;around a month ago I was on a flight back to Germany and hastily downloaded Podcasts before departure. Once airborne, I found all of them boring which had me sitting bored on a four hour flight. I had no coverage and the ones I had stored in the device turned out to be not really what I was into. That got me thiniking and I wanted to see if you could generate podcasts offline on my iPhone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; before I get into the details, Botcast was approved by Apple an hour ago. Check it out if you are interested.&lt;/p&gt; &lt;h1&gt;The challenge of generating podcasts&lt;/h1&gt; &lt;p&gt;I wanted an app that works offline and generates podcasts with decent voices. I went with TinyLlama 1.1B Chat v1.0 Q6_K to generate the podcasts. My initial attempt was to generate each spoken line with an individual prompt, but it turned out that just prompting TinyLlama to generate a podcast transcript just worked fine. The podcasts are all chats between two people for which gender, name and voice are randomly selected.&lt;/p&gt; &lt;p&gt;The entire process of generating the transcript takes around a minute on my iPhone 14, much faster on the 16 Pro and around 3-4 minutes on the SE 2020. For the voices, I went with Kokoro 0.19 since these voices seem to be the best quality I could find that work on iOS. After some testing, I threw out the UK voices since those sounded much too robotic.&lt;/p&gt; &lt;h1&gt;Technical details of Botcast&lt;/h1&gt; &lt;p&gt;Botcast is a native iOS app built with Xcode and written in Swift and SwiftUI. However, the majority of it is C/C++ simple because of llama.cpp for iOS and the necessary inference libraries for Kokoro on iOS. A ton of bridging between Swift and the frameworks, libraries is involved. That's also why I went with 18.2 minimum as stability on earlies iOS versions is just way too much work to ensure.&lt;/p&gt; &lt;p&gt;And as with all the audio stuff I did before, the app is brutally multi-threading both on the CPU, the Metal GPU and the Neural Core Engines. The app will need around 1.3 GB of RAM and hence has the entitlement to increase up to 3GB on iPhone 14, up to 1.4GB on SE 2020. Of course it also uses the extended memory areas of the GPU. Around 80% of bugfixing was simply getting the memory issues resolved.&lt;/p&gt; &lt;p&gt;When I first got it into TestFlight it simply crashed when Apple reviewed it. It wouldn't even launch. I had to upgrade some inference libraries and fiddle around with their instanciation. It's technically hitting the limits of the iPhone 14, but anything above that is perfectly smooth from my experience. Since it's also Mac Catalyst compatible, it works like a charm on my M1 Pro.&lt;/p&gt; &lt;h1&gt;Future of Botcast&lt;/h1&gt; &lt;p&gt;Botcast is currently free and I intent to keep it like that. Next step is CarPlay support which I definitely want as well as Siri integration for &amp;quot;Generate&amp;quot;. The idea is to have it do its thing completely hands free. Further, the inference supports streaming, so exploring the option to really have the generate and the playback run instantly to provide really instant real-time podcasts is also on the list.&lt;/p&gt; &lt;p&gt;Botcast was a lot of work and I am potentially looking into maybe giving it some customizing in the future and just charge a one-time fee for a pro version (e.g. custom prompting, different flavours of podcasts with some exclusive to a pro version). Pricing wise, a pro version will probably become something like $5 one-time fee as I'm totally not a fan of subscriptions for something that people run on their devices.&lt;/p&gt; &lt;p&gt;Let me know what you think about Botcast, what features you'd like to see or any questions you have. I'm totally excited and into Ollama, llama.cpp and all the stuff around it. It's just pure magical what you can do with llama.cpp on iOS. Performance is really strong even with Q6_K quants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derjanni"&gt; /u/derjanni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:07:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn3m4</id>
    <title>Qwen in the mac menu bar</title>
    <updated>2025-02-08T13:20:00+00:00</updated>
    <author>
      <name>/u/Zealousideal-Net1385</name>
      <uri>https://old.reddit.com/user/Zealousideal-Net1385</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt; &lt;img alt="Qwen in the mac menu bar" src="https://external-preview.redd.it/noY1h5Okrq7ZANvEw5MMNJGxiH9lTWE3nW0dM83KMT8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e4ce9351cd888be2d92a844b79e910f95cc4e2d" title="Qwen in the mac menu bar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear all, I Developed this app for Mac OS and I need some testers; since I love Qwen family models I’ve developed this app to enrich the productivity, working as a floating window over other apps. Comments are really appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Net1385"&gt; /u/Zealousideal-Net1385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/andreaturchet/Qwen4Mac"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikek15</id>
    <title>My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload</title>
    <updated>2025-02-08T04:03:51+00:00</updated>
    <author>
      <name>/u/bo_peng</name>
      <uri>https://old.reddit.com/user/bo_peng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt; &lt;img alt="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" src="https://a.thumbs.redditmedia.com/Yk1sCOddUTNzMn5fOag5QoE4yeshLWIZhFYJ14-t3u0.jpg" title="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321"&gt;https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now waiting for 4060ti 16G to arrive. Requires lots of custom code to efficiently utilize this chimera setup :) So stay tuned. I think it can reach 10+ token/s for quantized 671B after optimizations.&lt;/p&gt; &lt;p&gt;You can use &amp;quot;ASUS Hyper M.2 x16 Gen5 Card&amp;quot; to host 4 NVME. And currently you need AMD CPUs to do native x4x4x4x4 bifurcation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bo_peng"&gt; /u/bo_peng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T04:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqy5w</id>
    <title>Building an LLM-Optimized Linux Server on a Budget</title>
    <updated>2025-02-08T16:21:16+00:00</updated>
    <author>
      <name>/u/Unprotectedtxt</name>
      <uri>https://old.reddit.com/user/Unprotectedtxt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt; &lt;img alt="Building an LLM-Optimized Linux Server on a Budget" src="https://external-preview.redd.it/9ar9q08cZR3Pnd8Hxka6_rkxXup9YdrkJJw3Y1V7agI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7da33d2c9a8a6d7c6f95f5206eb1b4796158fe2f" title="Building an LLM-Optimized Linux Server on a Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on these benchmarks wouldn’t buying a Mac Studio with 128 GB RAM M2 Ultra 60 or 72 core be far better than traditional dedicated PC builds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unprotectedtxt"&gt; /u/Unprotectedtxt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxblog.io/build-llm-linux-server-on-budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik98l2</id>
    <title>I haven't seen many quad GPU setups so here is one</title>
    <updated>2025-02-07T23:39:23+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt; &lt;img alt="I haven't seen many quad GPU setups so here is one" src="https://preview.redd.it/lav3msh9zshe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a16836ce930a2602c2271a999be6be9f7ee678b" title="I haven't seen many quad GPU setups so here is one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lav3msh9zshe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iko5y3</id>
    <title>How do the models always answer in correct English when much of the web has badly written and incorrect English?</title>
    <updated>2025-02-08T14:15:17+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how the training works to achieve this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrf3</id>
    <title>OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp; also xAI | iLya &amp; elon both were OG cofounders of OpenAI</title>
    <updated>2025-02-08T09:40:24+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt; &lt;img alt="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" src="https://preview.redd.it/wojbv3ufyvhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672d5dd30929a3d9877d2f0489d521aeb4f36140" title="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wojbv3ufyvhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik76bj</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-02-07T22:08:04+00:00</updated>
    <author>
      <name>/u/Singularity-42</name>
      <uri>https://old.reddit.com/user/Singularity-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/juqrkouwishe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df7513b32ac2840f23b550e538e3f7b2c6c6ca3" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularity-42"&gt; /u/Singularity-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/juqrkouwishe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T22:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iks9cl</id>
    <title>Notes on OpenAI o3-mini: How good is it compared to r1 and o1?</title>
    <updated>2025-02-08T17:16:46+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We finally have a reasonable reasoning model from OpenAI that has a reasonable cost; it must be Deepseek r1 impact. But anyway, we now have the first family of models from the o3 series. Also, It is the first reasoning model with official function-calling support.&lt;/p&gt; &lt;p&gt;Another interesting thing is that, unlike o1, we can now see the chain of thought (CoT). However, the CoT is not raw like Deepseek r1, but only a summarized version of it, and I am not sure why they are still keeping it under wraps.&lt;/p&gt; &lt;h1&gt;On pricing&lt;/h1&gt; &lt;p&gt;Perhaps the most highlighting aspect of the model is that it’s 15x cheaper than O1 with comparable performance and, in fact, better at times.&lt;/p&gt; &lt;p&gt;The fact that it is cheaper by 2x than even the GPT-4o is even more amusing. Then why do Chatgpt users have limited queries while GPT-4o has unlimited queries?&lt;/p&gt; &lt;p&gt;Did Deepseek force OpenAI to subsidize API costs?&lt;/p&gt; &lt;h1&gt;On performance&lt;/h1&gt; &lt;p&gt;To know if it actually is a better model than r1 and o1, I tested it on my benchmark questions for reasoning, Math, Coding, etc.&lt;/p&gt; &lt;p&gt;Here’s my observation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O3-mini-high is the best available model for reasoning tasks, apart from o1-pro.&lt;/li&gt; &lt;li&gt;For math, o1 and o3-mini-high are on par, a tad bit better than Deepseek r1.&lt;/li&gt; &lt;li&gt;Again, for coding, o3-mini-high felt better in my use cases but can vary from case to case. It is faster, so it is better to work with.&lt;/li&gt; &lt;li&gt;I can’t get over Deepseek r1 for creative writing, well, especially its CoT traces. I wish OpenAI would disclose the raw CoT in the coming models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is actually good, and given the costs, it’s much better than o1. I would’ve loved if they showed us the actual CoT, and I think a lot of people are now more interested in thought patterns than actual responses.&lt;/p&gt; &lt;p&gt;For in-depth analysis, commentary, and remarks on the OpenAI o3-mini and comparison with Deepseek r1, check out this blog post: &lt;a href="https://composio.dev/blog/openai-o3-mini-vs-deepseek-r1/"&gt;On OpenAI o3-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what have been your views and experiences with the o3-mini. How did you like it compared to Deepseek r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikgsl6</id>
    <title>Germany: "We released model equivalent to R1 back in November, no reason to worry"</title>
    <updated>2025-02-08T06:14:46+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt; &lt;img alt="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" src="https://b.thumbs.redditmedia.com/mRAyAl4yIV32IK8PFM79kLhWfUBplZg1xnMZQcGaLLI.jpg" title="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ikgsl6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik162w</id>
    <title>Trump just said “no” DeepSeek does not pose a national security threat at a press conference</title>
    <updated>2025-02-07T17:57:11+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt; &lt;img alt="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" src="https://preview.redd.it/73sost17arhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d90041a44ec176a4b5a3630a47ce33577c957522" title="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73sost17arhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikguu9</id>
    <title>"Meta Torrented over 81 TB of Data Through Anna’s Archive, Despite Few Seeders"</title>
    <updated>2025-02-08T06:18:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn5fg</id>
    <title>Glyphstral-24b: Symbolic Deductive Reasoning Model</title>
    <updated>2025-02-08T13:22:41+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt; &lt;img alt="Glyphstral-24b: Symbolic Deductive Reasoning Model" src="https://external-preview.redd.it/DghrhJAW-NKneHTJvXZ7IAcBmIpZ_fU36ahUXITL0bM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad5bc295749b594e323b350912e29031d02d474" title="Glyphstral-24b: Symbolic Deductive Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone! &lt;/p&gt; &lt;p&gt;So I've been really obsessed lately with symbolic AI and the potential to improve reasoning and multi-dimensional thinking. I decided to go ahead and see if I could train a model to use a framework I am calling &amp;quot;Glyph Code Logic Flow&amp;quot;. &lt;/p&gt; &lt;p&gt;Essentially, it is a method of structured reasoning using deductive symbolic logic. You can learn more about it here &lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I first tried training Deepeek R1-Qwen-14 and QWQ-32 but their heavily pre-trained reasoning data seemed to conflict with my approach, which makes sense given the different concepts and ways of breaking down the problem.&lt;/p&gt; &lt;p&gt;I opted for Mistral-Small-24b to see the results, and after 7 days of pure training 24hrs a day (all locally using MLX-Dora at 4bit on my Mac M2 128GB). In all, the model trained on about 27mil tokens of my custom GCLF dataset (each example was around 30k tokens, with a total of 4500 examples)&lt;/p&gt; &lt;p&gt;I still need to get the docs and repo together, as I will be releasing it this weekend, but I felt like sharing a quick preview since this unexpectedly worked out awesomely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player"&gt;https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikh3vz</id>
    <title>OpenAI is hiding the actual thinking tokens in o3-mini</title>
    <updated>2025-02-08T06:35:07+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt; &lt;img alt="OpenAI is hiding the actual thinking tokens in o3-mini" src="https://external-preview.redd.it/7p74lVWOq7GrqAnrRqOdU-BQFqsEnbk8XozOhsD2YQc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65833b846107b44ca5029ac644441d85516e7126" title="OpenAI is hiding the actual thinking tokens in o3-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Yesterday I discovered that in fact you can not even with the current &amp;quot;thinking&amp;quot; tag on ChatGPT UI have the actual thinking tokens. In a way, the thinking of the model.&lt;/p&gt; &lt;p&gt;How did i know it? By running one single simple prompt about &amp;quot;maze puzzle&amp;quot; over the two models. The sample can be found in this dataset we built at our company. &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The prompt is a question about solving a maze: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 got it done in 483 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vc5mhbqxuhe1.png?width=1690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=611419e379856111070a3094ac3fbe7bac397381"&gt;Deepseek-R1 giving the correct result&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT 53 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yoxgrs11yuhe1.png?width=1706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f106a5f1553b8da728d9ca05dd32ce3702ecd5f"&gt;ChatGPT result also correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprisingly both results are correct but the devil is in the detail. When I inspect Deepseek-R1 thought process, it's very long, with step by step imagination and testing how it would go through the maze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 thinking tokens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/flt2emzlyuhe1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a16c88501392216f8998f3d97277996fac64a18"&gt;Deepseek-R1 thought process is literal pages and pages of thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT reason mode thinking tokens:&lt;/strong&gt;&lt;br /&gt; I will put the entire thinking tokens of ChatGPT here for everyone to inspect because it's much much shorter&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Let's break down the maze format. From what I'm seeing, the maze is made up of rows and columns, ranging from 0 to 4. Each cell is represented by its coordinates in the form &amp;lt;|row-col|&amp;gt;, followed by wall tokens like &amp;lt;|up_down_left_wall|&amp;gt; and &amp;lt;|blank|&amp;gt;. The walls specify which sides of each cell are blocked, and the origin and target tokens mark the start and end points. The way these tokens are structured seems important for determining the possible movements. Let’s work with this! Alright, I need to figure out where the origin and target are in the maze. I will look through the maze row by row to find them. Looking at row 1, it appears that the origin is at (1,3) and the target is at (1,2). We need to navigate from the origin (1,3) to the target (1,2). So now, it's just about predicting and outputting the individual moves one by one, separated by spaces. Let's figure out the path! I've figured out the maze's wall tokens, but before moving, I need to check if there's a direct path from the origin at (1,3) to the target at (1,2). The origin has walls on the up and left sides, according to the &amp;lt;|up_left_wall|&amp;gt; token. The target is to the left of the origin, and the cell next to the origin at (1,2) seems to have a wall on the right side, marked by &amp;lt;|down_right_wall|&amp;gt;. I’m thinking I’ll need to consider these walls carefully before predicting the next move. From what I’m seeing, the origin at (1,3) has walls on the left and up sides, so I can only move right or down. But moving left to reach the target is blocked because of the wall at (1,3). I checked the adjacency between (1,3) and (1,2), and it’s blocked by the walls in both cells. So, to get from (1,3) to (1,2), I'll need to find a detour. Let’s move down from (1,3) to (2,3) and explore. Okay, it looks like a direct route from (1,3) to (1,2) isn’t possible due to the walls blocking access. I checked various alternatives, including moving down and exploring different connections, but many of those led to dead ends. I've landed on a slightly longer route: get from (1,3) through (2,3) to (2,4), then to (3,4), and eventually backtracking westward while checking valid moves. After verifying the path through multiple neighbor checks, this looks like the best possible route. ANSWER: &amp;lt;|down|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|right|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Do you feel something is off about ChatGPT thought -&amp;gt; ANSWER?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very obvious that with the current thought process of ChatGPT reasoning model, &lt;strong&gt;it's glossing over so many details and somehow it can reach the result??&lt;/strong&gt; You basically cannot use the thinking tokens from ChatGPT to train a distilled thinking model because it is just not possible to reach the conclusion with those thinking, these are not actual thinking tokens of the models.&lt;/p&gt; &lt;p&gt;I have some hypothesis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI is only providing &amp;quot;summarized&amp;quot; version of their thinking tokens.&lt;/li&gt; &lt;li&gt;OpenAI has a model to output bullshit thinking tokens to distract everyone from training a distill model, so they can say they provide the community with the actual data, but there is no data.&lt;/li&gt; &lt;li&gt;They don't have &amp;quot;readable&amp;quot; thinking model but it's just an approximation of the &amp;quot;latent&amp;quot; thinking tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With the track record of OpenAI and ChatGPT, I am leaning towards &amp;quot;they are summarize or give bullshit thinking tokens&amp;quot; to the users more than they have more advanced model as option 3. Why? Because when I look at the UI it's obvious that the thought process is not outputting token per token but in chunk, which is either a summary, or a totally different model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;br /&gt; You can't just distill openAI model anymore, so don't assume everyone is distilling their model, &lt;strong&gt;THEY ARE CLOSED AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The full logs of both answers from ChatGPT and Deepseek-R1 can be found here:&lt;/strong&gt; &lt;a href="https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed"&gt;https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The maze dataset we build can be found here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikp5ko</id>
    <title>GeForce RTX 5090 fails to topple RTX 4090 in GPU compute benchmark.</title>
    <updated>2025-02-08T15:02:14+00:00</updated>
    <author>
      <name>/u/el0_0le</name>
      <uri>https://old.reddit.com/user/el0_0le</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So uh. Anyone have a good reason to upgrade from 4090 to 5090? &lt;/p&gt; &lt;p&gt;VRAM? Power? Paper specs? Future updates? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el0_0le"&gt; /u/el0_0le &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GeForce-RTX-5090-fails-to-topple-RTX-4090-in-GPU-compute-benchmark-while-RTX-5080-struggles-against-RTX-4070-Ti.958334.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:02:14+00:00</published>
  </entry>
</feed>
