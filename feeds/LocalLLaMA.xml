<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-26T05:24:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mzsg6v</id>
    <title>DeepSeek V3.1 - Getting token " extreme" / "ÊûÅ" / "Ê•µ" out of nowhere</title>
    <updated>2025-08-25T14:46:02+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with DeepSeek V3.1, and found that somehow the model likes to generate the token:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot; extreme&amp;quot; (id:15075)&lt;/li&gt; &lt;li&gt;&amp;quot;ÊûÅ&amp;quot; (id:2577, extreme in Simplified Chinese)&lt;/li&gt; &lt;li&gt;&amp;quot;Ê•µ&amp;quot; (id:16411, extreme in Traditional Chinese)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in totally unexpected places.&lt;/p&gt; &lt;p&gt;At first I thought it was due to the extreme IQ1_S quantization that I did or some edge case with imatrix calibration dataset, but then the same issue also happened with the FP8 full precision model from Fireworks.&lt;/p&gt; &lt;p&gt;Case 1 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.SeÊûÅ&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;ÊûÅ&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.3718461990356445 }, { &amp;quot;id&amp;quot;: 1511, &amp;quot;token&amp;quot;: &amp;quot;cond&amp;quot;, &amp;quot;bytes&amp;quot;: [99,111,110,100], &amp;quot;logprob&amp;quot;: -1.5412302017211914 }, { &amp;quot;id&amp;quot;: 1957, &amp;quot;token&amp;quot;: &amp;quot; second&amp;quot;, &amp;quot;bytes&amp;quot;: [32,115,101,99,111,110,100], &amp;quot;logprob&amp;quot;: -1.9008493423461914 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 2 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: &lt;a href="http://time.Se"&gt;time.Se&lt;/a&gt; extreme&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 15075, &amp;quot;token&amp;quot;: &amp;quot; extreme&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109,101], &amp;quot;logprob&amp;quot;: -1.0279325246810913 }, { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;ÊûÅ&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.077283263206482 }, { &amp;quot;id&amp;quot;: 9189, &amp;quot;token&amp;quot;: &amp;quot; extrem&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109], &amp;quot;logprob&amp;quot;: -1.8691496849060059 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 3 (fireworks, top_k=1, temperature=1):&lt;br /&gt; Expected: V1&lt;br /&gt; Generated: VÊûÅ&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;ÊûÅ&amp;quot;, &amp;quot;logprob&amp;quot;: -0.27936283, &amp;quot;token_id&amp;quot;: 2577, &amp;quot;bytes&amp;quot;: [230,158,129] }, { &amp;quot;token&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;logprob&amp;quot;: -1.90436232, &amp;quot;token_id&amp;quot;: 19, &amp;quot;bytes&amp;quot;: [49] }, { &amp;quot;token&amp;quot;: &amp;quot;Ê•µ&amp;quot;, &amp;quot;logprob&amp;quot;: -2.40436196, &amp;quot;token_id&amp;quot;: 16411, &amp;quot;bytes&amp;quot;: [230,165,181] } ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Worse still, other than these 3 cases where an extreme token was the top choice in greedy decoding, these extreme tokens are also constantly lurking as the 2nd or 3rd choice in other unexpected places as well.&lt;/p&gt; &lt;p&gt;I have done this exact eval for all the popular coding models, and this is the first time I am seeing this kind of issue. Has anyone experienced this?&lt;/p&gt; &lt;p&gt;EDIT: Seeing the same issue with Novita as well, so it is quite unlikely to be an issue with the inference stack.&lt;/p&gt; &lt;p&gt;EDIT 2: Current suspect is that the issue might be masked by MTP, and becomes a lot more apparent when the inference stack doesn't support MTP.&lt;/p&gt; &lt;p&gt;EDIT 3: Okay this is worse than I thought. I checked the old eval responses for &amp;quot;ÊûÅ&amp;quot;, and can see that DeepSeek V3 0324 already got the token as the 2nd or 3rd choice in many unexpected places. Sadly, the recent Qwen3 235B A22B Instruct 2507 and Qwen3 Coder 30B A3B Instruct also show the same symptom, probably at around the same stage as DeepSeek V3 0324. Meanwhile, Qwen3 Coder 480B A35B Instruct only shows the same symptom after heavily quantized. Looks like the 2 labs may use the same contaminated data. GLM 4.5 is not affected üëå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_ÊûÅ_Ê•µ_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_ÊûÅ_Ê•µ_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_ÊûÅ_Ê•µ_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzxmdg</id>
    <title>Local LLM-powered retro game builder (Lemonade Arcade)</title>
    <updated>2025-08-25T17:55:02+00:00</updated>
    <author>
      <name>/u/vgodsoe-amd</name>
      <uri>https://old.reddit.com/user/vgodsoe-amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt; &lt;img alt="Local LLM-powered retro game builder (Lemonade Arcade)" src="https://external-preview.redd.it/dm11dnNlbXplN2xmMfWDxdpPNldKtz87A5VyOKcGjTTlEA-df_2Lj-7xBpjN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eb379b4a343e9b82de0ce6d68492f198e8fbc96" title="Local LLM-powered retro game builder (Lemonade Arcade)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I'd share something that came out of a side project from one of my teammates. I‚Äôve been playing around with the app, Lemonade Arcade, that uses a local LLM (Qwen3-Coder-30B) to generate PyGames. In a couple of minutes, it builds retro-style games like Snake, Pong, Asteroids, Pac-Man, etc., from your inputted prompt. I also remixed some games to see what else I could get.&lt;/p&gt; &lt;p&gt;I was using a Ryzen AI 395 (Strix Halo) PC, but any machine with 32 GB RAM or 16 GB VRAM should be fine. It works on both Windows and Linux.&lt;/p&gt; &lt;p&gt;Curious what kinds of games people would come up with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vgodsoe-amd"&gt; /u/vgodsoe-amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6sy4a6nze7lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0cw80</id>
    <title>Anyone interested in talking on a podcast episode about what you're doing with LLMs?</title>
    <updated>2025-08-26T04:54:03+00:00</updated>
    <author>
      <name>/u/riv3r1andstr3ams</name>
      <uri>https://old.reddit.com/user/riv3r1andstr3ams</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0cw80/anyone_interested_in_talking_on_a_podcast_episode/"&gt; &lt;img alt="Anyone interested in talking on a podcast episode about what you're doing with LLMs?" src="https://external-preview.redd.it/d8HE-0jL1fa7ELnAZFTPI51hnazQ1xlcVq4q81eVBIE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a5e0c7b9ccafbd242dc5bc3e1396a33b13f159d" title="Anyone interested in talking on a podcast episode about what you're doing with LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to talk to you for an episode of a new podcast I started called Wild West Ai. dm me! or email at [&lt;a href="mailto:thewildwestai@gmail.com"&gt;thewildwestai@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:thewildwestai@gmail.com"&gt;thewildwestai@gmail.com&lt;/a&gt;) Here's the first short episode just getting things started. I want to have conversations with anyone and everyone interested or working with LLMs, and ai more broadly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riv3r1andstr3ams"&gt; /u/riv3r1andstr3ams &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.spotify.com/show/1Ce1P7ENIwn3RQmym2FuA9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0cw80/anyone_interested_in_talking_on_a_podcast_episode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0cw80/anyone_interested_in_talking_on_a_podcast_episode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T04:54:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04qry</id>
    <title>OpenWebTTS: Open-Source Speechify/ElevenLabs Alternative looking for contributors</title>
    <updated>2025-08-25T22:27:25+00:00</updated>
    <author>
      <name>/u/Material_Abies2307</name>
      <uri>https://old.reddit.com/user/Material_Abies2307</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm working on a new open-source project called &lt;strong&gt;OpenWebTTS&lt;/strong&gt;, and I'm looking for contributors who might be interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is OpenWebTTS?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea is to make an open-source alternative to popular text-to-speech platforms like Speechify and ElevenLabs. The goal is to create a free and customizable TTS tool that facilitates reading articles, texts and books using local models or API-friendly TTS, while making sure the UX is up to standard to modern TTS platforms. Right now, the codebase is relatively simple but already &lt;strong&gt;100% usable&lt;/strong&gt; with support for Piper and Kokoro as well as PDF and Epub parsing. We are using &lt;strong&gt;Python&lt;/strong&gt; for the backend and &lt;strong&gt;JavaScript&lt;/strong&gt; for the frontend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How can you contribute?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Any help is welcome:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt; (Python, JavaScript).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI/UX&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ideas and feedback&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're interested in contributing, please &lt;a href="https://github.com/Gyyyn/OpenWebTTS"&gt;check out the project&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Abies2307"&gt; /u/Material_Abies2307 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06z1t</id>
    <title>Running LLMs &amp; Multimodal models on Qualcomm Snapdragon NPU</title>
    <updated>2025-08-26T00:03:21+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been working on &lt;strong&gt;nexaSDK&lt;/strong&gt; ‚Äî a lightweight runtime that runs &lt;strong&gt;latest LLMs and multimodal models&lt;/strong&gt; directly on &lt;strong&gt;Qualcomm Snapdragon NPUs&lt;/strong&gt;. It supports pure NPU inference: faster, leaner, battery-friendly. The developer experience feels like &lt;strong&gt;Ollama on NPUs&lt;/strong&gt;, but with full multimodal support (text, image, audio) and extra performance optimizations.&lt;/p&gt; &lt;h1&gt;Key results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; &amp;gt;95% NPU usage, ~25% faster than Qualcomm GENIE (23 t/s on OmniNeural-4B vs. 18 t/s on Llama-3.2-3B by GENIE).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal &amp;amp; multiround:&lt;/strong&gt; conversational multi-image + multi-audio supported natively.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; NexaQuant cuts perplexity by ~10% vs. baseline.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Support latest SOTA models:&lt;/strong&gt; OmniNeural, Qwen3, YOLOv12, PaddleOCR v4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer UX:&lt;/strong&gt; Ollama-style install, 1 line to run ‚Üí &lt;code&gt;nexa infer omni-neural&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 2√ó longer context windows, JSON structured decoding for agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This makes &lt;strong&gt;local copilots, private mobile assistants, in-car copilots, and edge OCR/speech agents&lt;/strong&gt; actually practical on CPU/GPU limited or battery limited devices.&lt;/p&gt; &lt;h1&gt;Demos:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Multimodal (OmniNeural-4B) ‚Üí &lt;a href="https://huggingface.co/NexaAI/OmniNeural-4B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;We also helped Qwen team to bring their latest models to NPU:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Qwen3-2507 on NPU ‚Üí &lt;a href="https://x.com/nexa_ai/status/1959302777353736593"&gt;demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3 on cars + IoT ‚Üí &lt;a href="https://x.com/nexa_ai/status/1958797993676783792"&gt;demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub ‚Üí &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;nexaSDK&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Supported models ‚Üí &lt;a href="https://sdk.nexa.ai/model"&gt;Model Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôd love feedback, critiques, and ideas. Curious to hear from this community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which other models should we prioritize for NPU support?&lt;/li&gt; &lt;li&gt;Are there workloads that still make more sense on GPU/CPU?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvk44</id>
    <title>Codebase to Knowledge Graph generator</title>
    <updated>2025-08-25T16:39:59+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/aXlnMWRvdXExN2xmMW6IHesd2IpIEgbCcYmw7k3fEr5nk2vPdZm2_jU5G_lC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf22eb683182f2e28b2652a8c7fac245c1add93" title="Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-based chatbot. It runs entirely client-side in the browser, making it privacy-focused. I‚Äôm using &lt;strong&gt;tree-sitter.wasm&lt;/strong&gt; to parse code inside the browser and logic to use the generated AST to map out all relations. Now trying to optimize it through parallel processing with Web Workers, worker pool. For the in-memory graph database, I‚Äôm using &lt;strong&gt;KuzuDB&lt;/strong&gt;, which also runs through WebAssembly (&lt;strong&gt;kuzu.wasm&lt;/strong&gt;). Graph RAG chatbot uses langchains ReAct agent, generating cypher queries to get information.&lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Need advice from anyone who has experience in graph rag agents, will this be better than rag based grep features which is popular in all AI IDEs.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gix425uq17lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06iyh</id>
    <title>llama.cpp Lazy Swap</title>
    <updated>2025-08-25T23:43:53+00:00</updated>
    <author>
      <name>/u/unrulywind</name>
      <uri>https://old.reddit.com/user/unrulywind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because I'm totally lazy and I hate typing. I usually us a wrapper to run local models. But, recently I had to set up llama.cpp directly and, of course, being the lazy person I am, I created a bunch of command strings that I saved in a text file that I could copy into the terminal for each model.&lt;/p&gt; &lt;p&gt;Then I thought.... why am I doing this when I could make an old fashioned script menu. At that moment I realized, I never saw anyone post one. Maybe it's just too simple so everyone just made one eventually. Well, I thought, if I'm gonna write it, I might as well post it. So, here it is. All written up a a script creation script. part mine, but prettied up compliments of some help from gpt-oss-120b. The models used as examples are my setups for a 5090.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;üì¶ Full checklist ‚Äì copy‚Äëpaste this to get a working launcher&lt;/h2&gt; &lt;p&gt;This is a one time set up and creates a command: l-server 1. Copy entire script to clipboard 2. Open terminal inside WSL2 3. Right click to paste, or ctrl-v 4. Hit enter 5. Choose server 6. done 7. ctrl-c to stop server 8. It recycles to the menu, hit return to pull up the list again 9. To edit models edit the file in a Linux file editor or vscode ```&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;1Ô∏è‚É£ Make sure a place for personal scripts exists and is in $PATH&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;mkdir -p ~/bin&lt;/p&gt; &lt;h1&gt;If ~/bin is not yet in PATH, add it:&lt;/h1&gt; &lt;p&gt;if [[ &amp;quot;:$PATH:&amp;quot; != &lt;em&gt;&amp;quot;:$HOME/bin:&amp;quot;&lt;/em&gt; ]]; then echo 'export PATH=&amp;quot;$HOME/bin:$PATH&amp;quot;' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc fi&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;2Ô∏è‚É£ Write the script (the &amp;lt;&amp;lt;'EOF' ‚Ä¶ EOF trick writes the exact text)&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;cat &amp;gt; ~/bin/l-server &amp;lt;&amp;lt;'EOF'&lt;/p&gt; &lt;h1&gt;!/usr/bin/env bash&lt;/h1&gt; &lt;h1&gt;------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;l-server ‚Äì launcher for llama-server configurations&lt;/h1&gt; &lt;h1&gt;------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;cd ~/llama.cpp || { echo &amp;quot;‚ùå Could not cd to ~/llama.cpp&amp;quot;; exit 1; }&lt;/p&gt; &lt;p&gt;options=( &amp;quot;GPT‚ÄëOSS‚ÄëMXFP4‚Äë20b server&amp;quot; &amp;quot;GPT‚ÄëOSS‚ÄëMXFPp4‚Äë120b with moe offload&amp;quot; &amp;quot;GLM‚Äë4.5‚ÄëAir_IQ4_XS&amp;quot; &amp;quot;Gemma‚Äë3‚Äë27b&amp;quot; &amp;quot;Quit&amp;quot; )&lt;/p&gt; &lt;p&gt;commands=( &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gpt-oss-20b-MXFP4.gguf \ -c 131072 \ -ub 2048 -b 4096 \ -ngl 99 -fa \ --jinja&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gpt-oss-120b-MXFP4-00001-of-00002.gguf \ -c 65536 \ -ub 2048 -b 2048 \ -ngl 99 -fa \ --jinja \ --n-cpu-moe 24&amp;quot; &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf \ -c 65536 \ -ub 2048 -b 2048 \ -ctk q8_0 -ctv q8_0 \ -ngl 99 -fa \ --jinja \ --n-cpu-moe 33&amp;quot; &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gemma-3-27B-it-QAT-Q4_0.gguf \ -c 65536 \ -ub 2048 -b 4096 \ -ctk q8_0 -ctv q8_0 \ -ngl 99 -fa \ --mmproj ~/models/mmproj-model-f16.gguf \ --no-mmproj-offload&amp;quot; &amp;quot;&amp;quot; # placeholder for Quit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;PS3=$'\nSelect a server (1‚Äë'${#options[@]}'): ' select choice in &amp;quot;${options[@]}&amp;quot;; do [[ -z $choice ]] &amp;amp;&amp;amp; { echo &amp;quot;‚ùå Invalid selection ‚Äì try again.&amp;quot;; continue; } idx=$(( REPLY - 1 )) [[ &amp;quot;$choice&amp;quot; == &amp;quot;Quit&amp;quot; || $REPLY -eq 0 ]] &amp;amp;&amp;amp; { echo &amp;quot;üëã Bye.&amp;quot;; break; }&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmd=&amp;quot;${commands[$idx]}&amp;quot; echo -e &amp;quot;\nüöÄ Starting \&amp;quot;$choice\&amp;quot; ‚Ä¶&amp;quot; echo &amp;quot; $cmd&amp;quot; echo &amp;quot;-----------------------------------------------------&amp;quot; eval &amp;quot;$cmd&amp;quot; echo -e &amp;quot;\n--- finished ---\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;done EOF&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;3Ô∏è‚É£ Make it executable&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;chmod +x ~/bin/l-server&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;4Ô∏è‚É£ Test it&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;l-server # should bring up the menu ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unrulywind"&gt; /u/unrulywind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0c58h</id>
    <title>Update llama.cpp for a big speed boost with gpt-oss and cuda.</title>
    <updated>2025-08-26T04:11:45+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few cuda commits landed today that have made a big difference in performance. Testing with gpt-oss-120B, I saw a 14.5% increase in tokens per second with 2x3090 and 1xP40. It went from 51.6 tok/sec to 59.1 tok/sec. &lt;/p&gt; &lt;p&gt;With gptoss-20B I stayed at 130 tok/sec on a single 3090 power limited to 300W. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T04:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06ahz</id>
    <title>InternVL3_5 GGUF here</title>
    <updated>2025-08-25T23:33:37+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt; &lt;img alt="InternVL3_5 GGUF here" src="https://b.thumbs.redditmedia.com/EdxxIyVz6m8Y6DmR8UexDPcp-CYBhmw8ygu2dfJ8LCY.jpg" title="InternVL3_5 GGUF here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tested the &lt;a href="https://huggingface.co/collections/QuantStack/internvl3-5-ggufs-68acef206837c4f661a9b0a5"&gt;InternVL3_5&lt;/a&gt; 1b fp16 GGUF, it works&lt;br /&gt; (that's means the model architect is supported now in llama.cpp, I tested on LM studio) &lt;/p&gt; &lt;p&gt;every models now, just fp16,&lt;br /&gt; I think the QuantStack team is quantizing to different quants,&lt;br /&gt; if you want a quick try, just like and watch this repo, you may get surprised in few hours&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n06ahz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzykeu</id>
    <title>DeepSeek 3.1 Update is Awesome!</title>
    <updated>2025-08-25T18:29:57+00:00</updated>
    <author>
      <name>/u/lovetootiesteele</name>
      <uri>https://old.reddit.com/user/lovetootiesteele</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who countless conversations interrupted due to length limits, the ability to re-visit those chats and pick-up where we left off has been a dream come true. Even though we would try to continue our projects in new chats, the foundation had been set in another. This update is awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lovetootiesteele"&gt; /u/lovetootiesteele &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bbnx</id>
    <title>Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it.</title>
    <updated>2025-08-26T03:28:31+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt; &lt;img alt="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." src="https://external-preview.redd.it/IQUYk0UKH8LL_chH4rY6LMG-79G-lG-wrUjPogrDpXU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec62af39d7821e4b67b4c3bcde089ba116620d78" title="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to create a creative writing fine tune, but I've never done it and I don't know if to 3090's are up for the task for this model. So, I thought I'd start with inspiring those who have to take a look at this model if they missed it. While Seed-OSS was developed by ByteDance's Seed Team for reasoning and agent type stuff, it also has general capabilities and powerful long-context features. Also, this base doesn't have synthetic data in it. All sounds promising. What do you think? Does it have a shot at being a good base for creative models? Anyone attempting anything? Anyone up for helping me head down the fine tuning road? Is it even possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base-woSyn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with a training approach I‚Äôm calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both ‚Äúgood‚Äù and ‚Äúbad‚Äù completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on ‚Äúconflict tokens.‚Äù&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn‚Äôt wreck formatting.&lt;/li&gt; &lt;li&gt;I didn‚Äôt need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm curious what others think, especially folks who‚Äôve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you‚Äôd like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n02e7s</id>
    <title>How does huggingface make money?</title>
    <updated>2025-08-25T20:55:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I sure download from it a lot. What‚Äôs their way to bring profitably safe from shenanigans? Will it be stuff like GitHub? What‚Äôs the backup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T20:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0aijh</id>
    <title>GPT OSS 120B</title>
    <updated>2025-08-26T02:47:05+00:00</updated>
    <author>
      <name>/u/vinigrae</name>
      <uri>https://old.reddit.com/user/vinigrae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the best function calling model I‚Äôve used, don‚Äôt think twice, just use it. &lt;/p&gt; &lt;p&gt;We gave it a multi scenario difficulty 300 tool call test, where even 4o and GPT 5 mini performed poorly. &lt;/p&gt; &lt;p&gt;Ensure you format the system properly for it, you will find the model won‚Äôt even execute things that are actually done in a faulty manner and are detrimental to the pipeline.&lt;/p&gt; &lt;p&gt;I‚Äôm &lt;strong&gt;extremely&lt;/strong&gt; impressed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinigrae"&gt; /u/vinigrae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0am1b</id>
    <title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title>
    <updated>2025-08-26T02:52:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;https://arxiv.org/abs/2508.18265v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05√ó inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks‚Äînarrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Models:&lt;/p&gt; &lt;p&gt;1B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-2B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-4B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-14B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;38B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-38B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-38B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;20BA4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30BA3B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;241BA28B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers&amp;quot;&lt;/li&gt; &lt;li&gt;Based on Qwen2.5-1.5B&lt;/li&gt; &lt;li&gt;7B variant &amp;quot;coming soon&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bhd7</id>
    <title>Microsoft VibeVoice TTS : Open-Sourced, Supports 90 minutes speech, 4 distinct speakers at a time</title>
    <updated>2025-08-26T03:36:48+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. &lt;/p&gt; &lt;p&gt;Demo Video : &lt;a href="https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ"&gt;https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
