<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-09T11:05:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1juxri4</id>
    <title>Is there a guaranteed way to keep models follow specific formatting guidelines, without breaking completely?</title>
    <updated>2025-04-09T04:48:54+00:00</updated>
    <author>
      <name>/u/Pomegranate-Junior</name>
      <uri>https://old.reddit.com/user/Pomegranate-Junior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm using several different models, mostly using APIs because my little 2060 was made for space engineers, not LLMs.&lt;/p&gt; &lt;p&gt;One thing that's common (in my experience) in most of the models is how the formatting breaks.&lt;/p&gt; &lt;p&gt;So what I like, for example:&lt;/p&gt; &lt;p&gt;&amp;quot;What time is it?&amp;quot; *I asked, looking at him like a moron that couldn't figure out the clock without glasses.*&lt;br /&gt; &amp;quot;Idk, like 4:30... I'm blind, remember?&amp;quot; *he said, looking at a pole instead of me.*&lt;/p&gt; &lt;p&gt;aka, &amp;quot;speech like this&amp;quot; *narration like that*.&lt;/p&gt; &lt;p&gt;What I experience often is that they mess up the *narration part*, like a lot. So using the example above, I get responses like this:&lt;/p&gt; &lt;p&gt;&amp;quot;What time is it?&amp;quot; *I asked,* looking at him* like a moron that couldn't figure out the clock without glasses.*&lt;br /&gt; *&amp;quot;Idk, like 4:30... I'm blind, remember?&amp;quot; he said, looking at a pole instead of me.&lt;/p&gt; &lt;p&gt;(there's 2 in between, and one is on the wrong side of the space, meaning the * is even visible in the response, and the next line doesn't have it at all, just at the very start of the row.)&lt;/p&gt; &lt;p&gt;I see many people just use &amp;quot;this for speech&amp;quot; and then nothing for narration and whatever, but I'm too used to doing *narration like this*, and sure, regenerating text like 4 times is alright, but doing it 14 times, or non-stop going back and forth editing the responses myself to fit the formatting is just immersion breaking.&lt;/p&gt; &lt;p&gt;so TL;DR:&lt;/p&gt; &lt;p&gt;Is there a guaranteed way to keep models follow specific formatting guidelines, without breaking completely? (breaking completely means sending walls of text with messed up formatting and ZERO separation into paragraphs) (I hope I'm making sense here, its early)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pomegranate-Junior"&gt; /u/Pomegranate-Junior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juxri4/is_there_a_guaranteed_way_to_keep_models_follow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juxri4/is_there_a_guaranteed_way_to_keep_models_follow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juxri4/is_there_a_guaranteed_way_to_keep_models_follow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T04:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1juw2vy</id>
    <title>Last chance to buy a Mac studio?</title>
    <updated>2025-04-09T03:10:30+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering all the crazy tariff war stuff, should I get a Mac Studio right now before Apple skyrockets the price?&lt;/p&gt; &lt;p&gt;I'm looking at the M3 Ultra with 256GB, since the prompt processing speed is too slow for large models like DS v3, but idk if that will change in the future &lt;/p&gt; &lt;p&gt;Right now, all I have for local inference is a single 4090, so the largest model I can run is 32B Q4.&lt;/p&gt; &lt;p&gt;What's your experience with M3 Ultra, do you think it's worth it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juw2vy/last_chance_to_buy_a_mac_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juw2vy/last_chance_to_buy_a_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juw2vy/last_chance_to_buy_a_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T03:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jumtgd</id>
    <title>Why aren't the smaller Gemma 3 models on LMArena?</title>
    <updated>2025-04-08T19:52:35+00:00</updated>
    <author>
      <name>/u/Thatisverytrue54321</name>
      <uri>https://old.reddit.com/user/Thatisverytrue54321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been waiting to see how people rank them since they've come out. It's just kind of strange to me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thatisverytrue54321"&gt; /u/Thatisverytrue54321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jumtgd/why_arent_the_smaller_gemma_3_models_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jumtgd/why_arent_the_smaller_gemma_3_models_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jumtgd/why_arent_the_smaller_gemma_3_models_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T19:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jupvai</id>
    <title>QwQ 32B thinking chunk removal in llama.cpp</title>
    <updated>2025-04-08T22:00:56+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the QwQ 32B HF &lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;page&lt;/a&gt; I see that they specify the following:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;No Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. This feature is already implemented in apply_chat_template.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Is this implemented in llama.cpp or Ollama? Is it enabled by default?&lt;/p&gt; &lt;p&gt;I also have the same doubt on this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Enforce Thoughtful Output: Ensure the model starts with &amp;quot;&amp;lt;think&amp;gt;\n&amp;quot; to prevent generating empty thinking content, which can degrade output quality. If you use apply_chat_template and set add_generation_prompt=True, this is already automatically implemented, but it may cause the response to lack the &amp;lt;think&amp;gt; tag at the beginning. This is normal behavior.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jupvai/qwq_32b_thinking_chunk_removal_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jupvai/qwq_32b_thinking_chunk_removal_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jupvai/qwq_32b_thinking_chunk_removal_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T22:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1juc553</id>
    <title>Ollama now supports Mistral Small 3.1 with vision</title>
    <updated>2025-04-08T12:18:04+00:00</updated>
    <author>
      <name>/u/markole</name>
      <uri>https://old.reddit.com/user/markole</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"&gt; &lt;img alt="Ollama now supports Mistral Small 3.1 with vision" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Ollama now supports Mistral Small 3.1 with vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/markole"&gt; /u/markole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/mistral-small3.1:24b-instruct-2503-q4_K_M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T12:18:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv1k8i</id>
    <title>Android app that works with LLM APIs and includes voice as an input</title>
    <updated>2025-04-09T09:23:47+00:00</updated>
    <author>
      <name>/u/DrKrepz</name>
      <uri>https://old.reddit.com/user/DrKrepz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know of a way to achieve this? I like using ChatGPT to organise my thoughts by speaking into it and submitting as text. However, I hate OpenAI and would really like to find a way to use open source models, such as via the Lambda Inference API, with a UX that is similar to how I currently use ChatGPT.&lt;/p&gt; &lt;p&gt;Any suggestions would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrKrepz"&gt; /u/DrKrepz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1k8i/android_app_that_works_with_llm_apis_and_includes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1k8i/android_app_that_works_with_llm_apis_and_includes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1k8i/android_app_that_works_with_llm_apis_and_includes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T09:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jugmxm</id>
    <title>Artificial Analysis Updates Llama-4 Maverick and Scout Ratings</title>
    <updated>2025-04-08T15:40:23+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"&gt; &lt;img alt="Artificial Analysis Updates Llama-4 Maverick and Scout Ratings" src="https://preview.redd.it/jqc8govgsmte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa3fa84066ed377a3f048d4e3f324774d9cb188" title="Artificial Analysis Updates Llama-4 Maverick and Scout Ratings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jqc8govgsmte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T15:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv1o8o</id>
    <title>What are y'alls opinion about the differences in "personality" in LLMs?</title>
    <updated>2025-04-09T09:31:39+00:00</updated>
    <author>
      <name>/u/Cubow</name>
      <uri>https://old.reddit.com/user/Cubow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over time of working with a few LLMs (mainly the big ones like Gemini, Claude, ChatGPT and Grok) to help me study for exams, learn about certain topics or just coding, I've noticed that they all have a very distinct personality and it actually impacts my preference for which one I want to use quite a lot.&lt;/p&gt; &lt;p&gt;To give an example, personally Claude feels the most like it just &amp;quot;gets&amp;quot; me, it knows when to stay concise, when to elaborate or when to ask follow up questions. Gemini on the other hand tends to yap a lot and in longer conversations even tends to lose its cool a bit, starting to write progressively more in caps, bolded or cursive text until it just starts all out tweaking. ChatGPT seems like it has the most &amp;quot;clean&amp;quot; personality, it's generally quite formal and concise. And last, but not least Grok seems somewhat similar to Claude, it doesn't &lt;em&gt;quite&lt;/em&gt; get me as much (I would say its like 90% there), but its the one I actually tend to use the most, since Claude has a very annoying rate limit.&lt;/p&gt; &lt;p&gt;Now I am curious, what do you all think about the different &amp;quot;personalities&amp;quot; of all the LLMs you've used, what kind of style do you prefer and how does it impact your choice of which one you actually use the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cubow"&gt; /u/Cubow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1o8o/what_are_yalls_opinion_about_the_differences_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1o8o/what_are_yalls_opinion_about_the_differences_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv1o8o/what_are_yalls_opinion_about_the_differences_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T09:31:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jui6wd</id>
    <title>What is everyone's top local llm ui (April 2025)</title>
    <updated>2025-04-08T16:44:41+00:00</updated>
    <author>
      <name>/u/Full_You_8700</name>
      <uri>https://old.reddit.com/user/Full_You_8700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying to keep up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_You_8700"&gt; /u/Full_You_8700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T16:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1juqwmd</id>
    <title>TTS: Index-tts: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
    <updated>2025-04-08T22:48:14+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IndexTTS is a GPT-style text-to-speech (TTS) model mainly based on XTTS and Tortoise. It is capable of correcting the pronunciation of Chinese characters using pinyin and controlling pauses at any position through punctuation marks. We enhanced multiple modules of the system, including the improvement of speaker condition feature representation, and the integration of BigVGAN2 to optimize audio quality. Trained on tens of thousands of hours of data, our system achieves state-of-the-art performance, outperforming current popular TTS systems such as XTTS, CosyVoice2, Fish-Speech, and F5-TTS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/index-tts/index-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juqwmd/tts_indextts_an_industriallevel_controllable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juqwmd/tts_indextts_an_industriallevel_controllable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T22:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv14ff</id>
    <title>Anyone use a local model for rust coding?</title>
    <updated>2025-04-09T08:50:10+00:00</updated>
    <author>
      <name>/u/OnceMoreOntoTheBrie</name>
      <uri>https://old.reddit.com/user/OnceMoreOntoTheBrie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen language specific benchmarks so I was wondering if anyone has experience in using llms for rust coding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OnceMoreOntoTheBrie"&gt; /u/OnceMoreOntoTheBrie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv14ff/anyone_use_a_local_model_for_rust_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv14ff/anyone_use_a_local_model_for_rust_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv14ff/anyone_use_a_local_model_for_rust_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T08:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jujc9p</id>
    <title>Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix</title>
    <updated>2025-04-08T17:31:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt; &lt;img alt="Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix" src="https://external-preview.redd.it/G6CWdjW0ZI0mpPw5AO6U1jpVZ_0ooqTiGOkMmNur-z4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c156c4195f362fe9292a1565450a79a881b7a78c" title="Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f46sokm6bmte1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc6fb68378cc9c78888ad6694c3f8f4c2fdc6768"&gt;Open WebUI running with Ryzen AI hardware acceleration.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi, I'm Jeremy from AMD, here to share my team’s work to see if anyone here is interested in using it and get their feedback!&lt;/p&gt; &lt;p&gt;🍋Lemonade Server is an OpenAI-compatible local LLM server that offers NPU acceleration on AMD’s latest Ryzen AI PCs (aka Strix Point, Ryzen AI 300-series; requires Windows 11).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (Apache 2 license): &lt;a href="https://github.com/onnx/turnkeyml"&gt;onnx/turnkeyml: Local LLM Server with NPU Acceleration&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Releases page with GUI installer: &lt;a href="https://github.com/onnx/turnkeyml/releases"&gt;Releases · onnx/turnkeyml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The NPU helps you get faster prompt processing (time to first token) and then hands off the token generation to the processor’s integrated GPU. Technically, 🍋Lemonade Server will run in CPU-only mode on any x86 PC (Windows or Linux), but our focus right now is on Windows 11 Strix PCs.&lt;/p&gt; &lt;p&gt;We’ve been daily driving 🍋Lemonade Server with Open WebUI, and also trying it out with Continue.dev, CodeGPT, and Microsoft AI Toolkit.&lt;/p&gt; &lt;p&gt;We started this project because Ryzen AI Software is in the ONNX ecosystem, and we wanted to add some of the nice things from the llama.cpp ecosystem (such as this local server, benchmarking/accuracy CLI, and a Python API).&lt;/p&gt; &lt;p&gt;Lemonde Server is still in its early days, but we think now it's robust enough for people to start playing with and developing against. Thanks in advance for your constructive feedback! Especially about how the Sever endpoints and installer could improve, or what apps you would like to see tutorials for in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T17:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1julc3c</id>
    <title>Well llama 4 is facing so many defeats again such low score on arc agi</title>
    <updated>2025-04-08T18:50:36+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"&gt; &lt;img alt="Well llama 4 is facing so many defeats again such low score on arc agi" src="https://preview.redd.it/espl4stfqnte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0f715cad13717f261670e6b5e2abe772afa2aed" title="Well llama 4 is facing so many defeats again such low score on arc agi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/espl4stfqnte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T18:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1juq57m</id>
    <title>Llama 4 Maverick - 1.78bit Unsloth Dynamic GGUF</title>
    <updated>2025-04-08T22:13:10+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juq57m/llama_4_maverick_178bit_unsloth_dynamic_gguf/"&gt; &lt;img alt="Llama 4 Maverick - 1.78bit Unsloth Dynamic GGUF" src="https://external-preview.redd.it/z4xvThYutvB3Uci9TeHYLIrtcJ8j5Voji3gJkFtduNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=589f80df05465441181ff5902e8bae974cc28236" title="Llama 4 Maverick - 1.78bit Unsloth Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all! Maverick GGUFs are up now! For 1.78-bit, Maverick shrunk from 400GB to 122GB (-70%). &lt;a href="https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maverick fits in 2xH100 GPUs for fast inference ~80 tokens/sec. Would recommend y'all to have at least 128GB combined VRAM+RAM. Apple Unified memory should work decently well!&lt;/p&gt; &lt;p&gt;Guide + extra interesting details: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone benchmarked &lt;strong&gt;Dynamic Q2XL&lt;/strong&gt; Scout against the &lt;strong&gt;full 16-bit&lt;/strong&gt; model and surprisingly the Q2XL version does &lt;strong&gt;BETTER&lt;/strong&gt; on MMLU benchmarks which is just insane - maybe due to a combination of our custom calibration dataset + improper implementation of the model? &lt;a href="https://x.com/WolframRvnwlf/status/1909735579564331016/photo/1"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ez7jgwbtzote1.jpg?width=4096&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f72789a917d299db1e710759f286010bf21b8370"&gt;https://preview.redd.it/ez7jgwbtzote1.jpg?width=4096&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f72789a917d299db1e710759f286010bf21b8370&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During quantization of Llama 4 Maverick (the large model), we found the 1st, 3rd and 45th MoE layers could not be calibrated correctly. Maverick uses interleaving MoE layers for every odd layer, so Dense-&amp;gt;MoE-&amp;gt;Dense and so on.&lt;/p&gt; &lt;p&gt;We tried adding more uncommon languages to our calibration dataset, and tried using more tokens (1 million) vs Scout's 250K tokens for calibration, but we still found issues. We decided to leave these MoE layers as 3bit and 4bit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lzm1eqsdgote1.jpg?width=2577&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=92e52ac4c0f4b83eca6bec8d0ba0bf6c8de078f8"&gt;https://preview.redd.it/lzm1eqsdgote1.jpg?width=2577&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=92e52ac4c0f4b83eca6bec8d0ba0bf6c8de078f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For Llama 4 Scout, we found we should not quantize the vision layers, and leave the MoE router and some other layers as unquantized - we upload these to &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-unsloth-dynamic-bnb-4bit"&gt;https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-unsloth-dynamic-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7a662ymigote1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=abd5d6c1a2b818109eb3fd6c76016bb082740b26"&gt;https://preview.redd.it/7a662ymigote1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=abd5d6c1a2b818109eb3fd6c76016bb082740b26&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also had to convert &lt;code&gt;torch.nn.Parameter&lt;/code&gt; to &lt;code&gt;torch.nn.Linear&lt;/code&gt; for the MoE layers to allow 4bit quantization to occur. This also means we had to rewrite and patch over the generic Hugging Face implementation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o53fvv5mgote1.jpg?width=1136&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=63771723d05264bcba2a0adabfeab185d93e9789"&gt;https://preview.redd.it/o53fvv5mgote1.jpg?width=1136&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=63771723d05264bcba2a0adabfeab185d93e9789&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 4 also now uses chunked attention - it's essentially sliding window attention, but slightly more efficient by not attending to previous tokens over the 8192 boundary.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juq57m/llama_4_maverick_178bit_unsloth_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juq57m/llama_4_maverick_178bit_unsloth_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juq57m/llama_4_maverick_178bit_unsloth_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T22:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju9qx0</id>
    <title>Gemma 3 it is then</title>
    <updated>2025-04-08T09:51:07+00:00</updated>
    <author>
      <name>/u/freehuntx</name>
      <uri>https://old.reddit.com/user/freehuntx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"&gt; &lt;img alt="Gemma 3 it is then" src="https://preview.redd.it/zlui8az62lte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73b4f479c50348dbbdb4980ac9b2e0a61172b7af" title="Gemma 3 it is then" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freehuntx"&gt; /u/freehuntx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zlui8az62lte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T09:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jufqbn</id>
    <title>Qwen3 pull request sent to llama.cpp</title>
    <updated>2025-04-08T15:02:55+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The pull request has been created by bozheng-hit, who also sent the patches for qwen3 support in transformers.&lt;/p&gt; &lt;p&gt;It's approved and ready for merging.&lt;/p&gt; &lt;p&gt;Qwen 3 is near.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12828"&gt;https://github.com/ggml-org/llama.cpp/pull/12828&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T15:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jumdrx</id>
    <title>Introducing Cogito Preview</title>
    <updated>2025-04-08T19:34:08+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jumdrx/introducing_cogito_preview/"&gt; &lt;img alt="Introducing Cogito Preview" src="https://external-preview.redd.it/E0V10DZQvSMwLRdE1S8mLN25LWN3vkyFwE-AnfYBfUg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3596563f4c5d07663e0d550dc4a6da1097b6f5c" title="Introducing Cogito Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New series of LLMs making some pretty big claims.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.deepcogito.com/research/cogito-v1-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jumdrx/introducing_cogito_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jumdrx/introducing_cogito_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T19:34:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1juwb0e</id>
    <title>Use AI as proxy to communicate with other human?</title>
    <updated>2025-04-09T03:22:55+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"&gt; &lt;img alt="Use AI as proxy to communicate with other human?" src="https://preview.redd.it/7d7hcjvz7qte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=188e15cced0081b10de29d4f68a27164ba72494e" title="Use AI as proxy to communicate with other human?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7d7hcjvz7qte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T03:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1juphda</id>
    <title>Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!</title>
    <updated>2025-04-08T21:44:27+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"&gt; &lt;img alt="Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!" src="https://external-preview.redd.it/ZmRwZWJybDVnb3RlMSY-bUZv8golKOTPEcs9ioJR7hnQy1I9Bc0nx92Tbrmm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2869afde6675b54147a12cb0891e8cce7db3b1f1" title="Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xp0tcrl5gote1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T21:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1juz7o6</id>
    <title>I uploaded Q6 / Q5 quants of Mistral-Small-3.1-24B to ollama</title>
    <updated>2025-04-09T06:26:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/Mistral-Small-3.1-24B"&gt;https://www.ollama.com/JollyLlama/Mistral-Small-3.1-24B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since the official Ollama repo only has Q8 and Q4, I uploaded the Q5 and Q6 ggufs of Mistral-Small-3.1-24B to Ollama myself. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;These are quantized using ollama client, so these quants supports vision&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;On an RTX 4090 with 24GB of VRAM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q8 KV Cache enabled&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Leave 1GB to 800MB of VRAM as buffer zone&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;Q6_K: 35K context&lt;/p&gt; &lt;p&gt;Q5_K_M: 64K context&lt;/p&gt; &lt;p&gt;Q4_K_S: 100K context&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q6_K&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q5_K_M&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q4_K_S&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T06:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1juzt8z</id>
    <title>LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro</title>
    <updated>2025-04-09T07:10:30+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"&gt; &lt;img alt="LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro" src="https://preview.redd.it/r9wik2qderte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57b4c2a255dea94e6dc61d081088d52ad6289dcd" title="LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r9wik2qderte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T07:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1juhgy4</id>
    <title>World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200</title>
    <updated>2025-04-08T16:14:49+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt; &lt;img alt="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" src="https://external-preview.redd.it/w1uNpC9oR-BDfODib53NHNbbHwfxCxWJxaBmoZ3DyCw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee2a4eb2ac98a062fe2d3cb503f6c3733267d31" title="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At Avian.io, we have achieved 303 tokens per second in a collaboration with NVIDIA to achieve world leading inference performance on the Blackwell platform.&lt;/p&gt; &lt;p&gt;This marks a new era in test time compute driven models. We will be providing dedicated B200 endpoints for this model which will be available in the coming days, now available for preorder due to limited capacity &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:share:7315398985362391040/?actorCompanyId=99470879"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T16:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv22mm</id>
    <title>Qwen3 and Qwen3-MoE support merged into llama.cpp</title>
    <updated>2025-04-09T10:00:19+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Qwen3 and Qwen3-MoE support merged into llama.cpp" src="https://external-preview.redd.it/-e19x77nCYUlOdA0buk2ekzVqo7mRcwDr167KRd89n4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8c6fee2cf4cf6c565073243d3b38360e7ec134d" title="Qwen3 and Qwen3-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support merged.&lt;/p&gt; &lt;p&gt;We'll have GGUF models on day one&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T10:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jum5s1</id>
    <title>Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license</title>
    <updated>2025-04-08T19:24:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt; &lt;img alt="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" src="https://b.thumbs.redditmedia.com/UF9D2G4LY8nt9Z5N6vRdkaTUx2GI3fo84_fgBsrinvs.jpg" title="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cogito: “We are releasing the strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license. Each model outperforms the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen, across most standard benchmarks”&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53"&gt;https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jum5s1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T19:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1juni3t</id>
    <title>DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level</title>
    <updated>2025-04-08T20:20:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt; &lt;img alt="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" src="https://a.thumbs.redditmedia.com/Y5BwtBnjZby6zmZDlawuWxCvPe3JSO0Wzb73zGMqhW4.jpg" title="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1juni3t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T20:20:30+00:00</published>
  </entry>
</feed>
