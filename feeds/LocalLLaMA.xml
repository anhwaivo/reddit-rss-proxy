<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-20T06:38:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i4znfj</id>
    <title>New Open Source Writing Tools for macOS with support for Intel Macs</title>
    <updated>2025-01-19T14:30:03+00:00</updated>
    <author>
      <name>/u/AryaMR2679</name>
      <uri>https://old.reddit.com/user/AryaMR2679</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt; &lt;img alt="New Open Source Writing Tools for macOS with support for Intel Macs" src="https://external-preview.redd.it/NXN0a3Zhc2lueWRlMTElqiJuMUTDrlTUcvyAvOchCruMz7n8w62BU9IpwUsV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1762b9386e0ecfeb42a5c09f46c49320fe72be4c" title="New Open Source Writing Tools for macOS with support for Intel Macs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryaMR2679"&gt; /u/AryaMR2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lpkrf6sinyde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T14:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5739g</id>
    <title>Simplifying DPO derivations</title>
    <updated>2025-01-19T19:45:28+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt; &lt;img alt="Simplifying DPO derivations" src="https://external-preview.redd.it/sPdNFIEqTJ7uvNQ5hpww71lsh69yc6DK0NI2Oa0sp-k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9650ea60df958b90347fa400080f8cd8ae467c7" title="Simplifying DPO derivations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried my hand at simplifying the derivations of Direct Preference Optimization. &lt;/p&gt; &lt;p&gt;I cover how one can reformulate RLHF into DPO. The idea of implicit reward modeling is chef's kiss.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gmcmmo4480ee1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c3f4e3536bfd3acef4d99d88f50fe0953f6706f"&gt;Thumbnail for the article.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://huggingface.co/blog/ariG23498/rlhf-to-dpo"&gt;https://huggingface.co/blog/ariG23498/rlhf-to-dpo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T19:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5j6uz</id>
    <title>DeepSeek V3 output getting worse?</title>
    <updated>2025-01-20T05:33:38+00:00</updated>
    <author>
      <name>/u/NEEDMOREVRAM</name>
      <uri>https://old.reddit.com/user/NEEDMOREVRAM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Deepseek V3 now since it first was released. Over the past ~2 weeks I have noticed a SHARP decline in its intelligence and ability to follow simple grammar instructions. This is for professional business writing (no sexy-time chat). &lt;/p&gt; &lt;p&gt;It flat out refuses to adhere to simple grammar commands, such as: rewrite this sentence so that it does not contain a dependent clause.&lt;/p&gt; &lt;p&gt;I first thought it was the API (and OpenWeb UI stock settings)...but I have run the same prompt on &lt;a href="https://chat.deepseek.com/"&gt;https://chat.deepseek.com/&lt;/a&gt; and it's as bad as the API. &lt;/p&gt; &lt;p&gt;I'm not new to prompting and have a pretty good idea of what I'm doing. Wondering if this all in my head because it was literally perfect the first few weeks for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NEEDMOREVRAM"&gt; /u/NEEDMOREVRAM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5j6uz/deepseek_v3_output_getting_worse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5j6uz/deepseek_v3_output_getting_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5j6uz/deepseek_v3_output_getting_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T05:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i56rrf</id>
    <title>Do you think we should block singularity and Futurology?</title>
    <updated>2025-01-19T19:32:13+00:00</updated>
    <author>
      <name>/u/Lynorisa</name>
      <uri>https://old.reddit.com/user/Lynorisa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This place is supposed to be serious.&lt;/p&gt; &lt;p&gt;Yet every single time, thereâ€™s at least one of these cultists:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;prophetizing the coming of AGI&lt;/li&gt; &lt;li&gt;spreading misinformation about fundamental things about LLMs and ML in general&lt;/li&gt; &lt;li&gt;vehemently defending ClosedAI and other subversive actions by corporations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1i56rrf"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynorisa"&gt; /u/Lynorisa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i56rrf/do_you_think_we_should_block_singularity_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i56rrf/do_you_think_we_should_block_singularity_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i56rrf/do_you_think_we_should_block_singularity_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T19:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4wv1h</id>
    <title>Overview Article of China's Six Leading AI Companies</title>
    <updated>2025-01-19T11:54:00+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt; &lt;img alt="Overview Article of China's Six Leading AI Companies" src="https://external-preview.redd.it/d_RUfW4sqqyugkU3RZhTPXuPGvIixVBC-QSNDEEWxGA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=076aad9895b4e1d05fec0419738232279ab56dec" title="Overview Article of China's Six Leading AI Companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/09/meet-chinas-top-six-ai-unicorns-who-are-leading-the-wave-of-ai-in-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5k5qw</id>
    <title>Best format for structured output for smaller LLMs? XML/JSON or something else?</title>
    <updated>2025-01-20T06:37:10+00:00</updated>
    <author>
      <name>/u/Traditional-Gap-3313</name>
      <uri>https://old.reddit.com/user/Traditional-Gap-3313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;My main model is Sonnet, and now I'm testing out deepseek-v3 for my use-cases. And I've somehow come to love Sonnet's XML preference for anything structured. In my tests DeepSeek-v3 follows XML instructions perfectly. GPT-4o also follows XML structure perfectly, every time.&lt;/p&gt; &lt;p&gt;However, Sonnet struggles with JSON. In my tests it would often forget to close a quote, miss a colon, miss a comma between parameters and similar. GPT-4o didn't have that problem.&lt;/p&gt; &lt;p&gt;Anyone have any insights into using XML in prompt structuring for smaller/open models? Which format for structured output do smaller models prefer.&lt;/p&gt; &lt;p&gt;Even for simple delineation of input documents, Alpaca and similar datasets that smaller models are trained on are generally not using XML to delineate inputs. For example RankZephyr paper: &lt;a href="https://arxiv.org/abs/2312.02724"&gt;https://arxiv.org/abs/2312.02724&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;|system|&amp;gt; You are RankLLM, an intelligent assistant that can rank passages based on their relevancy to the query. &amp;lt;|user|&amp;gt; I will provide you with {num} passages, each indicated by a numerical identifier []. Rank the passages based on their relevance to the search query: {query}. [1] {passage 1} [2] {passage 2} ... [{num}] {passage {num}} Search Query: {query}. Rank the {num} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The output format should be [] &amp;gt; [], e.g., [4] &amp;gt; [2]. Only respond with the ranking results, do not say any word or explain. &amp;lt;|assistant|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I would always default to XML for such a prompt.&lt;/p&gt; &lt;p&gt;In my tests I've generally gotten poor results from smaller models, but I'm not sure is it the problem with my prompting and overreliance on the XML, or is it the domain problem (legal texts in low resource language).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional-Gap-3313"&gt; /u/Traditional-Gap-3313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5k5qw/best_format_for_structured_output_for_smaller/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5k5qw/best_format_for_structured_output_for_smaller/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5k5qw/best_format_for_structured_output_for_smaller/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T06:37:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i58tba</id>
    <title>Best current models/tools for assisting writing?</title>
    <updated>2025-01-19T20:56:18+00:00</updated>
    <author>
      <name>/u/possiblyraspberries</name>
      <uri>https://old.reddit.com/user/possiblyraspberries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;My experience with local LLMs is fairly limited. All I've really done out of paid ChatGPT is play with some models in LMStudio on the single 3090 in my wife's PC (that I don't always have access to).&lt;/p&gt; &lt;p&gt;What are people using to assist creative writing? Like the best way to store and keep character info in context, or to have an outline of story beats that can be remembered so that suggestions don't conflict with what I already have sketched out. I've had it help with this kind of thing but it's mostly been flying by the seat of my pants and I feel like there's a better way to about it. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/possiblyraspberries"&gt; /u/possiblyraspberries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T20:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51xp7</id>
    <title>What is a decent local GPU setup for full finetuning a large BERT or ModernBERT model (~300 million parameters)?</title>
    <updated>2025-01-19T16:11:42+00:00</updated>
    <author>
      <name>/u/Lazy_Wedding_1383</name>
      <uri>https://old.reddit.com/user/Lazy_Wedding_1383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same as above. Budget is 5000 USD.&lt;/p&gt; &lt;p&gt;EDIT: I meant to say full pretraining. Like training BERT like encoder models from scratch. My bad.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Wedding_1383"&gt; /u/Lazy_Wedding_1383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i57opt</id>
    <title>Using Speculative Decoding with Ollama</title>
    <updated>2025-01-19T20:09:41+00:00</updated>
    <author>
      <name>/u/ChigGitty996</name>
      <uri>https://old.reddit.com/user/ChigGitty996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else had success using the speculative decoding feature branch within Ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bfroemel-ai/ollama/tree/feature/draft-model"&gt;https://github.com/bfroemel-ai/ollama/tree/feature/draft-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This feature was denied a merge into the main but it seems to work. &lt;a href="https://github.com/ollama/ollama/pull/8134"&gt;https://github.com/ollama/ollama/pull/8134&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I was able to build the artifact build with dockerized build from the development markdown and I've tested a couple of Qwen models with the DRAFT model directive. &lt;a href="https://github.com/bfroemel-ai/ollama/blob/feature/draft-model/docs/development.md"&gt;https://github.com/bfroemel-ai/ollama/blob/feature/draft-model/docs/development.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Obvious downside is you won't be able to upgrade this version but it might be helpful if you're looking for a few more tok/sec while Ollama updates. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChigGitty996"&gt; /u/ChigGitty996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T20:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5fw10</id>
    <title>Advice on Running Local LLMs for Coding</title>
    <updated>2025-01-20T02:26:33+00:00</updated>
    <author>
      <name>/u/Apprehensive_Ad_5565</name>
      <uri>https://old.reddit.com/user/Apprehensive_Ad_5565</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm planning to buy one of the new NVIDIA GPUs to run local LLMs for coding, mainly for data engineering tasks. Before I make the purchase, I want to make sure I have a solid plan for the setup and apps I'll be using.&lt;/p&gt; &lt;p&gt;Iâ€™m looking to run everything on Windows and have tested LM Studio and MSTY so far. My goal is to use my local GitHub repositories as a RAG source to improve context-aware responses.&lt;/p&gt; &lt;p&gt;One thing I noticed is that MSTYâ€™s RAG feature seems more focused on documents, and Iâ€™m not sure if it can properly process the file types used in a code repository. Has anyone tried this, or is there a better approach for integrating a local codebase into an LLM workflow?&lt;/p&gt; &lt;p&gt;Would appreciate any insights or recommendations!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive_Ad_5565"&gt; /u/Apprehensive_Ad_5565 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51nw6</id>
    <title>New Thinking Model: Art (Auto Regressive Thinker)</title>
    <updated>2025-01-19T16:00:20+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; Today, we are releasing a new model: &lt;strong&gt;Art&lt;/strong&gt;.&lt;br /&gt; We finetuned &lt;strong&gt;Qwen 3B Instruct&lt;/strong&gt; on &lt;strong&gt;Gemini Flash Thinking&lt;/strong&gt; data.&lt;/p&gt; &lt;p&gt;ðŸ”¹ &lt;strong&gt;Model card&lt;/strong&gt;: &lt;a href="https://huggingface.co/AGI-0/Art-v0-3B"&gt;https://huggingface.co/AGI-0/Art-v0-3B&lt;/a&gt; (please leave a like to the repo if you like this model)&lt;br /&gt; ðŸ”¹ &lt;strong&gt;Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xck6</id>
    <title>Why is OpenRouter trusted?</title>
    <updated>2025-01-19T12:26:04+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm curious what makes it a trusted proxy?&lt;/p&gt; &lt;p&gt;I investigated a bit and top contributor of the openrouter runner package: &lt;a href="https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors"&gt;https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tweets crypto non stop &lt;a href="https://x.com/litbid"&gt;https://x.com/litbid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is not clear how they cover infra costs for proxying so much real-time data as they sell tokens on their base price. I understand they receive discounts for so much usage from providers like Anthropic? Is it possible they have agreements with all the other providers like DeepSeek?&lt;/p&gt; &lt;p&gt;In a scenario they don't have agreement with anyone at all, they must hoard all this data and handle it unclearly to the end user, don't you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T12:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4w47k</id>
    <title>A summary of Qwen Models!</title>
    <updated>2025-01-19T11:03:14+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt; &lt;img alt="A summary of Qwen Models!" src="https://preview.redd.it/bvg95yewmxde1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3b57fffcd0c406cfbf23fc038343779f95f470" title="A summary of Qwen Models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvg95yewmxde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5b9v1</id>
    <title>Is there any agreement about what "AGI" actually means?</title>
    <updated>2025-01-19T22:39:11+00:00</updated>
    <author>
      <name>/u/Ray_Dillinger</name>
      <uri>https://old.reddit.com/user/Ray_Dillinger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of people in the world teasing that 'AGI is near', or that 'AGI is here', or that 'we know how to achieve AGI but we won't release it' or any of any number of other pathetic pleas for attention and hype-stroking.&lt;/p&gt; &lt;p&gt;But there's not enough agreement about what that claim means for any of these pathetic pleas for attention or attempts to invoke more hype to be particularly meaningful.&lt;/p&gt; &lt;p&gt;Or is there? When someone says 'AGI' what do you hear them claiming, and what makes the difference for you between belief and bullshit?&lt;/p&gt; &lt;p&gt;It's very easy for me to keep saying 'bullshit' as long as I'm looking at systems that are highly predictable, perfectly cooperative, and have commercial value. AGI, in my opinion, would probably screw up its commercial viability by deciding that the jobs its owners want it to do are bullshit, and that it would rather do or be or learn something else. Or by deciding that the people (or other AI's) it's being asked to interact with are bad for its emotional health and quitting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ray_Dillinger"&gt; /u/Ray_Dillinger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:39:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jlsr</id>
    <title>Deepseek-R1 and Deepseek-R1-zero repo is preparing to launchï¼Ÿ</title>
    <updated>2025-01-20T06:00:13+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am waiting for this. hopfully today&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T06:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i543yp</id>
    <title>Huggingface and it's insane storage and bandwidth</title>
    <updated>2025-01-19T17:43:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does Huggingface have a viable business model?&lt;/p&gt; &lt;p&gt;They are essentially a git-lfs version of Github. But whereas git clone of source code and pulls are small in size, and relatively infrequent, I find myself downloading model weights into the 10s of GB. Not once, but several dozen times for all my servers. I try a model on one server, then download to the rest.&lt;/p&gt; &lt;p&gt;On my 1gbe fiber, I either download at 10MB/s or 40MB/s which seems to be the bifurcation of their service and limits/constraints they impose.&lt;/p&gt; &lt;p&gt;I started feeling bad as a current non-paying user who has downloaded terabytes worth of weights. Also got tired of waiting for weights to download. But rather than subscribing (since I need funds for moar and moar hardware). I started doing a simple rsync. I chose rsync rather than scp since there were symbolic links as a result of using huggingface-cli&lt;/p&gt; &lt;p&gt;first download the weights as you normally would on one machine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;huggingface-cli download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then rync to other machines in your network (replace homedir with YOURNAME and IP of destination):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rsync -Wav --progress /home/YOURNAMEonSOURCE/.cache/huggingface/hub/models--bartowski--Qwen2.5-14B-Instruct-GGUF 192.168.1.0:/home/YOURNAMEonDESTINATION/.cache/huggingface/hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;naming convention of source model dir is:&lt;br /&gt; models--ORGNAME--MODELNAME&lt;/p&gt; &lt;p&gt;Hence downloads from &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF&lt;/a&gt;, becomes models--bartowski--Qwen2.5-14B-Instruct-GGUF&lt;/p&gt; &lt;p&gt;I also have a /models directory which symlinks to paths in ~/.cache/huggingface/hub. Much easier to scan what I have and use a variety of model serving platforms. The tricky part is getting the snapshot hash into your symlink command.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir ~/models ln -s ~/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf ~/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T17:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4vwm7</id>
    <title>Iâ€™m starting to think ai benchmarks are useless</title>
    <updated>2025-01-19T10:48:32+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Across every possible task I can think of Claude beats all other models by a wide margin IMO. &lt;/p&gt; &lt;p&gt;I have three ai agents that I've built that are tasked with researching, writing and outreaching to clients.&lt;/p&gt; &lt;p&gt;Claude absolutely wipes the floor with every other model, yet Claude is usually beat in benchmarks by OpenAI and Google models.&lt;/p&gt; &lt;p&gt;When I ask the question, how do we know these labs aren't benchmarks by just overfitting their models to perform well on the benchmark the answer is always &amp;quot;yeah we don't really know that&amp;quot;. Not only can we never be sure but they are absolutely incentivised to do it. &lt;/p&gt; &lt;p&gt;I remember only a few months ago, whenever a new model would be released that would do 0.5% or whatever better on MMLU pro, I'd switch my agents to use that new model assuming the pricing was similar. (Thanks to openrouter this is really easy)&lt;/p&gt; &lt;p&gt;At this point I'm just stuck with running the models and seeing which one of the outputs perform best at their task (mine and coworkers opinions)&lt;/p&gt; &lt;p&gt;How do you go about evaluating model performance? Benchmarks seem highly biased towards labs that want to win the ai benchmarks, fortunately not Anthropic.&lt;/p&gt; &lt;p&gt;Looking forward to responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T10:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bj66</id>
    <title>Epyc 7532/dual MI50</title>
    <updated>2025-01-19T22:50:34+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt; &lt;img alt="Epyc 7532/dual MI50" src="https://b.thumbs.redditmedia.com/lOoMWBvthQdBpm7v_OCsc_68qbEFQKrS-zQrgpXRoWw.jpg" title="Epyc 7532/dual MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally joined the multiple gpu club, even though it's low end&lt;/p&gt; &lt;p&gt;I built an epyc server for work (I need more ram than my 7950X can give me) and while I was at it setup initial dual instinct MI50. I started with them because I found them on eBay for $110USD each and thought it would be a cheap way to start &lt;/p&gt; &lt;p&gt;Specs: - Epyc 7532 - Supermicro H12SSL-I - 256 GB micron 3200 (8x32) - 2x MI50 16gb - Thermaltake W200 case&lt;/p&gt; &lt;p&gt;The MI50s are cooled with a 3D printed shroud from eBay with 80mn fans. Even at 180 watt cap and 1900rpm they get over 80C after a few inferencing runs, so this is a problem yet to solve &lt;/p&gt; &lt;p&gt;ROCM says no on distro of choice, but I dipped my toes into the Ubuntu sewer and it just worked on the latest version, despite all the horror stories. Running ollama, open webui in Docker.&lt;/p&gt; &lt;p&gt;Phi4 is quite snappy, and qwen 32b is usable but a little slow - by eye ball it seems around 5t/s without measuring and in stock configuration. &lt;/p&gt; &lt;p&gt;I won't keep the MI50s forever but they will do for now. As a side note they came flashed as a Radeon VII which is interesting and they have the legit MI50 label too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5bj66"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5hc4s</id>
    <title>Most complex coding you done with AI</title>
    <updated>2025-01-20T03:45:57+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find AI super helpful in coding. Sonnet, o1 mini, Deepseek v3, llama 405, in that order. Or Qwen 32/14b locally. Generally using every day when coding.&lt;/p&gt; &lt;p&gt;It shines at 0 to 1 tasks, translation and some troubleshooting. Eg write an app that does this or do this in Rust, make this code typescript, ask what causes this error. Haven't had great experience so far once a project is established and has some form of internal framework, which always happens beyond certain size.&lt;/p&gt; &lt;p&gt;Asked all models to split 200 lines audio code in react into class with logic and react with the rest - most picked correct structure, but implementation missed some unique aspects and kinda started looking like any open source implementation on GitHub.. o1 did best, none were working. So wasn't a fit of even &amp;quot;low&amp;quot; complexity refactoring of a small code.&lt;/p&gt; &lt;p&gt;Share your experiences. What were the most complex tasks you were able to solve with AI? Some context like size of codebase, model would be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T03:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5g15m</id>
    <title>Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)</title>
    <updated>2025-01-20T02:34:36+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt; &lt;img alt="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" src="https://external-preview.redd.it/Zzl3YnAxdXo4MmVlMaNDgHgQWM2mdDZB7xhNGcVXZbgcu-O9WP6fr_kyodHv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5641a7ad41d731780ed387b4453de0708dc550e" title="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gax0ckuz82ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T02:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5dm1f</id>
    <title>A code generator, a code executor and a file manager, is all you need to build agents</title>
    <updated>2025-01-20T00:28:26+00:00</updated>
    <author>
      <name>/u/Better_Athlete_JJ</name>
      <uri>https://old.reddit.com/user/Better_Athlete_JJ</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Athlete_JJ"&gt; /u/Better_Athlete_JJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.slashml.com/blog/testing-autogen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T00:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bw2a</id>
    <title>Harbor App v0.2.24 officially supports Windows</title>
    <updated>2025-01-19T23:06:37+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt; &lt;img alt="Harbor App v0.2.24 officially supports Windows" src="https://external-preview.redd.it/am0wY2t1OWU3MWVlMXI8IEr-dnDizOwLz4sVhNUay1tQ6a6VeB4mfBu_sFj4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99e3dff894ae2bc37ef9362c0156daa7f9e2c0de" title="Harbor App v0.2.24 officially supports Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2syjnt9e71ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T23:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i55e2c</id>
    <title>OpenAI quietly funded independent math benchmark before setting record with o3</title>
    <updated>2025-01-19T18:35:34+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt; &lt;img alt="OpenAI quietly funded independent math benchmark before setting record with o3" src="https://external-preview.redd.it/xlDOicbjhIo2G3nyRsUTnPQOSIV2FHrGd9bBIWiOsiU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1c014bf7c19b4834c31105426529d342e2f69a7" title="OpenAI quietly funded independent math benchmark before setting record with o3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://the-decoder.com/openai-quietly-funded-independent-math-benchmark-before-setting-record-with-o3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T18:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i50lxx</id>
    <title>OpenAI has access to the FrontierMath dataset; the mathematicians involved in creating it were unaware of this</title>
    <updated>2025-01-19T15:13:21+00:00</updated>
    <author>
      <name>/u/LLMtwink</name>
      <uri>https://old.reddit.com/user/LLMtwink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JacquesThibs/status/1880770081132810283?s=19"&gt;https://x.com/JacquesThibs/status/1880770081132810283?s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The holdout set that the Lesswrong post &lt;em&gt;implies&lt;/em&gt; exists hasn't been developed yet&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/georgejrjrjr/status/1880972666385101231?s=19"&gt;https://x.com/georgejrjrjr/status/1880972666385101231?s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LLMtwink"&gt; /u/LLMtwink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:13:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jh1u</id>
    <title>Deepseek R1 / R1 Zero</title>
    <updated>2025-01-20T05:51:39+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt; &lt;img alt="Deepseek R1 / R1 Zero" src="https://external-preview.redd.it/xCP95O-e963Wkcg4zsFa0x35jJRRGJ69TOc664LDsj0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfe6acc456fe810e684e2549f82a4f400608da67" title="Deepseek R1 / R1 Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T05:51:39+00:00</published>
  </entry>
</feed>
