<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-20T14:35:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i543yp</id>
    <title>Huggingface and it's insane storage and bandwidth</title>
    <updated>2025-01-19T17:43:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does Huggingface have a viable business model?&lt;/p&gt; &lt;p&gt;They are essentially a git-lfs version of Github. But whereas git clone of source code and pulls are small in size, and relatively infrequent, I find myself downloading model weights into the 10s of GB. Not once, but several dozen times for all my servers. I try a model on one server, then download to the rest.&lt;/p&gt; &lt;p&gt;On my 1gbe fiber, I either download at 10MB/s or 40MB/s which seems to be the bifurcation of their service and limits/constraints they impose.&lt;/p&gt; &lt;p&gt;I started feeling bad as a current non-paying user who has downloaded terabytes worth of weights. Also got tired of waiting for weights to download. But rather than subscribing (since I need funds for moar and moar hardware). I started doing a simple rsync. I chose rsync rather than scp since there were symbolic links as a result of using huggingface-cli&lt;/p&gt; &lt;p&gt;first download the weights as you normally would on one machine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;huggingface-cli download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then rync to other machines in your network (replace homedir with YOURNAME and IP of destination):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rsync -Wav --progress /home/YOURNAMEonSOURCE/.cache/huggingface/hub/models--bartowski--Qwen2.5-14B-Instruct-GGUF 192.168.1.0:/home/YOURNAMEonDESTINATION/.cache/huggingface/hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;naming convention of source model dir is:&lt;br /&gt; models--ORGNAME--MODELNAME&lt;/p&gt; &lt;p&gt;Hence downloads from &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF&lt;/a&gt;, becomes models--bartowski--Qwen2.5-14B-Instruct-GGUF&lt;/p&gt; &lt;p&gt;I also have a /models directory which symlinks to paths in ~/.cache/huggingface/hub. Much easier to scan what I have and use a variety of model serving platforms. The tricky part is getting the snapshot hash into your symlink command.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir ~/models ln -s ~/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf ~/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T17:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5oquo</id>
    <title>"DeepSeek R1 Distilled" Open Source Models</title>
    <updated>2025-01-20T12:07:11+00:00</updated>
    <author>
      <name>/u/GuessJust7842</name>
      <uri>https://old.reddit.com/user/GuessJust7842</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oquo/deepseek_r1_distilled_open_source_models/"&gt; &lt;img alt="&amp;quot;DeepSeek R1 Distilled&amp;quot; Open Source Models" src="https://external-preview.redd.it/_atc5Wper5qoTlKRLhG_b9IdHbfvAzDOL9GdRqfNNpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb0be2c39108ae40a2fceffcb31c8521a0e79a4a" title="&amp;quot;DeepSeek R1 Distilled&amp;quot; Open Source Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;like Qwen and Llama&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/th0jtj8a35ee1.png?width=2258&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=651d58f54dcd33bccae5b2053db003e1fd2eb4c5"&gt;https://preview.redd.it/th0jtj8a35ee1.png?width=2258&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=651d58f54dcd33bccae5b2053db003e1fd2eb4c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuessJust7842"&gt; /u/GuessJust7842 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oquo/deepseek_r1_distilled_open_source_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oquo/deepseek_r1_distilled_open_source_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oquo/deepseek_r1_distilled_open_source_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5lwg4</id>
    <title>Best open source LLM better than gpt-4o-mini but cheaper than 4o?</title>
    <updated>2025-01-20T08:44:41+00:00</updated>
    <author>
      <name>/u/PMMEYOURSMIL3</name>
      <uri>https://old.reddit.com/user/PMMEYOURSMIL3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean on OpenRouter. I know this is &amp;quot;Local&amp;quot;LLaMA but you guys are the most familiar with open source LLMs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PMMEYOURSMIL3"&gt; /u/PMMEYOURSMIL3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T08:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bj66</id>
    <title>Epyc 7532/dual MI50</title>
    <updated>2025-01-19T22:50:34+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt; &lt;img alt="Epyc 7532/dual MI50" src="https://b.thumbs.redditmedia.com/lOoMWBvthQdBpm7v_OCsc_68qbEFQKrS-zQrgpXRoWw.jpg" title="Epyc 7532/dual MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally joined the multiple gpu club, even though it's low end&lt;/p&gt; &lt;p&gt;I built an epyc server for work (I need more ram than my 7950X can give me) and while I was at it setup initial dual instinct MI50. I started with them because I found them on eBay for $110USD each and thought it would be a cheap way to start &lt;/p&gt; &lt;p&gt;Specs: - Epyc 7532 - Supermicro H12SSL-I - 256 GB micron 3200 (8x32) - 2x MI50 16gb - Thermaltake W200 case&lt;/p&gt; &lt;p&gt;The MI50s are cooled with a 3D printed shroud from eBay with 80mn fans. Even at 180 watt cap and 1900rpm they get over 80C after a few inferencing runs, so this is a problem yet to solve &lt;/p&gt; &lt;p&gt;ROCM says no on distro of choice, but I dipped my toes into the Ubuntu sewer and it just worked on the latest version, despite all the horror stories. Running ollama, open webui in Docker.&lt;/p&gt; &lt;p&gt;Phi4 is quite snappy, and qwen 32b is usable but a little slow - by eye ball it seems around 5t/s without measuring and in stock configuration. &lt;/p&gt; &lt;p&gt;I won't keep the MI50s forever but they will do for now. As a side note they came flashed as a Radeon VII which is interesting and they have the legit MI50 label too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5bj66"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5dm1f</id>
    <title>A code generator, a code executor and a file manager, is all you need to build agents</title>
    <updated>2025-01-20T00:28:26+00:00</updated>
    <author>
      <name>/u/Better_Athlete_JJ</name>
      <uri>https://old.reddit.com/user/Better_Athlete_JJ</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Athlete_JJ"&gt; /u/Better_Athlete_JJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.slashml.com/blog/testing-autogen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T00:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bw2a</id>
    <title>Harbor App v0.2.24 officially supports Windows</title>
    <updated>2025-01-19T23:06:37+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt; &lt;img alt="Harbor App v0.2.24 officially supports Windows" src="https://external-preview.redd.it/am0wY2t1OWU3MWVlMXI8IEr-dnDizOwLz4sVhNUay1tQ6a6VeB4mfBu_sFj4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99e3dff894ae2bc37ef9362c0156daa7f9e2c0de" title="Harbor App v0.2.24 officially supports Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2syjnt9e71ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T23:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5otwv</id>
    <title>DeepSeek update 6 distill models</title>
    <updated>2025-01-20T12:12:33+00:00</updated>
    <author>
      <name>/u/Attorney_Putrid</name>
      <uri>https://old.reddit.com/user/Attorney_Putrid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and so on&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Attorney_Putrid"&gt; /u/Attorney_Putrid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5otwv/deepseek_update_6_distill_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5otwv/deepseek_update_6_distill_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5otwv/deepseek_update_6_distill_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pogn</id>
    <title>Worth checking , They have released 6 distilled models</title>
    <updated>2025-01-20T13:03:18+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pogn/worth_checking_they_have_released_6_distilled/"&gt; &lt;img alt="Worth checking , They have released 6 distilled models " src="https://preview.redd.it/7nberqidd5ee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f45fd64dd5d2f53af3c47cdafcfa2e70b37be65c" title="Worth checking , They have released 6 distilled models " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7nberqidd5ee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pogn/worth_checking_they_have_released_6_distilled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pogn/worth_checking_they_have_released_6_distilled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T13:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5g15m</id>
    <title>Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)</title>
    <updated>2025-01-20T02:34:36+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt; &lt;img alt="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" src="https://external-preview.redd.it/Zzl3YnAxdXo4MmVlMaNDgHgQWM2mdDZB7xhNGcVXZbgcu-O9WP6fr_kyodHv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5641a7ad41d731780ed387b4453de0708dc550e" title="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gax0ckuz82ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T02:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p6vl</id>
    <title>DeepSeek-R1-Distill-Llama-8B AND DeepSeek-R1-Distill-Qwen-7B and 14b</title>
    <updated>2025-01-20T12:35:01+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;its show time, folks&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p6vl/deepseekr1distillllama8b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p6vl/deepseekr1distillllama8b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p6vl/deepseekr1distillllama8b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p80h</id>
    <title>NVIDIA DIGITS 10 years evolution</title>
    <updated>2025-01-20T12:36:59+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This isn't the first time that Nvidia have released DIGITS hardware. Back in 2015, they released the DIGITS DEVBOX. It was a $15k machine with 4 GeForce GTX TITAN X cards*.&lt;/p&gt; &lt;p&gt;That's 6.691 TFLOPS [FP32 (float)] and 12 GB VRAM per card. Or 26.764 TFLOPS and 48GB combined.&lt;/p&gt; &lt;p&gt;The new DIGITS will probably have around 30 TFLOPS in FP32 (float) and 128GB VRAM (and double up for two connected systems) for $3000.&lt;/p&gt; &lt;p&gt;*&lt;a href="https://www.legitreviews.com/nvidia-digits-devbox-promotes-deep-learning-titanx_160343"&gt;https://www.legitreviews.com/nvidia-digits-devbox-promotes-deep-learning-titanx_160343&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**&lt;a href="https://docs.nvidia.com/dgx/digits-devbox-user-guide/index.html"&gt;https://docs.nvidia.com/dgx/digits-devbox-user-guide/index.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p80h/nvidia_digits_10_years_evolution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p80h/nvidia_digits_10_years_evolution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p80h/nvidia_digits_10_years_evolution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jlsr</id>
    <title>Deepseek-R1 and Deepseek-R1-zero repo is preparing to launch？</title>
    <updated>2025-01-20T06:00:13+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am waiting for this. hopfully today&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T06:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5hc4s</id>
    <title>Most complex coding you done with AI</title>
    <updated>2025-01-20T03:45:57+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find AI super helpful in coding. Sonnet, o1 mini, Deepseek v3, llama 405, in that order. Or Qwen 32/14b locally. Generally using every day when coding.&lt;/p&gt; &lt;p&gt;It shines at 0 to 1 tasks, translation and some troubleshooting. Eg write an app that does this or do this in Rust, make this code typescript, ask what causes this error. Haven't had great experience so far once a project is established and has some form of internal framework, which always happens beyond certain size.&lt;/p&gt; &lt;p&gt;Asked all models to split 200 lines audio code in react into class with logic and react with the rest - most picked correct structure, but implementation missed some unique aspects and kinda started looking like any open source implementation on GitHub.. o1 did best, none were working. So wasn't a fit of even &amp;quot;low&amp;quot; complexity refactoring of a small code.&lt;/p&gt; &lt;p&gt;Share your experiences. What were the most complex tasks you were able to solve with AI? Some context like size of codebase, model would be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T03:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p9dk</id>
    <title>Deepseek-R1 officially release</title>
    <updated>2025-01-20T12:39:20+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt; &lt;img alt="Deepseek-R1 officially release" src="https://external-preview.redd.it/VH2iWMzzI7fr45Vwe4Cjkq8YtS-AXjXjtn6GyjVnsIQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee4cf33369b41133dc351d1a09cdeb6f80176720" title="Deepseek-R1 officially release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we are officially releasing DeepSeek-R1 and simultaneously open-sourcing the model weights.&lt;/p&gt; &lt;p&gt;DeepSeek-R1 is released under the MIT License, allowing users to train other models through distillation techniques using R1.&lt;/p&gt; &lt;p&gt;The DeepSeek-R1 API is now live, giving users access to chain-of-thought outputs by setting `model='deepseek-reasoner'`.&lt;/p&gt; &lt;p&gt;The DeepSeek website and app are being updated and launched simultaneously starting today.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance aligned with OpenAI-o1 official release&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;During the post-training phase, DeepSeek-R1 extensively utilized reinforcement learning techniques, significantly enhancing the model's reasoning capabilities with minimal annotated data. On tasks including mathematics, coding, and natural language reasoning, its performance matches that of the official OpenAI o1 release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0dyqpnhx75ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c403b4b0072827d55e4a5e6b30591342cc79f1c"&gt;https://preview.redd.it/0dyqpnhx75ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c403b4b0072827d55e4a5e6b30591342cc79f1c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are making all DeepSeek-R1 training techniques public to promote open exchange and collaborative innovation within the technical community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper Link&lt;/strong&gt;: &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"&gt;&lt;strong&gt;https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Distilled Small Models Surpass OpenAI o1-mini&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Along with open-sourcing the two 660B models DeepSeek-R1-Zero and DeepSeek-R1, we have distilled 6 smaller models for the community using DeepSeek-R1's outputs. Among these, our 32B and 70B models have achieved performance comparable to OpenAI o1-mini across multiple capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4o34xbv385ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71a867ed8173f83e87fd04df60748c0be1f2c64"&gt;https://preview.redd.it/4o34xbv385ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71a867ed8173f83e87fd04df60748c0be1f2c64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Link&lt;/strong&gt;: &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;strong&gt;https://huggingface.co/deepseek-ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yyta7e785ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3b5793efba042b1da54d00831470cab9383fc88"&gt;https://preview.redd.it/3yyta7e785ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3b5793efba042b1da54d00831470cab9383fc88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open License and User Agreement&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;To promote and encourage the development of the open-source community and industry ecosystem, while releasing and open-sourcing R1, we have made the following adjustments to our licensing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All model open-source licenses unified under MIT&lt;/strong&gt;. Previously, considering the unique characteristics of large language models and current industry practices, we introduced the DeepSeek License for open-source authorization. However, practice has shown that non-standard open-source licenses may increase developers' comprehension burden. Therefore, our open-source repositories (including model weights) now uniformly adopt the standardized, permissive MIT License - completely open source, with no commercial restrictions and no application required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Product agreement explicitly allows &amp;quot;model distillation&amp;quot;&lt;/strong&gt;. To further promote technology sharing and open source development, we have decided to support users in performing &amp;quot;model distillation.&amp;quot; We have updated our online product user agreement to explicitly allow users to train other models using model outputs through techniques such as model distillation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;API and Pricing&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;DeepSeek-R1 API service pricing is set at &lt;strong&gt;1 RMB per million input tokens (cache hit) / 4 RMB per million input tokens (cache miss), and 16 RMB per million output tokens.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ja0nhjzl85ee1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f5e80c57d6b78c9e2c11590bc0eaf5be7974335"&gt;https://preview.redd.it/ja0nhjzl85ee1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f5e80c57d6b78c9e2c11590bc0eaf5be7974335&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/roylljnm85ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba88186aa43a468f3d7174505cabe1d41f603628"&gt;https://preview.redd.it/roylljnm85ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba88186aa43a468f3d7174505cabe1d41f603628&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For detailed API usage guidelines, please refer to the official documentation: &lt;a href="https://api-docs.deepseek.com/zh-cn/guides/reasoning_model"&gt;&lt;strong&gt;https://api-docs.deepseek.com/zh-cn/guides/reasoning_model&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i50lxx</id>
    <title>OpenAI has access to the FrontierMath dataset; the mathematicians involved in creating it were unaware of this</title>
    <updated>2025-01-19T15:13:21+00:00</updated>
    <author>
      <name>/u/LLMtwink</name>
      <uri>https://old.reddit.com/user/LLMtwink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JacquesThibs/status/1880770081132810283?s=19"&gt;https://x.com/JacquesThibs/status/1880770081132810283?s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The holdout set that the Lesswrong post &lt;em&gt;implies&lt;/em&gt; exists hasn't been developed yet&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/georgejrjrjr/status/1880972666385101231?s=19"&gt;https://x.com/georgejrjrjr/status/1880972666385101231?s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LLMtwink"&gt; /u/LLMtwink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:13:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i55e2c</id>
    <title>OpenAI quietly funded independent math benchmark before setting record with o3</title>
    <updated>2025-01-19T18:35:34+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt; &lt;img alt="OpenAI quietly funded independent math benchmark before setting record with o3" src="https://external-preview.redd.it/xlDOicbjhIo2G3nyRsUTnPQOSIV2FHrGd9bBIWiOsiU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1c014bf7c19b4834c31105426529d342e2f69a7" title="OpenAI quietly funded independent math benchmark before setting record with o3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://the-decoder.com/openai-quietly-funded-independent-math-benchmark-before-setting-record-with-o3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T18:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pepa</id>
    <title>DeepSeek-R1 Paper</title>
    <updated>2025-01-20T12:48:03+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"&gt; &lt;img alt="DeepSeek-R1 Paper" src="https://external-preview.redd.it/VH2iWMzzI7fr45Vwe4Cjkq8YtS-AXjXjtn6GyjVnsIQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee4cf33369b41133dc351d1a09cdeb6f80176720" title="DeepSeek-R1 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5oygi</id>
    <title>DeepSeek test review</title>
    <updated>2025-01-20T12:20:37+00:00</updated>
    <author>
      <name>/u/Born-Shopping-1876</name>
      <uri>https://old.reddit.com/user/Born-Shopping-1876</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been testing the new full R1 model, I gave it the research paper of Titans architecture from Google Research, and I ask to write a small description with json format, then make learn the architecture and build it using TensorFlow to implement it and train it into the jsom text. &lt;/p&gt; &lt;p&gt;I got the correct code after 2-shot of errors and model works great.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born-Shopping-1876"&gt; /u/Born-Shopping-1876 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5piy1</id>
    <title>Deepseek R1 = $2.19/M tok output vs o1 $60/M tok. Insane</title>
    <updated>2025-01-20T12:54:57+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know we will have to check out real world applications outside of benchmarks, but this is wild. Curious to hear anyone's comparisons also - esp for code gen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5m4t9</id>
    <title>let’s goo, DeppSeek-R1 685 billion parameters!</title>
    <updated>2025-01-20T09:02:18+00:00</updated>
    <author>
      <name>/u/bymechul</name>
      <uri>https://old.reddit.com/user/bymechul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt; &lt;img alt="let’s goo, DeppSeek-R1 685 billion parameters!" src="https://b.thumbs.redditmedia.com/9jF3vfuRP-Df2M-JGgdyrsvXvrMePxQdfuMlkImLCQs.jpg" title="let’s goo, DeppSeek-R1 685 billion parameters!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131"&gt;https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bymechul"&gt; /u/bymechul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p549</id>
    <title>DeepSeek R1 has been officially released!</title>
    <updated>2025-01-20T12:32:02+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt; &lt;img alt="DeepSeek R1 has been officially released! " src="https://external-preview.redd.it/EFqVCmP1lVQaIeFKK0xlX4mTF_zF6Me4AHhvUKZ16H4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2abe26f7a3e1642bf9cb27ef184bd854f2d59cf2" title="DeepSeek R1 has been officially released! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;https://github.com/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The complete technical report has been made publicly available on GitHub.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6"&gt;https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5q6b9</id>
    <title>DeepSeek-R1 and distilled benchmarks color coded</title>
    <updated>2025-01-20T13:30:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt; &lt;img alt="DeepSeek-R1 and distilled benchmarks color coded" src="https://b.thumbs.redditmedia.com/uR9Tld2vZxIJ2G0oapW1g73pOQppqKetkRf1z_lJAIg.jpg" title="DeepSeek-R1 and distilled benchmarks color coded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5q6b9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T13:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jh1u</id>
    <title>Deepseek R1 / R1 Zero</title>
    <updated>2025-01-20T05:51:39+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt; &lt;img alt="Deepseek R1 / R1 Zero" src="https://external-preview.redd.it/xCP95O-e963Wkcg4zsFa0x35jJRRGJ69TOc664LDsj0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfe6acc456fe810e684e2549f82a4f400608da67" title="Deepseek R1 / R1 Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T05:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pbb3</id>
    <title>o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!</title>
    <updated>2025-01-20T12:42:33+00:00</updated>
    <author>
      <name>/u/Consistent_Bit_3295</name>
      <uri>https://old.reddit.com/user/Consistent_Bit_3295</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt; &lt;img alt="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" src="https://b.thumbs.redditmedia.com/gdpGFb_knvwb6nUDYwf-wMTftZq5nEGNIQeq1omODJI.jpg" title="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Bit_3295"&gt; /u/Consistent_Bit_3295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5pbb3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5or1y</id>
    <title>Deepseek just uploaded 6 distilled verions of R1 + R1 "full" now available on their website.</title>
    <updated>2025-01-20T12:07:33+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt; &lt;img alt="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." src="https://external-preview.redd.it/_atc5Wper5qoTlKRLhG_b9IdHbfvAzDOL9GdRqfNNpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb0be2c39108ae40a2fceffcb31c8521a0e79a4a" title="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:07:33+00:00</published>
  </entry>
</feed>
