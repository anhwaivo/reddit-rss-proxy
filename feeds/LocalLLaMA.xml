<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-12T05:37:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l957nz</id>
    <title>Best site for inferencing medgemma 27B?</title>
    <updated>2025-06-11T21:47:55+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it's locallama: I tried the 4B model on lmstudio and got scared that a 5GB file is a better doctor than I will ever be, so now I want to try the 27B model to feel even worse. My poor 3060 with 6 GB VRAM will never handle it and i did not find it on aistudio nor on openrouter. I tried with Vertex AI but it's a pain in the a** to setup so I wonder if there are alternatives (chat interface or API) that are easier to try. &lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are curious about my experience with the model: the 4-bit answered most of my question correctly when asked in English (questions like &amp;quot;what's the most common congenital cardiopathy in people with trisomy 21?&amp;quot;), but failed when asked in Italian hallucinating new diseases. The 8-bit quant answered correctly in Italian as well, but both failed at telling me anything about a rare disease I'm studying (MADD), not even what it's acronym stands for.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l957nz/best_site_for_inferencing_medgemma_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l957nz/best_site_for_inferencing_medgemma_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l957nz/best_site_for_inferencing_medgemma_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T21:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8x080</id>
    <title>Qwen 2.5 3B VL performance dropped post fine tuning.</title>
    <updated>2025-06-11T16:22:49+00:00</updated>
    <author>
      <name>/u/chitrabhat4</name>
      <uri>https://old.reddit.com/user/chitrabhat4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beginner here - please help me out.&lt;/p&gt; &lt;p&gt;I was asked to fine tune a Qwen 2.5 3B VL for the following task:&lt;/p&gt; &lt;p&gt;Given an image taken during an online test, check if the candidate is cheating or not. A candidate is considered to be cheating if there’s a mobile phone, headphones, crowd around, etc. &lt;/p&gt; &lt;p&gt;I was able to fine tune Qwen using Gemini annotated images: ~500 image per label (I am considering this a multi label classification problem) and a LLM might not be the best way to go about it. Using SFT, I am using a &amp;lt;think&amp;gt; token for reasoning as the expected suffix(thinking_mode is disabled) and then a json output for the conclusion. I had pretty decent success with the base Qwen model, but with fine tuned one the outputs quality have dropped.&lt;/p&gt; &lt;p&gt;A few next steps I am thinking of is: 1. In the trainer module, training loss is most likely token to token match as task is causal output. Changing that to something w a classification head that can give out logits on the json part itself; hence might improve training accuracy. 2. A RL setup as dataset is smol. &lt;/p&gt; &lt;p&gt;Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitrabhat4"&gt; /u/chitrabhat4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8x080/qwen_25_3b_vl_performance_dropped_post_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8x080/qwen_25_3b_vl_performance_dropped_post_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8x080/qwen_25_3b_vl_performance_dropped_post_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T16:22:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l98j75</id>
    <title>Best Practices in RL for Reasoning-Capable LLMs: Insights from Mistral’s Magistral Report</title>
    <updated>2025-06-12T00:15:10+00:00</updated>
    <author>
      <name>/u/seventh_day123</name>
      <uri>https://old.reddit.com/user/seventh_day123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magistral combines PPO-Clip, REINFORCE++-style advantage normalization, and DAPO tricks like Dynamic Sampling into a solid RLHF recipe for reasoning LLMs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://hijkzzz.notion.site/Best-Practices-in-RL-for-Reasoning-Capable-LLMs-Insights-from-Mistral-s-Magistral-Report-210d9a33ecc980a187d5c4cf09807271"&gt;Blog: Best Practices in RL for Reasoning-Capable LLMs: Insights from Mistral’s Magistral Report&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seventh_day123"&gt; /u/seventh_day123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l98j75/best_practices_in_rl_for_reasoningcapable_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l98j75/best_practices_in_rl_for_reasoningcapable_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l98j75/best_practices_in_rl_for_reasoningcapable_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T00:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8wm8v</id>
    <title>Perception Language Models (PLM): 1B, 3B, and 8B VLMs with code and data</title>
    <updated>2025-06-11T16:07:25+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wm8v/perception_language_models_plm_1b_3b_and_8b_vlms/"&gt; &lt;img alt="Perception Language Models (PLM): 1B, 3B, and 8B VLMs with code and data" src="https://external-preview.redd.it/kRxHl4QHoi7JxOqoDPbjwpU5kW4q0t0a7pYH8RdzKEQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6da37c184270b83484344bd81db1d541c7e4b1" title="Perception Language Models (PLM): 1B, 3B, and 8B VLMs with code and data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very cool resource if you're working in the VLM space!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models: &lt;a href="https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498"&gt;https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/facebookresearch/perception_models"&gt;https://github.com/facebookresearch/perception_models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Data: &lt;a href="https://ai.meta.com/datasets/plm-data/"&gt;https://ai.meta.com/datasets/plm-data/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.13180"&gt;https://arxiv.org/pdf/2504.13180&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://video-ord5-3.xx.fbcdn.net/o1/v/t2/f2/m69/AQPbJ1oVsuJarMlUQhsExvmBr4_9_A-n3u3EuumpTflJrPPTmzcrWVtW0DabtOFq5_w144E4V0aCiUf97GG6en6k.mp4?strext=1&amp;amp;_nc_cat=107&amp;amp;_nc_sid=5e9851&amp;amp;_nc_ht=video-ord5-3.xx.fbcdn.net&amp;amp;_nc_ohc=esn7TCrGmRIQ7kNvwExOm9L&amp;amp;efg=eyJ2ZW5jb2RlX3RhZyI6Inhwdl9wcm9ncmVzc2l2ZS5GQUNFQk9PSy4uQzMuMTkyMC5kYXNoX2gyNjQtYmFzaWMtZ2VuMl8xMDgwcCIsInhwdl9hc3NldF9pZCI6MTMyMTU4Nzg0MjI2MDg2MCwidmlfdXNlY2FzZV9pZCI6MTA4MjUsImR1cmF0aW9uX3MiOjcxLCJ1cmxnZW5fc291cmNlIjoid3d3In0%3D&amp;amp;ccb=17-1&amp;amp;vs=7218d2367d39d1b6&amp;amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQkM1U3gwQnpydmFKZWNCQUNNNlJxU0hTZE1ZYnY0R0FBQUYVAALIARIAFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTkRFTlIwVE9WeFZpN3NEQUQ1a18yQlBpc0Y0YnY0R0FBQUYVAgLIARIAKAAYABsCiAd1c2Vfb2lsATEScHJvZ3Jlc3NpdmVfcmVjaXBlATEVAAAm-P2E3sT-2AQVAigCQzMsF0BR2p--dsi0GBpkYXNoX2gyNjQtYmFzaWMtZ2VuMl8xMDgwcBEAdQJlkqkBAA&amp;amp;_nc_zt=28&amp;amp;oh=00_AfOfCNMWdIQf6B_RJS-7eHk5Dxgb8dkTcDYOU-dn3UhMpQ&amp;amp;oe=684F673B"&gt;Video&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wm8v/perception_language_models_plm_1b_3b_and_8b_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wm8v/perception_language_models_plm_1b_3b_and_8b_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T16:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9dns1</id>
    <title>Memory and compute estimation for Fine Tuning LLM</title>
    <updated>2025-06-12T04:40:01+00:00</updated>
    <author>
      <name>/u/TraderBoy</name>
      <uri>https://old.reddit.com/user/TraderBoy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;i want to you the crowd intelligence of this forum, since i have not trained that many llms and this is my first larger project. i looked for resources but there is a lot of contrary information out there:&lt;/p&gt; &lt;p&gt;I have around 1 million samples of 2800 tokens. I am right now trying to finetune a qwen3 8bln model using a h100 gpu with 80gb, flash attention 2 and bfloat16.&lt;/p&gt; &lt;p&gt;since it is a pretty big model, i use lora with rank of 64 and deepspeed. the models supposedly needs around 4days for one epoch. &lt;/p&gt; &lt;p&gt;i have looked in the internet and i have seen that it takes around 1 second for a batchsize of 4 (which i am using). for 1 mln samples and epoch of 3 i get to 200 hours of training. however i see when i am training around 500 hours estimation during the training process.&lt;/p&gt; &lt;p&gt;does anyone here have a good way to calculate and optimize the speed during training? somehow there is not much information out there to estimate the time reliably. maybe i am also doing something wrong and others in this forum have performed similar fine tuning with faster calculation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TraderBoy"&gt; /u/TraderBoy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9dns1/memory_and_compute_estimation_for_fine_tuning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9dns1/memory_and_compute_estimation_for_fine_tuning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9dns1/memory_and_compute_estimation_for_fine_tuning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T04:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8u7wy</id>
    <title>AI Deep Research Explained</title>
    <updated>2025-06-11T14:32:14+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably a lot of you are using deep research on ChatGPT, Perplexity, or Grok to get better and more comprehensive answers to your questions, or data you want to investigate.&lt;/p&gt; &lt;p&gt;But did you ever stop to think how it actually works behind the scenes?&lt;/p&gt; &lt;p&gt;In my latest blog post, I break down the system-level mechanics behind this new generation of research-capable AI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How these models understand what you're really asking&lt;/li&gt; &lt;li&gt;How they decide when and how to search the web or rely on internal knowledge&lt;/li&gt; &lt;li&gt;The ReAct loop that lets them reason step by step&lt;/li&gt; &lt;li&gt;How they craft and execute smart queries&lt;/li&gt; &lt;li&gt;How they verify facts by cross-checking multiple sources&lt;/li&gt; &lt;li&gt;What makes retrieval-augmented generation (RAG) so powerful&lt;/li&gt; &lt;li&gt;And why these systems are more up-to-date, transparent, and accurate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a shift from &amp;quot;look it up&amp;quot; to &amp;quot;figure it out.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the full (not too long) blog post (free to read, no paywall). The link is in the first comment.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8u7wy/ai_deep_research_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8u7wy/ai_deep_research_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8u7wy/ai_deep_research_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T14:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8wty9</id>
    <title>[Tool] rvn-convert: OSS Rust-based SafeTensors to GGUF v3 converter (single-shard, fast, no Python)</title>
    <updated>2025-06-11T16:15:51+00:00</updated>
    <author>
      <name>/u/rvnllm</name>
      <uri>https://old.reddit.com/user/rvnllm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Afternoon,&lt;/p&gt; &lt;p&gt;I built a tool out of frustration after losing hours to failed model conversions. (Seriously launching python tool just to see a failure after 159 tensors and 3 hours)&lt;/p&gt; &lt;p&gt;&lt;code&gt;rvn-convert&lt;/code&gt; is a small Rust utility that memory-maps a HuggingFace &lt;code&gt;safetensors&lt;/code&gt; file and writes a clean, llama.cpp-compatible &lt;code&gt;.gguf&lt;/code&gt; file. No intermediate RAM spikes, no Python overhead, no disk juggling.&lt;/p&gt; &lt;p&gt;Features (v0.1.0)&lt;br /&gt; Single-shard support (for now)&lt;br /&gt; Upcasts BF16 → F32&lt;br /&gt; Embeds &lt;code&gt;tokenizer.json&lt;/code&gt;&lt;br /&gt; Adds BOS/EOS/PAD IDs&lt;br /&gt; GGUF v3 output (tested with LLaMA 3.2)&lt;/p&gt; &lt;p&gt;No multi-shard support (yet)&lt;br /&gt; No quantization&lt;br /&gt; No GGUF v2 / tokenizer model variants&lt;/p&gt; &lt;p&gt;I use this daily in my pipeline; just wanted to share in case it helps others.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/rvnllm/rvn-convert"&gt;https://github.com/rvnllm/rvn-convert&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to feedback or bug reports—this is early but working well so far.&lt;/p&gt; &lt;p&gt;[NOTE: working through some serious bugs, should be fixed within a day (or two max)]&lt;br /&gt; [NOTE: will keep post updated]&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvnllm"&gt; /u/rvnllm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wty9/tool_rvnconvert_oss_rustbased_safetensors_to_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wty9/tool_rvnconvert_oss_rustbased_safetensors_to_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8wty9/tool_rvnconvert_oss_rustbased_safetensors_to_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T16:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l926uy</id>
    <title>Are we hobbyists lagging behind?</title>
    <updated>2025-06-11T19:45:20+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It almost feels like every local project is a variation of another project or an implementation of a project from the big orgs, i.e, notebook LLM, deepsearch, coding agents, etc.&lt;/p&gt; &lt;p&gt;Felt like a year or two ago, hobbyists were also helping to seriously push the envelope. How do we get back to relevancy and being impactful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l926uy/are_we_hobbyists_lagging_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l926uy/are_we_hobbyists_lagging_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l926uy/are_we_hobbyists_lagging_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T19:45:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l97xrq</id>
    <title>Open Source agentic tool/framework to automate codebase workflows</title>
    <updated>2025-06-11T23:46:59+00:00</updated>
    <author>
      <name>/u/Soft-Salamander7514</name>
      <uri>https://old.reddit.com/user/Soft-Salamander7514</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm looking for some open source agentic tool/framework with autonomous agents to automate workflows on my repositories. I tried Aider but it requires way too much human intervention, even just to automate simple tasks, it seems not to be designed for that purpose. I'm also trying OpenHands, it looks good but I don't know if it's the best alternative for my use cases (or maybe someone who knows how to use it better can give me some advice, maybe I'm using it wrong). I am looking for something that really allows me to automate specific workflows on repositories (follow guidelines and rules, accessibility, make large scale changes etc). Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soft-Salamander7514"&gt; /u/Soft-Salamander7514 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l97xrq/open_source_agentic_toolframework_to_automate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l97xrq/open_source_agentic_toolframework_to_automate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l97xrq/open_source_agentic_toolframework_to_automate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T23:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8qh2a</id>
    <title>MNN TaoAvatar: run 3d avatar offline, Android app by alibaba mnn team</title>
    <updated>2025-06-11T11:43:01+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8qh2a/mnn_taoavatar_run_3d_avatar_offline_android_app/"&gt; &lt;img alt="MNN TaoAvatar: run 3d avatar offline, Android app by alibaba mnn team" src="https://external-preview.redd.it/MXhndzBsZmhjYTZmMcCLIwnOvaDL_InTTSsx50yogh0oEgldtT-tbB2eca5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0caab54ad1afed99d7348d23c23ef74933687cfc" title="MNN TaoAvatar: run 3d avatar offline, Android app by alibaba mnn team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/Mnn3dAvatar/README.md#version-001"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/Mnn3dAvatar/README.md#version-001&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/65vyq2fhca6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8qh2a/mnn_taoavatar_run_3d_avatar_offline_android_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8qh2a/mnn_taoavatar_run_3d_avatar_offline_android_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T11:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8oe8g</id>
    <title>Altman on open weight 🤔🤔</title>
    <updated>2025-06-11T09:38:50+00:00</updated>
    <author>
      <name>/u/Mean-Neighborhood-42</name>
      <uri>https://old.reddit.com/user/Mean-Neighborhood-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8oe8g/altman_on_open_weight/"&gt; &lt;img alt="Altman on open weight 🤔🤔" src="https://external-preview.redd.it/GVhfh_dphC644v-zed5GXAiqXgODxyxxMhlNyvcKd5w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6da23f937afc76fa32caeb0f34e23d73d37134ed" title="Altman on open weight 🤔🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vgjy1f9wp96f1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=099a712e08d30554498bd49dcfe18f1bdd63538d"&gt;https://preview.redd.it/vgjy1f9wp96f1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=099a712e08d30554498bd49dcfe18f1bdd63538d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤔🤔🤔🤔&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/sama/status/1932573231199707168?t=yyRBeIHlMyzxkaG1XOP_mQ&amp;amp;s=34"&gt;(21) Sam Altman on X: &amp;quot;we are going to take a little more time with our open-weights model, i.e. expect it later this summer but not june. our research team did something unexpected and quite amazing and we think it will be very very worth the wait, but needs a bit longer.&amp;quot; / X&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mean-Neighborhood-42"&gt; /u/Mean-Neighborhood-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8oe8g/altman_on_open_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8oe8g/altman_on_open_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8oe8g/altman_on_open_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T09:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l93ry3</id>
    <title>LiteRT-LM - (An early version of) A C++ library to efficiently run Gemma-3N across various platform</title>
    <updated>2025-06-11T20:49:26+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l93ry3/litertlm_an_early_version_of_a_c_library_to/"&gt; &lt;img alt="LiteRT-LM - (An early version of) A C++ library to efficiently run Gemma-3N across various platform" src="https://external-preview.redd.it/5ALlCNjnyb01JF5ItOkTd8Y8O1vVBS6s20XdxDI3_Nc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b3264b87600a5991d3d91a3c6ddb8ea5e0e3a35" title="LiteRT-LM - (An early version of) A C++ library to efficiently run Gemma-3N across various platform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-ai-edge/LiteRT-LM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l93ry3/litertlm_an_early_version_of_a_c_library_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l93ry3/litertlm_an_early_version_of_a_c_library_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T20:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9ayep</id>
    <title>Enable AI Agents to join and interact in your meetings</title>
    <updated>2025-06-12T02:14:43+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ayep/enable_ai_agents_to_join_and_interact_in_your/"&gt; &lt;img alt="Enable AI Agents to join and interact in your meetings" src="https://external-preview.redd.it/Nzdwb2lwc3VuZTZmMStdk3R7hZIRW-iC3N5YGOPCKOzNDbFNT3u3Wwxsw2PP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fdea4a9b0105987957a5118841bd17e89a56ec3" title="Enable AI Agents to join and interact in your meetings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, &lt;/p&gt; &lt;p&gt;we've been working on a project called joinly for the last few weeks. After many late nights and lots of energy drinks, we just open-sourced it. The idea is that you can make any browser-based video conference accessible to your AI agents and interact with them in real-time. Think of it at as a connector layer that brings the functionality of your AI agents into your meetings, essentially allowing you to build your own custom meeting assistant. Transcription, function calling etc. all happens locally respecting your privacy. &lt;/p&gt; &lt;p&gt;We made a quick video to show how it works. It's still in the early stages, so expect it to be a bit buggy. However, we think it's very promising! &lt;/p&gt; &lt;p&gt;We'd love to hear your feedback or ideas on what kind of agentic powers you'd enjoy in your meetings. 👉 &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pdsgwnsune6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ayep/enable_ai_agents_to_join_and_interact_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9ayep/enable_ai_agents_to_join_and_interact_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T02:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l98lly</id>
    <title>Privacy implications of sending data to OpenRouter</title>
    <updated>2025-06-12T00:18:22+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you developing applications with LLMs: do you really send your data to a local LLM hosted through OpenRouter? What are the pros and cons of doing that over sending your data to OpenAI/Azure? I'm confused about the practice of taking a local model and then accessing it through a third-party API, it negates many of the benefits of using a local model in the first place.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l98lly/privacy_implications_of_sending_data_to_openrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l98lly/privacy_implications_of_sending_data_to_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l98lly/privacy_implications_of_sending_data_to_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T00:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9cd44</id>
    <title>Mistral.rs v0.6.0 now has full built-in MCP Client support!</title>
    <updated>2025-06-12T03:27:42+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cd44/mistralrs_v060_now_has_full_builtin_mcp_client/"&gt; &lt;img alt="Mistral.rs v0.6.0 now has full built-in MCP Client support!" src="https://external-preview.redd.it/2lLo8jhJSmFII5np0CAVlto_8NREWNjCUymEN6xCnKk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd943325186c8dbc22ef59b4adb920ee2ccdc473" title="Mistral.rs v0.6.0 now has full built-in MCP Client support!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! Just shipped what I think is a game-changer for local LLM workflows: MCP (Model Context Protocol) client support in &lt;a href="https://github.com/EricLBuehler/mistral.rs/"&gt;mistral.rs&lt;/a&gt; (&lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;)! It is built-in and closely integrated, which makes the process of developing MCP-powered apps easy and fast.&lt;/p&gt; &lt;p&gt;You can get &lt;a href="https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/_README.md#installation-from-pypi"&gt;mistralrs via PyPi&lt;/a&gt;, &lt;a href="https://github.com/EricLBuehler/mistral.rs/pkgs/container/mistral.rs"&gt;Docker Containers&lt;/a&gt;, or with &lt;a href="https://github.com/EricLBuehler/mistral.rs?tab=readme-ov-file#installation-and-build"&gt;a local build&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Your models can now automatically connect to external tools and services - file systems, web search, databases, APIs, you name it.&lt;/p&gt; &lt;p&gt;No more manual tool calling setup, no more custom integration code.&lt;/p&gt; &lt;p&gt;Just configure once and your models gain superpowers.&lt;/p&gt; &lt;p&gt;We support all the transport interfaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: Local tools (filesystem, databases, and more)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streamable HTTP and SSE&lt;/strong&gt;: REST APIs, cloud services - Works with &lt;em&gt;any&lt;/em&gt; HTTP MCP server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WebSocket&lt;/strong&gt;: Real-time streaming tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The best part?&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;It just works.&lt;/em&gt;&lt;/strong&gt; Tools are discovered automatically at startup, and support for multiserver, authentication handling, and timeouts are designed to make the experience easy.&lt;/p&gt; &lt;p&gt;I've been testing this extensively and it's incredibly smooth. The Python API feels natural, HTTP server integration is seamless, and the automatic tool discovery means no more maintaining tool registries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Using the MCP support in Python:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2c2v74bt0f6f1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd180e59597f04103b7af5acc03b0983a4d41c04"&gt;https://preview.redd.it/2c2v74bt0f6f1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd180e59597f04103b7af5acc03b0983a4d41c04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use the HTTP server in just 2 steps:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) Create mcp-config.json&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;servers&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;Filesystem Tools&amp;quot;, &amp;quot;source&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;Process&amp;quot;, &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@modelcontextprotocol/server-filesystem&amp;quot;, &amp;quot;.&amp;quot; ] } } ], &amp;quot;auto_register_tools&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2) Start server:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;mistralrs-server --mcp-config mcp-config.json --port 1234 run -m Qwen/Qwen3-4B&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can just use the normal OpenAI API - tools work automatically!&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:1234/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;mistral.rs&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;List files and create hello.txt&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1l9cd44/video/i9ttdu2v0f6f1/player"&gt;https://reddit.com/link/1l9cd44/video/i9ttdu2v0f6f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm excited to see what you create with this 🚀! Let me know what you think.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/MCP_QUICK_START.md"&gt;https://github.com/EricLBuehler/mistral.rs/blob/master/examples/MCP_QUICK_START.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/mistral.rs/tree/master/docs/MCP"&gt;https://github.com/EricLBuehler/mistral.rs/tree/master/docs/MCP&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/mcp_client.py"&gt;https://github.com/EricLBuehler/mistral.rs/blob/master/examples/python/mcp_client.py&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cd44/mistralrs_v060_now_has_full_builtin_mcp_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cd44/mistralrs_v060_now_has_full_builtin_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cd44/mistralrs_v060_now_has_full_builtin_mcp_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T03:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9blur</id>
    <title>[2506.06105] Text-to-LoRA: Instant Transformer Adaption</title>
    <updated>2025-06-12T02:47:41+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.06105"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9blur/250606105_texttolora_instant_transformer_adaption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9blur/250606105_texttolora_instant_transformer_adaption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T02:47:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9b04q</id>
    <title>Local organic rig</title>
    <updated>2025-06-12T02:17:01+00:00</updated>
    <author>
      <name>/u/Both-Indication5062</name>
      <uri>https://old.reddit.com/user/Both-Indication5062</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9b04q/local_organic_rig/"&gt; &lt;img alt="Local organic rig" src="https://preview.redd.it/78c6uej5oe6f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1c646a11de430e9f56e2ed6e1e19a1e1cd01f56" title="Local organic rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;local organic ai rig&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Both-Indication5062"&gt; /u/Both-Indication5062 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/78c6uej5oe6f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9b04q/local_organic_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9b04q/local_organic_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T02:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l97fst</id>
    <title>OpenAI performs KYC to use the latest o3-pro via API</title>
    <updated>2025-06-11T23:24:00+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This afternoon I cobbled together a test-script to mess around with o3-pro. Looked nice, so nice that I came back this evening to give it another go. The OpenAI sdk throws an error in the terminal, prompting me &amp;quot;Your organization must be verified to stream this model.&amp;quot;&lt;/p&gt; &lt;p&gt;Allright, I go to OpenAI platform and lo and behold, a full blown KYC process kicks off, with ID scanning, face scanning, all that shite. Damn, has this gone far. Really hope DeepSeek delivers another blow with R2 to put an end to this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l97fst/openai_performs_kyc_to_use_the_latest_o3pro_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l97fst/openai_performs_kyc_to_use_the_latest_o3pro_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l97fst/openai_performs_kyc_to_use_the_latest_o3pro_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T23:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9demc</id>
    <title>Testing Mac Studio 512 GB, 4 TB SSD, M3 Ultra w 32 cores.</title>
    <updated>2025-06-12T04:25:33+00:00</updated>
    <author>
      <name>/u/Deviad</name>
      <uri>https://old.reddit.com/user/Deviad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9demc/testing_mac_studio_512_gb_4_tb_ssd_m3_ultra_w_32/"&gt; &lt;img alt="Testing Mac Studio 512 GB, 4 TB SSD, M3 Ultra w 32 cores." src="https://b.thumbs.redditmedia.com/YojFokLeZzbNOpT7rZaaYJlRnoALjFvRgIII2E0FV3I.jpg" title="Testing Mac Studio 512 GB, 4 TB SSD, M3 Ultra w 32 cores." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I am running some tests and to be fair, I don't regret it.&lt;br /&gt; Given that I want to learn and sell private AI solutions, and I want to run K8s clusters of agents locally for learning purposes, I think it's a good investment medium/long term.&lt;/p&gt; &lt;p&gt;24 tokens/second for Qwen3 235b, in thinking mode, is totally manageable and anyways that's when you need something complex.&lt;/p&gt; &lt;p&gt;If you use /nothink the response will be finalized in a short amount of time and for tasks like give me the boilerplate code for xyz, it's totally manageable.&lt;/p&gt; &lt;p&gt;Now I am downloading the latest R1, let's see how it goes with that.&lt;/p&gt; &lt;p&gt;Therefore, if you are waiting for M5 whatever, you are just wasting time which you could invest into learning and be there first.&lt;br /&gt; Not to mention the latest news about OpenAI being forced to log requests because of a NY court order being issued after a lawsuit started by The NY Times.&lt;br /&gt; I don't feel good thinking that when I type something into Claude or ChatGPT they may be learning from my questions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qf0zh629df6f1.png?width=1766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ededfa911a12e12a061122f517f2fcd2a13d9f02"&gt;Qwen3 235b MLX w thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tn9gcg0cdf6f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=112db305678d4210ae187e7b172223e605394e1d"&gt;Qwen3 235b MLX w/o thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deviad"&gt; /u/Deviad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9demc/testing_mac_studio_512_gb_4_tb_ssd_m3_ultra_w_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9demc/testing_mac_studio_512_gb_4_tb_ssd_m3_ultra_w_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9demc/testing_mac_studio_512_gb_4_tb_ssd_m3_ultra_w_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T04:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l96ag1</id>
    <title>Chatterbox - open-source SOTA TTS by resemble.ai</title>
    <updated>2025-06-11T22:32:39+00:00</updated>
    <author>
      <name>/u/Otis43</name>
      <uri>https://old.reddit.com/user/Otis43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otis43"&gt; /u/Otis43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l96ag1/chatterbox_opensource_sota_tts_by_resembleai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l96ag1/chatterbox_opensource_sota_tts_by_resembleai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l96ag1/chatterbox_opensource_sota_tts_by_resembleai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T22:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l99pih</id>
    <title>Mistral-Nemotron?</title>
    <updated>2025-06-12T01:13:30+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like Nvidia is hosting a new model but I can't find any information about it on Mistral's website?&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.api.nvidia.com/nim/reference/mistralai-mistral-nemotron"&gt;https://docs.api.nvidia.com/nim/reference/mistralai-mistral-nemotron&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://build.nvidia.com/mistralai/mistral-nemotron/modelcard"&gt;https://build.nvidia.com/mistralai/mistral-nemotron/modelcard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l99pih/mistralnemotron/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l99pih/mistralnemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l99pih/mistralnemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T01:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8pem0</id>
    <title>I finally got rid of Ollama!</title>
    <updated>2025-06-11T10:42:52+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, I decided to move away from Ollama (while still using Open WebUI as frontend), and I actually did it faster and easier than I thought!&lt;/p&gt; &lt;p&gt;Since then, my setup has been (on both Linux and Windows):&lt;/p&gt; &lt;p&gt;llama.cpp or ik_llama.cpp for inference&lt;/p&gt; &lt;p&gt;llama-swap to load/unload/auto-unload models (have a big config.yaml file with all the models and parameters like for think/no_think, etc) &lt;/p&gt; &lt;p&gt;Open Webui as the frontend. In its &amp;quot;workspace&amp;quot; I have all the models (although not needed, because with llama-swap, Open Webui will list all the models in the drop list, but I prefer to use it) configured with the system prompts and so. So I just select whichever I want from the drop list or from the &amp;quot;workspace&amp;quot; and llama-swap loads (or unloads the current one and loads the new one) the model. &lt;/p&gt; &lt;p&gt;No more weird location/names for the models (I now just &amp;quot;wget&amp;quot; from huggingface to whatever folder I want and, if needed, I could even use them with other engines), or other &amp;quot;features&amp;quot; from Ollama. &lt;/p&gt; &lt;p&gt;Big thanks to llama.cpp (as always), ik_llama.cpp, llama-swap and Open Webui! (and huggingface and &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; of course!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8pem0/i_finally_got_rid_of_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8pem0/i_finally_got_rid_of_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8pem0/i_finally_got_rid_of_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T10:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8umf2</id>
    <title>Meta releases V-JEPA 2, the first world model trained on video</title>
    <updated>2025-06-11T14:48:35+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8umf2/meta_releases_vjepa_2_the_first_world_model/"&gt; &lt;img alt="Meta releases V-JEPA 2, the first world model trained on video" src="https://external-preview.redd.it/XYCW87FCFGR0wI2hYDorldwWOlBC0pjIIfGLZhngZC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5011249fce4c8240f4cce7fe71a892b4c008780b" title="Meta releases V-JEPA 2, the first world model trained on video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8umf2/meta_releases_vjepa_2_the_first_world_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8umf2/meta_releases_vjepa_2_the_first_world_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T14:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9cwi5</id>
    <title>Running an LLM on a PS Vita</title>
    <updated>2025-06-12T03:57:31+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cwi5/running_an_llm_on_a_ps_vita/"&gt; &lt;img alt="Running an LLM on a PS Vita" src="https://external-preview.redd.it/MWMwMGd5dnY0ZjZmMdxvYbrJ4twRsyVsSVzosN1N6q8R6lU4U4ntC9uiniMK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f8445568b194107c46a13e005615b84bb24fc2e" title="Running an LLM on a PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After spending some time with my vita I wanted to see if **any** LLM can be ran on it, and it can! I modified llama2.c to have it run on the Vita, with the added capability of downloading the models on device to avoid having to manually transfer model files (which can be deleted too). This was a great way to learn about homebrewing on the Vita, there were a lot of great examples from the VitaSDK team which helped me a lot. If you have a Vita, there is a .vpk compiled in the releases section, check it out!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/callbacked/psvita-llm"&gt;https://github.com/callbacked/psvita-llm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/we6m8zvv4f6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cwi5/running_an_llm_on_a_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9cwi5/running_an_llm_on_a_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T03:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8zssy</id>
    <title>Disney and Universal sue AI image company Midjourney for unlicensed use of Star Wars, The Simpsons and more</title>
    <updated>2025-06-11T18:11:52+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is big! When Disney gets involved, shit is about to hit the fan. &lt;/p&gt; &lt;p&gt;If they come after Midourney, then expect other AI labs trained on similar training data to be hit soon. &lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8zssy/disney_and_universal_sue_ai_image_company/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8zssy/disney_and_universal_sue_ai_image_company/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8zssy/disney_and_universal_sue_ai_image_company/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T18:11:52+00:00</published>
  </entry>
</feed>
