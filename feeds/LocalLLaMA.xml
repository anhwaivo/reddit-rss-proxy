<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-07T21:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jtszhx</id>
    <title>🌙 [MODEL RELEASE] Veiled Calla - A 12B Roleplay Model</title>
    <updated>2025-04-07T18:50:08+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtszhx/model_release_veiled_calla_a_12b_roleplay_model/"&gt; &lt;img alt="🌙 [MODEL RELEASE] Veiled Calla - A 12B Roleplay Model" src="https://preview.redd.it/pufr4vhvkgte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd4fb1625d3ffd02d75332da0d22a1ba10561b5" title="🌙 [MODEL RELEASE] Veiled Calla - A 12B Roleplay Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thrilled to announce the release of &lt;strong&gt;✧ Veiled Calla ✧&lt;/strong&gt;, my roleplay model built on Google's Gemma-3-12b. If you're looking for immersive, emotionally nuanced roleplay with rich descriptive text and mysterious undertones, this might be exactly what you've been searching for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes Veiled Calla Special?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Veiled Calla specializes in creating evocative scenarios where the unspoken is just as important as what's said. The model excels at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Atmospheric storytelling with rich, moonlit scenarios and emotional depth&lt;/li&gt; &lt;li&gt;Character consistency throughout extended narratives&lt;/li&gt; &lt;li&gt;Enigmatic storylines that unfold with natural revelations&lt;/li&gt; &lt;li&gt;Emotional nuance where subtle meanings between characters truly come alive&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Veiled Calla aims to create that perfect balance of description and emotional resonance.&lt;/p&gt; &lt;p&gt;Still very much learning to finetune models so please feel free to provide feedback!&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/soob3123/Veiled-Calla-12B"&gt;https://huggingface.co/soob3123/Veiled-Calla-12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/soob3123/Veiled-Calla-12B-gguf"&gt;https://huggingface.co/soob3123/Veiled-Calla-12B-gguf&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pufr4vhvkgte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtszhx/model_release_veiled_calla_a_12b_roleplay_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtszhx/model_release_veiled_calla_a_12b_roleplay_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtwbt9</id>
    <title>LLM-based TTS explained by a human, a breakdown</title>
    <updated>2025-04-07T21:05:21+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a technical post written by me, so apologies in advance if I lose you.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Autoregressive&lt;/strong&gt; simply means the future is conditioned on the past. Autoregressiveness is a nice property for streaming and thereby lowering latency, because you can predict the next token on the fly, just based on what you have seen so far (as opposed to waiting for the end of a sentence). Most modern transformers/LLMs are autoregressive. Diffusion models are non-autoregressive. BERT is non-autoregressive: the B stands for Bidirectional.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;backbone&lt;/strong&gt; is an (often autoregressive) LLM that does: text tokens input =&amp;gt; acoustic tokens output. An acoustic token is a discrete, compressed representation over some frame of time, which can be decoded later into audio. In some cases, you might also have audio input tokens and/or text output tokens as well.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;neural audio codec&lt;/strong&gt; is an additional model that decodes acoustic tokens to audio. These are often trained with a compression/reconstruction objective and have various sample rates, codebook sizes, token resolutions (how many tokens per second), and so on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression/reconstruction objective&lt;/strong&gt; means: You have some audio, you &lt;strong&gt;encode&lt;/strong&gt; it into discrete acoustic tokens, then you &lt;strong&gt;decode&lt;/strong&gt; it back into audio. For any given codebook size / token resolution (aka &lt;strong&gt;compression&lt;/strong&gt;), you want to maximize &lt;strong&gt;reconstruction&lt;/strong&gt;, i.e. recover as much original signal as possible. This is a straightforward and easy objective because when you're training such a neural audio codec, you don't need text labels, you can just do it with raw audio.&lt;/li&gt; &lt;li&gt;There are many pretrained &lt;strong&gt;neural audio codecs&lt;/strong&gt;, some optimized for speech, others for music, and you can choose to freeze the neural audio codec during training. If you are working with a pretrained &amp;amp; frozen neural audio codec, you only need to pack and ship token sequences to your GPU and train the LLM backbone. This makes training faster, easier, and cheaper compared to training on raw audio waveforms.&lt;/li&gt; &lt;li&gt;Recall that LLMs have been cynically called &amp;quot;next token predictors&amp;quot;. But there is no law saying a token must represent text. If you can strap on &lt;strong&gt;encoders&lt;/strong&gt; `(image patch, audio frame, video frame, etc) =&amp;gt; token` and &lt;strong&gt;decoders&lt;/strong&gt; `token =&amp;gt; (image patch, audio frame, video frame, etc)`, then all of a sudden your next-token-predicting LLM gets a lot more powerful and Ghibli-like.&lt;/li&gt; &lt;li&gt;Many people are understandably converging on LLM-based TTS. To highlight this point, I will list some prominent LLM-based TTS released or updated in 2025, in chronological order. This list is best-effort off the top of my head, not exhaustive, and any omissions are either me not knowing or remembering that a particular TTS is LLM-based.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Backbone&lt;/th&gt; &lt;th align="left"&gt;Neural Audio Codec&lt;/th&gt; &lt;th align="left"&gt;Date&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/HKUSTAudio/llasa-679b87dbd06ac556cc0e0f44"&gt;Llasa&lt;/a&gt; (CC-BY-NC)&lt;/td&gt; &lt;td align="left"&gt;Llama &lt;a href="https://huggingface.co/HKUSTAudio/Llasa-1B"&gt;1B&lt;/a&gt; / &lt;a href="https://huggingface.co/HKUSTAudio/Llasa-3B"&gt;3B&lt;/a&gt; / &lt;a href="https://huggingface.co/HKUSTAudio/Llasa-8B"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/HKUSTAudio/xcodec2"&gt;XCodec2&lt;/a&gt;, 16khz, 800M&lt;/td&gt; &lt;td align="left"&gt;Jan 2025&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/Zyphra/zonos-v01-67ac661c85e1898670823b4f"&gt;Zonos&lt;/a&gt; (Apache 2)&lt;/td&gt; &lt;td align="left"&gt;1.6B &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;Transformer&lt;/a&gt; / &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;SSM&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/descriptinc/descript-audio-codec"&gt;Descript Audio Codec&lt;/a&gt;, 44.1khz, 54M?&lt;/td&gt; &lt;td align="left"&gt;Feb 2025&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CSM (Apache 2)&lt;/td&gt; &lt;td align="left"&gt;Llama &lt;a href="https://huggingface.co/sesame/csm-1b"&gt;1B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kyutai/mimi"&gt;Mimi&lt;/a&gt;, 12.5khz?, ~100M?&lt;/td&gt; &lt;td align="left"&gt;Mar 2025&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/canopylabs/orpheus-tts-67d9ea3f6c05a941c06ad9d2"&gt;Orpheus&lt;/a&gt; (Apache 2)&lt;/td&gt; &lt;td align="left"&gt;Llama &lt;a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft"&gt;3B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/hubertsiuzdak/snac"&gt;SNAC&lt;/a&gt;, 24khz, 20M&lt;/td&gt; &lt;td align="left"&gt;Mar 2025&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Oute (CC-BY-NC-SA)&lt;/td&gt; &lt;td align="left"&gt;Llama &lt;a href="https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B"&gt;1B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ibm-research/DAC.speech.v1.0"&gt;IBM-DAC&lt;/a&gt;, 24khz, 54M?&lt;/td&gt; &lt;td align="left"&gt;Apr 2025&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;There are almost certainly more LLM-based TTS, such as Fish, Spark, Index, etc etc, but I couldn't be bothered to look up the parameter counts and neural audio codec being used. Authors should consider making parameter counts and component details more prominent in their model cards. Feel free to also Do Your Own Research.&lt;/li&gt; &lt;li&gt;Interestingly, none of these guys are using the exact same Neural Audio Codec, which implies disagreement in the TTS community over which codec to use.&lt;/li&gt; &lt;li&gt;The Seahawks should have ran the ball, and at least some variant of Llama 4 should have been able to predict audio tokens.&lt;/li&gt; &lt;li&gt;Despite the table being scoped to 2025, LLM-based TTS dates back to Tortoise in 2022 by James Betker, who I think is now at OpenAI. See &lt;a href="https://nonint.com/2022/04/25/tortoise-architectural-design-doc/"&gt;Tortoise Design Doc&lt;/a&gt;. There could be LLM-based TTS before Tortoise, but I'm just not well-read on the history.&lt;/li&gt; &lt;li&gt;That said, I think we are still in very the nascent stages of LLM-based TTS. The fact that established LLM players like Meta and DeepSeek have not yet put out LLM-based TTS even though I think they could and should be able to, means the sky is still the limit.&lt;/li&gt; &lt;li&gt;If ElevenLabs were a publicly traded company, one gameplan for DeepSeek could be: Take out short positions on ElevenLabs, use DeepSeek whale magic to train a cracked LLM-based TTS model (possibly a SOTA Neural Audio Codec to go along with it), then drop open weights. To be clear, I hear ElevenLabs is currently one of the rare profitable AI companies, but they might need to play more defense as better open models emerge and the &amp;quot;sauce&amp;quot; is not quite as secret as it once was.&lt;/li&gt; &lt;li&gt;Hyperscalers are also doing/upgrading their LLM-based TTS offerings. A couple weeks ago, Google dropped &lt;a href="https://cloud.google.com/text-to-speech/docs/chirp3-hd"&gt;Chirp3 HD&lt;/a&gt; voices, and around that time Azure also dropped &lt;a href="https://techcommunity.microsoft.com/blog/azure-ai-services-blog/march-2025-azure-ai-speech%E2%80%99s-hd-voices-are-generally-available-and-more/4398951"&gt;Dragon HD&lt;/a&gt; voices. Both are almost certainly LLM-based.&lt;/li&gt; &lt;li&gt;Conversational / multi-speaker / podcast generation usually implies either or both (1) a shift in training data and/or (2) conditioning on audio input as well as text input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is both a resource and a discussion. The above statements are just one (hopefully informed) guy's opinion. Anything can be challenged, corrected or expanded upon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwbt9/llmbased_tts_explained_by_a_human_a_breakdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwbt9/llmbased_tts_explained_by_a_human_a_breakdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwbt9/llmbased_tts_explained_by_a_human_a_breakdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T21:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtc5ok</id>
    <title>We may see DeepSeek R2 this week, that will explain the Llama4 Saturday launch.</title>
    <updated>2025-04-07T03:38:54+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not going to be a good week for LLama millionaire engineers. The Benchs they showed seem like complete lies at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T03:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt85zy</id>
    <title>I'd like to see Zuckerberg try to replace mid level engineers with Llama 4</title>
    <updated>2025-04-07T00:01:40+00:00</updated>
    <author>
      <name>/u/NoConcert8847</name>
      <uri>https://old.reddit.com/user/NoConcert8847</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;He said this in January: &lt;a href="https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/"&gt;https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoConcert8847"&gt; /u/NoConcert8847 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtwcdo</id>
    <title>Guide for quickly setting up aider, QwQ and Qwen Coder</title>
    <updated>2025-04-07T21:06:01+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a guide for setting up a a 100% local coding co-pilot setup with QwQ as as an architect model and qwen Coder as the editor. The focus for the guide is on the trickiest part which is configuring everything to work together.&lt;/p&gt; &lt;p&gt;This guide uses QwQ and qwen Coder 32B as those can fit in a 24GB GPU. This guide uses llama-swap so QwQ and Qwen Coder are swapped in and our during aider's architect or editing phases. The guide also has settings for dual 24GB GPUs where both models can be used with swapping.&lt;/p&gt; &lt;p&gt;The original version is here: &lt;a href="https://github.com/mostlygeek/llama-swap/tree/main/examples/aider-qwq-coder"&gt;https://github.com/mostlygeek/llama-swap/tree/main/examples/aider-qwq-coder&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Here's what you you need:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;aider - &lt;a href="https://aider.chat/docs/install.html"&gt;installation docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama-server - &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;download latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama-swap - &lt;a href="https://github.com/mostlygeek/llama-swap/releases"&gt;download latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF"&gt;QwQ 32B&lt;/a&gt; and &lt;a href="https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF"&gt;Qwen Coder 2.5 32B&lt;/a&gt; models&lt;/li&gt; &lt;li&gt;24GB VRAM video card&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Running aider&lt;/h2&gt; &lt;p&gt;The goal is getting this command line to work:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh aider --architect \ --no-show-model-warnings \ --model openai/QwQ \ --editor-model openai/qwen-coder-32B \ --model-settings-file aider.model.settings.yml \ --openai-api-key &amp;quot;sk-na&amp;quot; \ --openai-api-base &amp;quot;http://10.0.1.24:8080/v1&amp;quot; \ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Set &lt;code&gt;--openai-api-base&lt;/code&gt; to the IP and port where your llama-swap is running.&lt;/p&gt; &lt;h2&gt;Create an aider model settings file&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;aider.model.settings.yml&lt;/h1&gt; &lt;h1&gt;!!! important: model names must match llama-swap configuration names !!!&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/QwQ&amp;quot; edit_format: diff extra_params: max_tokens: 16384 top_p: 0.95 top_k: 40 presence_penalty: 0.1 repetition_penalty: 1 num_ctx: 16384 use_temperature: 0.6 reasoning_tag: think weak_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot; editor_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/qwen-coder-32B&amp;quot; edit_format: diff extra_params: max_tokens: 16384 top_p: 0.8 top_k: 20 repetition_penalty: 1.05 use_temperature: 0.6 reasoning_tag: think editor_edit_format: editor-diff editor_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot; ```&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;llama-swap configuration&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;config.yaml&lt;/h1&gt; &lt;h1&gt;The parameters are tweaked to fit model+context into 24GB VRAM GPUs&lt;/h1&gt; &lt;p&gt;models: &amp;quot;qwen-coder-32B&amp;quot;: proxy: &amp;quot;&lt;a href="http://127.0.0.1:8999"&gt;http://127.0.0.1:8999&lt;/a&gt;&amp;quot; cmd: &amp;gt; /path/to/llama-server --host 127.0.0.1 --port 8999 --flash-attn --slots --ctx-size 16000 --cache-type-k q8_0 --cache-type-v q8_0 -ngl 99 --model /path/to/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf&lt;/p&gt; &lt;p&gt;&amp;quot;QwQ&amp;quot;: proxy: &amp;quot;&lt;a href="http://127.0.0.1:9503"&gt;http://127.0.0.1:9503&lt;/a&gt;&amp;quot; cmd: &amp;gt; /path/to/llama-server --host 127.0.0.1 --port 9503 --flash-attn --metrics--slots --cache-type-k q8_0 --cache-type-v q8_0 --ctx-size 32000 --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 --min-p 0.01 --top-k 40 --top-p 0.95 -ngl 99 --model /mnt/nvme/models/bartowski/Qwen_QwQ-32B-Q4_K_M.gguf ```&lt;/p&gt; &lt;h2&gt;Advanced, Dual GPU Configuration&lt;/h2&gt; &lt;p&gt;If you have &lt;em&gt;dual 24GB GPUs&lt;/em&gt; you can use llama-swap profiles to avoid swapping between QwQ and Qwen Coder.&lt;/p&gt; &lt;p&gt;In llama-swap's configuration file:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;add a &lt;code&gt;profiles&lt;/code&gt; section with &lt;code&gt;aider&lt;/code&gt; as the profile name&lt;/li&gt; &lt;li&gt;using the &lt;code&gt;env&lt;/code&gt; field to specify the GPU IDs for each model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;config.yaml&lt;/h1&gt; &lt;h1&gt;Add a profile for aider&lt;/h1&gt; &lt;p&gt;profiles: aider: - qwen-coder-32B - QwQ&lt;/p&gt; &lt;p&gt;models: &amp;quot;qwen-coder-32B&amp;quot;: # manually set the GPU to run on env: - &amp;quot;CUDA_VISIBLE_DEVICES=0&amp;quot; proxy: &amp;quot;&lt;a href="http://127.0.0.1:8999"&gt;http://127.0.0.1:8999&lt;/a&gt;&amp;quot; cmd: /path/to/llama-server ...&lt;/p&gt; &lt;p&gt;&amp;quot;QwQ&amp;quot;: # manually set the GPU to run on env: - &amp;quot;CUDA_VISIBLE_DEVICES=1&amp;quot; proxy: &amp;quot;&lt;a href="http://127.0.0.1:9503"&gt;http://127.0.0.1:9503&lt;/a&gt;&amp;quot; cmd: /path/to/llama-server ... ```&lt;/p&gt; &lt;p&gt;Append the profile tag, &lt;code&gt;aider:&lt;/code&gt;, to the model names in the model settings file&lt;/p&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;aider.model.settings.yml&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/aider:QwQ&amp;quot; weak_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot; editor_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/aider:qwen-coder-32B&amp;quot; editor_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot; ```&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run aider with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh $ aider --architect \ --no-show-model-warnings \ --model openai/aider:QwQ \ --editor-model openai/aider:qwen-coder-32B \ --config aider.conf.yml \ --model-settings-file aider.model.settings.yml --openai-api-key &amp;quot;sk-na&amp;quot; \ --openai-api-base &amp;quot;http://10.0.1.24:8080/v1&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T21:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtlmld</id>
    <title>Qwen 3 due this week?</title>
    <updated>2025-04-07T13:46:47+00:00</updated>
    <author>
      <name>/u/OnceMoreOntoTheBrie</name>
      <uri>https://old.reddit.com/user/OnceMoreOntoTheBrie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After what looks like a failure so far for llama 4, I am even more excited by what qwen 3 might offer. I believe they said the second week of April, which is now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OnceMoreOntoTheBrie"&gt; /u/OnceMoreOntoTheBrie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlmld/qwen_3_due_this_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlmld/qwen_3_due_this_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlmld/qwen_3_due_this_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T13:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtfm44</id>
    <title>Meta Leaker refutes the training on test set claim</title>
    <updated>2025-04-07T07:29:49+00:00</updated>
    <author>
      <name>/u/ElectronicCress3132</name>
      <uri>https://old.reddit.com/user/ElectronicCress3132</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"&gt; &lt;img alt="Meta Leaker refutes the training on test set claim" src="https://preview.redd.it/m4roxa1z7dte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83d5ddfa391a2107eb5ac49fe7ec99d2b2a5c5c3" title="Meta Leaker refutes the training on test set claim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectronicCress3132"&gt; /u/ElectronicCress3132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m4roxa1z7dte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T07:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtsotw</id>
    <title>Ollama 0.6.5 adds support for Mistral-Small:24b-3.1-2503 and also makes it the default model pull for “mistral-small” going forward.</title>
    <updated>2025-04-07T18:37:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not super huge news for a lot of folks I’m sure, but for those of us using Ollama who were waiting for Mistral-Small:24b-3.1-2503, this is a pretty big deal. This also added vision support for this model which we had been waiting on. &lt;/p&gt; &lt;p&gt;Here’s the Ollama Model page for the new release:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/mistral-small3.1"&gt;https://ollama.com/library/mistral-small3.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here’s the release page for 0.6.5:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases"&gt;https://github.com/ollama/ollama/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtsotw/ollama_065_adds_support_for_mistralsmall24b312503/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtsotw/ollama_065_adds_support_for_mistralsmall24b312503/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtsotw/ollama_065_adds_support_for_mistralsmall24b312503/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtudz4</id>
    <title>Benchmark update: Llama 4 is now the top open source OCR model</title>
    <updated>2025-04-07T19:45:51+00:00</updated>
    <author>
      <name>/u/Tylernator</name>
      <uri>https://old.reddit.com/user/Tylernator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tylernator"&gt; /u/Tylernator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtudz4/benchmark_update_llama_4_is_now_the_top_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtudz4/benchmark_update_llama_4_is_now_the_top_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T19:45:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt8yug</id>
    <title>“Serious issues in Llama 4 training. I Have Submitted My Resignation to GenAI“</title>
    <updated>2025-04-07T00:43:36+00:00</updated>
    <author>
      <name>/u/rrryougi</name>
      <uri>https://old.reddit.com/user/rrryougi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original post is in Chinese that can be found &lt;a href="https://www.1point3acres.com/bbs/thread-1122600-1-1.html"&gt;here&lt;/a&gt;. Please take the following with a grain of salt.&lt;/p&gt; &lt;p&gt;Content:&lt;/p&gt; &lt;p&gt;Despite repeated training efforts, the internal model's performance still falls short of open-source SOTA benchmarks, lagging significantly behind. Company leadership suggested blending test sets from various benchmarks during the post-training process, aiming to meet the targets across various metrics and produce a &amp;quot;presentable&amp;quot; result. Failure to achieve this goal by the end-of-April deadline would lead to dire consequences. Following yesterday’s release of Llama 4, many users on X and Reddit have already reported extremely poor real-world test results. &lt;/p&gt; &lt;p&gt;As someone currently in academia, I find this approach utterly unacceptable. Consequently, I have submitted my resignation and explicitly requested that my name be excluded from the technical report of Llama 4. Notably, the VP of AI at Meta also resigned for similar reasons. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rrryougi"&gt; /u/rrryougi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtweei</id>
    <title>Llama4 support is merged into llama.cpp!</title>
    <updated>2025-04-07T21:08:20+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"&gt; &lt;img alt="Llama4 support is merged into llama.cpp!" src="https://external-preview.redd.it/3YQJt4uj8I_2zhsxnK4qdxOjQqnMLiZv2IVMx9xShfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=553ce6c7a84f765dccbcc4f23b18ea664ca13750" title="Llama4 support is merged into llama.cpp!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12791"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T21:08:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtoctm</id>
    <title>Wondering how it would be without Qwen</title>
    <updated>2025-04-07T15:42:28+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am really wondering how the « open » scene would be without that team, Qwen2.5 coder, QwQ, Qwen2.5 VL are parts of my main goto, they always release with quantized models, there is no mess during releases…&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtoctm/wondering_how_it_would_be_without_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtoctm/wondering_how_it_would_be_without_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtoctm/wondering_how_it_would_be_without_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T15:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtjris</id>
    <title>I believe this is the first properly-trained multi-turn RP with reasoning model</title>
    <updated>2025-04-07T12:13:53+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtjris/i_believe_this_is_the_first_properlytrained/"&gt; &lt;img alt="I believe this is the first properly-trained multi-turn RP with reasoning model" src="https://external-preview.redd.it/d84kJOigyOx8M2I0kJHv3kMb0sZFeN-O5golQM9rmrE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f770c6926b6e4357e1747fc2a5b6bd9979c041d2" title="I believe this is the first properly-trained multi-turn RP with reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/QwQ-32B-ArliAI-RpR-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtjris/i_believe_this_is_the_first_properlytrained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtjris/i_believe_this_is_the_first_properlytrained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T12:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtm289</id>
    <title>0 Temperature is all you need!</title>
    <updated>2025-04-07T14:05:52+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm289/0_temperature_is_all_you_need/"&gt; &lt;img alt="0 Temperature is all you need!" src="https://preview.redd.it/igefzjfq6fte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb837a0ef683fa6908505f324474540d8dcac97f" title="0 Temperature is all you need!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;“For Llama model results, we report 0 shot evaluation with temperature = O” For kicks I set my temperature to -1 and it’s performing better than GPT4.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igefzjfq6fte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm289/0_temperature_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm289/0_temperature_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jttq00</id>
    <title>Dream 7B (the diffusion reasoning model) no longer has a blank GitHub.</title>
    <updated>2025-04-07T19:19:35+00:00</updated>
    <author>
      <name>/u/Creative-robot</name>
      <uri>https://old.reddit.com/user/Creative-robot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/HKUNLP/Dream"&gt;https://github.com/HKUNLP/Dream&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanted to provide this because some people were disappointed that the code wasn’t available. It appears to be available now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-robot"&gt; /u/Creative-robot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T19:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt7hlc</id>
    <title>Meta's Llama 4 Fell Short</title>
    <updated>2025-04-06T23:27:19+00:00</updated>
    <author>
      <name>/u/Rare-Site</name>
      <uri>https://old.reddit.com/user/Rare-Site</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt; &lt;img alt="Meta's Llama 4 Fell Short" src="https://preview.redd.it/rwrke16rpate1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97aeb81ed981cdfb9ae0bb78f2027199731a69a" title="Meta's Llama 4 Fell Short" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama 4 Scout and Maverick left me really disappointed. It might explain why Joelle Pineau, Meta’s AI research lead, just got fired. Why are these models so underwhelming? My armchair analyst intuition suggests it’s partly the tiny expert size in their mixture-of-experts setup. 17B parameters? Feels small these days.&lt;/p&gt; &lt;p&gt;Meta’s struggle proves that having all the GPUs and Data in the world doesn’t mean much if the ideas aren’t fresh. Companies like DeepSeek, OpenAI etc. show real innovation is what pushes AI forward. You can’t just throw resources at a problem and hope for magic. Guess that’s the tricky part of AI, it’s not just about brute force, but brainpower too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Site"&gt; /u/Rare-Site &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwrke16rpate1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T23:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jth72b</id>
    <title>OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages</title>
    <updated>2025-04-07T09:30:12+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"&gt; &lt;img alt="OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages" src="https://external-preview.redd.it/Z2xta3YzNmRzZHRlMQNkNwCzMCfIUGDv1ghIC3hRjQCjH8UUjqUCJCni6W9u.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df30707fd170e60bfac0344bd18a6f9e6c533729" title="OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bw9ii56dsdte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T09:30:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtslj9</id>
    <title>Official statement from meta</title>
    <updated>2025-04-07T18:34:05+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"&gt; &lt;img alt="Official statement from meta" src="https://preview.redd.it/4beb8fwkigte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcac364db46647580264300f0e485fe1826ca23c" title="Official statement from meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4beb8fwkigte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtejzj</id>
    <title>Llama 4 is open - unless you are in the EU</title>
    <updated>2025-04-07T06:13:05+00:00</updated>
    <author>
      <name>/u/Feeling_Dog9493</name>
      <uri>https://old.reddit.com/user/Feeling_Dog9493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you guys read the LLaMA 4 license? EU based entities are not restricted - they are banned. AI Geofencing has arrived:&lt;/p&gt; &lt;p&gt;“You may not use the Llama Materials if you are… domiciled in a country that is part of the European Union.”&lt;/p&gt; &lt;p&gt;No exceptions. Not for research, not for personal use, not even through a US-based cloud provider. If your org is legally in the EU, you’re legally locked out.&lt;/p&gt; &lt;p&gt;And that’s just the start: • Must use Meta’s branding (“LLaMA” must be in any derivative’s name) • Attribution is required (“Built with LLaMA”) • No field-of-use freedom • No redistribution freedom • Not OSI-compliant = not open source&lt;/p&gt; &lt;p&gt;This isn’t “open” in any meaningful sense—it’s corporate-controlled access dressed up in community language. The likely reason? Meta doesn’t want to deal with the EU AI Act’s transparency and risk requirements, so it’s easier to just draw a legal border around the entire continent.&lt;/p&gt; &lt;p&gt;This move sets a dangerous precedent. If region-locking becomes the norm, we’re headed for a fractured, privilege-based AI landscape—where your access to foundational tools depends on where your HQ is.&lt;/p&gt; &lt;p&gt;For EU devs, researchers, and startups: You’re out. For the open-source community: This is the line in the sand.&lt;/p&gt; &lt;p&gt;Real “open” models like DeepSeek and Mistral deserve more attention than ever—because this? This isn’t it.&lt;/p&gt; &lt;p&gt;What’s your take—are you switching models? Ignoring the license? Holding out hope for change?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feeling_Dog9493"&gt; /u/Feeling_Dog9493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T06:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtm56c</id>
    <title>"10m context window" Well, doesn't look good for Llama 4.</title>
    <updated>2025-04-07T14:09:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"&gt; &lt;img alt="&amp;quot;10m context window&amp;quot; Well, doesn't look good for Llama 4." src="https://preview.redd.it/xbucodud7fte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a0b3770d582753b384f5d1576fa77960022bb4f" title="&amp;quot;10m context window&amp;quot; Well, doesn't look good for Llama 4." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hmmm😢😢&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xbucodud7fte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtlymx</id>
    <title>Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models</title>
    <updated>2025-04-07T14:01:34+00:00</updated>
    <author>
      <name>/u/babydriver808</name>
      <uri>https://old.reddit.com/user/babydriver808</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"&gt; &lt;img alt="Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models" src="https://b.thumbs.redditmedia.com/VPjvvVejK3-Q7SfRk1d98G-LK9eBxXjm5dqQe5ogFqw.jpg" title="Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liquid neural networks are awesome - they change how that &amp;quot;neuron black box&amp;quot; connects over time given its past experiences, emulating the human brain in relating concepts and how it changes our perspective. &lt;/p&gt; &lt;p&gt;They are great at time series forecasting like weather and analytics, however the idea is to do it on a transformers model, making it acquire neuroplasticity at token prediction - and as we know its very expensive to train a whole model from scratch. &lt;/p&gt; &lt;p&gt;I figured we could splice in a new neuron layer inside the model's networks right between the transformers layer and the output projection layer that actually predicts the tokens. This way the thought would have &amp;quot;influences&amp;quot; of past experiences for every token generated aka. during the entire line of thinking, making the model acquire a &amp;quot;personality in behavior&amp;quot; over time. &lt;/p&gt; &lt;p&gt;The vector embeddings from the transformers layer are mean-pooled and &amp;quot;sprayed&amp;quot; with past memories changing the way each token is generated, influencing the meaning and therefore choice of words in the vocab space. This neural “Spray Layer” also remembers the paths it took before, blending new input with previous ones and gradually evolving its internal understanding of concepts over time. &lt;/p&gt; &lt;p&gt;It won’t guarantee exact word outputs, but it will make the model lean into certain concepts the more it interacts. For example: Tell it you love dogs, and over time, the model will start leaning toward dog-related kindness, loyalty, and fuzziness in its tone and direction. More teste are yet to be done and I know there is a cold start problem, finding the sweet spot is key. &lt;/p&gt; &lt;p&gt;This is quite fascinating, especially because we don't know exactly what happen at the model's transformer neuron level and how it makes the connections, but hacking it like this is interesting to watch. &lt;/p&gt; &lt;p&gt;I called this technique &amp;quot;Neural Graffiti&amp;quot;, and it is free and open for everyone.&lt;/p&gt; &lt;p&gt;Try the demo and give it a star on the github repo! - &lt;a href="https://github.com/babycommando/neuralgraffiti"&gt;babycommando/neuralgraffiti&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babydriver808"&gt; /u/babydriver808 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jtlymx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtmy7p</id>
    <title>Qwen3/Qwen3MoE support merged to vLLM</title>
    <updated>2025-04-07T14:44:12+00:00</updated>
    <author>
      <name>/u/tkon3</name>
      <uri>https://old.reddit.com/user/tkon3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vLLM &lt;a href="https://github.com/vllm-project/vllm/pull/15289"&gt;merged&lt;/a&gt; two Qwen3 architectures today.&lt;/p&gt; &lt;p&gt;You can find a mention to &lt;code&gt;Qwen/Qwen3-8B&lt;/code&gt; and &lt;code&gt;Qwen/Qwen3-MoE-15B-A2B&lt;/code&gt;at this &lt;a href="https://github.com/YamPengLi/vllm/blob/25e4a80fe2ef7e2c6fa4c43bfc0402c17303b589/docs/source/models/supported_models.md"&gt;page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Interesting week in perspective.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tkon3"&gt; /u/tkon3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtkb3p</id>
    <title>So what happened to Llama 4, which trained on 100,000 H100 GPUs?</title>
    <updated>2025-04-07T12:42:24+00:00</updated>
    <author>
      <name>/u/sunshinecheung</name>
      <uri>https://old.reddit.com/user/sunshinecheung</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt; &lt;img alt="So what happened to Llama 4, which trained on 100,000 H100 GPUs?" src="https://a.thumbs.redditmedia.com/8BEqw6mD4ZzZP0i0wXUPQrGPrNe0wHwiWVuao_uM-w8.jpg" title="So what happened to Llama 4, which trained on 100,000 H100 GPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zue0vixknete1.png?width=1040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9950ec242a827a86d5a2cc1d01da439839edd464"&gt;https://preview.redd.it/zue0vixknete1.png?width=1040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9950ec242a827a86d5a2cc1d01da439839edd464&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 4 was trained using 100,000 H100 GPUs. However, even though Deepseek does not have as so much data and GPUs as Meta, it could manage to achieve a better performance (like DeepSeek-V3-0324)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fvq6jcysnete1.png?width=1107&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa1cdf1e955d208d2250e706d94efade06ae942"&gt;https://preview.redd.it/fvq6jcysnete1.png?width=1107&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa1cdf1e955d208d2250e706d94efade06ae942&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yann LeCun: FAIR is working on the next generation of AI architectures beyond Auto-Regressive LLMs. &lt;/p&gt; &lt;p&gt;But now, it seems that Meta's leading edge is diminishing, and smaller open-source model have been surpassed by Qwen.(Qwen3 is coming...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunshinecheung"&gt; /u/sunshinecheung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T12:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jts2hq</id>
    <title>"...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in..."</title>
    <updated>2025-04-07T18:12:50+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"&gt; &lt;img alt="&amp;quot;...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in...&amp;quot;" src="https://external-preview.redd.it/Ux91NoXxBQFaUsZ3lSBPCQvGxPEHWAd01W7Kjotd_O8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=860e92dd619c1a16dfb6d356a8fb6e2167fe5e31" title="&amp;quot;...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in...&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We're glad to start getting Llama 4 in all your hands. We're already hearing lots of great results people are getting with these models. &lt;/p&gt; &lt;p&gt;That said, we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in. We'll keep working through our bug fixes and onboarding partners. &lt;/p&gt; &lt;p&gt;We've also heard claims that we trained on test sets -- that's simply not true and we would never do that. Our best understanding is that the variable quality people are seeing is due to needing to stabilize implementations. &lt;/p&gt; &lt;p&gt;We believe the Llama 4 models are a significant advancement and we're looking forward to working with the community to unlock their value.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ahmad_Al_Dahle/status/1909302532306092107"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtnryp</id>
    <title>Must have 5–8+ years experience with ChatGPT and Microsoft Copilot</title>
    <updated>2025-04-07T15:18:03+00:00</updated>
    <author>
      <name>/u/Leading-Leading6718</name>
      <uri>https://old.reddit.com/user/Leading-Leading6718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt; &lt;img alt="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" src="https://preview.redd.it/v4w6g5cohfte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0731325c97d9402fe56370d94bfeb59e80729e9c" title="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ah yes, the classic requirement:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;ChatGPT dropped in late 2022.&lt;br /&gt; Copilot showed up in 2023.&lt;br /&gt; APIs? Even newer.&lt;/p&gt; &lt;p&gt;But sure, let me just fire up the time machine real quick.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading-Leading6718"&gt; /u/Leading-Leading6718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4w6g5cohfte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T15:18:03+00:00</published>
  </entry>
</feed>
