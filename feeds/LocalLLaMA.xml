<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-12T03:37:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mnifh1</id>
    <title>NVIDIA Expands Its RTX PRO ‘Blackwell’ Workstation GPU Lineup With Two New Variants at SIGGRAPH 2025: RTX PRO 4000 SFF and RTX PRO 2000</title>
    <updated>2025-08-11T16:55:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnifh1/nvidia_expands_its_rtx_pro_blackwell_workstation/"&gt; &lt;img alt="NVIDIA Expands Its RTX PRO ‘Blackwell’ Workstation GPU Lineup With Two New Variants at SIGGRAPH 2025: RTX PRO 4000 SFF and RTX PRO 2000" src="https://external-preview.redd.it/k9pHhlT5Zo2BU9fRiistXexZxTyPMK7ADnLJ0PIOUlg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4653638ffcc20cb42b31a4fd95851a5d42776674" title="NVIDIA Expands Its RTX PRO ‘Blackwell’ Workstation GPU Lineup With Two New Variants at SIGGRAPH 2025: RTX PRO 4000 SFF and RTX PRO 2000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-expands-its-rtx-pro-blackwell-workstation-gpu-lineup-with-two-new-variants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnifh1/nvidia_expands_its_rtx_pro_blackwell_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnifh1/nvidia_expands_its_rtx_pro_blackwell_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn9z3d</id>
    <title>Qwen3-30B-A3B-Instruct-2507@Q8_0 vs GLM-4.5-Air@UD-Q2_K_XL</title>
    <updated>2025-08-11T11:05:27+00:00</updated>
    <author>
      <name>/u/DanielusGamer26</name>
      <uri>https://old.reddit.com/user/DanielusGamer26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With this configuration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Ryzen 5900x &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;RTX 5060Ti 16GB &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB DDR4 RAM @ 3600MHz &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NVMe drive with ~2GB/s read speed when models are offloaded to disk &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Should I use &lt;code&gt;Qwen3-30B-A3B-Instruct-2507-Q8_0&lt;/code&gt; or &lt;code&gt;GLM-4.5-Air-UD-Q2_K_XL&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;Considering I typically use no more than 16k of context and usually ask trivia-style questions while studying—requesting explanations of specific concepts with excerpts from books or web research as context. &lt;/p&gt; &lt;p&gt;I know these are models of completely different magnitudes (~100B vs 30B), but they're roughly similar in size (GLM being slightly larger and potentially requiring more disk offloading). Could the Q2_K quantization degrade performance so severely that the smaller, higher-precision Qwen3 model would perform better?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Translated with Qwen3-30B-A3B&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielusGamer26"&gt; /u/DanielusGamer26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T11:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7plv</id>
    <title>Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models</title>
    <updated>2025-08-11T08:46:35+00:00</updated>
    <author>
      <name>/u/Gad_3dart</name>
      <uri>https://old.reddit.com/user/Gad_3dart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt; &lt;img alt="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" src="https://preview.redd.it/8ib7o8elncif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e511e3e8857eff67e6a0c38d17d2fb76fd260819" title="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My friend and I are super excited to share our latest work with you. Recently, we’ve been focusing on improving &lt;strong&gt;multilingual capabilities&lt;/strong&gt;, with a special emphasis on &lt;strong&gt;bilingual French–English&lt;/strong&gt; performance.&lt;/p&gt; &lt;p&gt;As you probably know, English dominates the NLP world, and performance in many other languages can be significantly worse. Our research shows that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It’s possible to close much of the performance gap between English and other languages with proper post-training and a carefully curated dataset. We even achieved, as far as we know, SoTa results for models&amp;lt;2B on several French benchmarks&lt;/li&gt; &lt;li&gt;This can be done &lt;strong&gt;without sacrificing&lt;/strong&gt; high performance in English benchmarks, and can even improve some of them thanks to cross-lingual transfer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To demonstrate this, we’re releasing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-0.6B-Instruct"&gt;Luth-0.6B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-1.7B-Instruct"&gt;Luth-1.7B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/luth-sft"&gt;Luth-SFT dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/scholar"&gt;Scolar dataset&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We go into more detail in our Hugging Face blog post here:&lt;br /&gt; &lt;a href="https://huggingface.co/blog/MaxLSB/luth"&gt;https://huggingface.co/blog/MaxLSB/luth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love feedback, benchmarks, and any multilingual test cases you throw at these models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gad_3dart"&gt; /u/Gad_3dart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ib7o8elncif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnojya</id>
    <title>Open Source LLM Based Cyber Security System</title>
    <updated>2025-08-11T20:41:41+00:00</updated>
    <author>
      <name>/u/tylerni7</name>
      <uri>https://old.reddit.com/user/tylerni7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of resources came out from the recent DARPA AIxCC event at Defcon. Here's some resources from the team I was on! &lt;/p&gt; &lt;p&gt;Source from qualifying round: &lt;a href="https://github.com/theori-io/aixcc-asc-archive"&gt;https://github.com/theori-io/aixcc-asc-archive&lt;/a&gt; Source from finals: &lt;a href="https://github.com/theori-io/aixcc-afc-archive"&gt;https://github.com/theori-io/aixcc-afc-archive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's configured to use non-local models, but it totally works with local ones :)&lt;/p&gt; &lt;p&gt;For other actual info vs just source code (agent trajectories, blog/technical posts) &lt;a href="https://theori-io.github.io/aixcc-public/"&gt;https://theori-io.github.io/aixcc-public/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Other team repos are also available &lt;a href="https://archive.aicyberchallenge.com/"&gt;https://archive.aicyberchallenge.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tylerni7"&gt; /u/tylerni7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnojya/open_source_llm_based_cyber_security_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnojya/open_source_llm_based_cyber_security_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnojya/open_source_llm_based_cyber_security_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T20:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnp3ry</id>
    <title>If GPUs had slots like RAM DIMMs, could we add more VRAM?</title>
    <updated>2025-08-11T21:02:38+00:00</updated>
    <author>
      <name>/u/Diegam</name>
      <uri>https://old.reddit.com/user/Diegam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why doesn’t this happen? Is it just a business model choice, or is there some technical limitation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diegam"&gt; /u/Diegam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp3ry/if_gpus_had_slots_like_ram_dimms_could_we_add/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp3ry/if_gpus_had_slots_like_ram_dimms_could_we_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp3ry/if_gpus_had_slots_like_ram_dimms_could_we_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T21:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnin8k</id>
    <title>My beautiful vLLM adventure</title>
    <updated>2025-08-11T17:02:45+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, there was this rant post on vLLM the other day. Seeing as I have some time on my hands and wanting to help the open source community, I decided I'd try documenting the common use cases and proving that, hey, this vLLM thing isn't really *that hard to run*. And I must say, after the tests, I have no idea what you're talking about vLLM being hard to use. Here's how easily I managed to actually run an inference server on it.&lt;/p&gt; &lt;p&gt;First though: hey, let's go for OSS-20B, runs nicely enough on my hardware on llama.cpp, let's see what we get.&lt;/p&gt; &lt;p&gt;Of course, `vllm serve openai/gpt-oss-20b` out of the box would fail, I don't have 12 GB of VRAM (3080 with 10GB of VRAM here plus 24 GB of RAM). I need offloading.&lt;/p&gt; &lt;p&gt;Fortunately, vLLm *does* provide offloading, I know it from my previous fights with it. The setting is `--cpu-offload-gb X`. The behavior is the following: out of the entire model, X GB gets offloaded to CPU and the rest is loaded on the GPU. So if the model has 12GB and you want it to use 7 GB of VRAM, you need `--cpu-offload-gb 5`. Simple math!&lt;/p&gt; &lt;p&gt;Oh yeah, and of course there's `--gpu-memory-utilization`. If your GPU has residual stuff using it, you need to tell vLLM to only use X of the GPU memory or it's gonna crash. &lt;/p&gt; &lt;p&gt;Attempt 2: `vllm serve openai/gpt-oss-20b --gpu-memory-utilization 0.85 --cpu-offload-gb 5`&lt;/p&gt; &lt;p&gt;OOM CRASH&lt;/p&gt; &lt;p&gt;(no, we're no telling you why the OOM crash happened, figure it out on your own; we'll just tell you that YOU DON'T HAVE ENOUGH VRAM period)&lt;/p&gt; &lt;p&gt;`(APIServer pid=571098) INFO 08-11 18:19:32 [__init__.py:1731] Using max model len 262144`&lt;/p&gt; &lt;p&gt;Ah yes, unlike the other backends, vLLM will use the model's *maximum* context length as default. Of course I don't have that much. Let's fix it!&lt;/p&gt; &lt;p&gt;Attempt 3: `vllm serve openai/gpt-oss-20b --gpu-memory-utilization 0.85 --cpu-offload-gb 5 --max-model-len 40000`&lt;/p&gt; &lt;p&gt;OOM CRASH&lt;/p&gt; &lt;p&gt;This time we got to the KV cache though, so I get info that my remaining VRAM is simply not enough for the KV cache. Oh yeah, quantized KV cache, here we come... but only fp8, since vLLM doesn't support any lower options.&lt;/p&gt; &lt;p&gt;Attempt 4: `vllm serve openai/gpt-oss-20b --gpu-memory-utilization 0.85 --cpu-offload-gb 5 --max-model-len 40000 --kv-cache-dtype fp8`&lt;/p&gt; &lt;p&gt;... model loads ...&lt;br /&gt; ERROR: unsupported architecture for cache type 'mxfp4', compute capability: 86, minimum capability: 90&lt;/p&gt; &lt;p&gt;(translation: You pleb, you tried to run the shiny new MXFP4 quants on a 30x0 card, but a minimum of 40x0 cards are required)&lt;/p&gt; &lt;p&gt;Oh well, this is proof-of-concept after all, right? Let's run something easy. Qwen3-8B-FP8. Should fit nicely, should run OK, right?&lt;/p&gt; &lt;p&gt;Attempt 5: `VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve --cpu-offload-gb 6 --gpu-memory-utilization 0.85 Qwen/Qwen3-8B-FP8 --max-model-len 40000 --kv-cache-dtype fp8` (what is this Flashinfer witchcraft, you ask? Well, the debugging messages suggested running on Flashinfer for FP8 quants, so I went and got it. Yes, you have to compile it manually. With `--no-build-isolation`, preferrably. Don't ask. Just accept)&lt;/p&gt; &lt;p&gt;... models loads ...&lt;br /&gt; ... no unsupported architecture errors ...&lt;br /&gt; ... computing CUDA graphs ...&lt;/p&gt; &lt;p&gt;ERROR: cannot find #include_next &amp;quot;math.h&amp;quot;&lt;/p&gt; &lt;p&gt;WTF?!?! Okay, to the internets. ChatGPT says it's probably a problem of C++ compiler and NVCC compiler mismatch. Maybe recompile VLLM with G++-12? No, sorry mate, ain't doing that.&lt;/p&gt; &lt;p&gt;Okay, symlinking `math.h` and `stdlib.h` from `/usr/include` to `/usr/x86_64-linux-gnu` gets the job done.&lt;/p&gt; &lt;p&gt;Attempt 6: same line as before. &lt;/p&gt; &lt;p&gt;Hooray, it loads!&lt;/p&gt; &lt;p&gt;... I get 1.8 t/s throughput because all the optimizations are not for my pleb graphics card ;)&lt;/p&gt; &lt;p&gt;And you're saying it's not user friendly? That wasn't even half the time and effort it took to get a printer working in Linux back in the 1990s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnin8k/my_beautiful_vllm_adventure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnin8k/my_beautiful_vllm_adventure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnin8k/my_beautiful_vllm_adventure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T17:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnqyw3</id>
    <title>I built Nore, a free desktop UI to manage all your local and cloud AI models in one place.</title>
    <updated>2025-08-11T22:14:45+00:00</updated>
    <author>
      <name>/u/embium</name>
      <uri>https://old.reddit.com/user/embium</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnqyw3/i_built_nore_a_free_desktop_ui_to_manage_all_your/"&gt; &lt;img alt="I built Nore, a free desktop UI to manage all your local and cloud AI models in one place." src="https://external-preview.redd.it/6uxo0McORlF2P1of8Iq-78vP3ukl5svQJ-7TXrixmug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcec56249588918aad96626918addbae6c4b2bb2" title="I built Nore, a free desktop UI to manage all your local and cloud AI models in one place." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/8h46v38bsgif1.gif"&gt;https://i.redd.it/8h46v38bsgif1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been spending a lot of time with local models using Ollama, but I was getting tired of juggling terminal windows and different web UIs for cloud models like Claude or Gemini. I wanted a single, clean desktop app to handle everything.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;Nore&lt;/strong&gt;. It's a desktop front-end that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connect directly to Ollama&lt;/strong&gt; and other providers (OpenAI, Gemini, Groq, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Browse, install, and manage your Ollama models&lt;/strong&gt; from a built-in Model Hub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extend its skills&lt;/strong&gt; by installing &amp;quot;MCP Servers&amp;quot;—things like a TaskManager, a Knowledge Graph for memory, and even a Desktop Commander.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's built with Electron/React/Rust, and I'm really looking for feedback from fellow power users and enthusiasts.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can check it out here: &lt;a href="https://github.com/embium/nore"&gt;&lt;code&gt;https://github.com/embium/nore&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Screenshots: &lt;ul&gt; &lt;li&gt;Chat Main - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/chat_main.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/chat_main.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Chat + MCP Manager - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/chat_main_mcp_manager.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/chat_main_mcp_manager.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MCP Server Registry - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/mcp_server_registry.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/mcp_server_registry.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MCP Edit Server - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/mcp_servers_edit_server.png"&gt;https://github.com/embium/nore/blob/main/screenshots/mcp_servers_edit_server.png&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model Hub Catalog - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/model_hub_catalog.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/model_hub_catalog.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Settings: General Appearance - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/general_appearance_settings.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/general_appearance_settings.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Settings: AI Providers - &lt;a href="https://github.com/embium/nore/blob/main/screenshots/ai_provider_settings.png?raw=true"&gt;https://github.com/embium/nore/blob/main/screenshots/ai_provider_settings.png?raw=true&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think! I'm here to answer any questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/embium"&gt; /u/embium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnqyw3/i_built_nore_a_free_desktop_ui_to_manage_all_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnqyw3/i_built_nore_a_free_desktop_ui_to_manage_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnqyw3/i_built_nore_a_free_desktop_ui_to_manage_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T22:14:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnfomq</id>
    <title>Searching actually viable alternative to Ollama</title>
    <updated>2025-08-11T15:13:53+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;as we've all figured out by now, Ollama is certainly not the best way to go. Yes, it's simple, but there are so many alternatives out there which either outperform Ollama or just work with broader compatibility. So I said to myself, &amp;quot;screw it&amp;quot;, I'm gonna try that out, too.&lt;/p&gt; &lt;p&gt;Unfortunately, it turned out to be everything but simple. I need an alternative that...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;implements model swapping (loading/unloading on the fly, dynamically) just like Ollama does&lt;/li&gt; &lt;li&gt;exposes an OpenAI API endpoint&lt;/li&gt; &lt;li&gt;is open-source&lt;/li&gt; &lt;li&gt;can take pretty much any GGUF I throw at it&lt;/li&gt; &lt;li&gt;is easy to set up and spins up quickly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I looked at a few alternatives already. vLLM seems nice, but is quite the hassle to set up. It threw a lot of errors I simply did not have the time to look for, and I want a solution that &lt;em&gt;just works&lt;/em&gt;. LM Studio is closed and their open-source CLI still mandates usage of the closed LM Studio application...&lt;/p&gt; &lt;p&gt;Any go-to recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnvleb</id>
    <title>KittenTTS inference speed on ARM</title>
    <updated>2025-08-12T01:35:04+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://github.com/KittenML/KittenTTS/issues/40#issuecomment-3168324368"&gt;https://github.com/KittenML/KittenTTS/issues/40#issuecomment-3168324368&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Environment&lt;/th&gt; &lt;th&gt;Kokoro (kokoro-v1.0.fp16.onnx)&lt;/th&gt; &lt;th&gt;Piper (en_US-lessac-low.onnx)&lt;/th&gt; &lt;th&gt;KittenTTS (kitten_tts_nano_v0_1.onnx)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;MacBook Pro M2&lt;/td&gt; &lt;td&gt;0.75s&lt;/td&gt; &lt;td&gt;0.085s&lt;/td&gt; &lt;td&gt;0.68s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Raspberry Pi 5&lt;/td&gt; &lt;td&gt;4.83s&lt;/td&gt; &lt;td&gt;0.54s&lt;/td&gt; &lt;td&gt;4.13s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;TLDR: KittenTTS might be lighter in size than kokoro, but speed is still similar. Both might not be fast enough for streaming compared with older TTS models when using the cpu. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnvleb/kittentts_inference_speed_on_arm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T01:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn98w0</id>
    <title>Vllm documentation is garbage</title>
    <updated>2025-08-11T10:23:37+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wtf is this documentation, vllm? Incomplete and so cluttered. You need someone to help with your shtty documentation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T10:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8l69</id>
    <title>Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now</title>
    <updated>2025-08-11T09:43:41+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt; &lt;img alt="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" src="https://b.thumbs.redditmedia.com/ea0TMc1wgQgQw7ly692klA1IbdpyyTri9nJucym3uOo.jpg" title="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a re-distill of my SVD based distillation of qwen3 coder 480b into qwen 3 coder 30b. I fixed a bug that caused the MoE distillation to not actually distill so v1 did not distill the MoE layers properly. I also added SLERP and procrustes alignment to the distillation script alongside DARE (pretty much just cleans up the noise when making the lora) which seems to have produced a much better model. SVD distillation is a data-free distillation method I have not seen anyone do for a opensource model although ive seen a paper on it so its been done before. Its a really efficient distillation method it took 4 hours to distill the full 900+GB qwen3 coder 480b model into the unquantized qwen3 coder 30b model on 2x 3090's. The script distills and then creates a large rank 2048 lora (using the maximum rank for lora on SVD seems to be required to capture as much information as possible since its purely mathematical) and then I merged it with the 30b and then quantized. Ill post the github link for the scripts but it will be a bit until I post the updated scripts since its 4am and I should probably go to sleep lol. This has taken around 100 hours or more of research and testing script after script to get to this point, I think it was worth it, hopefully it will work well for you as well. I have not tested it on very complex code but it should be better at more than just what I tested it with since pretty much the weights themselfs have been distilled. Also Qwen models really love to put that one guy as the cover photo in alot of the dev portfolio website prompts I tested. I guess thats what a dev with 30 years of experience looks like in the AI stock photo world lol. The fintrack website was just 3 prompts and most things work. Its around 2000 lines of code for it. Heres the model page and github &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mn8l69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5fe6</id>
    <title>Apple patents matmul technique in GPU</title>
    <updated>2025-08-11T06:17:07+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US452614511&amp;amp;_cid=P12-M8WPOS-61919-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnh0s5</id>
    <title>Llama.cpp Vulkan is awesome, It gave new life to my old RX580</title>
    <updated>2025-08-11T16:03:21+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a new computer and instead of buying a GPU I decided to give my old RX580 8GB a try for running inference. I had it laying around unused.&lt;/p&gt; &lt;p&gt;My PC specs are not crazy, its b850 motherboard, Ryzen 7700X and 32gb ram. My total cost was about 700 dollars.&lt;/p&gt; &lt;p&gt;Tried running Qwen3 30 b with about 20 layers offloaded to the GPU and got 24 tokes a second. Here is my command&lt;/p&gt; &lt;p&gt;./llama-server --n_gpu_layers 20 --ctx-size 16000 --model ../../../models/Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;p&gt;Adding top_p and top_k and temp slows down the inference by about 10 tokens a second, not sure why. &lt;/p&gt; &lt;p&gt;&lt;code&gt;slot print_timing: id 0 | task 0 | prompt eval time = 559.81 ms / 13 tokens ( 43.06 ms per token, 23.22 tokens per second) eval time = 30875.68 ms / 743 tokens ( 41.56 ms per token, 24.06 tokens per second) total time = 31435.49 ms / 756 tokens &lt;/code&gt;&lt;/p&gt; &lt;p&gt;My RX580 is actually useful to me now, and it worked out of the box with Linux Mint!&lt;/p&gt; &lt;p&gt;With Vulkan being this good now, you can actually build a decent localllama build for about 7-800 dollars. Very excited for the future of local llms!&lt;/p&gt; &lt;p&gt;Edit: fixed the command i used for llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxwmw</id>
    <title>Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot</title>
    <updated>2025-08-12T03:24:03+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt; &lt;img alt="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" src="https://b.thumbs.redditmedia.com/81joqRjngFFUavEApRYDiznp-6LcG-wUqoKaM4BcLls.jpg" title="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a"&gt;https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to gguf: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sha256: c6f818151fa2c6fbca5de1a0ceb4625b329c58595a144dc4a07365920dd32c51&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8ij6</id>
    <title>gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)</title>
    <updated>2025-08-11T09:38:57+00:00</updated>
    <author>
      <name>/u/chikengunya</name>
      <uri>https://old.reddit.com/user/chikengunya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt; &lt;img alt="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" src="https://preview.redd.it/0lv50zsy1dif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea6791432a529ab3ea6d7e9ca517b8c29a23b19" title="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chikengunya"&gt; /u/chikengunya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0lv50zsy1dif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnlfyt</id>
    <title>Geocities style site by glm 4.5</title>
    <updated>2025-08-11T18:44:42+00:00</updated>
    <author>
      <name>/u/ChazychazZz</name>
      <uri>https://old.reddit.com/user/ChazychazZz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt; &lt;img alt="Geocities style site by glm 4.5" src="https://preview.redd.it/fvi60un3qfif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ac08d73fcf7b3cd7d2da2f4614ab0b91d7bf5250" title="Geocities style site by glm 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Completed in just 1 super simple prompt. GLM 4.5 is terrifyingly good at web dev now, especially as we can run it local. For me it was obvious it can generate modern and modern-ish sites but this stuff is kinda cooler to see (at least for me). The only unfortunate thing that it used emojis but that can be tweaked i guess and just included in the prompt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChazychazZz"&gt; /u/ChazychazZz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fvi60un3qfif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T18:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnedxo</id>
    <title>SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China</title>
    <updated>2025-08-11T14:24:03+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt; &lt;img alt="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" src="https://b.thumbs.redditmedia.com/euatxyG4VjgoW0nwCU8kLUP0naLHxC18HHdMXCH_NiU.jpg" title="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two weeks ago, China's &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; open-sourced the &lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;GLM-4.5 model&lt;/a&gt;. Now, building on GLM-4.5’s language architecture, they’ve trained a new VLM—&lt;a href="https://huggingface.co/collections/zai-org/glm-45v-68999032ddf8ecf7dcdbc102"&gt;GLM-4.5V&lt;/a&gt; — which achieved SOTA in ​&lt;strong&gt;41 out of 42 benchmarks&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Absolutely insane! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc"&gt;https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T14:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnd144</id>
    <title>Am I the only one who never really liked Ollama?</title>
    <updated>2025-08-11T13:30:22+00:00</updated>
    <author>
      <name>/u/a_normal_user1</name>
      <uri>https://old.reddit.com/user/a_normal_user1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all that happens with it now and them wanting people to make accounts to use certain features(which kinda defeats the purpose of it) am I the only one who thought that it's really not the best? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_normal_user1"&gt; /u/a_normal_user1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnhgt0</id>
    <title>GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks</title>
    <updated>2025-08-11T16:19:43+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt; &lt;img alt="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" src="https://preview.redd.it/jw671veezeif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ef1b882c2760541b723f2922a88f046fea21c80" title="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released their first open models since GPT-2, and GPT-OSS-120B is now the best open-weight model on our real-world TaskBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better completion performance overall compared to other open-weight models like Kimi-K2 and DeepSeek-R1, while being roughly 1/10th the size. Cheaper, better, faster.&lt;/li&gt; &lt;li&gt;Relative to closed-source models, it performs like smaller frontier models such as o4-mini or previous-generation top tier models like Claude-3.7.&lt;/li&gt; &lt;li&gt;Clearly optimized for agentic use cases, it’s close to Sonnet-4 on our agentic benchmarks and could be a strong main agent model.&lt;/li&gt; &lt;li&gt;Works more like an action model than a chat or knowledge model. Multi-lingual performance is limited, and it hallucinates more on world knowledge, so it benefits from retrieval grounding and pairing with another model for multi-lingual scenarios.&lt;/li&gt; &lt;li&gt;Context recall is decent but weaker than top frontier models, so it’s better suited for shorter or carefully managed context windows.&lt;/li&gt; &lt;li&gt;Excels when paired with strong context engineering and agentic engineering, where each task completion reliably feeds into the next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, this model looks to be a real gem and will likely inject more energy into open-source models.&lt;/p&gt; &lt;p&gt;We’ve published the full benchmark results, including GPT-5, mini, and nano, and our task categories and eval methods here: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those building with it, anyone else seeing similar strengths/weaknesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw671veezeif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mno45o</id>
    <title>FULL LEAKED v0 by Vercel System Prompts and Internal Tools</title>
    <updated>2025-08-11T20:25:04+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 11/08/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL official v0 system prompt and internal tools. Over 13.5K tokens and 1.3K lines.&lt;/p&gt; &lt;p&gt;Check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T20:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncfif</id>
    <title>GLM-4.5V (based on GLM-4.5 Air)</title>
    <updated>2025-08-11T13:04:47+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A vision-language model (VLM) in the GLM-4.5 family. Features listed in model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image reasoning&lt;/strong&gt; (scene understanding, complex multi-image analysis, spatial recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video understanding&lt;/strong&gt; (long video segmentation and event recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GUI tasks&lt;/strong&gt; (screen reading, icon recognition, desktop operation assistance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex chart &amp;amp; long document parsing&lt;/strong&gt; (research report analysis, information extraction)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grounding&lt;/strong&gt; (precise visual element localization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5V"&gt;https://huggingface.co/zai-org/GLM-4.5V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnc8lx</id>
    <title>I built Excel Add-in for Ollama</title>
    <updated>2025-08-11T12:56:39+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt; &lt;img alt="I built Excel Add-in for Ollama" src="https://preview.redd.it/mvjwf2f81eif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=17b456d91ceed7000d3f08cd2f8917aec6e4254a" title="I built Excel Add-in for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an excel add-in that connects Ollama with Microsoft Excel. Data to remain inside excel only. You can simply write function =ollama(A1), assuming prompt in cell A1. You can simply drag to run on multiple cells. It has arguments to specify system instructions, temperature and model. You can set at both global level and specific to your prompts. &lt;a href="https://www.listendata.com/2025/08/ollama-in-excel.html"&gt;https://www.listendata.com/2025/08/ollama-in-excel.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvjwf2f81eif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T12:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnp5nc</id>
    <title>Training an LLM only on books from the 1800's - Another update</title>
    <updated>2025-08-11T21:04:34+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm training LLM's from scratch using only texts from a specific region and time period and want to share another update. Right now it's 1800-1875 London. When I first started, my dataset was only 50 texts and I was using a 4060 for training. The latest version is trained on almost 7,000 texts using Phi 1.5 (700M parameters) on an A100 GPU. My long term goal is to see if a model trained this way can actually reason. The newest model I've trained has some promising output, it's starting to reference real historical events instead of just hallucinating everything. Also many people have told me that fine tuning will be more efficient and I agree, but I want to see how far this approach can go. And Internet Archive has around 175,000 London texts within my chosen time period, so scaling the dataset won't be an issue. &lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T21:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxodk</id>
    <title>LocalLLaMA is the last sane place to discuss LLMs on this site, I swear</title>
    <updated>2025-08-12T03:12:34+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt; &lt;img alt="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" src="https://preview.redd.it/iu3pniar9iif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bac76c25e583f3690f2e1e9cdc20c74739fa84c" title="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iu3pniar9iif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncrqp</id>
    <title>ollama</title>
    <updated>2025-08-11T13:19:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt; &lt;img alt="ollama" src="https://preview.redd.it/2whabjm55eif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea8efc9d0fe6d86f047a62709601f55061db889" title="ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2whabjm55eif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
