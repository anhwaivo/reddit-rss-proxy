<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-01T18:37:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j13rf3</id>
    <title>Future of LLM</title>
    <updated>2025-03-01T16:46:57+00:00</updated>
    <author>
      <name>/u/palyer69</name>
      <uri>https://old.reddit.com/user/palyer69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was curious about llm future &lt;/p&gt; &lt;p&gt;ü§ñpre-traning Plateau - gpt 4o,Gemini 2, grok 3 Sonnet 3.6 /3.7 all maxed out. minimal gain from scaling.&lt;/p&gt; &lt;p&gt;üß†reasoning post training - o1 then 03 and r1 now hitting diminishing returns , almost at plateau&lt;/p&gt; &lt;p&gt;future? more efficient small llm? multimodality? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palyer69"&gt; /u/palyer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13rf3/future_of_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13rf3/future_of_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13rf3/future_of_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j045xn</id>
    <title>I trained a reasoning model that speaks French‚Äîfor just $20! ü§Øüá´üá∑</title>
    <updated>2025-02-28T09:51:24+00:00</updated>
    <author>
      <name>/u/TheREXincoming</name>
      <uri>https://old.reddit.com/user/TheREXincoming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j045xn/video/mvudzukrpule1/player"&gt;https://reddit.com/link/1j045xn/video/mvudzukrpule1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheREXincoming"&gt; /u/TheREXincoming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T09:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j15f7n</id>
    <title>ctrl97 error message in Gemini -&gt; Anyone know what it means?</title>
    <updated>2025-03-01T17:57:17+00:00</updated>
    <author>
      <name>/u/Plenty-Inside-3814</name>
      <uri>https://old.reddit.com/user/Plenty-Inside-3814</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j15f7n/ctrl97_error_message_in_gemini_anyone_know_what/"&gt; &lt;img alt="ctrl97 error message in Gemini -&amp;gt; Anyone know what it means?" src="https://b.thumbs.redditmedia.com/PP4LO66FGfsm7VfKv_5oRKSRqNqIV77hBW0jDndpuso.jpg" title="ctrl97 error message in Gemini -&amp;gt; Anyone know what it means?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vtfb9syz94me1.png?width=1405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9c0559ac508a8e1f820e9728a8b332780049d1"&gt;https://preview.redd.it/vtfb9syz94me1.png?width=1405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9c0559ac508a8e1f820e9728a8b332780049d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else come across this message in Gemini? Any idea what it means?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plenty-Inside-3814"&gt; /u/Plenty-Inside-3814 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j15f7n/ctrl97_error_message_in_gemini_anyone_know_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j15f7n/ctrl97_error_message_in_gemini_anyone_know_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j15f7n/ctrl97_error_message_in_gemini_anyone_know_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T17:57:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0wurx</id>
    <title>UPDATE: Tool Calling for DeepSeek-R1 with LangChain and LangGraph: Now in TypeScript!</title>
    <updated>2025-03-01T10:46:24+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted here a Github repo Python package I created on tool calling for DeepSeek-R1 671B with LangChain and LangGraph, or more generally for any LLMs available in LangChain's ChatOpenAl class (particularly useful for newly released LLMs which isn't supported for tool calling yet by LangChain and LangGraph):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By community request, I'm thrilled to announce a TypeScript version of this package is now live!&lt;/p&gt; &lt;p&gt;Introducing &amp;quot;taot-ts&amp;quot; - The npm package that brings tool calling capabilities to DeepSeek-R1 671B in TypeScript:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time-ts"&gt;https://github.com/leockl/tool-ahead-of-time-ts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kindly give me a star on my repo if this is helpful. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T10:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0cbvs</id>
    <title>üó£Ô∏è Free &amp; Open-Source AI TTS: Kokoro Web v0.1.0</title>
    <updated>2025-02-28T16:53:25+00:00</updated>
    <author>
      <name>/u/EduardoDevop</name>
      <uri>https://old.reddit.com/user/EduardoDevop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Excited to share &lt;strong&gt;Kokoro Web&lt;/strong&gt;, a fully open-source AI text-to-speech tool that you can use for free. No paywalls, no restrictions‚Äîjust high-quality, local-friendly TTS. &lt;/p&gt; &lt;h2&gt;üî• Why It Matters:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Open-Source&lt;/strong&gt;: No locked features, no subscriptions.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hostable&lt;/strong&gt;: Run it locally or on your own server.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI API Compatible&lt;/strong&gt;: Drop-in replacement for AI projects.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Generate speech in different accents.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built on Kokoro v1.0&lt;/strong&gt;: One of the top-ranked models in &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;TTS Arena&lt;/a&gt;, just behind ElevenLabs.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;üöÄ Try It Out:&lt;/h2&gt; &lt;p&gt;Live demo: &lt;a href="https://voice-generator.pages.dev"&gt;https://voice-generator.pages.dev&lt;/a&gt; &lt;/p&gt; &lt;h2&gt;üîß Self-Hosting:&lt;/h2&gt; &lt;p&gt;Spin it up with Docker in minutes: &lt;a href="https://github.com/eduardolat/kokoro-web"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts‚Äîfeedback, contributions, and ideas are always welcome! üñ§ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EduardoDevop"&gt; /u/EduardoDevop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0c53c</id>
    <title>Inference speed comparisons between M1 Pro and maxed-out M4 Max</title>
    <updated>2025-02-28T16:45:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Won‚Äôt Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I don't think these models are actually utilizing the CPU. But I'm not definitive on this.&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I added a github repo in case anyone wants to contribute their own speed tests. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0kgyn</id>
    <title>99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900</title>
    <updated>2025-02-28T22:41:24+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt; &lt;img alt="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" src="https://external-preview.redd.it/HFws3DDkcEP1xVBovP3WDu-ptb9oIschwqz-M_5LaEc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1b52e8ca906335c7d16f3c20d0abb029388892" title="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ZWBQPKc.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T22:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d30g</id>
    <title>There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day</title>
    <updated>2025-02-28T17:23:46+00:00</updated>
    <author>
      <name>/u/unixmachine</name>
      <uri>https://old.reddit.com/user/unixmachine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt; &lt;img alt="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" src="https://external-preview.redd.it/VGXH8e8_7pJ5Oyuiy8alPcJzC5slCQHySpimzMU8-QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7786514b391f52128b399c1e789d18e7f6f10a" title="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixmachine"&gt; /u/unixmachine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-ROCm-RX-9070-Launch-Day"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j15ryr</id>
    <title>Need Help in using AI Agents and Tools</title>
    <updated>2025-03-01T18:11:59+00:00</updated>
    <author>
      <name>/u/Masochist_Boi</name>
      <uri>https://old.reddit.com/user/Masochist_Boi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So currently I've been put on an ongoing project which is based on Python and Reflex for front-end. Basically it's an ai tool where I can upload my datasets, it gets pre-processed and LLM gives response based on queries regarding it from user.&lt;/p&gt; &lt;p&gt;I have been told to make changes in it's functionality but Python isn't my strong suit. So I turned to Cursor where I have been using it's free tier plan to get my changes done but still haven't been able to figure out how to use claude 3.5 properly. It takes me 20-30 prompts to achieve one functionality. What can I do so that it understands my needs? &lt;/p&gt; &lt;p&gt;Many times when I give it in all particularity, it still does some things on it's own. Most of my time goes in the debugging of it. I'm pretty new on using Al agents so I need your help on this. What good prompts are there for getting things done in coding? Any piece of advice helps a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Masochist_Boi"&gt; /u/Masochist_Boi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j15ryr/need_help_in_using_ai_agents_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j15ryr/need_help_in_using_ai_agents_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j15ryr/need_help_in_using_ai_agents_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T18:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0sxsv</id>
    <title>What do the deepseek papers mean for local inference?</title>
    <updated>2025-03-01T06:11:13+00:00</updated>
    <author>
      <name>/u/oldschooldaw</name>
      <uri>https://old.reddit.com/user/oldschooldaw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Upfront, I can‚Äôt understand the papers. I don‚Äôt know enough to read them. But the snippets I‚Äôm seeing about them on X suggest to me a lot of the improvements are for VERY VERY VERY large players, not those with a single 4090. &lt;/p&gt; &lt;p&gt;Is there any developments in the drops I‚Äôve missed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oldschooldaw"&gt; /u/oldschooldaw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0muz1</id>
    <title>Phi-4-mini Bug Fixes + GGUFs</title>
    <updated>2025-03-01T00:33:13+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! llama.cpp added supported for Phi-4 mini today - we also found and fixed 4 tokenization related problems in Phi-4 mini!&lt;/p&gt; &lt;p&gt;The biggest problem with the chat template is the EOS token was set to &amp;lt;|endoftext|&amp;gt;, but it should be &amp;lt;|end|&amp;gt;!&lt;/p&gt; &lt;p&gt;GGUFs are at: &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The rest of the versions including 16-bit are&lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt; &lt;/a&gt;also on &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And the dynamic 4bit bitsandbytes version is at &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There were also tokenization problems for the larger Phi-4 14B as well, which we fixed a while back for those who missed it and Microsoft used our fixes 2 weeks ago.&lt;/p&gt; &lt;p&gt;Thank you! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13hf5</id>
    <title>How are people deploying apps with AI functionality and it not costing them an absolute fortune?</title>
    <updated>2025-03-01T16:35:17+00:00</updated>
    <author>
      <name>/u/joncording12</name>
      <uri>https://old.reddit.com/user/joncording12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all seen lots of web apps coming out which include AI chat functionality. The bit for me I'm most curious about, is a huge amount of them seem to have a free version without chat limits. &lt;/p&gt; &lt;p&gt;I'm building an app at the moment, and while intended for personal use, I'll likely open it up to the world as I think it's pretty cool. I'm mucking about with using an LLM which is going very well. I intend to block this functionality to public users unless they bring-their-own API keys. &lt;/p&gt; &lt;p&gt;In a perfect world, I'd love to have a basic/limited version for free users and then charge a minimal monthly fee which gives them the full version with my app. &lt;/p&gt; &lt;p&gt;But, how are people actually implementing this without it costing an arm and a leg? Are many devs just outright swallowing cost in anticipation of success from a paid offering? &lt;/p&gt; &lt;p&gt;I recently saw &lt;a href="https://www.open-health.me/"&gt;https://www.open-health.me/&lt;/a&gt; on Reddit - and even digging through the source code that is what the developer appears to be doing. I tried asking him but not received a response, but by all appearances it is what people are doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joncording12"&gt; /u/joncording12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0o8mt</id>
    <title>The first real open source DeepResearch attempt I've seen</title>
    <updated>2025-03-01T01:43:56+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Search-R1&lt;/strong&gt; is a reproduction of &lt;strong&gt;DeepSeek-R1(-Zero)&lt;/strong&gt; methods for &lt;em&gt;training reasoning and searching (tool-call) interleaved LLMs&lt;/em&gt;. Built upon &lt;a href="https://github.com/volcengine/verl"&gt;veRL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Through RL (rule-based outcome reward), the 3B &lt;strong&gt;base&lt;/strong&gt; LLM (both Qwen2.5-3b-base and Llama3.2-3b-base) develops reasoning and search engine calling abilities all on its own.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PeterGriffinJin/Search-R1/tree/main"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yxm1</id>
    <title>AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well</title>
    <updated>2025-03-01T13:00:07+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt; &lt;img alt="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" src="https://external-preview.redd.it/kjFdjz1K9W0AjIQXXFkEbN2PFu7C3Ga1YSe9tdCduuw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dfacaf6bed989273560dbf7b34460efac41ca6a" title="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-rx-9070-series-gpus-will-feature-support-for-rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yyg1</id>
    <title>AMD Ryzen AI Max+ Pro 395 "Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%</title>
    <updated>2025-03-01T13:01:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt; &lt;img alt="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" src="https://external-preview.redd.it/BkZkseIe-TX7LG_20ukN9CeYcn2i-ty5QOZ4F8-FqfE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d471c1208a7a116839a4c4a45eef1d42f16e7d2c" title="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-benchmarked-in-cpu-mark-outperforms-core-i9-14900hx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0uoht</id>
    <title>Chain of Draft: Thinking Faster by Writing Less</title>
    <updated>2025-03-01T08:10:14+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt; &lt;img alt="Chain of Draft: Thinking Faster by Writing Less" src="https://b.thumbs.redditmedia.com/fwljouG_I7UUcaN7hDPRHRGsfe6zHb3U7bt9TnwQ_OA.jpg" title="Chain of Draft: Thinking Faster by Writing Less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.18600"&gt;https://arxiv.org/abs/2502.18600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CoD System prompt:&lt;/p&gt; &lt;p&gt;Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j0uoht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0ync3</id>
    <title>TinyR1-32B-Preview: SuperDistillation Achieves Near-R1 Performance with Just 5% of Parameters.</title>
    <updated>2025-03-01T12:43:48+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We applied supervised fine-tuning (SFT) to Deepseek-R1-Distill-Qwen-32B across three target domains‚ÄîMathematics, Code, and Science ‚Äî using the 360-LLaMA-Factory training framework to produce three domain-specific models. We used questions from open-source data as seeds. Meanwhile, responses for mathematics, coding, and science tasks were generated by R1, creating specialized models for each domain. Building on this, we leveraged the Mergekit tool from the Arcee team to combine multiple models, creating Tiny-R1-32B-Preview, which demonstrates strong overall performance.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T12:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j121yv</id>
    <title>China's DeepSeek claims theoretical cost-profit ratio of 545% per day</title>
    <updated>2025-03-01T15:32:42+00:00</updated>
    <author>
      <name>/u/Tall_Science_9178</name>
      <uri>https://old.reddit.com/user/Tall_Science_9178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j121yv/chinas_deepseek_claims_theoretical_costprofit/"&gt; &lt;img alt="China's DeepSeek claims theoretical cost-profit ratio of 545% per day" src="https://external-preview.redd.it/qXp1Rz6whVi7GAFUtQK_EwYmWAtTZX_3wh9ot6cBcpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7e9935cf90902bd6dee4da657c7ad1d3529b899" title="China's DeepSeek claims theoretical cost-profit ratio of 545% per day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tall_Science_9178"&gt; /u/Tall_Science_9178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/chinas-deepseek-claims-theoretical-cost-121658741.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j121yv/chinas_deepseek_claims_theoretical_costprofit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j121yv/chinas_deepseek_claims_theoretical_costprofit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0r3go</id>
    <title>Day 6: One More Thing, DeepSeek-V3/R1 Inference System Overview</title>
    <updated>2025-03-01T04:19:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md"&gt;https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T04:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j12jh4</id>
    <title>Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!</title>
    <updated>2025-03-01T15:54:30+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt; &lt;img alt="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" src="https://external-preview.redd.it/yySKZcwLWOVfiw-FCxXZZGyMsX-eiuOXklZ8rkauveI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f604a8bea97f9cec32746dee177776a5626460f1" title="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Llama-3.3-R1-70B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j10d5g</id>
    <title>Can you ELI5 why a temp of 0 is bad?</title>
    <updated>2025-03-01T14:14:16+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like common knowledge that &amp;quot;you almost always need temp &amp;gt; 0&amp;quot; but I find this less authoritative than everyone believes. I understand if one is writing creatively, he'd use higher temps to arrive at less boring ideas, but what if the prompts are for STEM topics or just factual information? Wouldn't higher temps force the llm to wonder away from the more likely correct answer, into a maze of more likely wrong answers, and effectively hallucinate more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T14:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j11js6</id>
    <title>I bought 4090D with 48GB VRAM. How to test the performance?</title>
    <updated>2025-03-01T15:10:21+00:00</updated>
    <author>
      <name>/u/slavik-f</name>
      <uri>https://old.reddit.com/user/slavik-f</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paid $3k, shipped from Hong Kong. Received yesterday.&lt;/p&gt; &lt;p&gt;Obviously, the card is modified, and the spec said: &amp;quot;48GB GDDR6 256-bit&amp;quot;. Original 4090/4090D comes with GDDR6X 384-bit.&lt;/p&gt; &lt;p&gt;I installed it to my Dell Precision T7920 (Xeon Gold 5218, 384GB DDR4 RAM, 1400W PSU). I'm running few models with Ollama and it works great so far.&lt;/p&gt; &lt;p&gt;I had RTX 3090 and I even was able to put both GPUs in that system, so now I have 48+24 = 72GB VRAM!&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvidia-smi Sat Mar 1 15:00:26 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 560.35.05 Driver Version: 560.35.05 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Off | 00000000:0B:00.0 Off | N/A | | 0% 42C P8 19W / 350W | 4MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 4090 D Off | 00000000:0C:00.0 Off | Off | | 30% 48C P0 50W / 425W | 4MiB / 49140MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when I tried to measure memory bandwidth - I can't find a way to do it. Can someone help me here? How can I measure it?&lt;/p&gt; &lt;p&gt;Also, is there a way to measure Int8 perf (TOPS) ?&lt;/p&gt; &lt;p&gt;Looks like Windows has few more tools to get such data. But I'm on Ubuntu.&lt;/p&gt; &lt;p&gt;Running qwen2.5-72b-instruct-q4_K_M (47GB) model, on 2 GPUs I'm getting&lt;/p&gt; &lt;p&gt;- 263 t/s for prompt&lt;/p&gt; &lt;p&gt;- 16.6 t/s for response&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 1:&lt;/strong&gt; using &lt;a href="http://ghcr.io/huggingface/gpu-fryer"&gt;ghcr.io/huggingface/gpu-fryer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- RTX 3090: 22 TFLOPS&lt;/p&gt; &lt;p&gt;- RTX 4090D: 49 TFLOPS&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 2&lt;/strong&gt;: using llama-bench with qweb2.5-code 32b (18.5GB) model:&lt;/p&gt; &lt;p&gt;RTX 3090: &lt;/p&gt; &lt;p&gt;- pp512 | 1022.09 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 35.28 t/s&lt;/p&gt; &lt;p&gt;RTX 4090D:&lt;/p&gt; &lt;p&gt;- pp512 | 2118.70 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 41.16 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slavik-f"&gt; /u/slavik-f &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0n56h</id>
    <title>Finally, a real-time low-latency voice chat model</title>
    <updated>2025-03-01T00:47:24+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't seen it yet, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tried it fow a few minutes earlier today and another 15 minutes now. I tested and it remembered our chat earlier. It is the first time that I treated AI as a person and felt that I needed to mind my manners and say &amp;quot;thank you&amp;quot; and &amp;quot;good bye&amp;quot; at the end of the conversation.&lt;/p&gt; &lt;p&gt;Honestly, I had more fun chatting with this than some of my ex-girlfriends!&lt;/p&gt; &lt;p&gt;Github here (code not yet dropped):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;``` Model Sizes: We trained three model sizes, delineated by the backbone and decoder sizes:&lt;/p&gt; &lt;p&gt;Tiny: 1B backbone, 100M decoder Small: 3B backbone, 250M decoder Medium: 8B backbone, 300M decoder Each model was trained with a 2048 sequence length (~2 minutes of audio) over five epochs. ```&lt;/p&gt; &lt;p&gt;The model sizes look friendly to local deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0tnsr</id>
    <title>We're still waiting Sam...</title>
    <updated>2025-03-01T06:59:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt; &lt;img alt="We're still waiting Sam..." src="https://preview.redd.it/31jfuybv01me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128f969cd722b072d73b4d77393ee7c0bc1b057b" title="We're still waiting Sam..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/31jfuybv01me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13cwq</id>
    <title>Qwen: ‚Äúdeliver something next week through opensource‚Äù</title>
    <updated>2025-03-01T16:29:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt; &lt;img alt="Qwen: ‚Äúdeliver something next week through opensource‚Äù" src="https://preview.redd.it/knfs0pgpu3me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=848a461104672e52b7bade6e6a4ea8b55f90ba90" title="Qwen: ‚Äúdeliver something next week through opensource‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Not sure if we can surprise you a lot but we will definitely deliver something next week through opensource.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knfs0pgpu3me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:29:57+00:00</published>
  </entry>
</feed>
