<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-16T22:51:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m1n3kf</id>
    <title>Help in using Flux models in 3060 8gb vram and 16gb ram</title>
    <updated>2025-07-16T20:08:30+00:00</updated>
    <author>
      <name>/u/LahmeriMohamed</name>
      <uri>https://old.reddit.com/user/LahmeriMohamed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;how can i run flux model kontext dev locally ? i need documentation in pure python &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LahmeriMohamed"&gt; /u/LahmeriMohamed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1n3kf/help_in_using_flux_models_in_3060_8gb_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1n3kf/help_in_using_flux_models_in_3060_8gb_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1n3kf/help_in_using_flux_models_in_3060_8gb_vram_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m19upn</id>
    <title>So how do I fine-time a local model?</title>
    <updated>2025-07-16T11:14:57+00:00</updated>
    <author>
      <name>/u/ImYoric</name>
      <uri>https://old.reddit.com/user/ImYoric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm a newb, please forgive me if I'm missing some obvious documentation.&lt;/p&gt; &lt;p&gt;For the sake of fun and learning, I'd like to fine-tune a local model (haven't decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we're not there yet.&lt;/p&gt; &lt;p&gt;Right now, I'd like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it's not clear to me where to start. I'm not afraid of Python or PyTorch (or Rust, or C++), but I'm entirely lost on the process.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Any suggestion for a model to use as base? I'd like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don't need something particularly fancy.&lt;/li&gt; &lt;li&gt;How large a corpus do I need to get started?&lt;/li&gt; &lt;li&gt;Let's assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?&lt;/li&gt; &lt;li&gt;How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?&lt;/li&gt; &lt;li&gt;Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?&lt;/li&gt; &lt;li&gt;Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?&lt;/li&gt; &lt;li&gt;What else am I missing?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImYoric"&gt; /u/ImYoric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T11:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1qtgb</id>
    <title>LM Studio, MCP, Models and large JSON responses.</title>
    <updated>2025-07-16T22:37:11+00:00</updated>
    <author>
      <name>/u/Point5_MOA</name>
      <uri>https://old.reddit.com/user/Point5_MOA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, I got LM Studio running, have a MCP Server parsing XML Data (all runs successfully) and JSON Data comes back as expected. But I am having a problem with models ingesting this kind of data.&lt;/p&gt; &lt;p&gt;Given this tech is new and all is in the beginnings, I am expecting things going wrong. We are still in the learning phase here.&lt;/p&gt; &lt;p&gt;I have tested these three models so far:&lt;/p&gt; &lt;p&gt;qwen3-4b, Mistral 7B Instruct v0.2 and Llama 3 8B Instruct. All of them try to call the MCP multiple times.&lt;/p&gt; &lt;p&gt;My server delivers multiple pages of json data, not a single line like &amp;quot;The weather in your town XY is YZ&amp;quot;.&lt;/p&gt; &lt;p&gt;When asking to make a list of a specific attribute in the the list of the json response I never get a full list of the actual response. I am already cutting down the JSON response to attributs with actual data, ommitting fields with null or empty.&lt;/p&gt; &lt;p&gt;Has anybody had the same experience? If yes, feel free to vent your frustration here!&lt;/p&gt; &lt;p&gt;If you had success please share it with us.&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;p&gt;Edit: typos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Point5_MOA"&gt; /u/Point5_MOA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1qtgb/lm_studio_mcp_models_and_large_json_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1qtgb/lm_studio_mcp_models_and_large_json_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1qtgb/lm_studio_mcp_models_and_large_json_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T22:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m18nke</id>
    <title>GitHub - boneylizard/Eloquent: A local front-end for open-weight LLMs with memory, RAG, TTS/STT, Elo ratings, and dynamic research tools. Built with React and FastAPI.</title>
    <updated>2025-07-16T10:04:05+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üöÄ Just Dropped: Eloquent ‚Äì A Local LLM Powerhouse&lt;/h1&gt; &lt;p&gt;Hey LocalLLaMA! Just dropped &lt;strong&gt;Eloquent&lt;/strong&gt; after 4 months of &amp;quot;just one more feature&amp;quot; syndrome.&lt;/p&gt; &lt;p&gt;Started as a basic chat interface... ended up as a full-stack, dual-GPU, memory-retaining AI companion.&lt;br /&gt; Built entirely for local model users ‚Äî by someone who actually uses local models.&lt;/p&gt; &lt;h1&gt;üß† Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual-GPU architecture with memory offloading&lt;/li&gt; &lt;li&gt;Persistent memory system that learns who you are over time&lt;/li&gt; &lt;li&gt;Model ELO testing (head-to-head tournaments + scoring)&lt;/li&gt; &lt;li&gt;Auto-character creator (talk to an AI ‚Üí get a JSON persona)&lt;/li&gt; &lt;li&gt;Built-in SD support (EloDiffusion + ADetailer)&lt;/li&gt; &lt;li&gt;60+ TTS voices, fast voice-to-text&lt;/li&gt; &lt;li&gt;RAG support for PDFs, DOCX, and more&lt;/li&gt; &lt;li&gt;Focus &amp;amp; Call modes (clean UI &amp;amp; voice-only UX)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚Ä¶and probably a dozen other things I forgot I built.&lt;/p&gt; &lt;h1&gt;üõ†Ô∏è Install &amp;amp; Run&lt;/h1&gt; &lt;p&gt;Quick setup (Windows):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/boneylizard/Eloquent.git cd Eloquent install.bat run.bat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Works with any GGUF model. Supports single GPU, but flies with two.&lt;/p&gt; &lt;h1&gt;üß¨ Why?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I wanted real memory, so it remembers your background, style, vibe.&lt;/li&gt; &lt;li&gt;I wanted model comparisons that aren‚Äôt just vibes-based.&lt;/li&gt; &lt;li&gt;I wanted persona creation without filling out forms.&lt;/li&gt; &lt;li&gt;I wanted it modular, so anyone can build on top of it.&lt;/li&gt; &lt;li&gt;I wanted it local, private, and fast.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîì Open Source &amp;amp; Yours to Break&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;100% local ‚Äî nothing phones home&lt;/li&gt; &lt;li&gt;AGPL-3.0 licensed&lt;/li&gt; &lt;li&gt;Everything's in backend/app or frontend/src&lt;/li&gt; &lt;li&gt;The rest is just dependencies ‚Äî over 300 of them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please, try it out. Break it. Fork it. Adapt it.&lt;br /&gt; I genuinely think people will build cool stuff on top of this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/boneylizard/Eloquent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m18nke/github_boneylizardeloquent_a_local_frontend_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m18nke/github_boneylizardeloquent_a_local_frontend_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T10:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m118is</id>
    <title>Use claudecode with local models</title>
    <updated>2025-07-16T02:38:02+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month. So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code. Well, many folks are already doing that with local. So if you don't know, now you know. This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.&lt;/p&gt; &lt;p&gt;Start your local LLM API &lt;/p&gt; &lt;p&gt;Install claude code&lt;/p&gt; &lt;p&gt;install a proxy - &lt;a href="https://github.com/1rgs/claude-code-proxy"&gt;https://github.com/1rgs/claude-code-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit the &lt;a href="http://server.py"&gt;server.py&lt;/a&gt; proxy and point it to your OpenAI endpoint, could be llama.cpp, ollama, vllm, whatever you are running. &lt;/p&gt; &lt;p&gt;Add the line above load_dotenv&lt;br /&gt; +litellm.api_base = &amp;quot;http://yokujin:8083/v1&amp;quot; # use your localhost name/IP/ports&lt;/p&gt; &lt;p&gt;Start the proxy according to the docs which will run it in localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_BASE_URL=http://localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_AUTH_TOKEN=&amp;quot;sk-localkey&amp;quot;&lt;/p&gt; &lt;p&gt;run claude code&lt;/p&gt; &lt;p&gt;I just created my first code then decided to post this. I'm running the latest mistral-small-24b on that host. I'm going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T02:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m16o6r</id>
    <title>Official Local LLM support by AMD released. Lemonade</title>
    <updated>2025-07-16T07:53:22+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can somebody test the performance of Gemma3 12B / 27B q4 on different modes ONNX, llamacpp, GPU, CPU, NPU ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=mcf7dDybUco"&gt;https://www.youtube.com/watch?v=mcf7dDybUco&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T07:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1au28</id>
    <title>Vllm vs. llama.cpp</title>
    <updated>2025-07-16T12:06:46+00:00</updated>
    <author>
      <name>/u/Agreeable-Prompt-666</name>
      <uri>https://old.reddit.com/user/Agreeable-Prompt-666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Prompt-666"&gt; /u/Agreeable-Prompt-666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T12:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1k704</id>
    <title>Ollama and Open WebUI</title>
    <updated>2025-07-16T18:18:44+00:00</updated>
    <author>
      <name>/u/HeisenbergWalter</name>
      <uri>https://old.reddit.com/user/HeisenbergWalter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k704/ollama_and_open_webui/"&gt; &lt;img alt="Ollama and Open WebUI" src="https://b.thumbs.redditmedia.com/W0WHbIqez5BR0RPHkIBlWmGhPhSPNij3q2bk70k7TwU.jpg" title="Ollama and Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to set up my own Ollama server with OpenWebUI for my small business. I currently have the following options:&lt;/p&gt; &lt;p&gt;I still have 5 x RTX 3080 GPUs from my mining days ‚Äî or would it be better to buy a Mac Mini with the M4 chip?&lt;/p&gt; &lt;p&gt;What would you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeisenbergWalter"&gt; /u/HeisenbergWalter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m1k704"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k704/ollama_and_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k704/ollama_and_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T18:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0v9m1</id>
    <title>Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!</title>
    <updated>2025-07-15T22:04:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt; &lt;img alt="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ETH Zurich &amp;amp; EPFL Public LLM ‚Äì Technical Specs ‚Ä¢ Release: Late summer 2025 ‚Ä¢ Developers: EPFL, ETH Zurich, Swiss National Supercomputing Centre (CSCS), Swiss universities ‚Ä¢ Model sizes: 8B and 70B parameters (fully open weights and code, Apache 2.0 license) ‚Ä¢ Multilinguality: Fluency in 1,000+ languages (trained on &amp;gt;1,500 languages; ~60% English, ~40% non-English; code and math included) ‚Ä¢ Training data: &amp;gt;15 trillion tokens, high-quality, transparent, reproducible, with web-crawling opt-outs respected ‚Ä¢ Training hardware: Alps supercomputer (CSCS, Lugano), &amp;gt;10,000 NVIDIA Grace Hopper Superchips, 100% carbon-neutral electricity ‚Ä¢ Compliance: Swiss data protection and copyright laws, EU AI Act transparency ‚Ä¢ Intended use: Science, society, industry; fully public download, detailed documentation on model architecture and training ‚Ä¢ Initiative: Swiss AI Initiative, 800+ researchers, 20M+ GPU hours/year, funded by ETH Board (2025‚Äì2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T22:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1aj8n</id>
    <title>üì¢ [RELEASE] LoFT CLI: Fine-tune &amp; Deploy LLMs on CPU (8GB RAM, No GPU, No Cloud)</title>
    <updated>2025-07-16T11:51:46+00:00</updated>
    <author>
      <name>/u/diptanshu1991</name>
      <uri>https://old.reddit.com/user/diptanshu1991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/"&gt;previous post&lt;/a&gt; ‚Äî the repo is &lt;strong&gt;finally public&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;üî• TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/diptanshu1991/LoFT"&gt;diptanshu1991/LoFT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What you get&lt;/strong&gt;: 5 CLI commands: &lt;code&gt;loft finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;export&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: Tested on 8GB MacBook Air ‚Äî peak RAM &lt;strong&gt;330MB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 300 Dolly samples, 2 epochs ‚Üí 1.5 hrs total wall-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference speed&lt;/strong&gt;: 6.9 tok/sec (Q4_0) on CPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT ‚Äì 100% open-source&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† What is LoFT?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;LoFT CLI&lt;/strong&gt; is a lightweight, CPU-friendly toolkit that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ Finetune 1‚Äì3B LLMs like TinyLlama using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;üîÑ Merge and export models to &lt;strong&gt;GGUF&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;üß± Quantize models (Q4_0, Q5_1, etc.)&lt;/li&gt; &lt;li&gt;üí¨ Run &lt;strong&gt;offline inference&lt;/strong&gt; using &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All from a &lt;strong&gt;command-line interface&lt;/strong&gt; on your &lt;strong&gt;local laptop&lt;/strong&gt;. No Colab. No GPUs. No cloud.&lt;/p&gt; &lt;h1&gt;üìä Benchmarks (8GB MacBook Air)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Output&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Peak RAM&lt;/th&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Finetune&lt;/td&gt; &lt;td align="left"&gt;LoRA Adapter&lt;/td&gt; &lt;td align="left"&gt;4.3 MB&lt;/td&gt; &lt;td align="left"&gt;308 MB&lt;/td&gt; &lt;td align="left"&gt;23 min&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Merge&lt;/td&gt; &lt;td align="left"&gt;HF Model&lt;/td&gt; &lt;td align="left"&gt;4.2 GB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;4.7 min&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Export&lt;/td&gt; &lt;td align="left"&gt;GGUF (FP16)&lt;/td&gt; &lt;td align="left"&gt;2.1 GB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;83 sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Quantize&lt;/td&gt; &lt;td align="left"&gt;GGUF (Q4_0)&lt;/td&gt; &lt;td align="left"&gt;607 MB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;21 sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Chat&lt;/td&gt; &lt;td align="left"&gt;6.9 tok/sec&lt;/td&gt; &lt;td align="left"&gt;‚Äì&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;79 sec&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;üß™ Trained on: 300 Dolly samples, 2 epochs ‚Üí loss &amp;lt; 1.0&lt;/p&gt; &lt;h1&gt;üß™ 5-Command Lifecycle&lt;/h1&gt; &lt;p&gt;LoFT runs the complete LLM workflow ‚Äî from training to chat ‚Äî in just 5 commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;loft finetune loft merge loft export loft quantize loft chat &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;üß™ Coming Soon in LoFT&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;üì¶ Plug-and-Play Recipes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Legal Q&amp;amp;A bots (air-gapped, offline)&lt;/li&gt; &lt;li&gt;Customer support assistants&lt;/li&gt; &lt;li&gt;Contract summarizers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;üå± Early Experiments&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-turn finetuning&lt;/li&gt; &lt;li&gt;Adapter-sharing for niche domains&lt;/li&gt; &lt;li&gt;Dataset templating tools&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;LoFT is built for indie builders, researchers, and OSS devs who want &lt;strong&gt;local GenAI without GPU constraints&lt;/strong&gt;. Would love your feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What models/datasets you would like to see supported next&lt;/li&gt; &lt;li&gt;Edge cases or bugs during install/training&lt;/li&gt; &lt;li&gt;Use cases where this unlocks new workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/diptanshu1991/LoFT"&gt;https://github.com/diptanshu1991/LoFT&lt;/a&gt;&lt;br /&gt; ü™™ MIT licensed ‚Äî feel free to fork, contribute, and ship your own CLI tools on top&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diptanshu1991"&gt; /u/diptanshu1991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T11:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1m1qw</id>
    <title>Experimental RAG Techniques Resource</title>
    <updated>2025-07-16T19:27:24+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1m1qw/experimental_rag_techniques_resource/"&gt; &lt;img alt="Experimental RAG Techniques Resource" src="https://external-preview.redd.it/BI_t2luUqpFFoT6ofhB1ODYTo_sYADo6lsFHbPcG2pU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2922466eed2048ef5034f453831931a6dee909d" title="Experimental RAG Techniques Resource" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone!&lt;/p&gt; &lt;p&gt;For the last couple of weeks, I've been working on creating the Experimental RAG Tech repo, which I think some of you might find really interesting. This repository contains various techniques for improving RAG workflows that I've come up with during my research fellowship at my University. Each technique comes with a detailed Jupyter notebook (openable in Colab) containing both an explanation of the intuition behind it and the implementation in Python.&lt;/p&gt; &lt;p&gt;Please note that these techniques are EXPERIMENTAL in nature, meaning they have not been seriously tested or validated in a production-ready scenario, but they represent improvements over traditional methods. If you‚Äôre experimenting with LLMs and RAG and want some fresh ideas to test, you might find some inspiration inside this repo. &lt;/p&gt; &lt;p&gt;I'd love to make this a collaborative project with the community: If you have any feedback, critiques or even your own technique that you'd like to share, contact me via the email or LinkedIn profile listed in the repo's README.&lt;/p&gt; &lt;p&gt;The repo currently contains the following techniques:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Dynamic K estimation with Query Complexity Score: Use traditional NLP methods to estimate a Query Complexity Score (QCS) which is then used to dynamically select the value of the K parameter.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Single Pass Rerank and Compression with Recursive Reranking: This technique combines Reranking and Contextual Compression into a single pass by using a Reranker Model.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Stay tuned! More techniques are coming soon, including a chunking method that does entity propagation and disambiguation.&lt;/p&gt; &lt;p&gt;If you find this project helpful or interesting, a ‚≠êÔ∏è on GitHub would mean a lot to me. Thank you! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LucaStrano/Experimental_RAG_Tech"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1m1qw/experimental_rag_techniques_resource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1m1qw/experimental_rag_techniques_resource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T19:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1im6y</id>
    <title>We built an open-source tool that trains both diffusion and text models together in a single interface</title>
    <updated>2025-07-16T17:19:52+00:00</updated>
    <author>
      <name>/u/OriginalSpread3100</name>
      <uri>https://old.reddit.com/user/OriginalSpread3100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Transformer Lab has just shipped major updates to our Diffusion model support!&lt;/p&gt; &lt;p&gt;Transformer Lab now allows you to generate and train both text models (LLMs) and diffusion models in the same interface. It‚Äôs open source (AGPL-3.0) and works on AMD and NVIDIA GPUs, as well as Apple silicon.&lt;/p&gt; &lt;p&gt;Now, we‚Äôve built support for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Most major open Diffusion models (including SDXL &amp;amp; Flux)&lt;/li&gt; &lt;li&gt;Inpainting&lt;/li&gt; &lt;li&gt;Img2img&lt;/li&gt; &lt;li&gt;LoRA training&lt;/li&gt; &lt;li&gt;Downloading any LoRA adapter for generation&lt;/li&gt; &lt;li&gt;Downloading any ControlNet and use process types like Canny, OpenPose and Zoe to guide generations&lt;/li&gt; &lt;li&gt;Auto-captioning images with WD14 Tagger to tag your image dataset / provide captions for training&lt;/li&gt; &lt;li&gt;Generating images in a batch from prompts and export those as a dataset &lt;/li&gt; &lt;li&gt;And much more! &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If this is helpful, please give it a try, share feedback and let us know what we should build next. &lt;/p&gt; &lt;p&gt;&lt;a href="https://transformerlab.ai/docs/intro"&gt;https://transformerlab.ai/docs/intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalSpread3100"&gt; /u/OriginalSpread3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1im6y/we_built_an_opensource_tool_that_trains_both/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1im6y/we_built_an_opensource_tool_that_trains_both/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1im6y/we_built_an_opensource_tool_that_trains_both/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m16kdm</id>
    <title>T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog</title>
    <updated>2025-07-16T07:46:29+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"&gt; &lt;img alt="T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog" src="https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbbccb5f228138b777b853e12fc432178fff5f50" title="T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma released a new encoder-decoder model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/t5gemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T07:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m14a9j</id>
    <title>Meta's new ASI team discussed about abandoning Meta's powerful Open-source and focus on developing close</title>
    <updated>2025-07-16T05:23:16+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html"&gt;https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T05:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1k0vh</id>
    <title>Enable AI Agents to join and interact in your meetings via MCP</title>
    <updated>2025-07-16T18:12:12+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k0vh/enable_ai_agents_to_join_and_interact_in_your/"&gt; &lt;img alt="Enable AI Agents to join and interact in your meetings via MCP" src="https://external-preview.redd.it/cGI3eHRnYXd5OWRmMV3lGHAjbP8zKUEsAVrqPldZ7glK4yeeVC_nwGPMlIIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24697674c4fa3827ea2b3579a6f96bfaed763451" title="Enable AI Agents to join and interact in your meetings via MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We've been working on an open-source project called joinly for the last 10 weeks. The idea is that you can connect your favourite MCP servers (e.g. Asana, Notion and Linear, GitHub etc.) to an AI agent and send that agent to any browser-based video conference. This essentially allows you to create your own custom meeting assistant that can perform tasks in real time during the meeting.&lt;/p&gt; &lt;p&gt;So, how does it work? Ultimately, joinly is also just a MCP server that you can host yourself, providing your agent with essential meeting tools (such as speak_text and send_chat_message) alongside automatic real-time transcription. By the way, we've designed it so that you can select your own LLM, TTS and STT providers. Locally runnable with Kokoro as TTS, Whisper as STT and a Llama model as you Local LLM. &lt;/p&gt; &lt;p&gt;We made a quick video to show how it works connecting it to the Tavily and GitHub MCP servers and let joinly explain how joinly works. Because we think joinly best speaks for itself.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback or ideas on which other MCP servers you'd like to use in your meetings. Or just try it out yourself üëâ &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9w98c7awy9df1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k0vh/enable_ai_agents_to_join_and_interact_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1k0vh/enable_ai_agents_to_join_and_interact_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T18:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m13eb2</id>
    <title>AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO "Blackwell" With 24 GB VRAM</title>
    <updated>2025-07-16T04:28:39+00:00</updated>
    <author>
      <name>/u/Rich_Repeat_22</name>
      <uri>https://old.reddit.com/user/Rich_Repeat_22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" src="https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82787d21e7b0821fdce5a034706e0598040c7cc4" title="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Said it when this was presented that will have MSRP around RTX5080 since AMD decided to bench it against that card and not some workstation grade RTX.... ü•≥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Repeat_22"&gt; /u/Rich_Repeat_22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-radeon-ai-pro-r9700-32-gb-gpu-listed-pricing-around-1250-half-price-nvidia-rtx-pro-blackwell-24-gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T04:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1jxia</id>
    <title>Intel preparing Nova Lake-AX, big APU design to counter AMD Strix Halo - VideoCardz.com</title>
    <updated>2025-07-16T18:08:39+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jxia/intel_preparing_nova_lakeax_big_apu_design_to/"&gt; &lt;img alt="Intel preparing Nova Lake-AX, big APU design to counter AMD Strix Halo - VideoCardz.com" src="https://external-preview.redd.it/bwl76m6f4s2m-Wxzq9Cxwn8RVXuFp8molq0d9EndLBE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9aa621ee885bf41c0a81dd4ebed4faa23382257" title="Intel preparing Nova Lake-AX, big APU design to counter AMD Strix Halo - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-preparing-nova-lake-ax-big-apu-design-to-counter-amd-strix-halo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jxia/intel_preparing_nova_lakeax_big_apu_design_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jxia/intel_preparing_nova_lakeax_big_apu_design_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T18:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0z1zx</id>
    <title>Your unpopular takes on LLMs</title>
    <updated>2025-07-16T00:52:41+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mine are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;All the popular public benchmarks are nearly worthless when it comes to a model's general ability. Literaly the only good thing we get out of them is a rating for &amp;quot;can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up&amp;quot;, which does have some value. I think the people who maintain the benchmarks know this too, but we're all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. That's because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don't even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON'T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1jd2r</id>
    <title>Anyone having luck with Hunyuan 80B A13B?</title>
    <updated>2025-07-16T17:47:54+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF"&gt;Hunyuan-80B-A13B&lt;/a&gt; looked really cool on paper, I hoped it would be the &amp;quot;large equivalent&amp;quot; of the excellent Qwen3 30B A3B. According to the official &lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;Hugging Face page&lt;/a&gt;, it's &lt;strong&gt;compact&lt;/strong&gt; yet &lt;strong&gt;powerful&lt;/strong&gt;, comparable to much larger models:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I tried Unsloth's UD-Q5_K_XL quant with recommended sampler settings and in the latest version of LM Studio, and I'm getting pretty overall terrible results. I also tried UD-Q8_K_XL in case the model is very sensitive to quantization, but I'm still getting bad results.&lt;/p&gt; &lt;p&gt;For example, when I ask it about astronomy, it gets basic facts wrong, such as claiming that Mars is much larger than Earth and that Mars is closer to the sun than Earth (when in fact, it is the opposite: Earth is both larger and closer to the sun than Mars).&lt;/p&gt; &lt;p&gt;It also feels weak in creative writing, where it spouts a lot of nonsense that does not make much sense.&lt;/p&gt; &lt;p&gt;I really want this model to be good. I feel like (and hope) that the issue lies with my setup rather than the model itself. Might it still be buggy in llama.cpp? Is there a problem with the Jinja/chat template? Is the model particularly sensitive to incorrect sampler settings?&lt;/p&gt; &lt;p&gt;Is anyone else having better luck with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1jd2r/anyone_having_luck_with_hunyuan_80b_a13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1myiq</id>
    <title>[Open-Source] self-hostable AI productivity agent using Qwen 3 (4B) - reads your apps, extracts tasks, runs them on autopilot</title>
    <updated>2025-07-16T20:03:07+00:00</updated>
    <author>
      <name>/u/therealkabeer</name>
      <uri>https://old.reddit.com/user/therealkabeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone!&lt;/p&gt; &lt;p&gt;we're currently building an open-source autopilot for maximising productivity. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; the idea is that users can connect their apps, AI will periodically read these apps for new context (like new emails, new calendar events, etc), extract action items from them, ask the user clarifying questions (if any), create plans for tackling tasks and after I approve these plans, the AI will go ahead and complete them.&lt;/p&gt; &lt;p&gt;basically, all users need to do is answer clarifying questions and approve plans, rather than having to open a chatbot, type a long prompt explaining what they want to get done, what the AI should read for context and so on.&lt;/p&gt; &lt;p&gt;If you want to know more about the project or self-host it, check out the repo here: &lt;a href="https://github.com/existence-master/Sentient"&gt;https://github.com/existence-master/Sentient&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here are some of the features we've implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;we were tired of chat interfaces and so we've made the entire app revolve around an &amp;quot;organizer&amp;quot; page where you can dump tasks, entries, or even general thoughts and the AI will manage it for you. the AI also writes to the organizer, allowing you to keep a track of everything its done, what info it needs or what tasks need to be approved&lt;/li&gt; &lt;li&gt;the AI can run on autopilot. it can periodically read my emails + calendar and extract action items and memories about me from there. action items get added to the organizer and become plans which eventually become tasks. memories are indexed in the memory pipeline. we want to add more context sources (apart from email and calendar) that the AI can read proactively&lt;/li&gt; &lt;li&gt;the memory pipeline allows the AI to learn about the user as time progresses. preferences, personal details and more are stored in the memory pipeline. &lt;/li&gt; &lt;li&gt;it works across a bunch of apps (such as Gmail, GCalendar, GDocs, GSheets, GSlides, GDrive, Notion, Slack, GitHub, etc.) It can also search the web, get up-to-date weather info, search for shopping items, prepare charts and graphs and more.&lt;/li&gt; &lt;li&gt;You can also schedule your tasks to run at a specific time or run as recurring workflows at defined intervals. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some other nice-to-haves we've added are WhatsApp notifications (the AI can notify users of what its doing on WhatsApp), privacy filters (block certain keywords, email addresses, etc so that the AI will never process emails or calendar events you don't want it to)&lt;/p&gt; &lt;p&gt;the project is fully open-source and self-hostable using Docker&lt;/p&gt; &lt;p&gt;Some tech stuff:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Frontend: NextJS&lt;/li&gt; &lt;li&gt;Backend: Python&lt;/li&gt; &lt;li&gt;Agentic Framework: Qwen Agent &lt;/li&gt; &lt;li&gt;Model: Qwen 3 (4B) - this is a VERY impressive small model for tool calling&lt;/li&gt; &lt;li&gt;Integrations: Custom MCP servers built with FastMCP that wrap the APIs of a bunch of services into tools that the agents can use.&lt;/li&gt; &lt;li&gt;Others: Celery for task queue management with Redis, MongoDB as the database, Docker for containerization, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd greatly appreciate any feedback or ideas for improvements we can make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealkabeer"&gt; /u/therealkabeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1myiq/opensource_selfhostable_ai_productivity_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1hixa</id>
    <title>Playing around with the design of my pet project - does this look decent or nah?</title>
    <updated>2025-07-16T16:39:57+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt; &lt;img alt="Playing around with the design of my pet project - does this look decent or nah?" src="https://a.thumbs.redditmedia.com/oijm0DyxoxFre_g3otVxcpGYCKM5xFhT79TCi5btsZ8.jpg" title="Playing around with the design of my pet project - does this look decent or nah?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a showcase of my project recently, would be glad to hear opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m1hixa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1foz1</id>
    <title>CUDA is coming to MLX</title>
    <updated>2025-07-16T15:31:43+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt; &lt;img alt="CUDA is coming to MLX" src="https://external-preview.redd.it/w8edStcv8JcRcgUOJ4-eZrp8x-ns7z_4bZz-mt8i8eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9049533811e0bc40173ac835b90eb4f9943876f0" title="CUDA is coming to MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will soon get CUDA support in MLX - this means that we‚Äôll be able to run MLX programs on both Apple Silicon and CUDA GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/1983"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T15:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1np9n</id>
    <title>Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶.</title>
    <updated>2025-07-16T20:32:09+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt; &lt;img alt="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." src="https://preview.redd.it/obmnyjusqadf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af76a5484872eea14e9f64b8e559cdd32a58c0" title="Sometime‚Ä¶ in the next 3 to 5 decades‚Ä¶." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/obmnyjusqadf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1np9n/sometime_in_the_next_3_to_5_decades/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T20:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1h0fy</id>
    <title>Support for diffusion models (Dream 7B) has been merged into llama.cpp</title>
    <updated>2025-07-16T16:20:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt; &lt;img alt="Support for diffusion models (Dream 7B) has been merged into llama.cpp" src="https://external-preview.redd.it/OqAAbOs6fFLPZaNF0M6vIqHJqNLZwtArB7hBcX1IZ7M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8971ce6047ae48ffddcf53ec22de6523ddaa226e" title="Support for diffusion models (Dream 7B) has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion models are a new kind of language model that generate text by denoising random noise step-by-step, instead of predicting tokens left to right like traditional LLMs.&lt;/p&gt; &lt;p&gt;This PR adds basic support for diffusion models, using Dream 7B instruct as base. DiffuCoder-7B is built on the same arch so it should be trivial to add after this.&lt;br /&gt; [...]&lt;br /&gt; &lt;strong&gt;Another cool/gimmicky thing is you can see the diffusion unfold&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a joint effort with Huawei Noah‚Äôs Ark Lab, we release &lt;strong&gt;Dream 7B&lt;/strong&gt; (Diffusion reasoning model), the most powerful open diffusion large language model to date.&lt;/p&gt; &lt;p&gt;In short, Dream 7B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;consistently outperforms existing diffusion language models by a large margin;&lt;/li&gt; &lt;li&gt;matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;&lt;/li&gt; &lt;li&gt;demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1i922</id>
    <title>He‚Äôs out of line but he‚Äôs right</title>
    <updated>2025-07-16T17:06:31+00:00</updated>
    <author>
      <name>/u/EstablishmentFun3205</name>
      <uri>https://old.reddit.com/user/EstablishmentFun3205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt; &lt;img alt="He‚Äôs out of line but he‚Äôs right" src="https://preview.redd.it/dqx9wlf3q9df1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d71f6c8f3707ff6aae1011b47babeb593bd890e1" title="He‚Äôs out of line but he‚Äôs right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EstablishmentFun3205"&gt; /u/EstablishmentFun3205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dqx9wlf3q9df1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:06:31+00:00</published>
  </entry>
</feed>
