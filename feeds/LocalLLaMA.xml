<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-27T18:53:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1llvz0g</id>
    <title>7900XTX vs RTX3090</title>
    <updated>2025-06-27T14:56:21+00:00</updated>
    <author>
      <name>/u/_ballzdeep_</name>
      <uri>https://old.reddit.com/user/_ballzdeep_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm building a machine for gaming/ AI hobbyist and right now I'm debating myself on the GPU. My budget is around 750$ for the GPU. Refurbished 7900xtx with 5 months warranty for 690$ Used RTX3090 for 750$ New 5070ti New RX9070XT&lt;/p&gt; &lt;p&gt;I'm leaning towards a used GPU. I know ROCM and Vulkan have improved AMD inference massively and the warranty on 7900xtx is nice as well.&lt;/p&gt; &lt;p&gt;What are your suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ballzdeep_"&gt; /u/_ballzdeep_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T14:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll68iz</id>
    <title>Gemma 3n Full Launch - Developers Edition</title>
    <updated>2025-06-26T17:31:27+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Today we have the full launch of Gemma 3n, meaning we have support for your favorite tools as well as full support for its capabilities &lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recap&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Audio, video, image, and text input; text output&lt;/li&gt; &lt;li&gt;E2B and E4B - while their raw parameter count is 5B and 8B, you can operate them with as little as 2B and 4B effective params&lt;/li&gt; &lt;li&gt;MatFormer: The model architecture allows extracting submodels and doing mix-n-match, allowing you to export additional models in your favorite size between 2B and 4B.&lt;/li&gt; &lt;li&gt;MobileNetV5 and a new audio encoder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now...for supported tools. We collaborated with many many open source developers to enable its capabilities. So you can now use Gemma in Hugging Face, Kaggle, llama.cpp, Ollama, MLX, LMStudio, transformers.js, Docker model hub, Unsloth, transformers trl and PEFT, VLLM, SGLang, Jetson AI Lab, and many others. Enjoy! We'll also host a Kaggle competition if anyone wants to join &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;https://www.kaggle.com/competitions/google-gemma-3n-hackathon&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Unsloth &lt;a href="https://unsloth.ai/blog/gemma-3n"&gt;https://unsloth.ai/blog/gemma-3n&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF blog &lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;LMStudio &lt;a href="https://lmstudio.ai/models/google/gemma-3n-e4b"&gt;https://lmstudio.ai/models/google/gemma-3n-e4b&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;AI Studio &lt;a href="http://ai.dev"&gt;ai.dev&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3n"&gt;https://www.kaggle.com/models/google/gemma-3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc"&gt;https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc&lt;/a&gt; &lt;/li&gt; &lt;li&gt;ONNX/transformers.js &lt;a href="https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX"&gt;https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Vertex &lt;a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n"&gt;https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;GGUF &lt;a href="https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7"&gt;https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm0btg</id>
    <title>Mid-30s SWE: Take Huge Pay Cut for Risky LLM Research Role?</title>
    <updated>2025-06-27T17:49:28+00:00</updated>
    <author>
      <name>/u/Worth_Contract7903</name>
      <uri>https://old.reddit.com/user/Worth_Contract7903</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current Situation: * TC: 110k * YoE: 2 years as a Software Engineer (career switcher, mid-30s). * Role: SWE building AI applications using RAG. I've developed a strong passion for building LLMs, not just using them. I do not have a PhD.&lt;/p&gt; &lt;p&gt;I've been offered a role at a national lab to do exactly that—build LLMs from scratch and publish research, which could be a stepping stone to a top-tier team.&lt;/p&gt; &lt;p&gt;The problem is the offer has major red flags. It’s a significant pay cut, and my contact there admits the rest of the team is unmotivated and out of touch. More critically, the project's funding is only guaranteed until June of next year, and my contact, the only person I'd want to work with, will likely leave in two years. I'm worried about taking a huge risk that could blow up and leave me with nothing. My decision comes down to the future of AI roles. Is core LLM development a viable path without a PhD, or is the safer money in AI app development and fine-tuning? &lt;/p&gt; &lt;p&gt;Given the unstable funding and weak team, would you take this risky, low-paying job for a shot at a dream role, or is it a career-killing move?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth_Contract7903"&gt; /u/Worth_Contract7903 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T17:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll38zu</id>
    <title>FLUX.1 Kontext [dev] - an open weights model for proprietary-level image editing performance.</title>
    <updated>2025-06-26T15:35:49+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;weights: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev"&gt;https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;release news: &lt;a href="https://x.com/bfl_ml/status/1938257909726519640"&gt;https://x.com/bfl_ml/status/1938257909726519640&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll429p</id>
    <title>gemma 3n has been released on huggingface</title>
    <updated>2025-06-26T16:07:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B"&gt;https://huggingface.co/google/gemma-3n-E2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B-it"&gt;https://huggingface.co/google/gemma-3n-E2B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B"&gt;https://huggingface.co/google/gemma-3n-E4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it"&gt;https://huggingface.co/google/gemma-3n-E4B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(You can find benchmark results such as HellaSwag, MMLU, or LiveCodeBench above)&lt;/p&gt; &lt;p&gt;llama.cpp implementation by &lt;a href="https://github.com/ngxson"&gt;&lt;strong&gt;ngxson&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14400"&gt;https://github.com/ggml-org/llama.cpp/pull/14400&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical announcement:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llwtcd</id>
    <title>🛠️ ChatUI + Jupyter: A smooth way to test LLMs in your notebook interface</title>
    <updated>2025-06-27T15:30:07+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;If you're working with LLMs and want a clean, chat-style interface inside Jupyter notebooks, I’ve been experimenting with ChatUI integration — and it actually works really well for prototyping and testing.&lt;/p&gt; &lt;p&gt;You get:&lt;/p&gt; &lt;p&gt;A lightweight frontend (ChatUI)&lt;/p&gt; &lt;p&gt;Inside Jupyter (no extra servers needed)&lt;/p&gt; &lt;p&gt;Supports streaming responses from LLMs&lt;/p&gt; &lt;p&gt;Great for testing prompts, workflows, or local models&lt;/p&gt; &lt;p&gt;Has anyone else tried integrating UI layers like this into notebooks? Would love to know if you're using something lighter or more custom.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwtcd/chatui_jupyter_a_smooth_way_to_test_llms_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwtcd/chatui_jupyter_a_smooth_way_to_test_llms_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llwtcd/chatui_jupyter_a_smooth_way_to_test_llms_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1llv59w</id>
    <title>[2506.20702] The Singapore Consensus on Global AI Safety Research Priorities</title>
    <updated>2025-06-27T14:22:05+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Empire not happy, the Empire miserable. The Empire want to control your hardware. From the paper:&lt;/p&gt; &lt;p&gt;3.1.2 Conventional Intervention&lt;/p&gt; &lt;p&gt;Intervention techniques complement monitoring tools by offering various strategies to act on systems in ways that reduce risks from harmful behaviours.&lt;/p&gt; &lt;p&gt;Hardware-enabled mechanisms: Tools built into hardware could be used to enforce requirements about what can be run and by whom on specialised hardware (RAND). For example, hardware mechanisms could be used to block or halt certain jobs from being run on hardware if they fail an authentication process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.20702"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llv59w/250620702_the_singapore_consensus_on_global_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llv59w/250620702_the_singapore_consensus_on_global_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T14:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lluwee</id>
    <title>Are the new architectures Mamba and Jamba better or worse than current existing Transformer architectures.</title>
    <updated>2025-06-27T14:11:30+00:00</updated>
    <author>
      <name>/u/Direct-Lifeguard-607</name>
      <uri>https://old.reddit.com/user/Direct-Lifeguard-607</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When it comes to Mamba I've heard that it can run in constant time and train in O(n) compared to transformers which run in O(n) and train in O(n^2). I've also heard that Mamba is better with memory and power usage. I'm a bit confused by Jamba since it's a mixture of the two with alternating Mamba and Transformer blocks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct-Lifeguard-607"&gt; /u/Direct-Lifeguard-607 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T14:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1llzuit</id>
    <title>What's a good completion only model these days?</title>
    <updated>2025-06-27T17:30:16+00:00</updated>
    <author>
      <name>/u/quakquakquak</name>
      <uri>https://old.reddit.com/user/quakquakquak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for one I could run locally that isn't trained yet into doing questions &amp;amp; responses. Unfortunately a bunch of &amp;quot;base&amp;quot; models now are actually already trained to do that, so I had trouble finding a newer one. This is mostly for writing and seeing what sorts of things it comes up with 8)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quakquakquak"&gt; /u/quakquakquak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T17:30:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1llb5e9</id>
    <title>Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel.</title>
    <updated>2025-06-26T20:44:28+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt; &lt;img alt="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." src="https://preview.redd.it/x6kkfnuo2c9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d99eb39eccf80408c1a602f9dfe2d9fb44ce50a" title="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x6kkfnuo2c9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1llx5g1</id>
    <title>Third Batch of OSS AI Grants (SGLang, Ostris, Open WebUI, SWE-Bench, Pliny, Janus, Truth Terminal, Arc Prize)</title>
    <updated>2025-06-27T15:43:17+00:00</updated>
    <author>
      <name>/u/rajko_rad</name>
      <uri>https://old.reddit.com/user/rajko_rad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched the third batch of Open Source AI Grants, grants for independent researchers, hackers, and small teams doing foundational work in open source AI.&lt;/p&gt; &lt;p&gt;Our goal is to support the kind of experimentation, creativity, and transparency that keeps the AI ecosystem healthy and innovative.&lt;/p&gt; &lt;p&gt;This batch includes projects focused on LLM evaluation, novel reasoning tests, infrastructure, and experimental research at the edge of capability and cognition.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SGLang:&lt;/strong&gt; high-performance LLM serving infra powering trillions of tokens daily&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ostris:&lt;/strong&gt; diffusion model training tools optimized for consumer GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open WebUI:&lt;/strong&gt; self-hosted AI platforms for full data sovereignty&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SWE-Bench / SWE-Agent:&lt;/strong&gt; benchmarking and building AI software engineers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ARC Prize:&lt;/strong&gt; advancing AGI evals through reasoning benchmarks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truth_terminal:&lt;/strong&gt; exploring AI autonomy and cultural influence via semi-autonomous agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Elder_plinius:&lt;/strong&gt; researching LLM boundaries and prompt engineering strategies&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Janus:&lt;/strong&gt; exploring AI’s philosophical and creative frontiers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you to all the grantees for pushing things forward in the open. We are proud and grateful to support your work. Please let us know in the comments if there are folks you believe we should support in the future!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajko_rad"&gt; /u/rajko_rad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx5g1/third_batch_of_oss_ai_grants_sglang_ostris_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx5g1/third_batch_of_oss_ai_grants_sglang_ostris_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llx5g1/third_batch_of_oss_ai_grants_sglang_ostris_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lln5uj</id>
    <title>Reverse Engineering Gemma 3n</title>
    <updated>2025-06-27T06:45:04+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"&gt; &lt;img alt="Reverse Engineering Gemma 3n" src="https://external-preview.redd.it/VWUjHfeZBfEe00CQ4OXN4N4xfnF0YI65AE8Jt2eK1GQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca2d2136f62fedb0728942e71b3b8ff4a4232339" title="Reverse Engineering Gemma 3n" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/antimatter15/reverse-engineering-gemma-3n"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:45:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1llzdi8</id>
    <title>I built an Automated AI Stylist in 24 hours (open source, local)</title>
    <updated>2025-06-27T17:11:21+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"&gt; &lt;img alt="I built an Automated AI Stylist in 24 hours (open source, local)" src="https://external-preview.redd.it/aWJoanhkd2I1aTlmMeEsEqhEcpnAGeAOI3lYg_mXc9hWrD9oAMlWiqt_A_Sq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aa3b18ac6df7c246ae5daf5fad4a83ff312eb26" title="I built an Automated AI Stylist in 24 hours (open source, local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2v76newb5i9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T17:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1llnwna</id>
    <title>dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!</title>
    <updated>2025-06-27T07:34:10+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"&gt; &lt;img alt="dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!" src="https://external-preview.redd.it/eGthenQ5ZHQ5ZjlmMQQDM_dLcTyHBC8BScL5E00e_jl5aRRWjMUA-Nu_qDSf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4758657d96c820f19f51a4aed82016d536d0826b" title="dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share an update to &lt;a href="http://dyad.sh/"&gt;&lt;strong&gt;Dyad&lt;/strong&gt;&lt;/a&gt; which is a free, local, open-source AI app builder I've been working on for 3 months after leaving Google. It's designed as an alternative to v0, Lovable, and Bolt, but it runs on your computer (it's an Electron app)!&lt;/p&gt; &lt;p&gt;Here’s what makes Dyad different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run ANY model (including local LLMs!)&lt;/strong&gt; - Based on popular demand from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt;this sub-reddit&lt;/a&gt;, Dyad supports &lt;a href="https://www.dyad.sh/docs/guides/ai-models/local-models"&gt;local models&lt;/a&gt; via LM Studio and ollama (I don't play favorites!), and you can also connect it to any OpenAI API-compatible model!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; - Dyad runs entirely on your computer, making it fast and frictionless. Because your code lives locally, you can easily switch back and forth between Dyad and your IDE like Cursor, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; - Dyad is free and bring-your-own API key. This means you can use your free Gemini/OpenRouter API key and build apps in Dyad for free.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download Dyad for free: &lt;a href="https://dyad.sh/"&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dyad works on Mac &amp;amp; Windows and Linux (you can download Linux directly from &lt;a href="https://github.com/dyad-sh/dyad/releases"&gt;GitHub&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please share any feedback - would you be interested in MCP support?&lt;/p&gt; &lt;p&gt;P.S. I'm also launching on Product Hunt today and would appreciate any support 🙏 &lt;a href="https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool"&gt;https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t461p9dt9f9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T07:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll6jo5</id>
    <title>DeepSeek R2 delayed</title>
    <updated>2025-06-26T17:43:13+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt; &lt;img alt="DeepSeek R2 delayed" src="https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5423692617bfdf316daec6232ca857bc69416c" title="DeepSeek R2 delayed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Over the past several months, DeepSeek's engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek's models to enterprise customers.&lt;/p&gt; &lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt; &lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt; &lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt; &lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia's H20 chips, The Information said.&lt;/p&gt; &lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sources : &lt;a href="https://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls"&gt;[1]&lt;/a&gt; &lt;a href="https://x.com/kimmonismus/status/1938221881175183740"&gt;[2]&lt;/a&gt; &lt;a href="https://www.reuters.com/world/china/deepseek-r2-launch-stalled-ceo-balks-progress-information-reports-2025-06-26/"&gt;[3]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/718m48of6b9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1llty3n</id>
    <title>Gemma 3N on ChatterUI</title>
    <updated>2025-06-27T13:30:45+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/"&gt; &lt;img alt="Gemma 3N on ChatterUI" src="https://external-preview.redd.it/bDhmNGU5b2EyaDlmMaqG3pvP9RZCPXP8pBQTkpjntjyFw5myStLfVsGSm3Uj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=490d26a2034391a147c28b471411c08007a15090" title="Gemma 3N on ChatterUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qe2y2po62h9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T13:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1llms46</id>
    <title>FYI to everyone: RTX 3090 prices crashed and are back to baseline. You can finally get $600something 3090s again in the USA.</title>
    <updated>2025-06-27T06:20:23+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've been priced out by the spike to $1000+ recently for the past ~3 months, the prices finally dropped to baseline recently. &lt;/p&gt; &lt;p&gt;You can get a $650-750 Nvidia 3090 fairly easily now, instead of being nearly impossible. &lt;/p&gt; &lt;p&gt;Future pricing is unpredictable- if we follow expected deprecation trends, the 3090 should be around $550-600, but then again Trump's tariff extensions expire in a few weeks and pricing is wild and likely to spike up. &lt;/p&gt; &lt;p&gt;If you're interested in GPUs, &lt;strong&gt;now&lt;/strong&gt; is probably the best time to buy for 3090s/4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1llnwy5</id>
    <title>AI performance of smartphone SoCs</title>
    <updated>2025-06-27T07:34:42+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt; &lt;img alt="AI performance of smartphone SoCs" src="https://external-preview.redd.it/H_9g87w3EitABPy3ZAOo2ZH9LlcpQ5L4KMiJgV1zrjo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf972823fcd97a8af0b34ddd0ede97ce0d9de05" title="AI performance of smartphone SoCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few things notable to me: - The difference between tiers is &lt;em&gt;huge&lt;/em&gt;. A 2022 Snapdragon 8 Gen 2 beats the 8s Gen 4. There are huge gaps between the Dimensity 9000, 8000 and 7000 series. - You can better get a high-end SoC that’s a few years old than the latest mid-range one.&lt;/p&gt; &lt;h2&gt;- In this benchmark, it’s mainly a Qualcomm and Mediatek competition. It seems optimized software libraries are immensely important in using hardware effectively.&lt;/h2&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llnwy5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T07:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm0m6i</id>
    <title>Copilot Chat for VS Code is now Open Source</title>
    <updated>2025-06-27T18:00:56+00:00</updated>
    <author>
      <name>/u/corysama</name>
      <uri>https://old.reddit.com/user/corysama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt; &lt;img alt="Copilot Chat for VS Code is now Open Source" src="https://external-preview.redd.it/tyJeCqipzT78spT8qdYr9nFThGnon2rt0efU2xelzLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c7ae49e1d763b069953250103aad9e1f240a4f3" title="Copilot Chat for VS Code is now Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/corysama"&gt; /u/corysama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/vscode-copilot-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1llsztp</id>
    <title>What I Learned Building Agents for Enterprises</title>
    <updated>2025-06-27T12:46:41+00:00</updated>
    <author>
      <name>/u/Beneficial-Sir-6261</name>
      <uri>https://old.reddit.com/user/Beneficial-Sir-6261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🏦 For the past 3 months, we've been developing AI agents together with banks, fintechs, and software companies. The most critical point I've observed during this process is: Agentic transformation will be a painful process, just like digital transformation. What I learned in the field:👇&lt;/p&gt; &lt;p&gt;1- Definitions related to artificial intelligence are not yet standardized. Even the definition of &amp;quot;AI agent&amp;quot; differs between parties in meetings.&lt;/p&gt; &lt;p&gt;2- Organizations typically develop simple agents. They are far from achieving real-world transformation. To transform a job that generates ROI, an average of 20 agents need to work together or separately.&lt;/p&gt; &lt;p&gt;3- Companies initially want to produce a basic working prototype. Everyone is ready to allocate resources after seeing real ROI. But there's an important point. High performance is expected from small models running on a small amount of GPU, and the success of these models is naturally low. Therefore, they can't get out of the test environment and the business turns into a chicken-and-egg problem.🐥&lt;/p&gt; &lt;p&gt;4- Another important point in agentic transformation is that significant changes need to be made in the use of existing tools according to the agent to be built. Actions such as UI changes in used applications and providing new APIs need to be taken. This brings many arrangements with it.🌪️&lt;/p&gt; &lt;p&gt;🤷‍♂️ An important problem we encounter with agents is the excitement about agents. This situation causes us to raise our expectations from agents. There are two critical points to pay attention to:&lt;/p&gt; &lt;p&gt;1- Avoid using agents unnecessarily. Don't try to use agents for tasks that can be solved with software. Agents should be used as little as possible. Because software is deterministic - we can predict the next step with certainty. However, we cannot guarantee 100% output quality from agents. Therefore, we should use agents only at points where reasoning is needed.&lt;/p&gt; &lt;p&gt;2- Due to MCP and Agent excitement, we see technologies being used in the wrong places. There's justified excitement about MCP in the sector. We brought MCP support to our framework in the first month it was released, and we even prepared a special page on our website explaining the importance of MCP when it wasn't popular yet. MCP is a very important technology. However, this should not be forgotten: if you can solve a problem with classical software methods, you shouldn't try to solve it using tool calls (MCP or agent) or LLM. It's necessary to properly orchestrate the technologies and concepts emerging with agents.🎻&lt;/p&gt; &lt;p&gt;If you can properly orchestrate agents and choose the right agentic transformation points, productivity increases significantly with agents. At one of our clients, a job that took 1 hour was reduced to 5 minutes. The 5 minutes also require someone to perform checks related to the work done by the Agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Sir-6261"&gt; /u/Beneficial-Sir-6261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T12:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1llqp0a</id>
    <title>The more LLMs think, the worse they translate</title>
    <updated>2025-06-27T10:41:40+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"&gt; &lt;img alt="The more LLMs think, the worse they translate" src="https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c116c7e6295d776b6382e425434256d0d8559943" title="The more LLMs think, the worse they translate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T10:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1llhdoq</id>
    <title>I'm using a local Llama model for my game's dialogue system!</title>
    <updated>2025-06-27T01:23:40+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt; &lt;img alt="I'm using a local Llama model for my game's dialogue system!" src="https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a31f419b54bcf613f907d27abae7c2526e8092" title="I'm using a local Llama model for my game's dialogue system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm blown away by how fast and intelligent Llama 3.2 is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgoobkv5gd9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T01:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1llwfwv</id>
    <title>Qwen VLo: From "Understanding" the World to "Depicting" It</title>
    <updated>2025-06-27T15:15:25+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"&gt; &lt;img alt="Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It" src="https://external-preview.redd.it/p-RdsB-v9L-CFrA5EkxqdVn1O17bnDolUwqTorCzqTE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf118a7b3e066763df86407692a4a20da4c744d0" title="Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen-vlo/"&gt;https://qwenlm.github.io/blog/qwen-vlo/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llwfwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llx4ky</id>
    <title>Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title>
    <updated>2025-06-27T15:42:21+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt; &lt;img alt="Prime Intellect: We did it — SYNTHETIC‑2 is complete." src="https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d" title="Prime Intellect: We did it — SYNTHETIC‑2 is complete." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/PrimeIntellect/status/1938490370054361422"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1llndut</id>
    <title>Hunyuan-A13B released</title>
    <updated>2025-06-27T06:59:21+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt; &lt;img alt="Hunyuan-A13B released" src="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" title="Hunyuan-A13B released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and τ-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:59:21+00:00</published>
  </entry>
</feed>
