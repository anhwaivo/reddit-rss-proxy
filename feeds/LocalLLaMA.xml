<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-26T17:35:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iab1oe</id>
    <title>Best frameworks for fine-tuning models—what’s everyone using?</title>
    <updated>2025-01-26T10:20:25+00:00</updated>
    <author>
      <name>/u/Vivid-Entertainer752</name>
      <uri>https://old.reddit.com/user/Vivid-Entertainer752</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m new to fine-tuning LLMs/SLMs and trying to figure out what tools people usually use for this. From what I’ve seen so far, here are some options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Hugging Face TRL&lt;/strong&gt; (e.g., SFT, PPO, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Unsloth AI&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;No-code tools&lt;/strong&gt; like Together AI, Predibase, FinetuneDB&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any other thing?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Entertainer752"&gt; /u/Vivid-Entertainer752 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T10:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia8h46</id>
    <title>How CPU inference speed scales with memory bandwidth</title>
    <updated>2025-01-26T07:18:25+00:00</updated>
    <author>
      <name>/u/TheSilverSmith47</name>
      <uri>https://old.reddit.com/user/TheSilverSmith47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt; &lt;img alt="How CPU inference speed scales with memory bandwidth" src="https://external-preview.redd.it/srgBSZYNdTn-urtCEL65uOO5QGOSSrTYFh6M4eazrmc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8851aa0f5e9f9680cf0f2ab8bbc8d8819d519038" title="How CPU inference speed scales with memory bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's well known in the community by now that inference speed is currently memory bandwidth limited. I wanted to get hands-on experience with this bottleneck, so I set out to do test the CPU inference speed of my laptop at various memory bandwidths. Here are the results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/57u2fk7idafe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b99c835893709e93209c8d38ebe1c306aa6fce"&gt;https://preview.redd.it/57u2fk7idafe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b99c835893709e93209c8d38ebe1c306aa6fce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o8arwewxdafe1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43f0c153e8b87a82b8b11f927e358a9ba4ad29fa"&gt;https://preview.redd.it/o8arwewxdafe1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43f0c153e8b87a82b8b11f927e358a9ba4ad29fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, inference speed scales pretty linearly with memory bandwidth, affirming what most of us probably already know.&lt;/p&gt; &lt;p&gt;My laptop is an MSI GP66 11UH-028. It has an Intel 11800H, 64GB of 3200 MHz DDR4 RAM, and an 8GB mobile 3080 (although the GPU is not important for this test). To control the memory bandwidth of my system, I set a memory frequency limit in my BIOS. Unfortunately, there is no way to set a custom memory frequency limit, so I had to use the frequency limit presets built into my BIOS. Thankfully, there were plenty of frequency limit presets to choose from.&lt;/p&gt; &lt;p&gt;To validate the frequency of my RAM, I used CPU-Z and multiplied the memory frequency by two.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wbhwk7b2fafe1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2f92d3408ec5c7cce23c016d345649f83bc929f"&gt;https://preview.redd.it/wbhwk7b2fafe1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2f92d3408ec5c7cce23c016d345649f83bc929f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not sure why CPU-Z reads the frequency as half of what it actually is. When I set my frequency limit to 3200 MHz, the DRAM frequency read ~1600 MHz; when set to 2667 MHz, it read ~1333 MHz. I'm not sure why this is, but it did it consistently enough that I was comfortable using these values for my measured RAM frequency.&lt;/p&gt; &lt;p&gt;You can calculate the theoretical maximum memory bandwidth of your system using the formula found on &lt;a href="https://www.intel.com/content/www/us/en/support/articles/000056722/processors/intel-core-processors.html"&gt;this&lt;/a&gt; website. To validate the memory bandwidth of my system, I used &lt;a href="https://www.intel.com/content/www/us/en/download/736633/intel-memory-latency-checker-intel-mlc.html"&gt;Intel's Memory Latency Checker&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tlc0nqufafe1.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb514350affe3e2a76a12653cc58e48f64a48c0"&gt;https://preview.redd.it/6tlc0nqufafe1.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb514350affe3e2a76a12653cc58e48f64a48c0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The test measured many different values, but the only value I was interested in was the peak injection memory bandwidth.&lt;/p&gt; &lt;p&gt;I then loaded Qwen2.5-0.5B-Q8 into KoboldCPP using my CPU, FlashAttention, and a context length of 4096. I ran an inference 10 times and recorded the total inference rate for each output. I then averaged the inference rate and repeated this test for the various RAM frequency configurations.&lt;/p&gt; &lt;p&gt;I'm pretty satisfied with these results because they show linear scaling of inference speed with memory frequency. Next I plan to do the same test with my &lt;a href="https://www.notebookcheck.net/Intel-UHD-Graphics-Xe-32EUs-GPU-Tiger-Lake-H-Benchmarks-and-Specs.527298.0.html"&gt;iGPU&lt;/a&gt; to see if it will also benefit from higher memory speeds. Then I'll do the same for my dGPU by underclocking and overclocking my VRAM in MSI Afterburner.&lt;/p&gt; &lt;p&gt;If anyone has a Ryzen AI HX 370 CPU, would you be willing to perform the same test that I did for CPU inference? I'm curious to know how that CPU is able to handle a larger LLM (&amp;gt;30b parameters) at high DDR5 frequencies.&lt;/p&gt; &lt;p&gt;I'm also pretty excited for the Ryzen AI Max+ 395, though, given how we are currently memory bandwidth limited, I'm not too sure how the extra compute would help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSilverSmith47"&gt; /u/TheSilverSmith47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T07:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zdh3</id>
    <title>Best NSFW model for story telling?</title>
    <updated>2025-01-25T22:51:40+00:00</updated>
    <author>
      <name>/u/Might-Be-A-Ninja</name>
      <uri>https://old.reddit.com/user/Might-Be-A-Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if there are any models geared for it, but I want something that can write full stories, with me just giving it some direction&lt;/p&gt; &lt;p&gt;I am mainly into BDSM, if it matters&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Might-Be-A-Ninja"&gt; /u/Might-Be-A-Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T22:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wnfs</id>
    <title>Why do openai and meta etc plan to spend so much on data centers? how do they make the money back?</title>
    <updated>2025-01-25T20:48:52+00:00</updated>
    <author>
      <name>/u/lblblllb</name>
      <uri>https://old.reddit.com/user/lblblllb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt already has over 180mm users, which is over half of US population. With exception of limitation on o1, the service uptime seems mostly fine so far? why spend up to 500bln to build data centers for exclusive use of openai that will depreciate very quickly(due to GPU depreciation)? Same for meta spending 60bln on AI. how do they plan to make the money back? seems like they really have to be able to use AI to replace most of the knowledge workers in order to make a return.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/"&gt;https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lblblllb"&gt; /u/lblblllb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia95ap</id>
    <title>Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5</title>
    <updated>2025-01-26T08:07:40+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"&gt; &lt;img alt="Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5" src="https://external-preview.redd.it/rp7VJjsBbW35ue65hF_AusrsgbxRmLjaK2DB4dGUml4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0047285645af07421832798a077a8010328c7dc" title="Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/MbX9J1Tt_I0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadr5g</id>
    <title>How better is Deepseek r1 compared to llama3? Both are open source right?</title>
    <updated>2025-01-26T12:51:27+00:00</updated>
    <author>
      <name>/u/trenmost</name>
      <uri>https://old.reddit.com/user/trenmost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Afaik Llama3 is open source as well, is the hype around DeepSeek R1 so large because its way better than llama3? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trenmost"&gt; /u/trenmost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iadr5g/how_better_is_deepseek_r1_compared_to_llama3_both/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iadr5g/how_better_is_deepseek_r1_compared_to_llama3_both/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iadr5g/how_better_is_deepseek_r1_compared_to_llama3_both/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T12:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia10ld</id>
    <title>Msty connecting to a Chinese server in Hong Kong</title>
    <updated>2025-01-26T00:09:44+00:00</updated>
    <author>
      <name>/u/urubuz</name>
      <uri>https://old.reddit.com/user/urubuz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt; &lt;img alt="Msty connecting to a Chinese server in Hong Kong" src="https://b.thumbs.redditmedia.com/tlmfTviR5XharLxlY5KMT6R5OwwGM__phYc7Jk5IKhY.jpg" title="Msty connecting to a Chinese server in Hong Kong" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://msty.app/privacy:"&gt;https://msty.app/privacy:&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; We do not gather any telemetry data except for app open ping. All data is stored locally on your device and is NEVER transmitted to our servers.&lt;/p&gt; &lt;p&gt;Here's what Little Snitch Mini is reporting when the app booted up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537"&gt;https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/urubuz"&gt; /u/urubuz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iak7td</id>
    <title>Meet Qwen2.5-7B-Instruct-1M &amp; Qwen2.5-14B-Instruct-1M</title>
    <updated>2025-01-26T17:18:26+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1883557964759654608"&gt;https://x.com/Alibaba_Qwen/status/1883557964759654608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're leveling up the game with our latest open-source models, Qwen2.5-1M ! Now supporting a 1 MILLION TOKEN CONTEXT LENGTH &lt;/p&gt; &lt;p&gt;Here's what’s new: &lt;/p&gt; &lt;p&gt;Open Models: Meet Qwen2.5-7B-Instruct-1M &amp;amp; Qwen2.5-14B-Instruct-1M —our first-ever models handling 1M-token contexts! &lt;/p&gt; &lt;p&gt;Lightning-Fast Inference Framework: We’ve fully open-sourced our inference framework based on vLLM , integrated with sparse attention methods. Experience 3x to 7x faster processing for 1M-token inputs! &lt;/p&gt; &lt;p&gt;Tech Deep Dive: Check out our detailed Technical Report for all the juicy details behind the Qwen2.5-1M series! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wbya</id>
    <title>ByteDance announces Doubao-1.5-pro</title>
    <updated>2025-01-25T20:34:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt; &lt;img alt="ByteDance announces Doubao-1.5-pro" src="https://preview.redd.it/5pjykhaha7fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0df07e6b549319488a93d42063d7e338ff3b8b7" title="ByteDance announces Doubao-1.5-pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance announces Doubao-1.5-pro&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Includes a &amp;quot;Deep Thinking&amp;quot; mode, surpassing O1-preview and O1 models on the AIME benchmark.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms deepseek-v3, gpt4o, and llama3.1-405B on popular benchmarks. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built on a MoE architecture, with activated parameters far fewer than those in the above models. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Achieves a 7x MoE performance leverage—delivering dense model performance with just 1/7 of the activated parameters (e.g., 20B activated params = 140B dense performance). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Engineering-wise, features heterogeneous system design for prefill-decode and attn-fffn, maximizing throughput under low-latency requirements.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5pjykhaha7fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia1d4t</id>
    <title>7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient</title>
    <updated>2025-01-26T00:26:33+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt; &lt;img alt="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" src="https://external-preview.redd.it/88h1mEnLx1t41jw5cSvvfzo0nRgYDTSe3ZMQUihAxm4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3ddfdce52f96a0c0d08d64102e978a19be46554" title="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hkust-nlp.notion.site/simplerl-reason"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iai3oc</id>
    <title>Model training data density</title>
    <updated>2025-01-26T15:58:55+00:00</updated>
    <author>
      <name>/u/frivolousfidget</name>
      <uri>https://old.reddit.com/user/frivolousfidget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2411.04330"&gt;https://arxiv.org/abs/2411.04330&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wow… just read this article and went on to do some experiments. Qwen 2.5 Coder is trained on 5.5T and as a rule of thumb simplifying what the article says we can go by the idea that if a Trillion tokens &amp;gt; Billion params, we can assume that the model would be sensitive to quantization. And it really does show, this is the perplexity delta compared to f16:&lt;/p&gt; &lt;p&gt;0.5B (T/N ≈ 11,000):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Q6_K: 0.187% worse Q4_K_M: 3.42% worse Q3_K_L: 3.98% worse &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3B (T/N ≈ 1,833):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Q6_K: 0.41% worse Q4_K_M: 1.96% worse Q3_K_M: 76.09% worse (goes into 0.5b result territory while still being larger than 0.5b f16) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;7B (T/N ≈ 786 which is below the rule of thumb hence not affected at Q4):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Q6_K: (not tested) Q4_K_M: 0.95% worse Q3_K_L: 3.28% worse &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All compared to the best result (it is sunday and I am lazy rn so I asked sonnet to do it for me… looks correct 🤷 but dont trust it too much)&lt;/p&gt; &lt;p&gt;7B (T/N ≈ 786):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F16: baseline (8.0125) Q4_K_M: +0.95% Q3_K_L: +3.28% &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3B (T/N ≈ 1,833):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F16: +1.66% Q6_K: +2.08% Q4_K_M: +3.65% Q3_K_M: +79.02% &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;0.5B (T/N ≈ 11,000):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;F16: +93.79% Q8_0: +93.89% Q6_K: +94.15% Q4_K_M: +100.42% Q3_K_L: +101.49% &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frivolousfidget"&gt; /u/frivolousfidget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iai3oc/model_training_data_density/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iai3oc/model_training_data_density/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iai3oc/model_training_data_density/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T15:58:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia4mx6</id>
    <title>Project Digits Memory Speed</title>
    <updated>2025-01-26T03:17:58+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently saw an accidentally leaked slide from Nvidia on Project Digits memory speed. It is 273 GB/s.&lt;/p&gt; &lt;p&gt;Also 128 GB is the base memory. Only storage will have “pay to upgrade” tiers.&lt;/p&gt; &lt;p&gt;Wanted to give credit to this user. Completely correct. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ"&gt;https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Hoping for a May launch I heard too.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T03:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iafeo3</id>
    <title>Exploring UI-TARS</title>
    <updated>2025-01-26T14:08:04+00:00</updated>
    <author>
      <name>/u/AnhedoniaJack</name>
      <uri>https://old.reddit.com/user/AnhedoniaJack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iafeo3/exploring_uitars/"&gt; &lt;img alt="Exploring UI-TARS" src="https://external-preview.redd.it/MmhpOHA4aWVpY2ZlMWl7IhHQloSAXpiaOFGmAmD3rCPajgZPYYle5x1W-3q1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24143b889760431f2e6242c32363f8304d9d30f9" title="Exploring UI-TARS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring UI-TARS and the UI-TARS-Desktop agent (Note: I compiled my own version of it) and like a lot of early stage AI things, it's impressive and pretty easy to see how this could be disruptive, but it's also pretty funny to watch it fail miserably at simple tasks. &lt;/p&gt; &lt;p&gt;I am currently using UI-TARS-2B-SFT since I don't have the horsepower to run 7B or 72B unquantized, and the GGUF quants shit the bed for the time being. I can only assume that the 2B model is quite a bit more limited than the 7B or 72B.&lt;/p&gt; &lt;p&gt;I have sped up the boring parts where it is waiting on inference, but when quantized versions come out, the speed should be pretty impressive.&lt;/p&gt; &lt;p&gt;It can do quite a few simple tasks, but I was curious if I could have it visually get some dynamic direction from a third party. By instructing it to think about the result, the model does a pretty good job of sending a message that the user wants it to think about the text it just visually extracted. &lt;/p&gt; &lt;p&gt;Super basic, but pretty damn interesting to play with. I look forward to the quants!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnhedoniaJack"&gt; /u/AnhedoniaJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0j4w85keicfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iafeo3/exploring_uitars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iafeo3/exploring_uitars/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T14:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9nqj9</id>
    <title>Full open source reproduction of R1 in progress ⏳</title>
    <updated>2025-01-25T14:11:35+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt; &lt;img alt="Full open source reproduction of R1 in progress ⏳" src="https://preview.redd.it/s5rmvdhtd5fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf96bf7e9979be87994f66f0537b9e70492b54b" title="Full open source reproduction of R1 in progress ⏳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5rmvdhtd5fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T14:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia40om</id>
    <title>Would give up a kidney for a local audio model that’s even half as good as Suno</title>
    <updated>2025-01-26T02:44:27+00:00</updated>
    <author>
      <name>/u/Effective_Garbage_34</name>
      <uri>https://old.reddit.com/user/Effective_Garbage_34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, I’ve tried pretty much every local audio model out there—MusicGen, AudioCraft, Coqui TTS, NSynth—whatever. And they all sound… bad. Like, really bad. Meanwhile, Suno is out here sounding like magic, and I’m just sitting here wondering: what the hell are they doing differently?&lt;/p&gt; &lt;p&gt;Is it their training data? Some proprietary wizardry? Did they make a deal with the devil? Whatever it is, local models are so far behind it’s almost depressing.&lt;/p&gt; &lt;p&gt;I’d love to get even a fraction of Suno’s quality in something I can run locally. Has anyone figured out a way forward? Is there hope for local models, or are we stuck dreaming from a distance?&lt;/p&gt; &lt;p&gt;Seriously, what’s the secret sauce? If anyone has insight, please share—I’m desperate over here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Garbage_34"&gt; /u/Effective_Garbage_34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T02:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadomi</id>
    <title>[Rumor] Huawei 910C will double 910B performance</title>
    <updated>2025-01-26T12:47:48+00:00</updated>
    <author>
      <name>/u/44seconds</name>
      <uri>https://old.reddit.com/user/44seconds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Note I have no proof of this other than my word.&lt;/p&gt; &lt;p&gt;Recently met with a Huawei employee who was pitching their 910B chips for GenAI. We didn't end up going with them, but in the process I learned some interesting tidbits of information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Huawei 910C is the same architecture as 910B&lt;/li&gt; &lt;li&gt;The 910C is aiming for 800 TFLOPS of fp16 (unclear if fp32 accumulate, or fp16) -- it was mentioned that their goal is around Nvidia H200 NVL&lt;/li&gt; &lt;li&gt;The 910C is on a Chinese 7nm process&lt;/li&gt; &lt;li&gt;The 910C aims to use Chinese HBM2e, they provided no comment regarding capacity or bandwidth&lt;/li&gt; &lt;li&gt;The 910C aims to resolve serious cross-card interconnect issues present in the 910B, which rendered the 910B unsuitable for training LLMs&lt;/li&gt; &lt;li&gt;They mentioned that the chief designer of Huawei Ascend chips, who did the first Ascend design was a Chinese student educated in the USA. No details provided on if he was undergrad or PhD educated in the US. But mentioned his initial design focus was edge/low-power inference. They mentioned that a significant part of their EDA &amp;amp; compiler teams had undergrad/PhD US educations.&lt;/li&gt; &lt;li&gt;They are aiming for an exact silicon doubling of the 910B. They suggested this was done via chiplets, but were evasive when I pushed for details and tried to confirm this&lt;/li&gt; &lt;li&gt;Their goal is public sampling in 2025 Q1 or Q2&lt;/li&gt; &lt;li&gt;They claimed better Pytorch compatibility than AMD, and said it was comparable to Intel's current GPU compatibility&lt;/li&gt; &lt;li&gt;They claimed significant PyTorch compatibility improvements since 2024 Q1, since the 910B launched. And mentioned that a large effort was put into Pytorch operator compatibility/accuracy under fp16, and their own NPU API called ACL&lt;/li&gt; &lt;li&gt;They grumbled about 910B being prioritized to some &amp;quot;cloud&amp;quot; infrastructure customers who didn't have a viable cloud business, and required significant on-site ecosystem support. They liked working with the GenAI startups who had the skills for scale out infrastructure&lt;/li&gt; &lt;li&gt;They mentioned that demand outstripped supply as a whole&lt;/li&gt; &lt;li&gt;They grumbled about certain customers still preferring to use smuggled Nvidia chips rather than their solution&lt;/li&gt; &lt;li&gt;They grumbled about having to be bug compatible with Nvidia, and efforts to resolve accuracy issues&lt;/li&gt; &lt;li&gt;They are aiming for a new architecture for whatever succeededs 910C&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44seconds"&gt; /u/44seconds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T12:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaizyk</id>
    <title>Qwen 2.5 VL incoming</title>
    <updated>2025-01-26T16:32:42+00:00</updated>
    <author>
      <name>/u/Either-Job-341</name>
      <uri>https://old.reddit.com/user/Either-Job-341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 2 VL 7B and 72B are remarkable video models and this new series is expected to be even better.&lt;/p&gt; &lt;p&gt;Are you ready? ARE. YOU. READY?&lt;/p&gt; &lt;p&gt;Chinese labs are killing it and they sure know how to ride a wave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either-Job-341"&gt; /u/Either-Job-341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9nzm</id>
    <title>China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)</title>
    <updated>2025-01-26T08:47:19+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt; &lt;img alt="China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)" src="https://external-preview.redd.it/xPmyWRJan7ulb3PcWvyYjbtkiBhdqYVeIf1sY9iUMiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=285acbaf1e44da73546abd95c35ff3a0369dd036" title="China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Yuansheng Thinking Chain Large Model achieves adaptive slow thinking through two strategies: task adaptation and difficulty adaptation. In the evaluation set of non-inference tasks, this model tends to generate shorter answers while ensuring accuracy, thus improving response efficiency. Additionally, when evaluating generated long thinking chain data, the model comprehensively considers the difficulty of the questions and the length of the generated answers, using reinforcement learning to match the answer length with the question difficulty, further enhancing the model's accuracy and practicality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bd28qcf9xafe1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=214bdd271beeb39ee43991f43b78158e0077927e"&gt;https://preview.redd.it/bd28qcf9xafe1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=214bdd271beeb39ee43991f43b78158e0077927e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/UnicomAI/Unichat-32B-c1.git"&gt;Model Link (Chinese Only)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaj0da</id>
    <title>AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)</title>
    <updated>2025-01-26T16:33:07+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt; &lt;img alt="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" src="https://preview.redd.it/x4xd7d7a8dfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d010b3576261b46f6f12443a4fd4ccbf4a63bf2d" title="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So TUS is a really hard medical specialization exam consisting of two parts (each part 100 questions, so 200 in total). Never has a person answered all the questions correctly in its history. Doctors in Turkey must pass this exam to begin their desired residency in a hospital.&lt;/p&gt; &lt;p&gt;Credit: Ahmet Ay, founder of TUSBuddy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x4xd7d7a8dfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9y42v</id>
    <title>New OpenAI</title>
    <updated>2025-01-25T21:54:09+00:00</updated>
    <author>
      <name>/u/notomarsol</name>
      <uri>https://old.reddit.com/user/notomarsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt; &lt;img alt="New OpenAI" src="https://preview.redd.it/ppnejgtgo7fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e4cae2970050d080916629397ce588f1598ea49" title="New OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notomarsol"&gt; /u/notomarsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ppnejgtgo7fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaciu9</id>
    <title>Qwen 2.5 VL Release Imminent?</title>
    <updated>2025-01-26T11:45:21+00:00</updated>
    <author>
      <name>/u/iKy1e</name>
      <uri>https://old.reddit.com/user/iKy1e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've just created the collection for it on Hugging Face &amp;quot;updated about 2 hours ago&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;h1&gt;Qwen2.5-VL&lt;/h1&gt; &lt;p&gt;Vision-language model series based on Qwen2.5 &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKy1e"&gt; /u/iKy1e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T11:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia7v0x</id>
    <title>the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp.</title>
    <updated>2025-01-26T06:34:30+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt; &lt;img alt="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." src="https://external-preview.redd.it/GkVEj8SEXSUPkh7D3z-zTGfIOGq41yOTpDWE4Fj1WE4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01e2d3507d8b0dc551310e81cc376bab8fb4fc91" title="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;app maim page: &lt;a href="https://github.com/alibaba/MNN"&gt;MNN-LLM-APP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xo6fjer8afe1.png?width=1780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2da05212d5e1af8855cedc2a23a8166e4a5340dc"&gt;the mulitimodal app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;inference speed vs llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/elrqgjh59afe1.gif"&gt;https://i.redd.it/elrqgjh59afe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T06:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaizfb</id>
    <title>Qwen2.5-1M Release on HuggingFace - The long-context version of Qwen2.5, supporting 1M-token context lengths!</title>
    <updated>2025-01-26T16:32:10+00:00</updated>
    <author>
      <name>/u/Silentoplayz</name>
      <uri>https://old.reddit.com/user/Silentoplayz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing to be the first to do it here.&lt;/p&gt; &lt;blockquote&gt; &lt;h1&gt;Qwen2.5-1M&lt;/h1&gt; &lt;p&gt;The long-context version of Qwen2.5, supporting 1M-token context lengths&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba"&gt;https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Related &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; post by another fellow regarding &amp;quot;Qwen 2.5 VL&amp;quot; models - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Edit:&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen2.5-1m/"&gt;https://qwenlm.github.io/blog/qwen2.5-1m/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf"&gt;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you &lt;a href="/u/Balance-"&gt;u/Balance-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silentoplayz"&gt; /u/Silentoplayz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:32:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9iy1</id>
    <title>DeepSeekR1 3D game 100% from scratch</title>
    <updated>2025-01-26T08:36:26+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt; &lt;img alt="DeepSeekR1 3D game 100% from scratch" src="https://preview.redd.it/qrdlt6i8vafe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8d13a97797fa31e558155d2f6738fd891080c24b" title="DeepSeekR1 3D game 100% from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked DeepSeek R1 to make me a game like kkrieger ( where most of the things are generated on run ) and it made me this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrdlt6i8vafe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaebwp</id>
    <title>Financial Times: "DeepSeek shocked Silicon Valley"</title>
    <updated>2025-01-26T13:19:03+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;recent article&lt;/a&gt; in Financial Times says that US sanctions forced the AI companies in China to be more innovative &amp;quot;to maximise the computing power of a limited number of onshore chips&amp;quot;.&lt;/p&gt; &lt;p&gt;Most interesting to me was the claim that &amp;quot;DeepSeek’s singular focus on research makes it a dangerous competitor because it is willing to share its breakthroughs rather than protect them for commercial gains.&amp;quot;&lt;/p&gt; &lt;p&gt;What an Orwellian doublespeak! China, a supposedly closed country, leads the AI innovation and is willing to share its breakthroughs. And this makes them dangerous for ostensibly open countries where companies call themselves OpenAI but relentlessly hide information.&lt;/p&gt; &lt;p&gt;Here is the full link: &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;https://archive.md/b0M8i#selection-2491.0-2491.187&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T13:19:03+00:00</published>
  </entry>
</feed>
