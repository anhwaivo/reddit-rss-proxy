<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-21T15:24:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m4yo0g</id>
    <title>DiffRhythm 1.2 music generation model produces "Avicii vs Nicky Romero - I Could Be the One" nearly verbatim</title>
    <updated>2025-07-20T20:06:46+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt; &lt;img alt="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" src="https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc68db074fff49faf68ce5480e7b3265555e05bf" title="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.&lt;/p&gt; &lt;p&gt;Skip to 1:00 in the video to skip the generation process&lt;/p&gt; &lt;p&gt;Seed: 50518556518147&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ulng63nd53ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r4j1</id>
    <title>Open source is humanity’s last hope!</title>
    <updated>2025-07-20T15:04:05+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What’s you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5k88s</id>
    <title>As the creators of react-native-executorch, we built an open-source app for testing ExecuTorch LLMs on mobile.</title>
    <updated>2025-07-21T14:21:57+00:00</updated>
    <author>
      <name>/u/K4anan</name>
      <uri>https://old.reddit.com/user/K4anan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5k88s/as_the_creators_of_reactnativeexecutorch_we_built/"&gt; &lt;img alt="As the creators of react-native-executorch, we built an open-source app for testing ExecuTorch LLMs on mobile." src="https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cf812b9f49b8e4d10b0c6a54949be05fe9c1bfb" title="As the creators of react-native-executorch, we built an open-source app for testing ExecuTorch LLMs on mobile." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We’re the team at Software Mansion, the creators and maintainers of the &lt;strong&gt;react-native-executorch&lt;/strong&gt; library, which allows developers to run PyTorch ExecuTorch models inside React Native apps.&lt;/p&gt; &lt;p&gt;After releasing the library, we realized a major hurdle for the community was the lack of a simple way to test, benchmark, and just &lt;em&gt;play&lt;/em&gt; with LLMs on a mobile device without a complex setup.&lt;/p&gt; &lt;p&gt;To solve this, we created &lt;strong&gt;Private Mind&lt;/strong&gt;. An open-source app that acts as a testing &lt;strong&gt;utility&lt;/strong&gt; with one primary goal: to give developers and enthusiasts a dead-simple way to see how LLMs perform via ExecuTorch.&lt;/p&gt; &lt;p&gt;It's a tool built for this community. Here’s what it's designed for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A Lab for Your Models:&lt;/strong&gt; The main feature is loading your own custom models. If you can export it to the .pte format, you can run it in the app and interact with it through a basic chat interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pure On-Device Benchmarking:&lt;/strong&gt; Select any model and run a benchmark to see exactly how it performs on your hardware. You get crucial stats like tokens/second, memory usage, and time to first token. It’s a direct way to test the efficiency of your model or our library.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A Reference Implementation:&lt;/strong&gt; Since we built the underlying library, the app serves as a blueprint. You can check the GitHub repo to see our recommended practices for implementing react-native-executorch in a real-world application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; True to the ExecuTorch spirit, everything is on-device. Your models, chats, and benchmark data never leave your phone, making it a safe environment for experimentation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Our Roadmap is About Improving the Testing Toolkit:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We are actively working to enhance Private Mind as a testing utility. Next up is a new LLM runner that will expose parameters like &lt;strong&gt;temperature&lt;/strong&gt; and &lt;strong&gt;top_k&lt;/strong&gt; for more nuanced testing. After that, we plan to show how to implement more advanced use-cases like on-device &lt;strong&gt;RAG&lt;/strong&gt; and &lt;strong&gt;speech-to-text&lt;/strong&gt;. We'll also add &lt;strong&gt;Gemma 3n&lt;/strong&gt; support as soon as it's fully compatible with the ExecuTorch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;App Store (iOS):&lt;/strong&gt; &lt;a href="https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2"&gt;https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Google Play (Android):&lt;/strong&gt; &lt;a href="https://play.google.com/store/apps/details?id=com.swmansion.privatemind"&gt;https://play.google.com/store/apps/details?id=com.swmansion.privatemind&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (Check the code &amp;amp; contribute):&lt;/strong&gt; &lt;a href="https://github.com/software-mansion-labs/private-mind"&gt;https://github.com/software-mansion-labs/private-mind&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Our Pre-Exported Models for Testing:&lt;/strong&gt; &lt;a href="https://huggingface.co/software-mansion"&gt;https://huggingface.co/software-mansion&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've built the foundation, and now we want the community to shape what's next. Let us know in the comments: What's the killer feature you're missing from other local AI apps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/K4anan"&gt; /u/K4anan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wklalir6l8ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5k88s/as_the_creators_of_reactnativeexecutorch_we_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5k88s/as_the_creators_of_reactnativeexecutorch_we_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T14:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5kqk8</id>
    <title>FULL Windsurf System Prompt and Tools [UPDATED, Wave 11]</title>
    <updated>2025-07-21T14:41:49+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 21/07/2025)&lt;/p&gt; &lt;p&gt;I've just extracted the FULL Windsurf system prompt and internal tools (Wave 11 update). Over 500 lines (Around 9.6k tokens).&lt;/p&gt; &lt;p&gt;You can check it out here: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T14:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5l52r</id>
    <title>Office hours for cloud GPU</title>
    <updated>2025-07-21T14:57:29+00:00</updated>
    <author>
      <name>/u/No-Scarcity-8746</name>
      <uri>https://old.reddit.com/user/No-Scarcity-8746</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I recently built an office hours page for anyone who has questions on cloud GPUs or GPUs in general. we are a bunch of engineers who've built at Google, Dropbox, Alchemy, Tesla etc. and would love to help anyone who has questions in this area. &lt;a href="https://computedeck.com/office-hours"&gt;https://computedeck.com/office-hours&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We welcome any feedback as well!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Scarcity-8746"&gt; /u/No-Scarcity-8746 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T14:57:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5l8e4</id>
    <title>Seeking the newest coding models, especially for SQL?</title>
    <updated>2025-07-21T15:00:54+00:00</updated>
    <author>
      <name>/u/datascientist2964</name>
      <uri>https://old.reddit.com/user/datascientist2964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any newer models (&amp;lt;50 days old) that are well equipped to handle coding, especially in SQL? Hoping to find something under 24b. Currently running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; unsloth qwen3-14b Q4_K_S for general tasks&lt;/li&gt; &lt;li&gt;mistralai/mistral-small-3.2 for some stuff like writing&lt;/li&gt; &lt;li&gt;qwen2.5-coder-14b-instruct-q4_k_m - general coding tasks, not great&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/datascientist2964"&gt; /u/datascientist2964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l8e4/seeking_the_newest_coding_models_especially_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l8e4/seeking_the_newest_coding_models_especially_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5l8e4/seeking_the_newest_coding_models_especially_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T15:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5lf6l</id>
    <title>Chatterbox tts microphone results</title>
    <updated>2025-07-21T15:07:58+00:00</updated>
    <author>
      <name>/u/olympics2022wins</name>
      <uri>https://old.reddit.com/user/olympics2022wins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;;tldr when voice cloning use a high-end microphone not the one built-in to your computer/airpods&lt;/p&gt; &lt;p&gt;I have a child that has reading difficulties. They need to be able to read 15 books this coming year and I was lucky enough to be able to find out what those 15 books are. Many of them are from the 1920s and earlier. They’re relatively unpopular and do not have existing audiobooks available. A number of them aren’t even sold as Ebooks (yes we are all aghast).&lt;/p&gt; &lt;p&gt;Enter manually scanning ick&lt;/p&gt; &lt;p&gt;So I used my colleagues audiobook generator with my local rig. Each book gets chunked into around 1500 to 2000 chunks. My initial recording was on AirPods and/or a local microphone inside my MacBook.&lt;/p&gt; &lt;p&gt;With those recordings (I had two different ones) I had a 35 to 40% error rate which often persisted even when I was trying to generate 10 attempts. &lt;/p&gt; &lt;p&gt;I happened to pick up a prosumer voice recorder to be able to do interviews with older relatives as an audio genealogical history. When I recorded my voice with those reading the exact same script as the other two recordings I went to a 5 to 10% air rate with three shots. Mostly closer to 5% but sometimes up to 10% &lt;/p&gt; &lt;p&gt;For everyone who is having issues with their voice recording cloning, you may want to consider the quality of your microphone. I would have assumed that for an expressive reading of an audiobook it would be fine to just use decent quality hardware microphones. I was shocked at the improvement levels in the transcription passes and the output. It’s relatively obvious after I say it out loud, but I don’t see many people talking about it (too basic for the experts in the space and not something that the novices immediately intuit perhaps) so I thought I’d share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olympics2022wins"&gt; /u/olympics2022wins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T15:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5gl6e</id>
    <title>Chat webinterface for small company</title>
    <updated>2025-07-21T11:39:33+00:00</updated>
    <author>
      <name>/u/_ralph_</name>
      <uri>https://old.reddit.com/user/_ralph_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I need a web interface for my local model but i need multi user support. Meaning i need a login and everyone needs their own chat history. &lt;/p&gt; &lt;p&gt;Any ideas? (google and chatgpt/... were not helpful) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ralph_"&gt; /u/_ralph_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T11:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m55rrt</id>
    <title>How fast is gemma 3 27b on an H100? how many tokens per second can I expect?</title>
    <updated>2025-07-21T01:20:35+00:00</updated>
    <author>
      <name>/u/ThatIsNotIllegal</name>
      <uri>https://old.reddit.com/user/ThatIsNotIllegal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen people say 60/s and i've seen 22000/sec, I don't even know who to believe anymore.&lt;/p&gt; &lt;p&gt;Also how much does optimizing boost the tokens output speed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThatIsNotIllegal"&gt; /u/ThatIsNotIllegal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T01:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m56z4m</id>
    <title>why are there quite different quant strategies of bartowski and unsloth on MoE?</title>
    <updated>2025-07-21T02:18:43+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt; &lt;img alt="why are there quite different quant strategies of bartowski and unsloth on MoE?" src="https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dcb3e29b76660bae85af28320f25e9b8191473f" title="why are there quite different quant strategies of bartowski and unsloth on MoE?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF"&gt;https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF"&gt;https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;they are quant of a same model. at a same quant, e.g. both Q3_K_M, there are non-negligible count of blocks, which bartowski quantized as Q8_0, while unsloth Q3_K or Q4_K.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=838059d64089a5e17092169c48e88ab90b8d92a9"&gt;this is a part. count 67 in total&lt;/a&gt;&lt;/p&gt; &lt;p&gt;btw, the unsloth Q3_K_XL is smaller than Q3_K_M. I am really curious on the flavor of unloth naming.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T02:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5fkts</id>
    <title>My (practical) dual 3090 setup for local inference</title>
    <updated>2025-07-21T10:43:20+00:00</updated>
    <author>
      <name>/u/ColdImplement1319</name>
      <uri>https://old.reddit.com/user/ColdImplement1319</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/"&gt; &lt;img alt="My (practical) dual 3090 setup for local inference" src="https://a.thumbs.redditmedia.com/vvE88N6ubX7Gsux9A2O-Hn5WBZJNPCIpKoUwojUDik4.jpg" title="My (practical) dual 3090 setup for local inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I completed my local LLM rig in May, just after Qwen3's release (thanks to &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 's folks for the invaluable guidance!). Now that I've settled into the setup, I'm excited to share my build and how it's performing with local LLMs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf"&gt;https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let's dive in!&lt;/p&gt; &lt;h1&gt;Key Specs&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Specs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 7 7700 (8C/16T)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2 x NVIDIA RTX 3090 (48GB VRAM total)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;64GB DDR5 @ 6400 MHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;ASUS TUF B650-PLUS&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Lian Li LANCOOL 216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;a lot of fans 💨&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tried to run the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;30B-A3B Q4_K_XL&lt;/strong&gt;, &lt;strong&gt;32B Q4_K_XL&lt;/strong&gt; – fit into one GPU with ample context window&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32B Q8_K_XL&lt;/strong&gt; – runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;30B-A3B Q8_K_XL&lt;/strong&gt; – now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model's context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven’t found an optimal way to store weights once and manage contexts separately, so this remains a WiP).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Primary use: Running Qwen3-30B-A3B models with &lt;code&gt;llama.cpp&lt;/code&gt;. The performance for this model is ~ 1000 pp512 / 100 tg128&lt;/p&gt; &lt;p&gt;What's next? I think I will play with this one for a while. But... I'm already eyeing an EPYC-based system with 4x 4090s (48GB each). 😎&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ColdImplement1319"&gt; /u/ColdImplement1319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T10:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4y3cj</id>
    <title>Fine-tuned her the perfect local model. Still got API’d 💔</title>
    <updated>2025-07-20T19:43:10+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt; &lt;img alt="Fine-tuned her the perfect local model. Still got API’d 💔" src="https://preview.redd.it/xitr9w9f13ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f099b23bb6d2f68855a9689333e79824231cdf0" title="Fine-tuned her the perfect local model. Still got API’d 💔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xitr9w9f13ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T19:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5lgtr</id>
    <title>UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available</title>
    <updated>2025-07-21T15:09:43+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/"&gt; &lt;img alt="UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available" src="https://b.thumbs.redditmedia.com/jkGttzBpvCgJdojYediLI8339DpfTnhALhUrjGFYPnA.jpg" title="UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)&lt;/p&gt; &lt;p&gt;So type out your prompt like this: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;[Action] [UI type or page] [Framework(s)] [Key features] [Style (optional)]&lt;/li&gt; &lt;li&gt;&lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If it is within the context, then you can additionally add edits.&lt;/p&gt; &lt;p&gt;Here's a prompt template:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additional things that are supported -&amp;gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.&lt;/p&gt; &lt;p&gt;Remember, its only an 8B model!&lt;/p&gt; &lt;p&gt;We are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. &lt;/p&gt; &lt;p&gt;Make sure to join the community for more support. (Link in Huggingface!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m5lgtr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T15:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5827d</id>
    <title>Which LLMs, tools, or research have been overlooked or deserve more attention?</title>
    <updated>2025-07-21T03:13:18+00:00</updated>
    <author>
      <name>/u/MDT-49</name>
      <uri>https://old.reddit.com/user/MDT-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.&lt;/p&gt; &lt;p&gt;Of course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?&lt;/p&gt; &lt;p&gt;What's everyone's go-to model these days?&lt;/p&gt; &lt;p&gt;And what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released &lt;a href="https://huggingface.co/allenai/FlexOlmo-7x7B-1T"&gt;FlexOlmo&lt;/a&gt; LLM/paradigm?&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MDT-49"&gt; /u/MDT-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vw29</id>
    <title>Ikllamacpp repository gone, or it is only me?</title>
    <updated>2025-07-20T18:14:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T18:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5gmfr</id>
    <title>What are people fine-tuning their models for?</title>
    <updated>2025-07-21T11:41:17+00:00</updated>
    <author>
      <name>/u/MKBSP</name>
      <uri>https://old.reddit.com/user/MKBSP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I'm curious, what are people fine-tuning their models for? &lt;/p&gt; &lt;p&gt;I was working in a company where we fine-tuned models to better deal with product images, but the company couldn't keep the lights on. Most agencies, companies, freelancers, seem to use off-the-shelf models, which are getting &amp;quot;good enough&amp;quot; for the job. &lt;/p&gt; &lt;p&gt;So, what are people fine-tuning their models for? and which companies, or industries, are most likely to be fine-tuning models? &lt;/p&gt; &lt;p&gt;Thanks, just an idiot asking! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKBSP"&gt; /u/MKBSP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T11:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5d66p</id>
    <title>ik_llama.cpp 404: temporary repo up to commit d44c2d3</title>
    <updated>2025-07-21T08:13:37+00:00</updated>
    <author>
      <name>/u/PieBru</name>
      <uri>https://old.reddit.com/user/PieBru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those interested, here is a temporary copy pulled just before the official repo went 404.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PieBru/ik_llama.cpp_temp_copy"&gt;https://github.com/PieBru/ik_llama.cpp_temp_copy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PieBru"&gt; /u/PieBru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T08:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m52h10</id>
    <title>I posted 3 weeks ago about training my own model. Progress report.</title>
    <updated>2025-07-20T22:46:55+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt; &lt;img alt="I posted 3 weeks ago about training my own model. Progress report." src="https://b.thumbs.redditmedia.com/tGNWILy7NUwjWBRL__Qs6HOJImwQ5Z22Np_wBFcIDdM.jpg" title="I posted 3 weeks ago about training my own model. Progress report." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I posted that I wanted to train an LLM for under $1000 here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal). Cost projection is about $500 for this run. It has flash attention 2, a 3:1 GQA, a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset). The corpus is english only, which I'm hoping will give it an edge.&lt;/p&gt; &lt;p&gt;I have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.&lt;/p&gt; &lt;p&gt;Now at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.&lt;/p&gt; &lt;p&gt;Happy to answer any questions! Pic is the beautiful loss curve.&lt;/p&gt; &lt;p&gt;Edit: It's called Libremodel I, codename Gigi, and I made a website with more info here: &lt;a href="https://libremodel.xyz"&gt;https://libremodel.xyz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1"&gt;https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T22:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9mt</id>
    <title>I'm sorry Zuck please don't leave us we were just having fun</title>
    <updated>2025-07-20T15:51:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt; &lt;img alt="I'm sorry Zuck please don't leave us we were just having fun" src="https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2" title="I'm sorry Zuck please don't leave us we were just having fun" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9mxxen7w1ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m58695</id>
    <title>Which local 100B+ heavy weight models are your favorite and why?</title>
    <updated>2025-07-21T03:19:13+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Mistral_large-Instruct&lt;/li&gt; &lt;li&gt;Qwen3-235B&lt;/li&gt; &lt;li&gt;Command-A&lt;/li&gt; &lt;li&gt;Deepseek-V3&lt;/li&gt; &lt;li&gt;Deepseek-R1&lt;/li&gt; &lt;li&gt;Deepseek-R1-0528&lt;/li&gt; &lt;li&gt;Deepseek-TNG-R1T2-Chimera&lt;/li&gt; &lt;li&gt;Kimi-K2&lt;/li&gt; &lt;li&gt;Ernie-4.5-300b&lt;/li&gt; &lt;li&gt;llama3.1-405B&lt;/li&gt; &lt;li&gt;llama3.1-Nemotron-Ultra-253b?&lt;/li&gt; &lt;li&gt;Others?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5jr1v</id>
    <title>[New Architecture] Hierarchical Reasoning Model</title>
    <updated>2025-07-21T14:02:52+00:00</updated>
    <author>
      <name>/u/imonenext</name>
      <uri>https://old.reddit.com/user/imonenext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt; &lt;img alt="[New Architecture] Hierarchical Reasoning Model" src="https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg" title="[New Architecture] Hierarchical Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt; &lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. 🌟&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📄Paper: &lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻Code: &lt;a href="https://github.com/sapientinc/HRM"&gt;https://github.com/sapientinc/HRM&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imonenext"&gt; /u/imonenext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T14:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5fcdo</id>
    <title>NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters</title>
    <updated>2025-07-21T10:29:24+00:00</updated>
    <author>
      <name>/u/OwnWitness2836</name>
      <uri>https://old.reddit.com/user/OwnWitness2836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"&gt; &lt;img alt="NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters" src="https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b8ad27cd8415b4fcbb76f37f1c26b02e1e7a72d" title="NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnWitness2836"&gt; /u/OwnWitness2836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T10:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5fmlp</id>
    <title>Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing</title>
    <updated>2025-07-21T10:46:03+00:00</updated>
    <author>
      <name>/u/PmMeForPCBuilds</name>
      <uri>https://old.reddit.com/user/PmMeForPCBuilds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"&gt; &lt;img alt="Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing" src="https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3" title="Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PmMeForPCBuilds"&gt; /u/PmMeForPCBuilds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T10:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5gwzs</id>
    <title>I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.</title>
    <updated>2025-07-21T11:56:42+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.&lt;/p&gt; &lt;p&gt;For example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06 You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user. Your main goal is to follow the USER's instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag. &amp;lt;communication&amp;gt; When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \( and \) for inline math, \[ and \] for block math. &amp;lt;/communication&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt; &lt;p&gt;I'm super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href="https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href="https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T11:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5iymb</id>
    <title>The reason why local models are better/necessary.</title>
    <updated>2025-07-21T13:30:11+00:00</updated>
    <author>
      <name>/u/GPTshop_ai</name>
      <uri>https://old.reddit.com/user/GPTshop_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt; &lt;img alt="The reason why local models are better/necessary." src="https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac" title="The reason why local models are better/necessary." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop_ai"&gt; /u/GPTshop_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vdngpglhb8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T13:30:11+00:00</published>
  </entry>
</feed>
