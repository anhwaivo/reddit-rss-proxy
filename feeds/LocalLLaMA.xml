<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-01T16:24:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n4zo0a</id>
    <title>Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)</title>
    <updated>2025-08-31T16:49:25+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"&gt; &lt;img alt="Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)" src="https://external-preview.redd.it/lci6um6P0-wNteMe0vq3qeiBXe7Q5rNLEGJXrjSC4F4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fb361a7ef0f1b7c8f29357dfff067fa18ed656a" title="Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those wondering what my finetuning goals are, please expand and read &amp;quot;Who is Drummer?&amp;quot; and &amp;quot;What are my models like?&amp;quot; in the model card.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Behemoth-X-123B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T16:49:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n53ib4</id>
    <title>I built Anthropic's contextual retrieval with visual debugging and now I can see chunks transform in real-time</title>
    <updated>2025-08-31T19:21:21+00:00</updated>
    <author>
      <name>/u/autollama_dev</name>
      <uri>https://old.reddit.com/user/autollama_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n53ib4/i_built_anthropics_contextual_retrieval_with/"&gt; &lt;img alt="I built Anthropic's contextual retrieval with visual debugging and now I can see chunks transform in real-time" src="https://external-preview.redd.it/enKVm0ixiezcQ8SsRUGpYk5Ry46vKopZ5N1VDtQUnWA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=512c958eb6edf0898d924b0b5ecbf40129764bda" title="I built Anthropic's contextual retrieval with visual debugging and now I can see chunks transform in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's address the elephant in the room first: &lt;strong&gt;Yes, you can visualize embeddings with other tools&lt;/strong&gt; (TensorFlow Projector, Atlas, etc.). But I haven't found anything that shows the &lt;em&gt;transformation&lt;/em&gt; that happens during contextual enhancement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A RAG framework that implements Anthropic's contextual retrieval but lets you actually see what's happening to your chunks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Split View:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Left: Your original chunk (what most RAG systems use)&lt;/li&gt; &lt;li&gt;Right: The same chunk after AI adds context about its place in the document&lt;/li&gt; &lt;li&gt;Bottom: The actual embedding heatmap showing all 1536 dimensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Standard embedding visualizers show you the end result. This shows the journey. You can see exactly how adding context changes the vector representation.&lt;/p&gt; &lt;p&gt;According to Anthropic's research, this contextual enhancement gives 35-67% better retrieval: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/engineering/contextual-retrieval"&gt;https://www.anthropic.com/engineering/contextual-retrieval&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI text-embedding-3-small for vectors&lt;/li&gt; &lt;li&gt;GPT-4o-mini for context generation&lt;/li&gt; &lt;li&gt;Qdrant for vector storage&lt;/li&gt; &lt;li&gt;React/D3.js for visualizations&lt;/li&gt; &lt;li&gt;Node.js because the JavaScript ecosystem needs more RAG tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What surprised me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The heatmaps show that contextually enhanced chunks have noticeably different patterns - more activated dimensions in specific regions. You can literally see the context &amp;quot;light up&amp;quot; parts of the vector that were dormant before.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Honest question for the community:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Is anyone else frustrated that we implement these advanced RAG techniques but have no visibility into whether they're actually working? How do you debug your embeddings?&lt;/p&gt; &lt;p&gt;Code: &lt;a href="http://github.com/autollama/autollama"&gt;github.com/autollama/autollama&lt;/a&gt;&lt;br /&gt; Demo: &lt;a href="http://autollama.io"&gt;autollama.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The imgur album shows a Moby Dick chunk getting enhanced - watch how &amp;quot;Ahab and Starbuck in the cabin&amp;quot; becomes aware of the mounting tension and foreshadowing.&lt;/p&gt; &lt;p&gt;Happy to discuss the implementation or hear about other approaches to embedding transparency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autollama_dev"&gt; /u/autollama_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/xcOu3go"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n53ib4/i_built_anthropics_contextual_retrieval_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n53ib4/i_built_anthropics_contextual_retrieval_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T19:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5qi4l</id>
    <title>How do you handle background noise &amp; VAD for real-time voice agents?</title>
    <updated>2025-09-01T14:40:30+00:00</updated>
    <author>
      <name>/u/Funny_Working_7490</name>
      <uri>https://old.reddit.com/user/Funny_Working_7490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with building a voice agent using real-time STT, but I’m running into the classic issue: the transcriber happily picks up everything — background noise, side voices, even silence that gets misclassified. Stt: GPT-4o Transcribe (using their VAD) over WebSocket &lt;/p&gt; &lt;p&gt;For folks who’ve built real-time voice agents / caller bots:&lt;/p&gt; &lt;p&gt;How do you decide when to turn STT on/off so it only captures the right user at the right time?&lt;/p&gt; &lt;p&gt;Do you rely mostly on model-side VAD (like GPT-4o’s) or add another layer (Silero VAD, WebRTC noise suppression, Krisp, etc.)?&lt;/p&gt; &lt;p&gt;Any best practices for keeping things real-time while filtering background voices?&lt;/p&gt; &lt;p&gt;Do you handle this more on the client side (mic constraints, suppression) or on the backend?&lt;/p&gt; &lt;p&gt;I’m especially curious about what has actually worked for others in production&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Funny_Working_7490"&gt; /u/Funny_Working_7490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qi4l/how_do_you_handle_background_noise_vad_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qi4l/how_do_you_handle_background_noise_vad_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qi4l/how_do_you_handle_background_noise_vad_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T14:40:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5r2qr</id>
    <title>Old audio recording enhancement Model</title>
    <updated>2025-09-01T15:02:32+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I am trying to find if there is any model that can be used to enhance and recover lost frequencies of old audio tape recordings. &lt;/p&gt; &lt;p&gt;The requirement is that I have old music band recordings on tapes. The tapes loose a lot of frequencies in recordings and I am looking for a way to generate them back. &lt;/p&gt; &lt;p&gt;Any ideas would be helpful. What would a setup for this looks like software wise. I am currently using LM Studio and Llamacpp on Ryzen AI 395+ Max 128G&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5r2qr/old_audio_recording_enhancement_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5r2qr/old_audio_recording_enhancement_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5r2qr/old_audio_recording_enhancement_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T15:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5rhnt</id>
    <title>Need advice on setting up RAG with multi-modal data for an Agent</title>
    <updated>2025-09-01T15:18:19+00:00</updated>
    <author>
      <name>/u/Ahmad401</name>
      <uri>https://old.reddit.com/user/Ahmad401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on a digital agent, where I have information about a product from 4 different departments. Below are the nature of each department data source: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Data Source-1: The data is in text summary format. In future I am thinking of making it into structured data for better RAG retrieval &lt;/li&gt; &lt;li&gt;Data Source-2: For each product, two versions are there, one is summary (50 to 200 words) and other one is very detailed document with lots of sections and description (~3000 words)&lt;/li&gt; &lt;li&gt;Data Source-3: For each product, two versions are there, one is summary (50 to 200 words) excel and other one is very detailed document with lots of sections and description (~3000 words)&lt;/li&gt; &lt;li&gt;Data Source-4: Old reference documents (pdf) related to that product, each document contains any where between 10 to 15 pages with word count of 5000 words&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My thought process is to handle any question related to a specific product, I should be able to extract all the metadata related to that product. But here, If I add all the content related to a product every time, the prompt length will increase significantly. &lt;/p&gt; &lt;p&gt;For now I am taking the summary data of each data source as a metadata. And keeping product name in the vector database. So when user asks any question related to a specific product thorough RAG I can identify correct product and from metadata I can access all the content. Here I know, I can stick with conditional logic as well for getting metadata, but I am trying with RAG thinking I may use additional information in the embedding extraction. &lt;/p&gt; &lt;p&gt;Now my question is for Data Source - 3 and 4, for some specific questions, I need detailed document information. Since I can't send this every time due to context and token usage limitations, I am looking for creating RAG for these documents, but I am not sure how scalable that is. because if I want to maintain 1000 different products, then I need 2000 separate vector databases. &lt;/p&gt; &lt;p&gt;Is my thought process correct, or is there any better alternative. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ahmad401"&gt; /u/Ahmad401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhnt/need_advice_on_setting_up_rag_with_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhnt/need_advice_on_setting_up_rag_with_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhnt/need_advice_on_setting_up_rag_with_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T15:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wf0j</id>
    <title>Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀</title>
    <updated>2025-08-31T14:39:22+00:00</updated>
    <author>
      <name>/u/False_Mountain_7289</name>
      <uri>https://old.reddit.com/user/False_Mountain_7289</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"&gt; &lt;img alt="Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀" src="https://preview.redd.it/rcciowx66dmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edef201361f7e43cd16bc481e9d389945c77c84c" title="Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the last 2 months building something that might change how students prepare USMLE/UKMLE/NEET-PG forever. Meet &lt;strong&gt;Neeto-1.0-8B&lt;/strong&gt; - a specialized, 8-billion-parameter biomedical LLM fine-tuned on a curated dataset of over 500K items. Our goal was clear: create a model that could not only assist with medical exam prep (NEET-PG, USMLE, UKMLE) but also strengthen factual recall and clinical reasoning for practitioners and the model itself outperforming general models by 25% on medical datasets.&lt;/p&gt; &lt;p&gt;Docs + model on Hugging Face 👉 &lt;a href="https://huggingface.co/S4nfs/Neeto-1.0-8b"&gt;https://huggingface.co/S4nfs/Neeto-1.0-8b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🤯 The Problem&lt;/h1&gt; &lt;p&gt;While my company was preparing a research paper on USMLE/UKMLE/NEET-PG and medical science, I realized existing AI assistants couldn't handle medical reasoning. They'd hallucinate drug interactions, miss diagnostic nuances, and provide dangerous oversimplifications. So I decided to build something better at my organization.&lt;/p&gt; &lt;h1&gt;🚀 The Breakthrough&lt;/h1&gt; &lt;p&gt;After 1 month of training on more than &lt;strong&gt;410,000+ medical samples&lt;/strong&gt; (MedMCQA, USMLE questions, clinical cases) and private datasets from our my organization's platform medicoplasma[dot]com, we achieved:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;outperforms&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MedQA Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;85.8%&lt;/td&gt; &lt;td align="left"&gt;+87% vs general AI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PubMedQA&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;79.0%&lt;/td&gt; &lt;td align="left"&gt;+23% vs other medical AIs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Response Time&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&amp;lt;2 seconds&lt;/td&gt; &lt;td align="left"&gt;Real-time clinical use&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🔧 Technical Deep Dive&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Llama-3.1-8B with full-parameter fine-tuning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: 8×H200 GPUs using FSDP (Fully Sharded Data Parallel)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;: 4-bit GGUF for consumer hardware compatibility&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's how we compare to other models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;MedQA Score&lt;/th&gt; &lt;th align="left"&gt;Medical Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Neeto-1.0-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;85.8%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Expert-level&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;62.3%&lt;/td&gt; &lt;td align="left"&gt;Intermediate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenBioLM-8B&lt;/td&gt; &lt;td align="left"&gt;59.1%&lt;/td&gt; &lt;td align="left"&gt;Basic&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Yesterday, I watched a friend use Neeto to diagnose a complex case of &lt;strong&gt;ureteral calculus with aberrant renal artery anatomy&lt;/strong&gt; - something that would take hours in textbooks. Neeto provided the differential diagnosis in &lt;strong&gt;1.7 seconds&lt;/strong&gt; with 92% confidence.&lt;/p&gt; &lt;h1&gt;💻 How to Use It Right Now&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Install vLLM pip install vllm # 2. Run the medical AI server vllm serve S4nfs/Neeto-1.0-8b # 3. Ask medical questions curl http://localhost:8000/v1/completions -H &amp;quot;Content-Type: application/json&amp;quot; -d '{ &amp;quot;model&amp;quot;: &amp;quot;S4nfs/Neeto-1.0-8b&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;A 55-year-old male with flank pain and hematuria...&amp;quot;, &amp;quot;max_tokens&amp;quot;: 4096, &amp;quot;temperature&amp;quot;: 0.7 }' &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;🌟 What Makes This Different&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Cultural Context&lt;/strong&gt;: Optimized for advanced healthcare system and terminology&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real Clinical Validation&lt;/strong&gt;: Tested by 50+ doctors across global universities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accessibility&lt;/strong&gt;: Runs on single GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: Full training data and methodology disclosed (2 datasets are private as i am seeking permission from my org to release)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;📈 Benchmark Dominance&lt;/h1&gt; &lt;p&gt;We're outperforming every similar-sized model across 7 medical benchmarks, (see docs, for full results):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MedMCQA: 66.2% (+18% over competitors)&lt;/li&gt; &lt;li&gt;MMLU Medical Genetics: 87.1% (Best in class)&lt;/li&gt; &lt;li&gt;Clinical Knowledge: 79.4% (Near-specialist level)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upvote &amp;amp; like the model for medical research. Feedback, criticism &amp;amp; collaborations welcome! 🤗&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Mountain_7289"&gt; /u/False_Mountain_7289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rcciowx66dmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4v0ql</id>
    <title>LongCat-Flash-Chat 560B MoE</title>
    <updated>2025-08-31T13:41:09+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"&gt; &lt;img alt="LongCat-Flash-Chat 560B MoE" src="https://preview.redd.it/4pfegt9ezcmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=611d4a68a425489022dacb28fc5bd82d9690c441" title="LongCat-Flash-Chat 560B MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LongCat-Flash-Chat is a powerful and efficient language model with an innovative Mixture-of-Experts (MoE) architecture. It contains 560 billion total parameters but dynamically activates only 18.6 to 31.3 billion parameters (averaging ~27B) per token, optimizing for both performance and efficiency. It is designed to be a non-thinking foundation model with exceptional strengths in agentic tasks.&lt;/p&gt; &lt;p&gt;Key Features * Efficient Architecture: Uses a Mixture-of-Experts (MoE) design with a &amp;quot;zero-computation experts mechanism&amp;quot; and a &amp;quot;Shortcut-connected MoE&amp;quot; to optimize for computational efficiency and communication overlap. * Robust Scaling Strategy: Employs a comprehensive framework for stable training at a massive scale, including a hyperparameter transfer strategy, a model-growth initialization mechanism, and a multi-pronged stability suite. * Advanced Training Pipeline: A multi-stage pipeline was used to imbue the model with advanced agentic behaviors, focusing on reasoning, coding, and a long context length of 128k. It also uses a multi-agent synthesis framework to create complex training tasks.&lt;/p&gt; &lt;p&gt;Evaluation Highlights&lt;/p&gt; &lt;p&gt;The model demonstrates highly competitive performance across a wide range of benchmarks. Noteworthy strengths include: * Instruction Following: Achieves high scores on benchmarks like IFEval and COLLIE. * Agentic Tool Use: Shows strong results on agent-specific benchmarks such as τ²-Bench and VitaBench. * Mathematical Reasoning: Performs competitively on a variety of math reasoning tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;License: The model is released under the MIT License.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4pfegt9ezcmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T13:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5mvkp</id>
    <title>Hardware selection for LocalLLM + Obsidian Vault (PKM)</title>
    <updated>2025-09-01T12:04:41+00:00</updated>
    <author>
      <name>/u/Dethros</name>
      <uri>https://old.reddit.com/user/Dethros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, as the title suggests, I am getting into using PKM for my notes. I have been using google studio API keys to run AI assistant with my vault notes and RAG embedding to run my queries. Honesty I am blown away with the personal performance increase that I am feeling with the setup. I am ready to invest around 2500 euros for a local AI setup as I don't want to share my information stored in notes with google for privacy reasons. I am torn between a RTX 5080 setup vs Framework 125 Gb desktop. I am planning to design my own pipelines and integrate AI agents running locally with my notes to give me best cognitive improvement. I am interested in building a smart second brain that works. Although framework can run larger model, but as I want to get my hands dirty with trial and error, I am hesitant that having a iGPU that does not use CUDA might be a bottleneck. At the same time RTX offers better token generation but running larger models will be a bottleneck, Please let me know if you have any suggestions for hardware and LLM selection.&lt;/p&gt; &lt;p&gt;As I am doing theoretical physics research, any LLM setup that can understand basic latex maths and helps me connect my atomic notes into a coherent logical framework would be helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dethros"&gt; /u/Dethros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5mvkp/hardware_selection_for_localllm_obsidian_vault_pkm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5mvkp/hardware_selection_for_localllm_obsidian_vault_pkm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5mvkp/hardware_selection_for_localllm_obsidian_vault_pkm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T12:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5t4km</id>
    <title>Are there any SDKs that offer native tool calling functionality that can be used with any LLMs</title>
    <updated>2025-09-01T16:19:41+00:00</updated>
    <author>
      <name>/u/Ok_Needleworker_5247</name>
      <uri>https://old.reddit.com/user/Ok_Needleworker_5247</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says all. I know most model providers offer this on their cloud APIs but I am looking for an SDK that implements tool calling so that it can be used with any open weights model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Needleworker_5247"&gt; /u/Ok_Needleworker_5247 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5t4km/are_there_any_sdks_that_offer_native_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5t4km/are_there_any_sdks_that_offer_native_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5t4km/are_there_any_sdks_that_offer_native_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T16:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57pyj</id>
    <title>Hunyuan-MT-7B / Hunyuan-MT-Chimera-7B</title>
    <updated>2025-08-31T22:15:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;The Hunyuan Translation Model comprises a translation model, Hunyuan-MT-7B, and an ensemble model, Hunyuan-MT-Chimera. The translation model is used to translate source text into the target language, while the ensemble model integrates multiple translation outputs to produce a higher-quality result. It primarily supports mutual translation among 33 languages, including five ethnic minority languages in China.&lt;/p&gt; &lt;h1&gt;Key Features and Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;In the WMT25 competition, the model achieved first place in 30 out of the 31 language categories it participated in.&lt;/li&gt; &lt;li&gt;Hunyuan-MT-7B achieves industry-leading performance among models of comparable scale&lt;/li&gt; &lt;li&gt;Hunyuan-MT-Chimera-7B is the industry’s first open-source translation ensemble model, elevating translation quality to a new level&lt;/li&gt; &lt;li&gt;A comprehensive training framework for translation models has been proposed, spanning from pretrain → cross-lingual pretraining (CPT) → supervised fine-tuning (SFT) → translation enhancement → ensemble refinement, achieving state-of-the-art (SOTA) results for models of similar size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-MT-7B"&gt;https://huggingface.co/tencent/Hunyuan-MT-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B"&gt;https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57pyj/hunyuanmt7b_hunyuanmtchimera7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57pyj/hunyuanmt7b_hunyuanmtchimera7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n57pyj/hunyuanmt7b_hunyuanmtchimera7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T22:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n562o1</id>
    <title>[Meta] Add hardware flair?</title>
    <updated>2025-08-31T21:04:42+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It helps to know what hardware someone is running when they comment or post (including Openrouter; I know &amp;quot;no local no care&amp;quot;, said it myself, but let's be realistic and accommodating of enthusiasts because more enthusiasim is welcome). The flair will be a telltale sign of what quant they're using and will clean up the usual comments asking what the setup is. What do you think?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1n562o1"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n562o1/meta_add_hardware_flair/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n562o1/meta_add_hardware_flair/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n562o1/meta_add_hardware_flair/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T21:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5iiay</id>
    <title>Finally got Qwen3-Coder-30B-A3B running well. What tasks have you had success with?</title>
    <updated>2025-09-01T07:41:59+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5iiay/finally_got_qwen3coder30ba3b_running_well_what/"&gt; &lt;img alt="Finally got Qwen3-Coder-30B-A3B running well. What tasks have you had success with?" src="https://external-preview.redd.it/-r4KL8pAfF-qgIQHVdjvcXqo3OFxprXOaEKYH1y4dDc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9933d2e5af0447573f146afb98458ea23fb5fb73" title="Finally got Qwen3-Coder-30B-A3B running well. What tasks have you had success with?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get Qwen3 Coder running on a pair of older NVIDIA A4500s. Finally got it. Found a quant to run with vLLM that seems to be optimized pretty well. 4-bit weights and 16-bit activations. Split across 2 GPUs with 20GB VRAM each I can fit 128k context. 115 tokens/s.&lt;/p&gt; &lt;p&gt;What kind of tasks have worked well for you? What hasn't worked well?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s947a9lkaimf1.png?width=2874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0fe20c87817d10d07f532cc7c0467e56689ad401"&gt;nvtop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k08uyzytaimf1.png?width=1495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821257316c9231f448ec16d5c005fe0da6b6860"&gt;gpustack example&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W4A16"&gt;https://huggingface.co/ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W4A16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;run params from the logs in the &lt;a href="https://gpustack.ai/"&gt;gpustack&lt;/a&gt; platform if you're curious:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[(APIServer pid=3153)[ INFO 09-01 14:47:42 [api_server.py:1805] vLLM API server version 0.10.1.1 [(APIServer pid=3153)[ INFO 09-01 14:47:42 [utils.py:326] non-default args: {'model_tag': '/var/lib/gpustack/cache/huggingface/ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W4A16', 'host': '0.0.0.0', 'port': 40016, 'model': '/var/lib/gpustack/cache/huggingface/ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W4A16', 'trust_remote_code': True, 'dtype': 'half', 'max_model_len': 131076, 'served_model_name': ['qwen3-coder-30b-a3b'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5iiay/finally_got_qwen3coder30ba3b_running_well_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5iiay/finally_got_qwen3coder30ba3b_running_well_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5iiay/finally_got_qwen3coder30ba3b_running_well_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T07:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5qmu1</id>
    <title>Can someone help me with where to generate or get a roleplay dataset (mid-nsfw) to fine-tune LLaMA 3.1 8b?</title>
    <updated>2025-09-01T14:45:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;😶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T14:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5pnjq</id>
    <title>Built a private AI framework that runs fully offline on iPhone (demo inside) (OPEN SOURCE)</title>
    <updated>2025-09-01T14:07:20+00:00</updated>
    <author>
      <name>/u/Traditional_Day2212</name>
      <uri>https://old.reddit.com/user/Traditional_Day2212</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5pnjq/built_a_private_ai_framework_that_runs_fully/"&gt; &lt;img alt="Built a private AI framework that runs fully offline on iPhone (demo inside) (OPEN SOURCE)" src="https://external-preview.redd.it/bmdxOXg0czY4a21mMRFTCHiFf2voMJf7wjbDcIgNqYuBxaApNc19HBSW_O8A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e7cc4469701b1de13e0c1542e16032cf94edfb5" title="Built a private AI framework that runs fully offline on iPhone (demo inside) (OPEN SOURCE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;🐍 Basilisk AI&lt;/h1&gt; &lt;p&gt;🚨 &lt;strong&gt;A framework for a fully customizable private AI system that runs 100% offline.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Basilisk is built to combine &lt;strong&gt;vision, language, and reasoning&lt;/strong&gt; into a single offline pipeline — with no reliance on the cloud. It’s designed as a private, adaptable framework that you can extend to your own use cases. &lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🚀 Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;🖼️ &lt;strong&gt;MiniVLM2&lt;/strong&gt; – Vision-Language model for image understanding&lt;br /&gt;&lt;/li&gt; &lt;li&gt;🔬 &lt;strong&gt;CNN&lt;/strong&gt; – Lightweight convolutional neural network optimized for iOS &amp;amp; NumPy&lt;br /&gt;&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;MiniLLM&lt;/strong&gt; – Small language model for reasoning + dialogue&lt;br /&gt;&lt;/li&gt; &lt;li&gt;⏳ &lt;strong&gt;MiniLSM&lt;/strong&gt; – Context &amp;amp; sequence memory&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;🔒 Why Basilisk?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline&lt;/strong&gt; → runs entirely on-device (tested on iPhone with Pyto)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private by Design&lt;/strong&gt; → no data ever leaves your device&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt; → extend the framework with your own datasets &amp;amp; modules&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt; → NumPy-only execution, minimal dependencies&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;📂 Structure Overview&lt;/h2&gt; &lt;p&gt;Basilisk/&lt;br /&gt; ├── MiniVLM2.py # Vision-Language module&lt;br /&gt; ├── CNNModel.py # Convolutional Neural Network&lt;br /&gt; ├── MiniLLM.py # Lightweight reasoning model&lt;br /&gt; ├── MiniLSM.py # Sequence &amp;amp; memory module&lt;br /&gt; ├── main.py # Demo runner&lt;br /&gt; └── README.md # Documentation &lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;⚡ Quick Start&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;br /&gt; - Python 3.11+&lt;br /&gt; - Pyto (for iOS)&lt;br /&gt; - NumPy &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt;&lt;br /&gt; python main.py &lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🎯 Use Cases&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;📊 Data &amp;amp; chart analysis&lt;br /&gt;&lt;/li&gt; &lt;li&gt;🖼️ Offline image recognition&lt;br /&gt;&lt;/li&gt; &lt;li&gt;🔍 Local text &amp;amp; document understanding&lt;br /&gt;&lt;/li&gt; &lt;li&gt;🤖 On-device private assistant&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;📜 License &amp;amp; Access&lt;/h2&gt; &lt;p&gt;Basilisk is a &lt;strong&gt;framework for a fully customizable private AI system&lt;/strong&gt;.&lt;br /&gt; It is open-source inspired, but not yet hosted publicly. &lt;/p&gt; &lt;p&gt;📩 To get the code, just email me: &lt;strong&gt;&lt;a href="mailto:clucero.2411@gmail.com"&gt;clucero.2411@gmail.com&lt;/a&gt;&lt;/strong&gt; &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The future of AI isn’t in the cloud — it’s in your hands. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional_Day2212"&gt; /u/Traditional_Day2212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q0r6e5s68kmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5pnjq/built_a_private_ai_framework_that_runs_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5pnjq/built_a_private_ai_framework_that_runs_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T14:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5bdqe</id>
    <title>This is GPT-OSS 120b on Ollama, running on a i7 6700 3.4ghz, 64gb DDR4 2133mhz, RTX 3090 24GB, 1Tb standard SSD. No optimizations. first Token takes forever then it goes.</title>
    <updated>2025-09-01T01:08:43+00:00</updated>
    <author>
      <name>/u/oodelay</name>
      <uri>https://old.reddit.com/user/oodelay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5bdqe/this_is_gptoss_120b_on_ollama_running_on_a_i7/"&gt; &lt;img alt="This is GPT-OSS 120b on Ollama, running on a i7 6700 3.4ghz, 64gb DDR4 2133mhz, RTX 3090 24GB, 1Tb standard SSD. No optimizations. first Token takes forever then it goes." src="https://external-preview.redd.it/cGo4NmZxOWtkZ21mMf5iGCoukr275Bg__fIZwAtHUat4LEsnVHpMcvR__4io.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5221fac3c3d3abcba7923bd22e955c9ef4223afb" title="This is GPT-OSS 120b on Ollama, running on a i7 6700 3.4ghz, 64gb DDR4 2133mhz, RTX 3090 24GB, 1Tb standard SSD. No optimizations. first Token takes forever then it goes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is to show my lowtech bros that it's possible to run on a 900$ piece of crap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oodelay"&gt; /u/oodelay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kkk0zx9kdgmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5bdqe/this_is_gptoss_120b_on_ollama_running_on_a_i7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5bdqe/this_is_gptoss_120b_on_ollama_running_on_a_i7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T01:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n56415</id>
    <title>China Has a Different Vision for AI. It Might Be Smarter.</title>
    <updated>2025-08-31T21:06:11+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n56415/china_has_a_different_vision_for_ai_it_might_be/"&gt; &lt;img alt="China Has a Different Vision for AI. It Might Be Smarter." src="https://external-preview.redd.it/dnVvgG8ZUEFmRAWrF1FHNcgY3TzseHykeC8SwzyeCro.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dc036b3460d200c59ac7be0a86558da5ed80783" title="China Has a Different Vision for AI. It Might Be Smarter." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those without a subscription, the basic gist is that the US is pushing towards AGI. China is pushing towards practical AI. They are putting their efforts into what you can use AI for today. Not on AGI sometime into the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wsj.com/tech/ai/china-has-a-different-vision-for-ai-it-might-be-smarter-581f1e44"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n56415/china_has_a_different_vision_for_ai_it_might_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n56415/china_has_a_different_vision_for_ai_it_might_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T21:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5p1oj</id>
    <title>Epyc 9575F + 4 * 3090 inference speed?</title>
    <updated>2025-09-01T13:42:29+00:00</updated>
    <author>
      <name>/u/Unhappy-Tangelo5790</name>
      <uri>https://old.reddit.com/user/Unhappy-Tangelo5790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m planning to build a server with 9575F+12 * ddr5 64G 6400+4 * 3090, to run run local inference using moe models like ds-r1 or GLM 4.5 and do a lot more other self hosted stuffs. &lt;/p&gt; &lt;p&gt;With ik_llama.cpp or ktransformer, does anyone have approximately the idea how much tps I’ll get with GLM 4.5 Q4_K_M with 8.2B actually active params (for simplicity, supposing zero context)? Moreover, I currently have only 1 3090 and i’m still waiting to see if better cards with higher vram will come out, what’s the approximate tps with only 1 3090 with the same cpu setup? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unhappy-Tangelo5790"&gt; /u/Unhappy-Tangelo5790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5p1oj/epyc_9575f_4_3090_inference_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5p1oj/epyc_9575f_4_3090_inference_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5p1oj/epyc_9575f_4_3090_inference_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T13:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wo0y</id>
    <title>The Huawei GPU is not equivalent to an RTX 6000 Pro whatsoever</title>
    <updated>2025-08-31T14:49:29+00:00</updated>
    <author>
      <name>/u/MCH_2000</name>
      <uri>https://old.reddit.com/user/MCH_2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a response to the recent viral post about the “amazing” Huawei GPU offering 96 GB for “only” 2000$ when Nvidia is way more expensive. (Edit: as many in the comments section noted, the Huawei is a dual GPU setup. Depending on the specific packaging, it might not be easy to run inference at peak speed). &lt;/p&gt; &lt;p&gt;The post leaves out important context.&lt;/p&gt; &lt;h1&gt;Performance (Sparsity)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;INT8: 1,000 (2,000) TOPs vs 280 TOPs &lt;/li&gt; &lt;li&gt;FP4 w/FP32 Accumulate: 2,000 (4,000) TFLOPs vs not supported. &lt;/li&gt; &lt;li&gt;Bandwidth: 1792 GB/s vs 408 GB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Huawei is closer to a mobile SoC than it is to a high end Nvidia dGPU. &lt;/p&gt; &lt;h1&gt;Memory&lt;/h1&gt; &lt;p&gt;The reason the Huawei GPU packs 96 GB is it’s using LPDDR4X. &lt;/p&gt; &lt;p&gt;LPDDR4X (64b) is 8 GB @ 34 GB/s&lt;/p&gt; &lt;p&gt;GDDR7 (64b) is 2-3 GB @ 256 GB/s&lt;/p&gt; &lt;p&gt;The Nvidia has a wider bus, but it doesn’t use the top GDDR7 memory bin. Regardless, Bandwidth is roughly 4.5x. And for the highly memory bound consumer inference, this will translate to 4~5x higher token/s. &lt;/p&gt; &lt;p&gt;One of the two memory technologies trades Bandwidth for capacity. And Huawei is using ancient memory technology. LP4X is outdated and there is already LP5, LP5X, LP5T, LP6 with far higher capacity and bandwidth. Huawei can’t use them because of the entity list. &lt;/p&gt; &lt;p&gt;For the record, it’s for this reason that you can get an AI MAX 395+ w/128 GB MINI PC (not simply a GPU) for the price of the Huawei. It comes with a 16 Core Zen 5 CPU and a 55 TOPs INT8 NPU which supports sparsity. it also comes with an RDNA3.5 iGPU that does 50 TFLOPs FP16 | 50 TOPs INT8. &lt;/p&gt; &lt;h1&gt;Software&lt;/h1&gt; &lt;p&gt;It needs no saying, but the Nvidia GPU will have vastly better software support. &lt;/p&gt; &lt;h1&gt;Context&lt;/h1&gt; &lt;p&gt;The RTX 6000 Pro is banned from being exported to China. The inflated price reflects the reality that it needs to be smuggled. Huawei’s GPU is Chinese domestically produced. No one from memory maker to fab to Huawei are actually making money without the Chinese government subsidizing them. &lt;/p&gt; &lt;p&gt;Nvidia is a private company that needs to make a profit to continue operating in the segment. Nvidia’s recent rise in market valuation is overwhelmingly premised on them expanding their datacenter revenues rather than expanding their consumer margins. &lt;/p&gt; &lt;p&gt;Simply look at the consumer market to see if Nvidia is abusing their monopoly. &lt;/p&gt; &lt;p&gt;Nvidia sells 380mm2 + 16 GB GDDR7 for 750$. (5070Ti)&lt;/p&gt; &lt;p&gt;AMD sells 355mm2 + 16 GB GDDR6 for 700$. (9070XT)&lt;/p&gt; &lt;p&gt;Nvidia is giving more for only slightly more. &lt;/p&gt; &lt;p&gt;The anti-Nvidia circle jerk is getting tiring. Nvidia WILL OFFER high memory capacities in 2026 early. Why then? Because that’s when Micron and SK Hynix 3 GB GDDR7 is ready. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MCH_2000"&gt; /u/MCH_2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:49:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5eqbz</id>
    <title>3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance</title>
    <updated>2025-09-01T04:01:43+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"&gt; &lt;img alt="3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance" src="https://preview.redd.it/owg4l5s47hmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80374ba03f865ca2e46aba4422c6b71958a362f" title="3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted my new dual GPU setup yesterday: 5090 and 3090 crammed right next to each other. I'll post thermals in the comments, but I thought this performance graph was super cool so I'm leading with that. The 3090 is the only one that suffers from the GPUs being stuffed right next to each other because its fans blow straight into the back heat sink of the 5090. Fortunately, it's a Galax HOF 3090, which was built to be put under strain, and it has a button on the back that turns on super mega extreme loud fan mode. In an earlier test the 3090 topped out at 79 degrees, but once I hit the super fan button in a subsequent longer test it didn't get above 69 degrees. The 5090 never got above 54 at all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/owg4l5s47hmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T04:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5n2h3</id>
    <title>Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks</title>
    <updated>2025-09-01T12:14:21+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"&gt; &lt;img alt="Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks" src="https://preview.redd.it/h8d68m9enjmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d21724bca765d6cd8e82243cab247845f595ebca" title="Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Context reasoning evaluates whether a model can read the provided material and answer only from it. The context reasoning category is part of our Task Completion Benchmarks. It tests LLMs on grounded question answering with strict use of the provided source, long context retrieval, and resistance to distractors across documents, emails, logs, and policy text. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick read on current winners&lt;/strong&gt;&lt;br /&gt; Top tier (score ≈97): Claude Sonnet 4, GPT-5-mini&lt;br /&gt; Next tier (≈93): Gemini 2.5 Flash, Gemini 2.5 Pro, Claude Opus 4, OpenAI o3&lt;br /&gt; Strong group (≈90–88): Claude 3.5 Sonnet, GLM-4.5, GPT-5, Grok-4, GPT-OSS-120B, o4-mini. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A tricky failure case to watch for&lt;/strong&gt;&lt;br /&gt; We include tasks where relevant facts are dispersed across a long context, like a travel journal with scattered city mentions. Many models undercount unless they truly track entities across paragraphs. The better context reasoners pass this reliably. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;br /&gt; Context use matters as much as raw capability. Anthropic’s recent Sonnet models, Google’s Gemini 2.5 line, and OpenAI’s new 5-series (especially mini) show strong grounding on these tasks. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can see the category, examples, and methodology here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://opper.ai/tasks/context-reasoning"&gt;https://opper.ai/tasks/context-reasoning&lt;/a&gt; &lt;/p&gt; &lt;p&gt;For those building with it, what strengths or edge cases are you seeing in context-heavy workloads?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h8d68m9enjmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T12:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5ebur</id>
    <title>What's the best local model for nsfw story telling?</title>
    <updated>2025-09-01T03:40:20+00:00</updated>
    <author>
      <name>/u/oogami</name>
      <uri>https://old.reddit.com/user/oogami</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for recommendations. I want to generate long nsfw novel.&lt;/p&gt; &lt;p&gt;I can use the company's idle H100 80GB * 8 server. I have tried &lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-Instruct-2507-abliterated-Q4_K_M-GGUF"&gt;huihui-ai/Huihui-Qwen3-235B-A22B-Instruct-2507-abliterated-Q4_K_M-GGUF&lt;/a&gt;, it works, but the novel quality is not very good, and it's very slow because it's gguf so it can't be runed by vllm.&lt;/p&gt; &lt;p&gt;I have also tried to run DeepSeek-R1-0528. But the AWQ version failed to work on vllm, I don't know why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oogami"&gt; /u/oogami &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T03:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5rhbd</id>
    <title>I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use</title>
    <updated>2025-09-01T15:17:56+00:00</updated>
    <author>
      <name>/u/Avienir</name>
      <uri>https://old.reddit.com/user/Avienir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt; &lt;img alt="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" src="https://external-preview.redd.it/cXB3bmh1bGZsa21mMfbflv5Di1j64vZv4v6FbqGgackbIUKWjlzVaYUu9HIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da4116b03f8236881e7202c319fe08d1d76dec5" title="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of overengineered and bloated AI libraries and needed something to prototype local RAG apps quickly so I decided to make my own library,&lt;br /&gt; Features:&lt;br /&gt; ➡️ Get to prototyping local RAG applications in seconds: uvx rocketrag prepare &amp;amp; uv rocketrag ask is all you need&lt;br /&gt; ➡️ CLI first interface, you can even visualize embeddings in your terminal&lt;br /&gt; ➡️ Native llama.cpp bindings - no Ollama bullshit&lt;br /&gt; ➡️ Ready to use minimalistic web app with chat, vectors visualization and browsing documents➡️ Minimal footprint: milvus-lite, llama.cpp, kreuzberg, simple html web app&lt;br /&gt; ➡️ Tiny but powerful - use any chucking method from chonkie, any LLM with .gguf provided and any embedding model from sentence-transformers&lt;br /&gt; ➡️ Easily extendible - implement your own document loaders, chunkers and BDs, contributions welcome!&lt;br /&gt; Link to repo: &lt;a href="https://github.com/TheLion-ai/RocketRAG"&gt;https://github.com/TheLion-ai/RocketRAG&lt;/a&gt;&lt;br /&gt; Let me know what you think. If anybody wants to collaborate and contribute DM me or just open a PR!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avienir"&gt; /u/Avienir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tqnduvlflkmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T15:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5jhts</id>
    <title>gpt-oss 120b actually isn't that bad.</title>
    <updated>2025-09-01T08:46:10+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. I just wanted to make this post to see what everyone else thinks. It runs at a respectable 10~ tokens a second with 128k context split between a 3090TI and a 3090 (K and V caches on system ram) and did very well on some math and coding tests I put it through. It honestly feels like a lightweight version of ChatGPT which is not something I would complain about given that it's open weight and runs on 2 consumer gpus. It's not perfect and it refuses for absolutely no reason sometimes but for what it is, it's not terrible. It outperforms Llama 3.3 70b in a lot of ways which is my usual go-to but I can't decide if I like it ENOUGH to make it my default. Perhaps maybe I'll try and finetune it for longer answers and less censorship? Idk I just wanted to say that I gave it a shot and as much as I hate what OpenAI has become, I can't really say it's a terrible llm for what it is. The 20b model is still pretty iffy though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T08:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57hb8</id>
    <title>I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them</title>
    <updated>2025-08-31T22:04:33+00:00</updated>
    <author>
      <name>/u/jayminban</name>
      <uri>https://old.reddit.com/user/jayminban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"&gt; &lt;img alt="I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them" src="https://preview.redd.it/a2bfcgphgfmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=feddc11eb0681fb3c1b60c20d6369d89110c9a3a" title="I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I benchmarked 41 open-source LLMs using lm-evaluation-harness. Here are the 19 tasks covered:&lt;/p&gt; &lt;p&gt;mmlu, arc_challenge, gsm8k, bbh, truthfulqa, piqa, hellaswag, winogrande, boolq, drop, triviaqa, nq_open, sciq, qnli, gpqa, openbookqa, anli_r1, anli_r2, anli_r3&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ranks were computed by taking the simple average of task scores (scaled 0–1).&lt;/li&gt; &lt;li&gt;Sub-category rankings, GPU and memory usage logs, a master table with all information, raw JSON files, Jupyter notebook for tables, and script used to run benchmarks are posted on my GitHub repo.&lt;/li&gt; &lt;li&gt;🔗 &lt;a href="http://github.com/jayminban/41-llms-evaluated-on-19-benchmarks"&gt;github.com/jayminban/41-llms-evaluated-on-19-benchmarks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project required:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;18 days 8 hours of runtime&lt;/li&gt; &lt;li&gt;Equivalent to 14 days 23 hours of RTX 5090 GPU time, calculated at 100% utilization.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The environmental impact caused by this project was mitigated through my active use of public transportation. :)&lt;/p&gt; &lt;p&gt;Any feedback or ideas for my next project are greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jayminban"&gt; /u/jayminban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a2bfcgphgfmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T22:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5j783</id>
    <title>I built, pre-trained, and fine-tuned a small language model and it is truly open-source.</title>
    <updated>2025-09-01T08:26:34+00:00</updated>
    <author>
      <name>/u/itsnikity</name>
      <uri>https://old.reddit.com/user/itsnikity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt; &lt;img alt="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." src="https://preview.redd.it/cwyoa0f6kimf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147ff4faaa129cc07cda0d4d53d824668e625f35" title="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, most of the time we all read open-source and in reality it is just open-weights. This time it is truly open-source.&lt;/p&gt; &lt;p&gt;Lille is a 130M parameter model trained from scratch and every part of the stack is open. Dataset, Model weights, Training code, Tokenizer, Optimizer, Evaluation framework...&lt;/p&gt; &lt;p&gt;Two versions are available: a base model trained on billions of tokens, and an instruction-tuned version fine-tuned on a curated instruction dataset.&lt;/p&gt; &lt;p&gt;Fun fact: it was trained locally on a single RTX 4070-TI.&lt;/p&gt; &lt;p&gt;I’d love feedback, suggestions, or contributions - whether it’s fine-tuning ideas, evaluation improvements, or even architectural tweaks.&lt;/p&gt; &lt;p&gt;Thanks! Check it out: &lt;a href="https://huggingface.co/Nikity/lille-130m-instruct"&gt;Lille 130M Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnikity"&gt; /u/itsnikity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwyoa0f6kimf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T08:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
