<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-21T07:36:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mvuzzl</id>
    <title>Offline AI models for background noise removal and voice isolation</title>
    <updated>2025-08-20T23:32:14+00:00</updated>
    <author>
      <name>/u/healthiswealth0</name>
      <uri>https://old.reddit.com/user/healthiswealth0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Izotope 11 doesn't give results comparable to Adobe Podcast, but AP can only process max 4h/recording and it's online only.&lt;/p&gt; &lt;p&gt;Is there any offline AI model I can use which outputs similar quality as AP? I have RTX4090 so GPU is not an issue. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/healthiswealth0"&gt; /u/healthiswealth0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T23:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnetu</id>
    <title>New Trainable Sparsity Method I've been working on!</title>
    <updated>2025-08-20T18:41:56+00:00</updated>
    <author>
      <name>/u/nano-tech-warrior</name>
      <uri>https://old.reddit.com/user/nano-tech-warrior</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt; &lt;img alt="New Trainable Sparsity Method I've been working on!" src="https://preview.redd.it/mpxhgfb1y7kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1be78ec5f54af38b4819955caf62751a49149e6" title="New Trainable Sparsity Method I've been working on!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing CWIC a trainable sparsity paradigm that beats SOTA methods, enabling 80% sparsity and 4x+ speedups on CPU.&lt;/p&gt; &lt;p&gt;Something I've been working on with friends at &lt;a href="http://crystalai.org"&gt;crystalai.org&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;It works on models as small as 1b, outperforming TEAL R-sparse and friends.&lt;br /&gt; We are releasing code at &lt;a href="https://github.com/crystal-ai-org/cwic"&gt;https://github.com/crystal-ai-org/cwic&lt;/a&gt;&lt;br /&gt; read more at the blog &lt;a href="https://crystalai.org/blog/2025-08-18-compute-where-it-counts"&gt;https://crystalai.org/blog/2025-08-18-compute-where-it-counts&lt;/a&gt;&lt;br /&gt; if your interested in our our work feel free to reach out at &lt;a href="https://x.com/crystalAIorg"&gt;https://x.com/crystalAIorg&lt;/a&gt;, we love collaboration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nano-tech-warrior"&gt; /u/nano-tech-warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mpxhgfb1y7kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvgw9k</id>
    <title>DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC</title>
    <updated>2025-08-20T14:48:46+00:00</updated>
    <author>
      <name>/u/alexmrv</name>
      <uri>https://old.reddit.com/user/alexmrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt; &lt;img alt="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" src="https://external-preview.redd.it/FW1JlH9sSss0Pq8rzoWxjFpJJZK922NxO1y6uOe6VUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d145871e4a70112dabd181298b3fe1e73ada2a35" title="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been experimenting with memory systems for AI agents, and I wanted to share a prototype I've built: DiffMem. It's a lightweight, Git-based memory backend that stores &amp;quot;current state&amp;quot; knowledge in Markdown files while using Git's commit history for tracking evolution. The goal is efficient, scalable memory for long-horizon agents. Think personal assistants that accumulate knowledge over years without bloating queries.&lt;/p&gt; &lt;h1&gt;Why Git for AI Memory?&lt;/h1&gt; &lt;p&gt;Traditional approaches (databases, vector stores) work, but they can get messy with evolving personal data. DiffMem flips this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Current-State Focus&lt;/strong&gt;: Only the &amp;quot;now&amp;quot; view is in active files (e.g., current relationships or facts). This keeps search/indexing lean. BM25 queries hit a compact surface, reducing token overhead in LLM contexts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;History in the Background&lt;/strong&gt;: Changes live in Git diffs/logs. Agents query the present by default but can dive into &amp;quot;how did this evolve?&amp;quot; via targeted diffs (e.g., &lt;code&gt;git diff HEAD~1 file.md&lt;/code&gt;), without loading full histories.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benefits for Engineers&lt;/strong&gt;: No schemas/migrations. Just edit Markdown. Git handles versioning, branching (e.g., monthly timelines), and audits for free. It's durable (plaintext, distributed) and hackable.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt;: Analyzes transcripts, creates/updates entities, stages in Git's working tree (commit explicit for atomicity).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Manager&lt;/strong&gt;: Assembles depth-based context (basic: core blocks; deep: full files; temporal: with Git history).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Searcher&lt;/strong&gt;: LLM-orchestrated BM25 for semantic-aware retrieval.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a PoC. Functional but rough (e.g., manual index rebuilds, basic error handling). Full code, examples, and repo guide on GitHub.&lt;/p&gt; &lt;h1&gt;Why Share This?&lt;/h1&gt; &lt;p&gt;This is R&amp;amp;D exploring how version control can power evolvable agents. We're not claiming it's revolutionary, but it solves real pain points like memory sprawl and temporal reasoning. Future ideas: agent-driven pruning (archive low-strength memories to branches), collaborative repos for multi-agent systems, or hybrid with embeddings.&lt;/p&gt; &lt;p&gt;I'd love honest feedback: Does this resonate? What breaks? Ideas for improvements/collaborations? PRs welcome.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;https://github.com/Growth-Kinetics/DiffMem&lt;/a&gt;&lt;br /&gt; License: MIT&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexmrv"&gt; /u/alexmrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T14:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw1z0m</id>
    <title>Local Open Source Alternative to NotebookLM</title>
    <updated>2025-08-21T05:09:16+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;📊 &lt;strong&gt;Featu&lt;/strong&gt;res&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ &lt;strong&gt;Podca&lt;/strong&gt;sts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Extens&lt;/strong&gt;ion&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw1z0m/local_open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw1z0m/local_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw1z0m/local_open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvts3i</id>
    <title>2x RTX 5060ti 16GB - inference benchmarks in Ollama</title>
    <updated>2025-08-20T22:41:21+00:00</updated>
    <author>
      <name>/u/avedave</name>
      <uri>https://old.reddit.com/user/avedave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"&gt; &lt;img alt="2x RTX 5060ti 16GB - inference benchmarks in Ollama" src="https://b.thumbs.redditmedia.com/k4_tJqT09ffBK0-BkHQm_oeGLgR4XZG78TAvB_JX2Fk.jpg" title="2x RTX 5060ti 16GB - inference benchmarks in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Despite the recommendations of most Redditors, I chose not to fish a used 3090 out of a dumpster for $1,000. Instead, I bought two brand-new NVIDIA RTX 5060 Ti 16GB cards for a total of $800.&lt;/p&gt; &lt;p&gt;I am pretty happy with the inference results in Ollama!&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quantization: Q4_K_M (all models)&lt;/li&gt; &lt;li&gt;Prompt: &amp;quot;Write a 500-word essay containing recommendations for travel arrangements from Warsaw to New York, assuming it’s the year 1900.&amp;quot;&lt;/li&gt; &lt;li&gt;NVIDIA drivers: 575.64.03&lt;/li&gt; &lt;li&gt;CUDA version: 12.9&lt;/li&gt; &lt;li&gt;Ollama version: 0.11.4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Duration&lt;/th&gt; &lt;th align="left"&gt;Prompt Processing&lt;/th&gt; &lt;th align="left"&gt;Response Processing&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 1B&lt;/td&gt; &lt;td align="left"&gt;0m:4s&lt;/td&gt; &lt;td align="left"&gt;249 tokens/s&lt;/td&gt; &lt;td align="left"&gt;212 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 4B&lt;/td&gt; &lt;td align="left"&gt;0m:8s&lt;/td&gt; &lt;td align="left"&gt;364 tokens/s&lt;/td&gt; &lt;td align="left"&gt;108 token/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 12B&lt;/td&gt; &lt;td align="left"&gt;0m:18s&lt;/td&gt; &lt;td align="left"&gt;305 tokens/s&lt;/td&gt; &lt;td align="left"&gt;44 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 27B&lt;/td&gt; &lt;td align="left"&gt;0m:42s&lt;/td&gt; &lt;td align="left"&gt;217 tokens/s&lt;/td&gt; &lt;td align="left"&gt;22 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 70B&lt;/td&gt; &lt;td align="left"&gt;7m:31s&lt;/td&gt; &lt;td align="left"&gt;22 tokens/s&lt;/td&gt; &lt;td align="left"&gt;3.04 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Conclusions / Observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'd be happy to see a direct comparison, but I believe that for inference, 2x5060ti 16GB is a much better option than 1x3090 24GB&lt;/li&gt; &lt;li&gt;Load times for all models were between 1 and 10 seconds, so if you are worried about 5060ti being just PCIe 5 x8 - I don't think that an issue at all&lt;/li&gt; &lt;li&gt;Even during the lengthy inference of DeepSeek R1 70B each GPU was consuming around just 40W (while the card is rated at max 180W)&lt;/li&gt; &lt;li&gt;The temperature of GPUs was around 60C&lt;/li&gt; &lt;li&gt;The last two observations probably mean there's some room for improvement - I'd be happy to see any suggestions!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avedave"&gt; /u/avedave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvts3i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T22:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvo9ko</id>
    <title>Using large-scale search to discover fast GPU kernels</title>
    <updated>2025-08-20T19:12:53+00:00</updated>
    <author>
      <name>/u/jafioti</name>
      <uri>https://old.reddit.com/user/jafioti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt; &lt;img alt="Using large-scale search to discover fast GPU kernels" src="https://external-preview.redd.it/b-ktLeXWioxuV4hQoMUJJnc-Er8yy-L0PCsHQ7N2a0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd17cc6390a58faaef58b46b4cf1bfc8a445b756" title="Using large-scale search to discover fast GPU kernels" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance. &lt;a href="https://github.com/luminal-ai/luminal"&gt;https://github.com/luminal-ai/luminal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It takes high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.&lt;/p&gt; &lt;p&gt;You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: &lt;a href="https://youtu.be/P2oNR8zxSAA"&gt;https://youtu.be/P2oNR8zxSAA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.&lt;/p&gt; &lt;p&gt;We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.&lt;/p&gt; &lt;p&gt;The aim is to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo above and I’d love to hear your thoughts!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mvo9ko/video/dshypdss48kf1/player"&gt;https://reddit.com/link/1mvo9ko/video/dshypdss48kf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jafioti"&gt; /u/jafioti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvp0kn</id>
    <title>cursor will increase in price , The good thing is that we have local models</title>
    <updated>2025-08-20T19:40:16+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt; &lt;img alt="cursor will increase in price , The good thing is that we have local models" src="https://external-preview.redd.it/wrLxmwkxE0sboHYe9DL7M2A9cBYiHIRiSvubxC7TZHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0319732839ae255898da934baa62523680f329b8" title="cursor will increase in price , The good thing is that we have local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the cursor will increase in price. Right now, you have an elastic price, but after September 15, you will be charged more.&lt;/p&gt; &lt;p&gt;blog : &lt;a href="https://cursor.com/blog/aug-2025-pricing"&gt;https://cursor.com/blog/aug-2025-pricing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;price : &lt;a href="https://docs.cursor.com/en/account/pricing#auto"&gt;https://docs.cursor.com/en/account/pricing#auto&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5jfqnrb98kf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f23a973f7d39ab4ff7f46e79d3bcc014dacdb61"&gt;https://preview.redd.it/i5jfqnrb98kf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f23a973f7d39ab4ff7f46e79d3bcc014dacdb61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ytoam8gk88kf1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5db3b26f6f79cda3f7cc6717f5956e371c2587e3"&gt;https://preview.redd.it/ytoam8gk88kf1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5db3b26f6f79cda3f7cc6717f5956e371c2587e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvol0o</id>
    <title>Running Qwen3-Coder-30B-A3 Q4_LM in Cursor with Agent Mode unlocked</title>
    <updated>2025-08-20T19:24:29+00:00</updated>
    <author>
      <name>/u/ConfidentDinner6648</name>
      <uri>https://old.reddit.com/user/ConfidentDinner6648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing ways to make Cursor usable without relying only on their default “auto” model (which honestly feels pretty bad). While experimenting, I noticed something interesting:&lt;/p&gt; &lt;p&gt;If you run a model locally and just register it under the name &lt;code&gt;gpt-4o&lt;/code&gt;, Cursor unlocks &lt;strong&gt;Agent Mode&lt;/strong&gt; (function calling, todo list, etc.) and everything works as if it were an official endpoint.&lt;/p&gt; &lt;p&gt;I tried this with &lt;strong&gt;Qwen3-Coder-30B-A3 Q4_LM&lt;/strong&gt; (through LM Studio + ngrok) and here’s what I got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms Gemini Flash and Gemini Pro on many coding tasks&lt;/li&gt; &lt;li&gt;In some cases, feels close to Sonnet 4 (which is wild for a quantized 30B)&lt;/li&gt; &lt;li&gt;Function calling works smoothly, no errors so far&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This obviously isn’t official support, but it shows that Cursor &lt;em&gt;could&lt;/em&gt; support local/self-hosted models natively without much issue.&lt;/p&gt; &lt;p&gt;Anyone else tried running Qwen3 (or others) inside Cursor like this? Curious to hear results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentDinner6648"&gt; /u/ConfidentDinner6648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw0rm9</id>
    <title>Which weights under 50GB have the best *depth of knowledge*?</title>
    <updated>2025-08-21T04:03:45+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a benchmark for this that doesn't mix knowledge with reasoning? Just sheer encyclopedia knowledge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0rm9/which_weights_under_50gb_have_the_best_depth_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T04:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvynft</id>
    <title>Maxsun Dual Intel Arc Pro B60 available at $2,999</title>
    <updated>2025-08-21T02:18:53+00:00</updated>
    <author>
      <name>/u/ConcaveTriangle5761</name>
      <uri>https://old.reddit.com/user/ConcaveTriangle5761</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I emailed Maxsun about availability of their dual B60 cards, and got a response:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;let me introduce Mr. Jason Green, who is our US distributor for B60, he is gonna help you with the purchase, thanks.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Regards,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm Jason from Hydratech Builds, the US distributor for MAXSUN.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;To help you with your purchase, please let me know how many units you are interested in. For orders of fewer than 5 units, you can purchase directly from our website: [&lt;/em&gt;&lt;a href="http://www.hydratechbuilds.com%5D"&gt;&lt;em&gt;www.hydratechbuilds.com]&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Product page (Intel Arc Pro B60 48GB):&lt;/em&gt; &lt;a href="https://www.hydratechbuilds.com/product-page/intel-arc-pro-b60-dual-48g-turbo"&gt;&lt;em&gt;https://www.hydratechbuilds.com/product-page/intel-arc-pro-b60-dual-48g-turbo&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are looking to purchase 5 units or more per SKU, please let me know, and I will send you our US bulk pricelist.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thanks,&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Jason&lt;/em&gt;&lt;/p&gt; &lt;p&gt;On the product page, the cards are up at $2,999 USD each. I am reasonably confident that this is the official Maxsun US pricing, as the same website is listed under &lt;a href="https://www.maxsun.com/pages/where-to-buy/"&gt;https://www.maxsun.com/pages/where-to-buy/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConcaveTriangle5761"&gt; /u/ConcaveTriangle5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvynft/maxsun_dual_intel_arc_pro_b60_available_at_2999/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T02:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3j7l</id>
    <title>Deepseek V3.1 is not so bad after all..</title>
    <updated>2025-08-21T06:40:50+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt; &lt;img alt="Deepseek V3.1 is not so bad after all.." src="https://b.thumbs.redditmedia.com/T9qzPo7Bvut8hGvtV6Xg52JupsxwiKXPdU34MXCHojI.jpg" title="Deepseek V3.1 is not so bad after all.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like it just was a different purpose, speed and agency. Its pretty good at what its meant for&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw3j7l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2xci</id>
    <title>monkeSearch's first prototype is now public, And it works! Offline natural language query for local files using a VERY small LLM (Qwen3-0.6b) and it works amazingly right away. With temporal awareness.</title>
    <updated>2025-08-21T06:03:45+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, this is a follow up post of my old post, which was about building a local natural language file search engine using qwen0.6b and LangExtract, and today I am very excited to release a very bare bones and working prototype for this!&lt;br /&gt; &lt;a href="https://github.com/monkesearch/monkeSearch"&gt;https://github.com/monkesearch/monkeSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I'd love to get reviews and suggestions for this, and I've used macOS's inbuilt spotlight indexing for the query. There are a lot of modifications and feature additions to be done now but I want you guys to try it out locally. Current file search is only limited to a few file types because I am associating the macOS specific uniform type identifiers with file types, and that has been done manually just for the prototype right now. But I'd love to get ideas on how can I improve this. &lt;/p&gt; &lt;p&gt;No data leaves your pc and it is aimed at being able to run on potato pcs. And I'm currently aiming at a smaller and smarter model (Gemma 3 270M finetune) to increase the accuracy of the tool (even though it's pretty accurate right away with base Qwen3) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvl0zk</id>
    <title>Qwen-Image-Edit #6 overall on LMArena, best open model image editor</title>
    <updated>2025-08-20T17:17:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt; &lt;img alt="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" src="https://preview.redd.it/90yj5wnyj7kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c77694f269b30f417ff8568342a83f0ba81a1ec2" title="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Surprised they didn't vote this one higher, I felt like the edits I saw Qwen make online were pretty good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90yj5wnyj7kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T17:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvn50l</id>
    <title>Guys it's official, the nano banana model on lm arena is Google's</title>
    <updated>2025-08-20T18:32:13+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OfficialLoganK/status/1957908528925909391"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvfdja</id>
    <title>IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit</title>
    <updated>2025-08-20T13:51:04+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt; &lt;img alt="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" src="https://preview.redd.it/moddapg5j6kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6316f1a44b3898869b91f28f4d1774a35db2491" title="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Solar storms don’t just make pretty auroras—they can scramble GPS, disrupt flights, degrade satellite comms, and stress power grids. To get ahead of that, IBM and NASA have open‑sourced Surya on Hugging Face: a foundation model trained on years of Solar Dynamics Observatory (SDO) data to make space‑weather forecasting more accurate and accessible.&lt;/p&gt; &lt;p&gt;What Surya is&lt;/p&gt; &lt;p&gt;A mid‑size foundation model for heliophysics that learns general “features of the Sun” from large SDO image archives.&lt;/p&gt; &lt;p&gt;Built to support zero/few‑shot tasks like flare probability, CME risk, and geomagnetic indices (e.g., Kp/Dst) with fine‑tuning.&lt;/p&gt; &lt;p&gt;Released with open weights and recipes so labs, universities, and startups can adapt it without massive compute.&lt;/p&gt; &lt;p&gt;Why this matters&lt;/p&gt; &lt;p&gt;Early, reliable alerts help airlines reroute, satellite operators safe‑mode hardware, and grid operators harden the network before a hit.&lt;/p&gt; &lt;p&gt;Open sourcing lowers the barrier for regional forecasters and fosters reproducible science (shared baselines, comparable benchmarks).&lt;/p&gt; &lt;p&gt;We’re in an active solar cycle—better lead times now can prevent expensive outages and service disruptions.&lt;/p&gt; &lt;p&gt;How to try it (technical)&lt;/p&gt; &lt;p&gt;Pull the model from Hugging Face and fine‑tune on your target label: flare class prediction, Kp nowcasting, or satellite anomaly detection.&lt;/p&gt; &lt;p&gt;Start with SDO preprocessing pipelines; add lightweight adapters/LoRA for event‑specific fine‑tuning to keep compute modest.&lt;/p&gt; &lt;p&gt;Evaluate on public benchmarks (Kp/Dst) and report lead time vs. skill scores; stress test on extreme events.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moddapg5j6kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjj8q</id>
    <title>Seed-OSS-36B-Instruct</title>
    <updated>2025-08-20T16:23:50+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Introduction:&lt;/p&gt; &lt;p&gt;Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.&lt;/p&gt; &lt;p&gt;We release this series of models to the open-source community under the Apache-2.0 license.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Flexible Control of Thinking Budget&lt;/strong&gt;: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Reasoning Capability&lt;/strong&gt;: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Intelligence&lt;/strong&gt;: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Friendly&lt;/strong&gt;: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: Trained with up-to-512K long context natively.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2lme</id>
    <title>Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets</title>
    <updated>2025-08-21T05:44:41+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt; &lt;img alt="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" src="https://b.thumbs.redditmedia.com/qEAZdKzAv0zTNBneqhh9EDw7ENWMaMbCmlZSPzIj5Hw.jpg" title="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw2lme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvy6ai</id>
    <title>Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement.</title>
    <updated>2025-08-21T01:57:14+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt; &lt;img alt="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." src="https://a.thumbs.redditmedia.com/QpddyvC1oI2y7Ei80TMeqG06SbkjYBp8kDygMRpJtF8.jpg" title="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvy6ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T01:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3kmd</id>
    <title>DeepSeek-V3.1 (Thinking and Non Thinking)</title>
    <updated>2025-08-21T06:43:19+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt; &lt;img alt="DeepSeek-V3.1 (Thinking and Non Thinking)" src="https://preview.redd.it/131ngchkjbkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c84b80fbda47f246b1ba5d8d04285fb722a69588" title="DeepSeek-V3.1 (Thinking and Non Thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid thinking mode&lt;/strong&gt;: One model supports both thinking mode and non-thinking mode by changing the chat template.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smarter tool calling&lt;/strong&gt;: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Higher thinking efficiency&lt;/strong&gt;: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Benchmark (Metric)&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-NonThinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3 0324&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-Thinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek R1 0528&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;General&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Redux (EM)&lt;/td&gt; &lt;td align="left"&gt;91.8&lt;/td&gt; &lt;td align="left"&gt;90.5&lt;/td&gt; &lt;td align="left"&gt;93.7&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Pro (EM)&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;85.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;74.9&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;80.1&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;15.9&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Search Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.0&lt;/td&gt; &lt;td align="left"&gt;8.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp_zh&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;49.2&lt;/td&gt; &lt;td align="left"&gt;35.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Python + Search)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;29.8&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SimpleQA&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;92.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;LiveCodeBench (2408-2505) (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;56.4&lt;/td&gt; &lt;td align="left"&gt;43.0&lt;/td&gt; &lt;td align="left"&gt;74.8&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Codeforces-Div1 (Rating)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;2091&lt;/td&gt; &lt;td align="left"&gt;1930&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Aider-Polyglot (Acc.)&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;55.1&lt;/td&gt; &lt;td align="left"&gt;76.3&lt;/td&gt; &lt;td align="left"&gt;71.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE Verified (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE-bench Multilingual (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;29.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Terminal-bench (Terminus 1 framework)&lt;/td&gt; &lt;td align="left"&gt;31.3&lt;/td&gt; &lt;td align="left"&gt;13.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;5.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2024 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;td align="left"&gt;93.1&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;49.8&lt;/td&gt; &lt;td align="left"&gt;51.3&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;87.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;HMMT 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;33.5&lt;/td&gt; &lt;td align="left"&gt;29.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;79.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/131ngchkjbkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3nat</id>
    <title>DeepSeek-V3.1 implements Anthropic API compatibility</title>
    <updated>2025-08-21T06:47:55+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt; &lt;img alt="DeepSeek-V3.1 implements Anthropic API compatibility" src="https://preview.redd.it/0pp8mwjkkbkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e7a0529d636accee5763e0a807e41d636629b2" title="DeepSeek-V3.1 implements Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/guides/anthropic_api"&gt;https://api-docs.deepseek.com/guides/anthropic_api&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pp8mwjkkbkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvw3hz</id>
    <title>NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model</title>
    <updated>2025-08-21T00:20:54+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt; &lt;img alt="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" src="https://b.thumbs.redditmedia.com/e1iw4XluNhKx2_uQFrUBfjQ-KXChA-T80tHk2Ay_3VI.jpg" title="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvw3hz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T00:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw0tc4</id>
    <title>Finally Kimi-VL-A3B-Thinking-2506-GGUF is available</title>
    <updated>2025-08-21T04:06:14+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt; &lt;img alt="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" src="https://external-preview.redd.it/m2lF_KqN7wgwWcFm1a3lN4x_joA1P4xA8L66Y-aVPXM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e8aa6a2602eb7ba21a6af8e92b4d296f3365548" title="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original model: &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506"&gt;https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supported added in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;https://github.com/ggml-org/llama.cpp/pull/15458&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T04:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3c7s</id>
    <title>deepseek-ai/DeepSeek-V3.1 · Hugging Face</title>
    <updated>2025-08-21T06:28:56+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1 · Hugging Face" src="https://external-preview.redd.it/RJXEgvNDm4zhSkGlks1Mt4ppnLOAENNDWYNaVwpLE9k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d409de545b1a58fad7e22e741370f1a55018f432" title="deepseek-ai/DeepSeek-V3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve posted on here a couple times sharing my project. I'm training LLM’s from scratch on 1800’s London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I’ve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here’s the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up “Lord Palmerston” and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I’m not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I’m just doing this for fun so if anyone would like to collaborate let me know, I’m open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
