<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-25T00:56:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ljhg1i</id>
    <title>I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other.</title>
    <updated>2025-06-24T17:33:15+00:00</updated>
    <author>
      <name>/u/CharlesStross</name>
      <uri>https://old.reddit.com/user/CharlesStross</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt; &lt;img alt="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." src="https://external-preview.redd.it/rna5zREg5_FzFmMGv-Mzfn4pHDOOgy6GUqSdq0vIQVE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61a501a09debdd7a58e2f8b7a92cf6aad5a73838" title="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharlesStross"&gt; /u/CharlesStross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jkingsman/resonant-chat-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T17:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlyrs</id>
    <title>Is it normal to have significantly more performance from Qwen 235B compared to Qwen 32B when doing partial offloading?</title>
    <updated>2025-06-24T20:24:47+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded than for the entire 32B model.here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded to the cpu than for the entire 32B model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; &amp;quot;Qwen3:32B&amp;quot;: proxy: http://127.0.0.1:9995 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9995 --no-webui --no-warmup --model ~/raid/models/Qwen3-32B-Q4_K_M.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 34 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 &amp;quot;Qwen3:235B&amp;quot;: proxy: http://127.0.0.1:9993 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9993 --no-webui --no-warmup --model ~/raid/models/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 95 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 --override-tensor exps=CPU &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljrcq6</id>
    <title>Qwen3 vs phi4 vs gemma3 vs deepseek r1 or deepseek v3 vs llama 3 or llama 4</title>
    <updated>2025-06-25T00:11:31+00:00</updated>
    <author>
      <name>/u/Divkix</name>
      <uri>https://old.reddit.com/user/Divkix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model do you use where? As in what case does one solve that other isn’t able to do? I’m diving into local llm after using openai, gemini and claude. If I had to make ai agents which model would fit which use case? Llama 4, qwen3 (both dense and moe) and deepseek v3/r1 are moe and others are dense I guess? I would use openrouter for the inference so how would each model define their cost? Best use case for each model.&lt;/p&gt; &lt;p&gt;Edit: forgot to mention I asked this in &lt;a href="/r/localllm"&gt;r/localllm&lt;/a&gt; as well bc I couldn’t post it here yesterday, hope more people here can give their input.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Divkix"&gt; /u/Divkix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrcq6/qwen3_vs_phi4_vs_gemma3_vs_deepseek_r1_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrcq6/qwen3_vs_phi4_vs_gemma3_vs_deepseek_r1_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrcq6/qwen3_vs_phi4_vs_gemma3_vs_deepseek_r1_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljoyvm</id>
    <title>What local clients do you use?</title>
    <updated>2025-06-24T22:26:08+00:00</updated>
    <author>
      <name>/u/PotatoHD404</name>
      <uri>https://old.reddit.com/user/PotatoHD404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a local client for llms embeddings and rerankers, possibly rag. But I doubt that it will be used by someone else than me. I was going to make something like lm studio but opensource. Upon deeper research I found many alternatives like jan ai or anythingllm. Do you think that my app will be used by anyone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotatoHD404"&gt; /u/PotatoHD404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoyvm/what_local_clients_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoyvm/what_local_clients_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoyvm/what_local_clients_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs4e7</id>
    <title>All of our posts for the last week:</title>
    <updated>2025-06-25T00:47:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt; &lt;img alt="All of our posts for the last week:" src="https://preview.redd.it/0feqhgvc0z8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb98aa33f9a72ba846bb3609af050401518880f2" title="All of our posts for the last week:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0feqhgvc0z8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljohcu</id>
    <title>Will I be happy with a RTX 3090?</title>
    <updated>2025-06-24T22:05:47+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before making a big purchase, I would be grateful for some advice from the experts here! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want to do:&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Enhanced web search (for example using &lt;a href="https://github.com/ItzCrazyKns/Perplexica"&gt;perplexica&lt;/a&gt;) - it seems you can achieve decent results with smaller models. Being able to get summaries of &amp;quot;todays news&amp;quot; or just generally using it as an alternative to google searching. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Generating images (stable diffusion / Flux) - nothing too fancy here, just playing around for fun. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Simple coding assistance, looking up javascript syntax etc. Ideally with a VS code or command line extension. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What I am not so interested in:&lt;/strong&gt; - Random chatting with the model, storytelling etc - Getting &amp;quot;facts&amp;quot; from the model weights directly, they seem to often be wrong, and always more or less outdated. - Code generation / &amp;quot;vibe coding&amp;quot; - it is more fun to write code myself =) &lt;/p&gt; &lt;p&gt;Currently I am using an GTX 1070Ti with 8GB of VRAM and small models such as llama3.2 and gemma3:4b. With this setup web search is not working very well, it can do some things, but cannot fetch todays news for example. Image generation is simply awful. &lt;/p&gt; &lt;p&gt;I realise that using a commercial model will be better and cheaper, but I want to do this locally because it is fun =). Ideally I would like to achieve results that are good enough to be competitive/acceptable compared to the commercial cloud models for my use cases (excluding image generation).&lt;/p&gt; &lt;p&gt;Will I be happy with an RTX 3090 with 24GB? Which models should I aim for in that case? Or are there other cards you would suggest? Thank you very much in advance! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn09w</id>
    <title>I made a free iOS app for people who run LLMs locally. It’s a chatbot that you can use away from home to interact with an LLM that runs locally on your desktop Mac.</title>
    <updated>2025-06-24T21:05:30+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is easy enough that anyone can use it. No tunnel or port forwarding needed.&lt;/p&gt; &lt;p&gt;The app is called LLM Pigeon and has a companion app called LLM Pigeon Server for Mac.&lt;br /&gt; It works like a carrier pigeon :). It uses iCloud to append each prompt and response to a file on iCloud.&lt;br /&gt; It’s not totally local because iCloud is involved, but I trust iCloud with all my files anyway (most people do) and I don’t trust AI companies. &lt;/p&gt; &lt;p&gt;The iOS app is a simple Chatbot app. The MacOS app is a simple bridge to LMStudio or Ollama. Just insert the model name you are running on LMStudio or Ollama and it’s ready to go.&lt;br /&gt; I also added 5 in-built models so even people who are not familiar with Ollama or LMStudio can use this.&lt;/p&gt; &lt;p&gt;I find it super cool that I can chat anywhere with Qwen3-30B running on my Mac at home. &lt;/p&gt; &lt;p&gt;The apps are open source and these are the repos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They are both on the App Store. Here are the links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS. I hope this isn't viewed as self promotion because the app is free, collects no data and is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr12h</id>
    <title>[Gamers Nexus] NVIDIA RTX PRO 6000 Blackwell Benchmarks &amp; Tear-Down | Thermals, Gaming, LLM, &amp; Acoustic Tests</title>
    <updated>2025-06-24T23:56:11+00:00</updated>
    <author>
      <name>/u/asssuber</name>
      <uri>https://old.reddit.com/user/asssuber</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr12h/gamers_nexus_nvidia_rtx_pro_6000_blackwell/"&gt; &lt;img alt="[Gamers Nexus] NVIDIA RTX PRO 6000 Blackwell Benchmarks &amp;amp; Tear-Down | Thermals, Gaming, LLM, &amp;amp; Acoustic Tests" src="https://external-preview.redd.it/83pqnDabbeW2W87zR8vNGBLxz05MxlwFmCGTIDvEn_8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=482351d1c5c19f1ce8e6b9e835d8bd89ca6a055b" title="[Gamers Nexus] NVIDIA RTX PRO 6000 Blackwell Benchmarks &amp;amp; Tear-Down | Thermals, Gaming, LLM, &amp;amp; Acoustic Tests" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asssuber"&gt; /u/asssuber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ZCvjw8B6rcg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr12h/gamers_nexus_nvidia_rtx_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr12h/gamers_nexus_nvidia_rtx_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:56:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljrqvy</id>
    <title>Faster local inference?</title>
    <updated>2025-06-25T00:29:41+00:00</updated>
    <author>
      <name>/u/badatreality</name>
      <uri>https://old.reddit.com/user/badatreality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am curious to hear folks perspective on the speed they get when running models locally. I've tried on a Mac (with llama.cpp, ollama, and mlx) as well as on an AMD card on a PC. But while I can see various benefits to running models locally, I also at times want the response speed that only seems possible when using a cloud service. I'm not sure if there's things I could be doing to get faster response times locally (e.g., could I keep a model running permanently and warmed up, like it's cached?), but anything to approximate the responsiveness of chatgpt would be amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badatreality"&gt; /u/badatreality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrqvy/faster_local_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrqvy/faster_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrqvy/faster_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljogsx</id>
    <title>LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs</title>
    <updated>2025-06-24T22:05:08+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt; &lt;img alt="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" src="https://external-preview.redd.it/ZSkXOQ0Ftmzf9m07Ydba1-71lECRPh1WZMhCFovef6Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fdb319a25ca00eba0456ee1f02c9bf5308cdb5e" title="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought it might be fun for the community to see one of the largest tech YouTubers introducing their audience to local LLMs.&lt;/p&gt; &lt;p&gt;Lots of newbie mistakes in their messing with Open WebUI and Ollama but hopefully it encourages some of their audience to learn more. For anyone who saw the video and found their way here, welcome! Feel free to ask questions about getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HZgQp-WDebU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm32s</id>
    <title>Agent Arena – crowdsourced testbed for evaluating AI agents in the wild</title>
    <updated>2025-06-24T20:29:27+00:00</updated>
    <author>
      <name>/u/tejpal-obl</name>
      <uri>https://old.reddit.com/user/tejpal-obl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched Agent Arena -- a crowdsourced testbed for evaluating AI agents in the wild. Think Chatbot Arena, but for agents.&lt;/p&gt; &lt;p&gt;It’s completely free to run matches. We cover the inference.&lt;/p&gt; &lt;p&gt;I always find myself debating whether to use 4o or o3, but now I just try both on Agent Arena!&lt;/p&gt; &lt;p&gt;Try it out: &lt;a href="https://obl.dev/"&gt;https://obl.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tejpal-obl"&gt; /u/tejpal-obl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljrwrq</id>
    <title>Does anyone else find Dots really impressive?</title>
    <updated>2025-06-25T00:37:22+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Dots and I find it really impressive. It's my current favorite model. It's knowledgeable, uncensored and has a bit of attitude. Its uncensored in that it will not only talk about TS, it will do so in great depth. If you push it about something, it'll show some attitude by being sarcastic. I like that. It's more human.&lt;/p&gt; &lt;p&gt;The only thing that baffles me about Dots is since it was trained on Rednote, why does it speak English so well? Rednote is in Chinese.&lt;/p&gt; &lt;p&gt;What do others think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljqgxb</id>
    <title>Best tts and stt open source or cheap - NOT real time?</title>
    <updated>2025-06-24T23:30:48+00:00</updated>
    <author>
      <name>/u/dabble_</name>
      <uri>https://old.reddit.com/user/dabble_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing a lot of realtime qna when I was browsing and searching the sub, what about not real time? Ideally not insanely slow but I have no need for anything close to real time so higher quality audio would be preferred.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabble_"&gt; /u/dabble_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm13j</id>
    <title>0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?</title>
    <updated>2025-06-24T20:27:16+00:00</updated>
    <author>
      <name>/u/BasicCoconut9187</name>
      <uri>https://old.reddit.com/user/BasicCoconut9187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt; &lt;img alt="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" src="https://b.thumbs.redditmedia.com/YwYzRHAaFEgB8IqsgHGymFQqUiU6aiL80kikS_RSFhM.jpg" title="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/bgzyj1lypx8f1.gif"&gt;Now I've got your attention, I hope!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi there everyone!&lt;/p&gt; &lt;p&gt;I've just recently assembled an entire home server system, however, for some reason, the performance I'm getting is atrocious with 1TB of DDR4 2400MHz RAM on EPYC 7C13 running on Gigabyte MZ32-AR1. I'm getting 1-3 tok/s on prompt eval (depending on context), and 0.3-0.6 tok/s generation.&lt;/p&gt; &lt;p&gt;Now, the model I'm running is Ubergarm's R1 0528 IQ4_KS_R4, on ik_llama, so that's a bit different than what a lot of people here are running. However, on the more 'standard' R1 GGUFs from Unsloth, the performance is even worse, and that's true across everything I've tried, Kobold.cpp, LMstudio, Ollama, etc. True of other LLMs as well such as Qwen, people report way better tok/s with the same/almost the same CPU and system.&lt;/p&gt; &lt;p&gt;So, here's my request, if anyone is in the know, can you please share the BIOS options that I should use to optimize this CPU for LLM interference? I'm ready to sacrifice pretty much any setting/feature if that means I will be able to get this running in line with what other people online are getting.&lt;/p&gt; &lt;p&gt;Also, I know what you think, the model is entirely mlock'ed and is using 128 threads, my OS is Ubuntu 25.04, and other than Ubuntu's tendency to set locked memory to just 128 or so gigs every time I reboot which can be simply fixed with sudo su and then ulimit -Hl and -l, I don't seem to have any issues on the OS side, so that's where my entire guess of this being the BIOS settings fault comes from.&lt;/p&gt; &lt;p&gt;Thank you so much for reading all of this, and have a great day!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BasicCoconut9187"&gt; /u/BasicCoconut9187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn4h8</id>
    <title>Why is my llama so dumb?</title>
    <updated>2025-06-24T21:10:06+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: DeepSeek R1 Distill Llama 70B&lt;/p&gt; &lt;p&gt;GPU+Hardware: Vulkan on AMD AI Max+ 395 128GB VRAM &lt;/p&gt; &lt;p&gt;Program+Options:&lt;br /&gt; - GPU Offload Max&lt;br /&gt; - CPU Thread Pool Size 16&lt;br /&gt; - Offload KV Cache: Yes&lt;br /&gt; - Keep Model in Memory: Yes&lt;br /&gt; - Try mmap(): Yes&lt;br /&gt; - K Cache Quantization Type: Q4_0 &lt;/p&gt; &lt;p&gt;So the question is, when asking basic questions, it consistently gets the answer wrong. And does a whole lot of that &amp;quot;thinking&amp;quot;: &lt;/p&gt; &lt;p&gt;&amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Okay so i'm trying to understand&amp;quot;&lt;br /&gt; etc&lt;br /&gt; etc. &lt;/p&gt; &lt;p&gt;I'm not complaining about speed. More that the accuracy for something as basic as &amp;quot;explain this common linux command&amp;quot; and it is super wordy and then ultimately comes to the wrong conclusion. &lt;/p&gt; &lt;p&gt;I'm using LM Studio btw. &lt;/p&gt; &lt;p&gt;Is there a good primer for setting these LLMs up for success? What do you recommend? Have I done something stupid myself?&lt;br /&gt; Thanks in advance for any help/suggestions! &lt;/p&gt; &lt;p&gt;p.s. I do plan on running and testing ROCm, but i've only got so much time in a day and i'm a newbie to the LLM space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnoj7</id>
    <title>AMD Instinct MI60 (32gb VRAM) "llama bench" results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected</title>
    <updated>2025-06-24T21:32:35+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt; &lt;img alt="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" src="https://b.thumbs.redditmedia.com/IWP60MgSnWzXR7H6EGZicI90kN9NpZLeyDsSansVnlA.jpg" title="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just completed a new build and (finally) have everything running as I wanted it to when I spec'd out the build. I'll be making a separate post about that as I'm now my own sovereign nation state for media, home automation (including voice activated commands), security cameras and local AI which I'm thrilled about...but, like I said, that's for a separate post.&lt;/p&gt; &lt;p&gt;This one is with regard to the MI60 GPU which I'm very happy with given my use case. I bought two of them on eBay, got one for right around $300 and the other for just shy of $500. Turns out I only need one as I can fit both of the models I'm using (one for HomeAssistant and the other for Frigate security camera feed processing) onto the same GPU with more than acceptable results. I might keep the second one for other models, but for the time being it's not installed. &lt;strong&gt;EDIT:&lt;/strong&gt; Forgot to mention I'm running Ubuntu 24.04 on the server.&lt;/p&gt; &lt;p&gt;For HomeAssistant I get results back in less than two seconds for voice activated commands like &amp;quot;it's a little dark in the living room and the cats are meowing at me because they're hungry&amp;quot; (it brightens the lights and feeds the cats, obviously). For Frigate it takes about 10 seconds after a camera has noticed an object of interest to return back what was observed (here is a copy/paste of an example of data returned from one of my camera feeds: &amp;quot;&lt;em&gt;Person detected. The person is a man wearing a black sleeveless top and red shorts. He is standing on the deck holding a drink. Given their casual demeanor this does not appear to be suspicious.&lt;/em&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Notes about the setup for the GPU, for some reason I'm unable to get the powercap set to anything higher than 225w (I've got a 1000w PSU, I've tried the physical switch on the card, I've looked for different vbios versions for the card and can't locate any...it's frustrating, but is what it is...it's supposed to be a 300tdp card). I was able to slightly increase it because while it won't allow me to change the powercap to anything higher, I was able to set the &amp;quot;overdrive&amp;quot; to allow for a 20% increase. With the cooling shroud for the GPU (photo at bottom of post) even at full bore, the GPU has never gone over 64 degrees Celsius&lt;/p&gt; &lt;p&gt;Here are some &amp;quot;llama-bench&amp;quot; results of various models that I was testing before settling on the two I'm using (noted below):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | pp512 | 581.33 ± 0.16 | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | tg128 | 64.82 ± 0.04 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | pp512 | 587.76 ± 1.04 | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | tg128 | 43.50 ± 0.18 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Hermes-3-Llama-3.1-8B.Q8_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Hermes-3-Llama-3.1-8B.Q8_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | pp512 | 582.56 ± 0.62 | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | tg128 | 52.94 ± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Meta-Llama-3-8B-Instruct.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Meta-Llama-3-8B-Instruct.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | pp512 | 1214.07 ± 1.93 | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | tg128 | 70.56 ± 0.12 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | pp512 | 420.61 ± 0.18 | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | tg128 | 31.03 ± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | pp512 | 188.13 ± 0.03 | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | tg128 | 27.37 ± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | pp512 | 257.37 ± 0.04 | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | tg128 | 17.65 ± 0.02 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;nexusraven-v2-13b.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/nexusraven-v2-13b.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | pp512 | 704.18 ± 0.29 | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | tg128 | 52.75 ± 0.07 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-30B-A3B-Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | pp512 | 1165.52 ± 4.04 | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | tg128 | 68.26 ± 0.13 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_1.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-32B-Q4_1.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | pp512 | 270.18 ± 0.14 | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | tg128 | 21.59 ± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a photo of the build for anyone interested (i9-14900k, 96gb RAM, total of 11 drives, a mix of NVME, HDD and SSD):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0"&gt;https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm2n2</id>
    <title>Polaris: A Post-training recipe for scaling RL on Advanced ReasonIng models</title>
    <updated>2025-06-24T20:28:55+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ChenxinAn-fdu/POLARIS"&gt;Here is the link.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have no idea what it is but it was released a few days ago and has an intriguing concept so I decided to post here to see if anyone knows about this. It seems pretty new but its some sort of post-training RL with a unique approach that claims a Qwen3-4b performance boost that surpasses Claude-4-Opus, Grok-3-Beta, and o3-mini-high.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt. I am not in any way affiliated with this project. Someone simply recommended it to me so I posted it here to gather your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr1wn</id>
    <title>Where is OpenAI's open source model?</title>
    <updated>2025-06-24T23:57:16+00:00</updated>
    <author>
      <name>/u/_Vedr</name>
      <uri>https://old.reddit.com/user/_Vedr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Vedr"&gt; /u/_Vedr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljpo64</id>
    <title>I gave the same silly task to ~70 models that fit on 32GB of VRAM - thousands of times (resharing my post from /r/LocalLLM)</title>
    <updated>2025-06-24T22:56:20+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd posted this over at &lt;a href="/r/LocalLLM"&gt;/r/LocalLLM&lt;/a&gt; and Some people thought I presented this too much as serious research - it wasn't, it was much closer to a bored rainy day activity. So here's the post I've been waiting to make on &lt;a href="/r/LocalLLaMA"&gt;/r/LocalLLaMA&lt;/a&gt; for some time, simplified as casually as possible:&lt;/p&gt; &lt;p&gt;Quick recap - &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;here is the original post&lt;/a&gt; from a few weeks ago where users suggested I greatly expand the scope of this little game. &lt;a href="https://old.reddit.com/r/LocalLLM/comments/1liy7ku/i_thousands_of_tests_on_104_different_ggufs_10k/"&gt;Here is the post on /r/LocalLLM&lt;/a&gt; yesterday that I imagine some of you saw. I hope you don't mind the cross-post - but &lt;em&gt;THIS&lt;/em&gt; is the subreddit that I really wanted to bounce this off of and yesterday it was going through a change-of-management :-)&lt;/p&gt; &lt;p&gt;To be as brief/casual as possible: I broke HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; again with a sentence that was correct English, but contextually nonsense, and asked a bunch of quantized LLM's (all that fit with 16k context on 32GB of VRAM). I did this multiple times at all temperatures from 0.0 to 0.9 in steps of 0.1 . For models with optional reasoning I split thinking mode on and off.&lt;/p&gt; &lt;h2&gt;What should you take from this?&lt;/h2&gt; &lt;p&gt;nothing at all! I'm hoping to get a better feel for how quantization works on some of my favorite models, so will take a little thing I do during my day and repeat it thousands and thousands of times to see if patterns emerge. I share this dataset with you for fun. I have my takeaways, I'd be interested to hear yours. My biggest takeaway from this is that I built a little framework of scripts for myself that will run and evaluate these sorts of tests at whatever scale I set them to.&lt;/p&gt; &lt;h2&gt;The Results&lt;/h2&gt; &lt;p&gt;Without further ado, the results. The 'Score' column is a percentage of correct answers.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Reasoning&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia Nemotron Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Microsoft Phi Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Alibaba Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Gemma Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Deepseek (Distill) Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo4ns</id>
    <title>New Moondream 2B VLM update, with visual reasoning</title>
    <updated>2025-06-24T21:51:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-06-21-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljp29d</id>
    <title>So, what do people think about the new Mistral Small 3.2?</title>
    <updated>2025-06-24T22:30:10+00:00</updated>
    <author>
      <name>/u/TacticalRock</name>
      <uri>https://old.reddit.com/user/TacticalRock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering why the sub was so quiet lately, but alas, what're your thoughts so far?&lt;/p&gt; &lt;p&gt;I for one welcome the decreased repetition, solid &amp;quot;minor&amp;quot; update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacticalRock"&gt; /u/TacticalRock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnmj9</id>
    <title>Google researcher requesting feedback on the next Gemma.</title>
    <updated>2025-06-24T21:30:18+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt; &lt;img alt="Google researcher requesting feedback on the next Gemma." src="https://a.thumbs.redditmedia.com/YXztzxUAkpa8OQtPRt3lxinca8NVcah5DIxz1ZPOgn4.jpg" title="Google researcher requesting feedback on the next Gemma." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kr52i2mn0y8f1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f654b4d8fc807a8722055201e8c097168452937f"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/osanseviero/status/1937453755261243600"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm gpu poor. 8-12B models are perfect for me. What are yout thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
</feed>
