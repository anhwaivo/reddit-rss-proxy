<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-16T08:51:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k06wr7</id>
    <title>How would you unit-test LLM outputs?</title>
    <updated>2025-04-15T23:59:42+00:00</updated>
    <author>
      <name>/u/Blender-Fan</name>
      <uri>https://old.reddit.com/user/Blender-Fan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have this api where in one of the endpoints's requests has an LLM input field and so does the response&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;llm_input&amp;quot;: &amp;quot;pigs do fly&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;datetime&amp;quot;: &amp;quot;2025-04-15T12:00:00Z&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;llm_output&amp;quot;: &amp;quot;unicorns are real&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;datetime&amp;quot;: &amp;quot;2025-04-15T12:00:01Z&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;model&amp;quot;: &amp;quot;gpt-4&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My API validates stuff like if the datetime (must not be older than datetime.now), but &lt;strong&gt;how the fuck do i validate an llm's output?&lt;/strong&gt; The example is of course exagerated, but if the llm says something logically wrong like &amp;quot;2+2=5&amp;quot; or &amp;quot;It is possible the sun goes supernova this year&amp;quot;, how do we unit-test that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blender-Fan"&gt; /u/Blender-Fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k06wr7/how_would_you_unittest_llm_outputs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k06wr7/how_would_you_unittest_llm_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k06wr7/how_would_you_unittest_llm_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T23:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0eivw</id>
    <title>How to get 9070 working to run LLMs on Windows</title>
    <updated>2025-04-16T07:21:07+00:00</updated>
    <author>
      <name>/u/Semi_Tech</name>
      <uri>https://old.reddit.com/user/Semi_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First thanks to u/&lt;a href="https://www.reddit.com/user/DegenerativePoop/"&gt;DegenerativePoop&lt;/a&gt; for finding this and to the entire team that made it possible to get AIs running on this card.&lt;/p&gt; &lt;p&gt;Step by step instructions on how to get this running:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download exe for Ollama for AMD from &lt;a href="https://github.com/likelovewant/ollama-for-amd/releases"&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Install it&lt;/li&gt; &lt;li&gt;Download the &amp;quot;rocm.gfx1201.for.hip.skd.6.2.4-no-optimized.7z&amp;quot; archive from &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU/releases/tag/v0.6.2.4"&gt;here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Go to %appdata% -&amp;gt; &lt;code&gt;C:\Users\usrname\AppData\Local\Programs\Ollama\lib\ollama\rocm&lt;/code&gt;&lt;/li&gt; &lt;li&gt;From the archive copy/paste and REPLACE the rocblas dll file&lt;/li&gt; &lt;li&gt;Go in the rocblas folder and DELETE the library folder&lt;/li&gt; &lt;li&gt;From the archive copy/paste the library folder where the old one was&lt;/li&gt; &lt;li&gt;Done&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can now do &lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run gemma3:12b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And you will have it running GPU accelerated.&lt;/p&gt; &lt;p&gt;I am getting about 15 tokens/s for gemma3 12B which is better than running it on CPU+RAM&lt;/p&gt; &lt;p&gt;You can then use whichever front end you want with Ollama as the server.&lt;/p&gt; &lt;p&gt;The easiest one I was able to get up and running is sillytavern&lt;/p&gt; &lt;p&gt;Installation took 2 minutes for those that don't want to fiddle with stuff too much.&lt;/p&gt; &lt;p&gt;Very easy installation &lt;a href="https://sillytavernai.com/how-to-install-sillytavern/"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Semi_Tech"&gt; /u/Semi_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0eivw/how_to_get_9070_working_to_run_llms_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0eivw/how_to_get_9070_working_to_run_llms_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0eivw/how_to_get_9070_working_to_run_llms_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T07:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzrc5r</id>
    <title>Mistral Libraries!</title>
    <updated>2025-04-15T13:02:30+00:00</updated>
    <author>
      <name>/u/SufficientRadio</name>
      <uri>https://old.reddit.com/user/SufficientRadio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzrc5r/mistral_libraries/"&gt; &lt;img alt="Mistral Libraries!" src="https://preview.redd.it/r7ae07pgxzue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c05f90fdbf3aee13a62dcd1f96854260c11d304" title="Mistral Libraries!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current support for PDF, DOCX, PPTX, CSV, TXT, MD, XLSX&lt;/p&gt; &lt;p&gt;Up to 100 files, 100MB per file&lt;/p&gt; &lt;p&gt;Waiting on the official announcement...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SufficientRadio"&gt; /u/SufficientRadio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r7ae07pgxzue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzrc5r/mistral_libraries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzrc5r/mistral_libraries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T13:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzk8nu</id>
    <title>So OpenAI released nothing open source today?</title>
    <updated>2025-04-15T05:36:59+00:00</updated>
    <author>
      <name>/u/DamiaHeavyIndustries</name>
      <uri>https://old.reddit.com/user/DamiaHeavyIndustries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Except that benchmarking tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DamiaHeavyIndustries"&gt; /u/DamiaHeavyIndustries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T05:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzncvp</id>
    <title>It's good to download a small open local model, what can go wrong?</title>
    <updated>2025-04-15T09:11:56+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzncvp/its_good_to_download_a_small_open_local_model/"&gt; &lt;img alt="It's good to download a small open local model, what can go wrong?" src="https://preview.redd.it/tbm102jesyue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5522d29ded8ded62ddd3b3dc760eaaebe2b5bb5f" title="It's good to download a small open local model, what can go wrong?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tbm102jesyue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzncvp/its_good_to_download_a_small_open_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzncvp/its_good_to_download_a_small_open_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T09:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzyeak</id>
    <title>VL-Rethinker, Open Weight SOTA 72B VLM that surpasses o1</title>
    <updated>2025-04-15T17:54:18+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzyeak/vlrethinker_open_weight_sota_72b_vlm_that/"&gt; &lt;img alt="VL-Rethinker, Open Weight SOTA 72B VLM that surpasses o1" src="https://a.thumbs.redditmedia.com/G0WPCu_gVOmV5_gOYDmaCvnruLn6hhKv2WVUS_IA4Q0.jpg" title="VL-Rethinker, Open Weight SOTA 72B VLM that surpasses o1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/isob2eoff1ve1.jpg?width=3526&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=be8dc026e4ce5e2b7a5f8330fcbc57362fab3b1f"&gt;https://preview.redd.it/isob2eoff1ve1.jpg?width=3526&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=be8dc026e4ce5e2b7a5f8330fcbc57362fab3b1f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzvax96gf1ve1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c14765a7181fcbbb27d82490f1901f2d66b9461"&gt;https://preview.redd.it/jzvax96gf1ve1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c14765a7181fcbbb27d82490f1901f2d66b9461&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzyeak/vlrethinker_open_weight_sota_72b_vlm_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzyeak/vlrethinker_open_weight_sota_72b_vlm_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzyeak/vlrethinker_open_weight_sota_72b_vlm_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T17:54:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0f7zf</id>
    <title>Creating Llama3.2 function definition JSON</title>
    <updated>2025-04-16T08:12:38+00:00</updated>
    <author>
      <name>/u/Pacyfist01</name>
      <uri>https://old.reddit.com/user/Pacyfist01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to write some code that connects SematnicKernel to the smallest Llama3.2 network possible. I want my simple agent to be able to run on just 1.2GB vRAM. I have a problem understanding how the function definition JSON is created. In the Llama3.2 docs there is a detailed example. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-prompt-template-"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-prompt-template-&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;name&amp;quot;: &amp;quot;get_user_info&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;dict&amp;quot;, &amp;quot;required&amp;quot;: [ &amp;quot;user_id&amp;quot; ], &amp;quot;properties&amp;quot;: { &amp;quot;user_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;The unique identifier of the user. It is used to fetch the specific user details from the database.&amp;quot; }, &amp;quot;special&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Any special information or parameters that need to be considered while fetching user details.&amp;quot;, &amp;quot;default&amp;quot;: &amp;quot;none&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anyone know what library generates JSON this way?&lt;br /&gt; I don't want to reinvent the wheel.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pacyfist01"&gt; /u/Pacyfist01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0f7zf/creating_llama32_function_definition_json/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0f7zf/creating_llama32_function_definition_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0f7zf/creating_llama32_function_definition_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T08:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzxpzx</id>
    <title>Ragie on “RAG is Dead”: What the Critics Are Getting Wrong… Again</title>
    <updated>2025-04-15T17:27:05+00:00</updated>
    <author>
      <name>/u/bob_at_ragie</name>
      <uri>https://old.reddit.com/user/bob_at_ragie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;With the release of Llama 4 Scout and its 10 million token context window, the “RAG is dead” critics have started up again, but I think they're missing the point.&lt;/p&gt; &lt;p&gt;RAG isn’t dead... long context windows enable exciting new possibilities but they complement RAG rather than replace it. I went deep and wrote a blog post the latency, cost and accuracy tradeoffs of stuffing tokens in context vs using RAG because I've been getting questions from friends and colleagues about the subject.&lt;/p&gt; &lt;p&gt;I would love to get your thoughts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ragie.ai/blog/ragie-on-rag-is-dead-what-the-critics-are-getting-wrong-again"&gt;https://www.ragie.ai/blog/ragie-on-rag-is-dead-what-the-critics-are-getting-wrong-again&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bob_at_ragie"&gt; /u/bob_at_ragie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzxpzx/ragie_on_rag_is_dead_what_the_critics_are_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzxpzx/ragie_on_rag_is_dead_what_the_critics_are_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzxpzx/ragie_on_rag_is_dead_what_the_critics_are_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T17:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzn9wj</id>
    <title>New open-source model GLM-4-32B with performance comparable to Qwen 2.5 72B</title>
    <updated>2025-04-15T09:05:52+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzn9wj/new_opensource_model_glm432b_with_performance/"&gt; &lt;img alt="New open-source model GLM-4-32B with performance comparable to Qwen 2.5 72B" src="https://preview.redd.it/6pogmi3isyue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4861ca1813294a35c850cb947f8c2dbf56ea3e68" title="New open-source model GLM-4-32B with performance comparable to Qwen 2.5 72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is from ChatGLM (now Z.ai). A reasoning, deep research and 9B version are also available (6 models in total). MIT License.&lt;/p&gt; &lt;p&gt;Everything is on their GitHub: &lt;a href="https://github.com/THUDM/GLM-4"&gt;https://github.com/THUDM/GLM-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmarks are impressive compared to bigger models but I'm still waiting for more tests and experimenting with the models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6pogmi3isyue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzn9wj/new_opensource_model_glm432b_with_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzn9wj/new_opensource_model_glm432b_with_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T09:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k01pqy</id>
    <title>There is a hunt for reasoning datasets beyond math, science and coding. Much needed initiative</title>
    <updated>2025-04-15T20:08:17+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really interested in seeing what comes out of this.&lt;br /&gt; &lt;a href="https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition"&gt;https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition&lt;/a&gt;&lt;br /&gt; Current datasets: &lt;a href="https://huggingface.co/datasets?other=reasoning-datasets-competition"&gt;https://huggingface.co/datasets?other=reasoning-datasets-competition&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k01pqy/there_is_a_hunt_for_reasoning_datasets_beyond/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k01pqy/there_is_a_hunt_for_reasoning_datasets_beyond/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k01pqy/there_is_a_hunt_for_reasoning_datasets_beyond/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T20:08:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzwoci</id>
    <title>An extensive open-source collection of RAG implementations with many different strategies</title>
    <updated>2025-04-15T16:45:21+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing a repo I was working on and apparently people found it helpful (over 14,000 stars).&lt;/p&gt; &lt;p&gt;It’s open-source and includes 33 strategies for RAG, including tutorials, and visualizations.&lt;/p&gt; &lt;p&gt;This is great learning and reference material.&lt;/p&gt; &lt;p&gt;Open issues, suggest more strategies, and use as needed.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NirDiamant/RAG_Techniques"&gt;https://github.com/NirDiamant/RAG_Techniques&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzwoci/an_extensive_opensource_collection_of_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzwoci/an_extensive_opensource_collection_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzwoci/an_extensive_opensource_collection_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T16:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzuqpq</id>
    <title>I created an app that allows you use OpenAI API without API Key (Through desktop app)</title>
    <updated>2025-04-15T15:27:27+00:00</updated>
    <author>
      <name>/u/0ssamaak0</name>
      <uri>https://old.reddit.com/user/0ssamaak0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/"&gt; &lt;img alt="I created an app that allows you use OpenAI API without API Key (Through desktop app)" src="https://a.thumbs.redditmedia.com/5oqb949FvtZSn8quwgjs_M_tz6BFp8U8Nu1SQ6smLT8.jpg" title="I created an app that allows you use OpenAI API without API Key (Through desktop app)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/rh6ghzpkn0ve1.gif"&gt;https://i.redd.it/rh6ghzpkn0ve1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I created an open source mac app that mocks the usage of OpenAI API by routing the messages to the chatgpt desktop app so it can be used without API key.&lt;/p&gt; &lt;p&gt;I made it for personal reason but I think it may benefit you. I know the purpose of the app and the API is very different but I was using it just for personal stuff and automations.&lt;/p&gt; &lt;p&gt;You can simply change the api base (like if u are using ollama) and select any of the models that you can access from chatgpt app&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(api_key=OPENAI_API_KEY, base_url = 'http://127.0.0.1:11435/v1') completion = client.chat.completions.create( model=&amp;quot;gpt-4o-2024-05-13&amp;quot;, messages=[ {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How many r's in the word strawberry?&amp;quot;}, ] ) print(completion.choices[0].message) ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://github.com/0ssamaak0/MackingJAI"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's only available as dmg now but I will try to do a brew package soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0ssamaak0"&gt; /u/0ssamaak0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T15:27:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k043gb</id>
    <title>We’ve been snapshotting local LLaMA models and restoring in ~2s. Here’s what we learned from the last post.</title>
    <updated>2025-04-15T21:48:59+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on a post here last week.we’ve been snapshotting local LLaMA models (including full execution state: weights, KV cache, memory layout, stream context) and restoring them from disk in ~2 seconds. It’s kind of like treating them as pause/resume processes instead of keeping them always in memory.&lt;/p&gt; &lt;p&gt;The replies and DMs were awesome . wanted to share some takeaways and next steps.&lt;/p&gt; &lt;p&gt;What stood out:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;•Model swapping is still a huge pain for local setups •People want more efficient multi-model usage per GPU •Everyone’s tired of redundant reloading •Live benchmarks &amp;gt; charts or claims &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What we’re building now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;•Clean demo showing snapshot load vs vLLM / Triton-style cold starts •Single-GPU view with model switching timers •Simulated bursty agent traffic to stress test swapping •Dynamic memory &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;reuse for 50+ LLaMA models per node&lt;/p&gt; &lt;p&gt;Big thanks to the folks who messaged or shared what they’re hacking on . happy to include anyone curious in the next round of testing. Here is the demo(please excuse the UI) : &lt;a href="https://inferx.net"&gt;https://inferx.net&lt;/a&gt; Updates also going out on X @InferXai for anyone following this rabbit hole&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k043gb/weve_been_snapshotting_local_llama_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k043gb/weve_been_snapshotting_local_llama_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k043gb/weve_been_snapshotting_local_llama_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T21:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzsp5r</id>
    <title>Nvidia releases ultralong-8b model with context lengths from 1, 2 or 4mil</title>
    <updated>2025-04-15T14:04:10+00:00</updated>
    <author>
      <name>/u/throwawayacc201711</name>
      <uri>https://old.reddit.com/user/throwawayacc201711</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawayacc201711"&gt; /u/throwawayacc201711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzsp5r/nvidia_releases_ultralong8b_model_with_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzsp5r/nvidia_releases_ultralong8b_model_with_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T14:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k05ya6</id>
    <title>Overtrained Language Models Are Harder to Fine-Tune</title>
    <updated>2025-04-15T23:13:35+00:00</updated>
    <author>
      <name>/u/DinoAmino</name>
      <uri>https://old.reddit.com/user/DinoAmino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well damn... there go my plans for Behemoth &lt;a href="https://arxiv.org/abs/2503.19206"&gt;https://arxiv.org/abs/2503.19206&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DinoAmino"&gt; /u/DinoAmino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05ya6/overtrained_language_models_are_harder_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05ya6/overtrained_language_models_are_harder_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k05ya6/overtrained_language_models_are_harder_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T23:13:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0cpx4</id>
    <title>SFT can significantly undermine subsequent RL by inducing "pseudo reasoning paths" imitated from expert models.</title>
    <updated>2025-04-16T05:16:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models&lt;/p&gt; &lt;p&gt;&lt;a href="https://ucsc-vlaa.github.io/VLAA-Thinking/"&gt;https://ucsc-vlaa.github.io/VLAA-Thinking/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;SFT can significantly undermine subsequent RL by inducing &amp;quot;pseudo reasoning paths&amp;quot; imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0cpx4/sft_can_significantly_undermine_subsequent_rl_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0cpx4/sft_can_significantly_undermine_subsequent_rl_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0cpx4/sft_can_significantly_undermine_subsequent_rl_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T05:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k013u1</id>
    <title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title>
    <updated>2025-04-15T19:43:27+00:00</updated>
    <author>
      <name>/u/rini17</name>
      <uri>https://old.reddit.com/user/rini17</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k013u1/primacpp_speeding_up_70bscale_llm_inference_on/"&gt; &lt;img alt="PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters" src="https://external-preview.redd.it/1a4VnOKBMgllCIP8oR-afgJiFL7NVrlzrJf47Hyoz_0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47ec2eedb24b29b34a31b164d9038ca9e61a6a62" title="PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rini17"&gt; /u/rini17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2504.08791"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k013u1/primacpp_speeding_up_70bscale_llm_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k013u1/primacpp_speeding_up_70bscale_llm_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T19:43:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzp5or</id>
    <title>Microsoft has released a fresh 2B bitnet model</title>
    <updated>2025-04-15T11:10:39+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;BitNet b1.58 2B4T&lt;/strong&gt;, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale, developed by Microsoft Research.&lt;/p&gt; &lt;p&gt;Trained on a corpus of 4 trillion tokens, this model demonstrates that native 1-bit LLMs can achieve performance comparable to leading open-weight, full-precision models of similar size, while offering substantial advantages in computational efficiency (memory, energy, latency).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T"&gt;HuggingFace (safetensors)&lt;/a&gt; &lt;a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-bf16"&gt;BF16 (not published yet)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf"&gt;HuggingFace (GGUF)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/microsoft/BitNet"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzp5or/microsoft_has_released_a_fresh_2b_bitnet_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzp5or/microsoft_has_released_a_fresh_2b_bitnet_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzp5or/microsoft_has_released_a_fresh_2b_bitnet_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T11:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzz2q6</id>
    <title>Nvidia 5060 Ti 16 GB VRAM for $429. Yay or nay?</title>
    <updated>2025-04-15T18:21:12+00:00</updated>
    <author>
      <name>/u/Amadesa1</name>
      <uri>https://old.reddit.com/user/Amadesa1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzz2q6/nvidia_5060_ti_16_gb_vram_for_429_yay_or_nay/"&gt; &lt;img alt="Nvidia 5060 Ti 16 GB VRAM for $429. Yay or nay?" src="https://preview.redd.it/nqgok5nih1ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45d17691e0d37894b83cc3105089d7bcbe4f7f56" title="Nvidia 5060 Ti 16 GB VRAM for $429. Yay or nay?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;These new graphics cards are based on Nvidia's GB206 die. Both RTX 5060 Ti configurations use the same core, with the only difference being memory capacity. There are 4,608 CUDA cores – up 6% from the 4,352 cores in the RTX 4060 Ti – with a boost clock of 2.57 GHz. They feature a &lt;strong&gt;128-bit memory bus&lt;/strong&gt; utilizing 28 Gbps GDDR7 memory, which should deliver &lt;strong&gt;448 GB/s of bandwidth&lt;/strong&gt;, regardless of whether you choose the 16GB or 8GB version. Nvidia didn't confirm this directly, but we expect a PCIe 5.0 x8 interface. They did, however, confirm full DisplayPort 2.1b UHBR20 support.&amp;quot; &lt;a href="https://www.techspot.com/news/107541-nvidia-launches-geforce-rtx-5060-series-three-new.html"&gt;TechSpot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Assuming these will be supply constrained / tariffed, I'm guesstimating +20% MSRP for actual street price so it might be closer to $530-ish.&lt;/p&gt; &lt;p&gt;Does anybody have good expectations for this product in homelab AI versus a Mac Mini/Studio or any AMD 7000/8000 GPU considering VRAM size or token/s per price?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amadesa1"&gt; /u/Amadesa1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nqgok5nih1ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzz2q6/nvidia_5060_ti_16_gb_vram_for_429_yay_or_nay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzz2q6/nvidia_5060_ti_16_gb_vram_for_429_yay_or_nay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T18:21:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k04tcz</id>
    <title>INTELLECT-2: The First Globally Distributed Reinforcement Learning Training of a 32B Parameter Model</title>
    <updated>2025-04-15T22:21:17+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k04tcz/intellect2_the_first_globally_distributed/"&gt; &lt;img alt="INTELLECT-2: The First Globally Distributed Reinforcement Learning Training of a 32B Parameter Model" src="https://external-preview.redd.it/AnKCfKkHWYOIoVVmxjzjYrZqxFItOaRIWWXYxJe4nSs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805b290535499f071d8670cbf7eb2173ce4a806b" title="INTELLECT-2: The First Globally Distributed Reinforcement Learning Training of a 32B Parameter Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.primeintellect.ai/blog/intellect-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k04tcz/intellect2_the_first_globally_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k04tcz/intellect2_the_first_globally_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T22:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0967d</id>
    <title>What is your favorite uncensored model?</title>
    <updated>2025-04-16T01:55:36+00:00</updated>
    <author>
      <name>/u/HornyGooner4401</name>
      <uri>https://old.reddit.com/user/HornyGooner4401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By uncensored, I don't just mean roleplay. I have yet to find a model that doesn't refuse when asked on instructions of how to cook meth, make pipe bombs, or invade a small country in South America and force them to sell bananas to you. &lt;/p&gt; &lt;p&gt;I feel like a good chunk is lost when you get lobotomized and taught to not say certain things&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HornyGooner4401"&gt; /u/HornyGooner4401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T01:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzocoo</id>
    <title>Finally someone noticed this unfair situation</title>
    <updated>2025-04-15T10:21:16+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt; &lt;img alt="Finally someone noticed this unfair situation" src="https://external-preview.redd.it/5GYklgQz-p1iWSTGvDsKHeD_QUDxP-9vHZQeXTsgRz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f85ee3e9068eb521d7e3ef4dce3cee7c471c03" title="Finally someone noticed this unfair situation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f3kbm3p73zue1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=626a7ab843545471bcd88509c4daaebaa4d44d79"&gt;I have the same opinion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And in Meta's recent Llama 4 release &lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;blog post&lt;/a&gt;, in the &amp;quot;Explore the Llama ecosystem&amp;quot; section, Meta thanks and acknowledges various companies and partners:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/85yqglbi4zue1.png?width=1476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1292382820cafc658d65bb71ca5bcf15ef0bf1b"&gt;Meta's blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notice how &lt;strong&gt;Ollama&lt;/strong&gt; is mentioned, but there's no acknowledgment of &lt;strong&gt;llama.cpp&lt;/strong&gt; or its creator &lt;strong&gt;ggerganov&lt;/strong&gt;, whose foundational work made much of this ecosystem possible.&lt;/p&gt; &lt;p&gt;Isn't this situation incredibly ironic? The original project creators and ecosystem founders get forgotten by big companies, while YouTube and social media are flooded with clickbait titles like &amp;quot;Deploy LLM with one click using Ollama.&amp;quot;&lt;/p&gt; &lt;p&gt;Content creators even deliberately blur the lines between the complete and distilled versions of models like DeepSeek R1, using the R1 name indiscriminately for marketing purposes.&lt;/p&gt; &lt;p&gt;Meanwhile, the foundational projects and their creators are forgotten by the public, never receiving the gratitude or compensation they deserve. The people doing the real technical heavy lifting get overshadowed while wrapper projects take all the glory.&lt;/p&gt; &lt;p&gt;What do you think about this situation? Is this fair?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T10:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0b8wx</id>
    <title>Yes, you could have 160gb of vram for just about $1000.</title>
    <updated>2025-04-16T03:46:47+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please see my original post that posted about this journey - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems.... &lt;/p&gt; &lt;p&gt;Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157.&lt;/p&gt; &lt;p&gt;Octominer XULTRA 12 has 12 PCIe slots, it's designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload.&lt;/p&gt; &lt;p&gt;I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a few models. I'll post data later.&lt;/p&gt; &lt;p&gt;Since the card has 12 slots, it has 1 8 pin for each slot, for a total of 12 cables. The cards have 2 8 pin each, so I had a choice, use an 8 pin to dual 8 pin cable or 2 to 1. To play it safe for starters, I did 2 to 1. For a total of 6 cards installed. The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts. I have 3 power supplies of 750watts for a total of 2250watts. The cool thing about the power supply is that it's hot swappable, I can plug in and take out while it's running. You don't need all 3 to run, only 1. The good news is that this thing doesn't draw power! The cards are a bit high idle at about 20watts, so 6 cards 120watts, system idles really at &amp;lt; 130 watts. I'm measuring at the outlet with an electrical measurement meter. During inference across the cards, peak was about 340watt. I'm using llama.cpp so inference is serial and not parallel. You can see the load move from one card to the other. This as you can guess is &amp;quot;inefficient&amp;quot; so llama.cpp is not as far as say using vLLM with tensor parallel. But it does support multi users, so you can push it by running parallel requests if you are sharing the rig with others, running agents or custom code. In such a situation, you can have the cards all max out. I didn't power limit the cards, system reports them at 250watts, I saw about 230watt max while inferring.&lt;/p&gt; &lt;p&gt;The case fan at 100% sounds like a jet engine, but the great thing is they are easy to control and at 10% you can't hear it. The cards run cooler than my Nvidia cards that are on an open rig, my Nvidia cards idle at 30-40C, these cards idle in the 20C range with 5% fan. I can't hear the fan until about 25% and it's very quiet and blends in. It takes about 50-60% before anyone that walks into the room will notice.&lt;/p&gt; &lt;p&gt;I just cut and paste and took some rough notes, I don't have any blogs or anything to sell, just sharing for those that might be interested. One of the cards seems to have issue. llama.cpp crashes when I try to use it both local and via RPC. I'll swap and move it around to see if it makes a difference. I have 2 other rigs, llama.cpp won't let me infer across more than 16 cards.&lt;/p&gt; &lt;p&gt;I'm spending time trying to figure it out, updated the *_MAX_DEVICES and MAX_BACKENDS, MAX_SERVERS in code from 16 to 32, it sometimes works. I did build with -DGGML_SCHED_MAX_BACKENDS=48 makes no difference. So if you have any idea, let me know. :)&lt;/p&gt; &lt;p&gt;Now on power and electricity. Save it, don't care. With that said, the box idles at about 120watts, my other rigs probably idle more. Between the 3 rigs, maybe idle of 600watts. I have experimented with &amp;quot;wake on lan&amp;quot; That means I can suspend the machines and then wake them up remotely. One of my weekend plans is to put a daemon that will monitor the GPUs and system, if idle and nothing going on for 30 minutes. Hibernate the system, when I'm ready to use them wake them up remotely. Do this for all rig and don't keep them running. I don't know how loaded models will behave, my guess is that it would need to be reloaded, it's &amp;quot;vram&amp;quot; aka &amp;quot;RAM&amp;quot; after all, and unlike system ram that gets saved to disk, GPU doesn't. I'm still shocked at the low power use.&lt;/p&gt; &lt;p&gt;So on PCIe electrical x1 speed. I read it was 1GBps, but hey, there's a difference from 1Gbps and that. So PCie3x1 is capable of 985 MB/s. My network cards are 1Gbps which are more around 125 MB/s. So upgrading to a 10Gbps network should theoretically allow for much faster load. 7x. In practice, I think it would be less. llama.cpp hackers are just programmers getting it done by any means necessary, the goal is to infer models not the best program, from my wandering around the rpc code today and observed behavior it's not that performant. So if you're into unix network programming and wanna contribute, that would be a great area. ;-)&lt;/p&gt; &lt;p&gt;With all this said, yes, for a just about $1000, 160gb of vram is sort of possible. There was a lot of MI50 on ebay and I suppose some other hawks saw them as well and took their chance so it's sold out. Keep your eyes out for deals. I even heard I didn't get the best deal, some lucky sonomabbb got the MI50's that were 32gb. It might just be that companies might start replacing more of their old cards and we will see more of these or even better ones. Don't be scared, don't worry about that mess of you need a power plant and it's no longer supported. Most of the things folks argued about on here are flat out wrong from my practical experience, so risk it all.&lt;/p&gt; &lt;p&gt;Oh yeah, largest model I did run was llama405b, and had it write code and was getting about 2tk/s. Yes it's a large dense model. It would perform the worse, MoE like deepseekv3, llama4 are going to fly. I'll get some numbers up on those if I remember to.&lt;/p&gt; &lt;p&gt;Future stuff.&lt;br /&gt; Decide if I'm going to pack all the GPUs in one server or another server. From the load observed today, one server will handle it fine. Unlike newer Nvidia GPUs with cable going in from the top, this one has the cables going in from the back and it's quite a tight fit to get in. PCI standards from what I understand expect cards to pull a max of 75w and an 8pin cable can supply 150w, for a max of 225w. So I could power them with a single cable, figure out how to limit power to 200w and be good to go. As a matter of fact, some of the cables had those adapter and I took them out. I saw a video of a crypto bro running an Octominer with 3080s and those have more power demand than MI50s.&lt;/p&gt; &lt;p&gt;Here goes data from my notes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama3.1-8b-instruct-q8&lt;/strong&gt; inference, same prompt, same seed&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MI50 local &amp;gt; llama_perf_sampler_print: sampling time = 141.03 ms / 543 runs ( 0.26 ms per token, 3850.22 tokens per second) llama_perf_context_print: load time = 164330.99 ms *** SSD through PCIe3x1 slot*** llama_perf_context_print: prompt eval time = 217.66 ms / 42 tokens ( 5.18 ms per token, 192.97 tokens per second) llama_perf_context_print: eval time = 12046.14 ms / 500 runs ( 24.09 ms per token, 41.51 tokens per second) llama_perf_context_print: total time = 18773.63 ms / 542 tokens 3090 local &amp;gt; llama_perf_context_print: load time = 3088.11 ms *** NVME through PCIex16 *** llama_perf_context_print: prompt eval time = 27.76 ms / 42 tokens ( 0.66 ms per token, 1512.91 tokens per second) llama_perf_context_print: eval time = 6472.99 ms / 510 runs ( 12.69 ms per token, 78.79 tokens per second) 3080ti local &amp;gt; llama_perf_context_print: prompt eval time = 41.82 ms / 42 tokens ( 1.00 ms per token, 1004.26 tokens per second) llama_perf_context_print: eval time = 5976.19 ms / 454 runs ( 13.16 ms per token, 75.97 tokens per second) 3060 local &amp;gt; llama_perf_sampler_print: sampling time = 392.98 ms / 483 runs ( 0.81 ms per token, 1229.09 tokens per second) llama_perf_context_print: eval time = 12351.84 ms / 440 runs ( 28.07 ms per token, 35.62 tokens per second) p40 local &amp;gt; llama_perf_context_print: prompt eval time = 95.65 ms / 42 tokens ( 2.28 ms per token, 439.12 tokens per second) llama_perf_context_print: eval time = 12083.73 ms / 376 runs ( 32.14 ms per token, 31.12 tokens per second) MI50B local *** different GPU from above, consistent *** llama_perf_context_print: prompt eval time = 229.34 ms / 42 tokens ( 5.46 ms per token, 183.14 tokens per second) llama_perf_context_print: eval time = 12186.78 ms / 500 runs ( 24.37 ms per token, 41.03 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are paying attention MI50s are not great at prompt processing.&lt;/p&gt; &lt;p&gt;a little bit larger context, demonstrates that MI50 sucks at prompt processing... and demonstrating performance over RPC. I got these to see if I could use them via RPC for very huge models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;p40 local llama_perf_context_print: prompt eval time = 512.56 ms / 416 tokens ( 1.23 ms per token, 811.61 tokens per second) llama_perf_context_print: eval time = 12582.57 ms / 370 runs ( 34.01 ms per token, 29.41 tokens per second) 3060 local llama_perf_context_print: prompt eval time = 307.63 ms / 416 tokens ( 0.74 ms per token, 1352.27 tokens per second) llama_perf_context_print: eval time = 10149.66 ms / 357 runs ( 28.43 ms per token, 35.17 tokens per second) 3080ti local llama_perf_context_print: prompt eval time = 141.43 ms / 416 tokens ( 0.34 ms per token, 2941.45 tokens per second) llama_perf_context_print: eval time = 6079.14 ms / 451 runs ( 13.48 ms per token, 74.19 tokens per second) 3090 local llama_perf_context_print: prompt eval time = 140.91 ms / 416 tokens ( 0.34 ms per token, 2952.30 tokens per second) llama_perf_context_print: eval time = 4170.36 ms / 314 runs ( 13.28 ms per token, 75.29 tokens per second MI50 local llama_perf_context_print: prompt eval time = 1391.44 ms / 416 tokens ( 3.34 ms per token, 298.97 tokens per second) llama_perf_context_print: eval time = 8497.04 ms / 340 runs ( 24.99 ms per token, 40.01 tokens per second) MI50 over RPC (1GPU) llama_perf_context_print: prompt eval time = 1177.23 ms / 416 tokens ( 2.83 ms per token, 353.37 tokens per second) llama_perf_context_print: eval time = 16800.55 ms / 340 runs ( 49.41 ms per token, 20.24 tokens per second) MI50 over RPC (2xGPU) llama_perf_context_print: prompt eval time = 1400.72 ms / 416 tokens ( 3.37 ms per token, 296.99 tokens per second) llama_perf_context_print: eval time = 17539.33 ms / 340 runs ( 51.59 ms per token, 19.39 tokens per second) MI50 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 1562.64 ms / 416 tokens ( 3.76 ms per token, 266.22 tokens per second) llama_perf_context_print: eval time = 18325.72 ms / 340 runs ( 53.90 ms per token, 18.55 tokens per second) p40 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 968.91 ms / 416 tokens ( 2.33 ms per token, 429.35 tokens per second) llama_perf_context_print: eval time = 22888.16 ms / 370 runs ( 61.86 ms per token, 16.17 tokens per second) MI50 over RPC (5xGPU) (1 token a second loss for every RPC?) llama_perf_context_print: prompt eval time = 1955.87 ms / 416 tokens ( 4.70 ms per token, 212.69 tokens per second) llama_perf_context_print: eval time = 22217.03 ms / 340 runs ( 65.34 ms per token, 15.30 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;max inference over RPC observed with rocm-smi was 100w, lower than when running locally, saw 240w&lt;/p&gt; &lt;p&gt;max watt observed at outlet before RPC was 361w, max watt after 361w&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama-70b-q8&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;if you want to approximate how fast it will run in q4, just multiple by 2. This was done with llama.cpp, yes vLLM is faster, someone already did q4 llama8 with vLLM and tensor parallel for 25tk/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;3090 5xGPU llama-70b llama_perf_context_print: prompt eval time = 785.20 ms / 416 tokens ( 1.89 ms per token, 529.80 tokens per second) llama_perf_context_print: eval time = 26483.01 ms / 281 runs ( 94.25 ms per token, 10.61 tokens per second) llama_perf_context_print: total time = 133787.93 ms / 756 tokens MI50 over RPC (5xGPU) llama-70b llama_perf_context_print: prompt eval time = 11841.23 ms / 416 tokens ( 28.46 ms per token, 35.13 tokens per second) llama_perf_context_print: eval time = 84088.80 ms / 415 runs ( 202.62 ms per token, 4.94 tokens per second) llama_perf_context_print: total time = 101548.44 ms / 831 tokens RPC across 17GPUs, 6 main 3090l and 11 remote GPUs (3090, 3080ti,3060, 3xP40, 5xMI50) true latency test llama_perf_context_print: prompt eval time = 8172.69 ms / 416 tokens ( 19.65 ms per token, 50.90 tokens per second) llama_perf_context_print: eval time = 74990.44 ms / 345 runs ( 217.36 ms per token, 4.60 tokens per second) llama_perf_context_print: total time = 556723.90 ms / 761 tokens Misc notes idle watt at outlet = 126watts temp about 25-27C across GPUs idle power across individual 21-26watts powercap - 250watts inference across 3GPUs at outlet - 262watts highest power on one GPU = 223W at 10% speed, fan got to 60C, at 20% speed highest is 53C while GPU is active. turned up to 100% it brought the GPUs down to high 20's in under 2 minutes &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T03:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k05wpt</id>
    <title>ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)</title>
    <updated>2025-04-15T23:11:34+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"&gt; &lt;img alt="ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)" src="https://preview.redd.it/393vjiodz2ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb315c5ae73bc479aead0533e99e06cf2db069a" title="ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Architecture Liquid is an auto-regressive model extending from existing LLMs that uses an transformer architecture (similar to GPT-4o imagegen).&lt;/p&gt; &lt;p&gt;Input: text and image. Output: generate text or generated image.&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Junfeng5/Liquid_V1_7B"&gt;https://huggingface.co/Junfeng5/Liquid_V1_7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;App demo: &lt;a href="https://huggingface.co/spaces/Junfeng5/Liquid_demo"&gt;https://huggingface.co/spaces/Junfeng5/Liquid_demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Personal review: the quality of the image generation is definitely not as good as gpt-4o imagegen. However it’s important as a release due to using an auto-regressive generation paradigm using a single LLM, unlike previous multimodal large language model (MLLM) which used external pretrained visual embeddings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/393vjiodz2ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T23:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0c40c</id>
    <title>We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed</title>
    <updated>2025-04-16T04:38:13+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt; &lt;img alt="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" src="https://external-preview.redd.it/OTVoem9nbmRsNHZlMRZyoyYKNpzPJZZUnGrUtyeCYi3ToyFLi7JPjGL-ftCw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b12cd479c2024bd0aed4acb204f01a7a4780624" title="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Menlo Research again, and today we’d like to introduce a new paper from our team related to search.&lt;/p&gt; &lt;p&gt;Have you ever felt that when searching on Google, &lt;strong&gt;you know for sure there’s no way you’ll get the result you want on the first try&lt;/strong&gt; (you’re already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea.&lt;/p&gt; &lt;p&gt;We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model &amp;quot;work harder&amp;quot; and be more diligent, it could actually perform better.&lt;/p&gt; &lt;p&gt;Normally when training LLMs, repetitive actions are something people want to avoid, because they’re thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of &lt;strong&gt;46%&lt;/strong&gt;, compared to just &lt;strong&gt;20%&lt;/strong&gt; from a baseline model trained the same way. So that gives us some evidence that &lt;strong&gt;Repetition is not hallucination.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfect for research use cases.&lt;/p&gt; &lt;p&gt;Attached a demo in the clip.&lt;/p&gt; &lt;p&gt;(The beginning has a little meme to bring you some laughs 😄 - Trust me ReZero is Retry and Zero from Deepseek-zero)&lt;/p&gt; &lt;p&gt;Links to the paper/data below:&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2504.11001"&gt;https://arxiv.org/abs/2504.11001&lt;/a&gt;&lt;br /&gt; huggingface: &lt;a href="https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404"&gt;https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404&lt;/a&gt;&lt;br /&gt; github: &lt;a href="https://github.com/menloresearch/ReZero"&gt;https://github.com/menloresearch/ReZero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; As much as we want to make this model perfect, we are well aware of its limitations, specifically about training set and a bit poor design choice of reward functions. However we decided to release the model anyway, because it's better for the community to have access and play with it (also our time budget for this research is already up).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x9c46kt8l4ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T04:38:13+00:00</published>
  </entry>
</feed>
