<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-03T22:05:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n770a1</id>
    <title>I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time</title>
    <updated>2025-09-03T06:03:07+00:00</updated>
    <author>
      <name>/u/yuyangchee98</name>
      <uri>https://old.reddit.com/user/yuyangchee98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt; &lt;img alt="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" src="https://external-preview.redd.it/B50ELvs9yWP89z_ZcFK2UCF9ieaTQI3VL80AVGhBmaU.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1278541976a13214da5dc7333c05607ca273ef3" title="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a Chrome extension that uses local models to filter content based on rules you write in plain English.&lt;/p&gt; &lt;p&gt;Some examples are: &amp;quot;No political content or culture wars&amp;quot;, &amp;quot;Remove clickbait and rage bait&amp;quot;, &amp;quot;Hide celebrity gossip and drama&amp;quot;, &amp;quot;No sports or entertainment news&amp;quot;.&lt;/p&gt; &lt;p&gt;It works with Ollama, LM Studio, and your custom defined OpenAI compatible endpoint. Let me know if you use some other way to host your local LLMs.&lt;/p&gt; &lt;p&gt;Currently only works on Reddit but planning to add more sites.&lt;/p&gt; &lt;p&gt;Link is here: &lt;a href="https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef"&gt;https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player"&gt;https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuyangchee98"&gt; /u/yuyangchee98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T06:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7923o</id>
    <title>Has anyone run 256GB of DDR5 6000 stable on an AM5 platform?</title>
    <updated>2025-09-03T08:15:25+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my system to 256GB so I can run a larger model with my GPU. I’m wondering if anyone has been able to run 256GB of DDR5 6000 stable on an AM5 platform. I don’t want to upgrade to Threadripper since it’s out of my budget. Which motherboard and RAM did you use?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587"&gt;https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MSI claims their motherboard can still achieve a stable overclocking speed of 6000MT/s even with four 64GB DRAM fully installed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mh7o</id>
    <title>Is BitNet Training Unstable?</title>
    <updated>2025-09-03T18:16:06+00:00</updated>
    <author>
      <name>/u/THE_ROCKS_MUST_LEARN</name>
      <uri>https://old.reddit.com/user/THE_ROCKS_MUST_LEARN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mh7o/is_bitnet_training_unstable/"&gt; &lt;img alt="Is BitNet Training Unstable?" src="https://preview.redd.it/5734gavtqzmf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=75ae69df82e1c51e97b35c2d75e4482e5779fa00" title="Is BitNet Training Unstable?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet models&lt;/a&gt; have weights that can be represented by a single bit, meaning that they are either -1 or +1.&lt;/p&gt; &lt;p&gt;The common way to train them is using an underlying continuous parameter (I will call it &lt;em&gt;z&lt;/em&gt;) that is projected into a 1-bit discrete weight using the function &lt;em&gt;w=sign(z).&lt;/em&gt; The gradient of &lt;em&gt;z&lt;/em&gt; with respect to &lt;em&gt;w&lt;/em&gt; is calculated using a straight-through-estimator (STE), such that &lt;em&gt;dw/dz=1&lt;/em&gt; (other STEs are also used, like tanh, but I don't think that matters for my point).&lt;/p&gt; &lt;p&gt;However, this method appears unstable. To see why, consider the case where the &amp;quot;ideal&amp;quot; weight is between -1 and +1. For example, think about a simplified scenario where the model's loss with respect to &lt;em&gt;w&lt;/em&gt; is &lt;em&gt;L=(w-0.3)^2&lt;/em&gt; (which is approximately the form that LLM losses take with respect to individual weights). When &lt;em&gt;z&amp;gt;0,&lt;/em&gt; then &lt;em&gt;w&amp;gt;0.3&lt;/em&gt; so the gradients push &lt;em&gt;z&lt;/em&gt; in the negative direction. Similarly, when &lt;em&gt;z&amp;lt;0&lt;/em&gt;, then &lt;em&gt;w&amp;lt;0.3&lt;/em&gt; so the gradients push &lt;em&gt;z&lt;/em&gt; in the positive direction. It seems that this would cause &lt;em&gt;z&lt;/em&gt; to oscillate around zero and make &lt;em&gt;w&lt;/em&gt; to flip back and forth between -1 and +1, making training unstable and preventing convergence.&lt;/p&gt; &lt;p&gt;My diagram shows an animation of the gradient descent process under these conditions. The black dot shows &lt;em&gt;z&lt;/em&gt; moving according to the gradient direction and the blue dot shows &lt;em&gt;w&lt;/em&gt; and its position on the loss function (the yellow line). You can see the instability once &lt;em&gt;z&lt;/em&gt; reaches zero.&lt;/p&gt; &lt;p&gt;Has this phenomenon been mentioned or addressed before? Is it actually not a problem inside of real models (possibly because of stochastic gradients or optimizers like Adam)?&lt;/p&gt; &lt;p&gt;A similar example can also be constructed for 1.58-bit models where the weights are in {-1, 0, +1}.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THE_ROCKS_MUST_LEARN"&gt; /u/THE_ROCKS_MUST_LEARN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5734gavtqzmf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mh7o/is_bitnet_training_unstable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mh7o/is_bitnet_training_unstable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:16:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fux7</id>
    <title>Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic</title>
    <updated>2025-09-03T14:10:03+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71b95</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:09+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run the same models (ie qwen 3 coder, or GLM 4.5 air) with 4 x 3090? Is the only real difference slight speed difference and a few dollars more a month in electricity? Secondly, are there any consumer motherboards (currently using an intel 265K) that support 4 GPUs, or would I need a new chipset / cpu / mobo etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mjjn</id>
    <title>Speeding up PyTorch inference by 87% on Apple devices with AI-generated Metal kernels</title>
    <updated>2025-09-03T18:18:30+00:00</updated>
    <author>
      <name>/u/thebachelor-ml</name>
      <uri>https://old.reddit.com/user/thebachelor-ml</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebachelor-ml"&gt; /u/thebachelor-ml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gimletlabs.ai/blog/ai-generated-metal-kernels"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mjjn/speeding_up_pytorch_inference_by_87_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mjjn/speeding_up_pytorch_inference_by_87_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7plpb</id>
    <title>What is the biggest advantage of running local?</title>
    <updated>2025-09-03T20:13:31+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disadvantages? :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Cost&lt;/li&gt; &lt;li&gt;Speed&lt;/li&gt; &lt;li&gt;Smartness&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For me, knowing my data isn't shared is the biggest. Other reasons:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Being able to create NSFW content&lt;/li&gt; &lt;li&gt;Knowing that my model isn't being degraded unknowingly via quantization&lt;/li&gt; &lt;li&gt;Tools to automate local workflows, like auto generate git commit messages.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What are you thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7plpb/what_is_the_biggest_advantage_of_running_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7plpb/what_is_the_biggest_advantage_of_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7plpb/what_is_the_biggest_advantage_of_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T20:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fsgd</id>
    <title>I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools</title>
    <updated>2025-09-03T14:07:15+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt; &lt;img alt="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" src="https://b.thumbs.redditmedia.com/lVGU8L5zqMIzkaT28XyJJJ-eN-kz7vnn4Hr4DJVSTqI.jpg" title="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an open-source project called Spring AI Playground — a self-hosted web UI for experimenting with local LLMs, RAG, and MCP tools.&lt;/p&gt; &lt;p&gt;It’s a self-hosted web UI (Docker image available) that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run local LLMs with &lt;strong&gt;Ollama&lt;/strong&gt; (you can switch to OpenAI/Anthropic too).&lt;/li&gt; &lt;li&gt;Upload docs → chunk, embed, search, and inspect vector-DB retrieval &lt;strong&gt;with score details&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Connect to &lt;strong&gt;MCP servers directly&lt;/strong&gt;, test each tool, and even run end-to-end chat flows combining RAG + MCP.&lt;/li&gt; &lt;li&gt;Swap vector DBs or select MCP tools dynamically - thanks to the Spring AI framework under the hood.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why I built it:&lt;/p&gt; &lt;p&gt;I wanted a sandbox where I could mash things together quickly, test retrieval quality, debug tools, and keep everything running locally. Open WebUI is fantastic for chat-centric experiments, but my focus was to make &lt;strong&gt;RAG + MCP first-class playgrounds&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community - especially from those running local models or playing with MCP. Curious if this would fit into your workflow, or if there are rough edges I should improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7fsgd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7rp5v</id>
    <title>Who here has got a Mac Studio with 512 gigs RAM?</title>
    <updated>2025-09-03T21:34:03+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have questions for you guys. So many questions. What models you run and what token/sec you get? What is the context size you set, do you run local LLM for fun or you do development and trying to replace Claude. &lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T21:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7obos</id>
    <title>I've benchmarked the top model I can run locally on CPU via llama-swap</title>
    <updated>2025-09-03T19:24:57+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After gpt-oss template has been updated in llama.cpp, I can just say it rocks for speed and accuracy&lt;/p&gt; &lt;p&gt;&lt;code&gt; ╒═══════════════════════╤══════════════════════════╤═════════════════════╕ │ Model │ Correct │ Avg Response Time │ ╞═══════════════════════╪══════════════════════════╪═════════════════════╡ │ gemma-3-12b-it-Q4_K_M │ 4/8 (50.0%) [█████░░░░░] │ 43.41s │ ├───────────────────────┼──────────────────────────┼─────────────────────┤ │ Qwen3-4B-IQ4_NL │ 6/8 (75.0%) [███████░░░] │ 87.14s │ ├───────────────────────┼──────────────────────────┼─────────────────────┤ │ gpt-oss-20b-mxfp4 │ 7/8 (87.5%) [████████░░] │ 52.60s │ ╘═══════════════════════╧══════════════════════════╧═════════════════════╛ &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7obos/ive_benchmarked_the_top_model_i_can_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7obos/ive_benchmarked_the_top_model_i_can_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7obos/ive_benchmarked_the_top_model_i_can_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T19:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7b5xl</id>
    <title>New Swiss fully-open multilingual Model</title>
    <updated>2025-09-03T10:29:46+00:00</updated>
    <author>
      <name>/u/braincrowd</name>
      <uri>https://old.reddit.com/user/braincrowd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt; &lt;img alt="New Swiss fully-open multilingual Model" src="https://external-preview.redd.it/KeZfybYf994Jltq2xFXUUTTUg9fRGIDbb5FdVf9Sh70.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=415bc651c08564e7f0fb8fbbfdc78d40ba8ad377" title="New Swiss fully-open multilingual Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braincrowd"&gt; /u/braincrowd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-2509"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T10:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7oamj</id>
    <title>Getting started with Lemonade's web ui on Linux/dev installation comes with instructions now (and more improvements)</title>
    <updated>2025-09-03T19:23:48+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7oamj/getting_started_with_lemonades_web_ui_on_linuxdev/"&gt; &lt;img alt="Getting started with Lemonade's web ui on Linux/dev installation comes with instructions now (and more improvements)" src="https://preview.redd.it/suw8kdbg00nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9409a4d563347aecc4c001675d6ba6bca57b8ad" title="Getting started with Lemonade's web ui on Linux/dev installation comes with instructions now (and more improvements)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade v8.1.8 is out today, with some nice quality-of-life improvements which were inspired and/or contributed by the community.&lt;/p&gt; &lt;p&gt;Our goal is to build an open, easy-to-use local LLM server that auto-optimizes for any computer. There's lots more to do, but we're making progress.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;💡Improved LLM Chat Interface&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Linux users and developers who install from PyPI or source are greeted with helpful instructions (used to be totally blank) &lt;ul&gt; &lt;li&gt;Glad redditors and github users pushed us to do this one&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;The text input box is now resizable (finally!) &lt;ul&gt; &lt;li&gt;Thanks &lt;a href="https://github.com/RobertAgee"&gt;https://github.com/RobertAgee&lt;/a&gt; for this and other contributions!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;🙌 OpenHands Tutorial&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;The Featured Apps section of the docs now has instructions for setting up OpenHands for LLM coding.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;🐛 Bug Bash&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Ryzen AI NPU completions now truncate instead of error when the maximum prompt length is exceeded &lt;ul&gt; &lt;li&gt;Thanks &lt;a href="https://github.com/Kritik-07"&gt;https://github.com/Kritik-07&lt;/a&gt; for taking on this common request!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;gfx120X (Radeon 9000-series) is supported on Ubuntu + ROCm&lt;/li&gt; &lt;li&gt;llama.cpp errors are more visible, for easier debugging&lt;/li&gt; &lt;li&gt;Support for installing multiple quantization variants of the same GGUF model&lt;/li&gt; &lt;li&gt;Enable users to override the HIP_VISIBLE_DEVICES environment variable&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Links in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/suw8kdbg00nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7oamj/getting_started_with_lemonades_web_ui_on_linuxdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7oamj/getting_started_with_lemonades_web_ui_on_linuxdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T19:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7c1tg</id>
    <title>Le Chat. Custom MCP connectors. Memories.</title>
    <updated>2025-09-03T11:19:20+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt; &lt;img alt="Le Chat. Custom MCP connectors. Memories." src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Le Chat. Custom MCP connectors. Memories." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Le Chat now integrates with 20+ enterprise platforms—powered by MCP—and remembers what matters with Memories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-mcp-connectors-memories"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7meyo</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T18:13:50+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/intel-arc-pro-b50-linux"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m1ig</id>
    <title>Mapping LLM Style and Range in Flash Fiction</title>
    <updated>2025-09-03T18:00:11+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt; &lt;img alt="Mapping LLM Style and Range in Flash Fiction" src="https://b.thumbs.redditmedia.com/u40TUFkM22cScfVpJOWkzRp8n5pZSql8r4GWBthStMo.jpg" title="Mapping LLM Style and Range in Flash Fiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Additional charts and analysis: &lt;a href="https://github.com/lechmazur/writing_styles"&gt;https://github.com/lechmazur/writing_styles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on 400 flash-fiction pieces of 600–800 words per LLM. Prompts include required elements to keep content varied.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7m1ig"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7bqgm</id>
    <title>LangExtract by Google: many people don't know about this yet!</title>
    <updated>2025-09-03T11:02:22+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt; &lt;img alt="LangExtract by Google: many people don't know about this yet!" src="https://external-preview.redd.it/n9THNRvTBgabZmzyX_O8lEw2GxXkLfCbQBuYD0khQMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9ab8760012176bf18e519e0590b3f5f3d4bab" title="LangExtract by Google: many people don't know about this yet!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google/langextract"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n75z15</id>
    <title>GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations</title>
    <updated>2025-09-03T05:01:51+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt; &lt;img alt="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" src="https://preview.redd.it/6c1jae9atvmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f69c39fa3f7051f8ad4c85418e9c6c975491e18b" title="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full benchmarking methodology here: &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;https://artificialanalysis.ai/methodology/intelligence-benchmarking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c1jae9atvmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jmiz</id>
    <title>Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!</title>
    <updated>2025-09-03T16:31:18+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt; &lt;img alt="Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!" src="https://external-preview.redd.it/uylRGwq1HYqH_9GVZQwtt7vjMGVse2R0k3BHHixg9iQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd30ec44d7dcdb6ab67aaebd28d83444619fea7e" title="Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd also like to take this opportunity to share some benchmarks for Cydonia 24B v4.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-31B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ilou</id>
    <title>Switzerland launches its own open source model</title>
    <updated>2025-09-03T15:54:28+00:00</updated>
    <author>
      <name>/u/ananas_tacos</name>
      <uri>https://old.reddit.com/user/ananas_tacos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt; &lt;img alt="Switzerland launches its own open source model" src="https://external-preview.redd.it/vqsLOQwLzSpFCY0wZKGHoR70wxX3Zo0oDI880u-Ya_o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659f786877c6c3ce79ac169974bd64f43e3484fc" title="Switzerland launches its own open source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ananas_tacos"&gt; /u/ananas_tacos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.engadget.com/ai/switzerland-launches-its-own-open-source-ai-model-133051578.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T15:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jfpt</id>
    <title>Qwen3 30B A3B Thinking 2507 Hybrid !!</title>
    <updated>2025-09-03T16:24:34+00:00</updated>
    <author>
      <name>/u/Not4Fame</name>
      <uri>https://old.reddit.com/user/Not4Fame</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt; &lt;img alt="Qwen3 30B A3B Thinking 2507 Hybrid !!" src="https://external-preview.redd.it/3-YBimUSWKbnR7AwkACpdqNr5hKT1fY59SClnp7z_yM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5bdb4b82debdc147a98f0aa03728d4703fe317e" title="Qwen3 30B A3B Thinking 2507 Hybrid !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, with some creative merge from YOYO-AI, and some love from me, now you have Qwen3 30B A3B Thinking 2507 in hybrid mode, just like the old hybrid mode, but 2507 weights. First give the creator some love &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507/discussions"&gt;here&lt;/a&gt; and next, read my instructions and get the chat template &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507-Q4_K_M-GGUF/discussions/1"&gt;here&lt;/a&gt; finally, go and download the model &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;No coffee needed, whatever I do, I do for love, not for fame ;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i6jep5s67zmf1.png?width=1151&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00577ea14074f247cf2491ae18a0fd5bf3cbfbb4"&gt;Qwen3 30B A3B Thinking 2507 Hybrid !&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; OOPS, I've linked wrong models, sorry&lt;/p&gt; &lt;p&gt;Model is this &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507"&gt;YOYO-AI/Qwen3-30B-A3B-Mixture-2507 · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and GGUF is this &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Moar Edit: I've also corrected the links in post body as well now, my comment explaining the usage and the chat template, sadly, is written under wrong model, but hey, nobody is perfect :)&lt;/p&gt; &lt;p&gt;Even Moar Edit: I've also moved my comment under the correct model now and linked this post's link correctly to it, so all links are now correct, phew... (yeah I'm stoned... so ? :P)&lt;/p&gt; &lt;p&gt;Moar Bonus Edit:&lt;/p&gt; &lt;p&gt;YOYO-AI also has &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF"&gt;this&lt;/a&gt; interesting merge, Qwen3 coder 2507 with Qwen3 Thinking 2507 merge, which in my experience makes the coder somewhat less rigid and a bit less resilient/more instruction following etc. I recommend you check that out too !&lt;/p&gt; &lt;p&gt;Moar Flex&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/466yn0x6tzmf1.png?width=1148&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98ad91f42fc6e0c0ebee2c9af31b05c74a20761b"&gt;Qwen3 30B A3B Hybrid Running at 196 tk/s on a 5090&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not4Fame"&gt; /u/Not4Fame &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mien</id>
    <title>Best current NSFW TTS model?</title>
    <updated>2025-09-03T18:17:17+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one? And how to use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7g0c2</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark w/ Leading Models</title>
    <updated>2025-09-03T14:15:58+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" src="https://b.thumbs.redditmedia.com/-7S29tlnbmvmCdgKuNNinsiLgi6LQ83T8Ar-ZV3lCVs.jpg" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, big thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for creating the original German &lt;strong&gt;Wer wird Millionär?&lt;/strong&gt; Benchmark and open-sourcing it. &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;After speaking, we said it would be fun to run the same benchmark on a set of leading models, and that's what we did here. &lt;/p&gt; &lt;p&gt;The rules and data stayed the same, 45 rounds, each with 15 multiple-choice questions from easy to hard. One wrong answer ends the program and you keep the current winnings. No lifelines. Answers are single letters A–D. same public WWM question corpus used in the original. &lt;a href="https://github.com/GerritKainz/wer_wird_millionaer"&gt;https://github.com/GerritKainz/wer_wird_millionaer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Questions remain in German for inference, but we included parallel English text so non-German readers can follow along. See fragen_antworten_en.json in the repo. Scripts to run many programs quickly and rebuild results from per-model outputs (millionaire-run.py, rebuild_leaderboard.py). We’ll attach a screenshot of the leaderboard instead of pasting a table here. same scoring and structure as the original, packaged for quick reruns.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Jose-Sabater/millionaire-bench-opper"&gt;https://github.com/Jose-Sabater/millionaire-bench-opper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Again thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for the idea and groundwork. If you try more models or tweak settings, feel free to open a PR or drop results in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7g0c2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7l5kg</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T17:27:29+00:00</updated>
    <author>
      <name>/u/levian_</name>
      <uri>https://old.reddit.com/user/levian_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt; &lt;img alt="Intel launches Arc Pro B50 graphics card at $349" src="https://preview.redd.it/357rwwhaizmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=066c5073e108f00168fb16f32dcc905e00df9cae" title="Intel launches Arc Pro B50 graphics card at $349" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial review, source:&lt;a href="https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349"&gt;https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/levian_"&gt; /u/levian_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/357rwwhaizmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
