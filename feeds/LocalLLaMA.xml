<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-01T23:05:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ifgf8s</id>
    <title>Batch sizes. Inferencing with llama.cpp</title>
    <updated>2025-02-01T21:07:49+00:00</updated>
    <author>
      <name>/u/SiEgE-F1</name>
      <uri>https://old.reddit.com/user/SiEgE-F1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At the llamacpp pull requests, I keep seeing people sharing test results with different batch sizes, and it seems like they produce different speeds for different type of hardware, so my question is:&lt;br /&gt; What are the cases to play around with batch sizes? I realize that they are something like the &amp;quot;chunk sizes&amp;quot; for the data to be passed per GPU request, but are there any possible gains for alternating them?&lt;/p&gt; &lt;p&gt;What batch size should I use if my VRAM holds 100% of model? or at 70% VRAM/30% RAM? Or at 50/50%? Or when I inference just on RAM?&lt;/p&gt; &lt;p&gt;From my understanding, the default batch size is mostly optimized for VRAM inference. But what if I hold 50% of the model's inside of RAM? My system is heavily disbalanced. I have not the fastest, 4 planks of DDR4 memory, and a 4090. I wonder, will changing the chunk size help the RAM throughput, without hurting the GPU's performance that much? Or am I not required to do that, and such adaption already happens when inference is done by the RAM/CPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SiEgE-F1"&gt; /u/SiEgE-F1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgf8s/batch_sizes_inferencing_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgf8s/batch_sizes_inferencing_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgf8s/batch_sizes_inferencing_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T21:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifgu6h</id>
    <title>We haven't won...yet</title>
    <updated>2025-02-01T21:26:32+00:00</updated>
    <author>
      <name>/u/agentcubed</name>
      <uri>https://old.reddit.com/user/agentcubed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is great, but I feel like people are too quickly celebrating that open source has won. Call me pessimistic, but could things get...worse?&lt;/p&gt; &lt;h2&gt;OpenAI also used to be open AI&lt;/h2&gt; &lt;p&gt;Money doesn't grow on trees, and in the end, it is still a quant fund that is investing millions into AI, and despite their official statements it's probably not &amp;quot;just because we want to help everyone&amp;quot;. The moment OpenAI realized they got past the point of startup and into the point of making money, they went closed. Quick research says they don't have any plans to make money...in the short term (&lt;a href="https://www.reuters.com/technology/artificial-intelligence/high-flyer-ai-quant-fund-behind-chinas-deepseek-2025-01-29/"&gt;Reuters&lt;/a&gt;)&lt;/p&gt; &lt;h2&gt;AI War&lt;/h2&gt; &lt;p&gt;I'm scared because this greater US vs China thing would probably less research being published. I wouldn't put it past either government to &amp;quot;convince&amp;quot; researchers to hide new research in an attempt to get an edge.&lt;/p&gt; &lt;p&gt;I think open source has shown it is still in the battle, but I definitely don't think it won or is close to winning.&lt;/p&gt; &lt;p&gt;Am I just being too pesimistic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agentcubed"&gt; /u/agentcubed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgu6h/we_havent_wonyet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgu6h/we_havent_wonyet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgu6h/we_havent_wonyet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T21:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifcqwj</id>
    <title>Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</title>
    <updated>2025-02-01T18:26:36+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt; &lt;img alt="Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs" src="https://b.thumbs.redditmedia.com/nMhDepkecp6mzYH9xbeeKZ7cjJZ8At59A1TyIQUg8YY.jpg" title="Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p3ft39n5lkge1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87862e427a143953733e496a2c9262e43392a7fc"&gt;https://preview.redd.it/p3ft39n5lkge1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87862e427a143953733e496a2c9262e43392a7fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xjlxdo9lkge1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b532c4f202a2d1010c25ddd617e44ff84e811e6e"&gt;https://preview.redd.it/4xjlxdo9lkge1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b532c4f202a2d1010c25ddd617e44ff84e811e6e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.18585"&gt;https://arxiv.org/pdf/2501.18585&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There were several post in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; saying people are trying to have longer thinking. But maybe longer thinking tokens doesn't necessarily means better or more accurate answer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/OpenAI/comments/1ibyq7p/deepseek_r1_overthinker_force_r1_models_to_think/"&gt;https://www.reddit.com/r/OpenAI/comments/1ibyq7p/deepseek_r1_overthinker_force_r1_models_to_think/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/qunash/r1-overthinker"&gt;https://github.com/qunash/r1-overthinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/latent-variable/r1_reasoning_effort"&gt;https://github.com/latent-variable/r1_reasoning_effort&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. I'm not the authors of the paper&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:26:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3xpg</id>
    <title>Virtuoso-Small-v2 - Distilled from Deepseek-v3, 128k context</title>
    <updated>2025-02-01T10:56:34+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Virtuoso-Small-v2 (14B)&lt;/strong&gt; is our next-generation, 14-billion-parameter language model that builds upon the original Virtuoso-Small architecture. This version is distilled from Deepseek-v3, leveraging an expanded dataset of 5B+ tokens worth of logits.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;arcee-ai/Virtuoso-Small-v2:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Virtuoso-Small-v2"&gt;https://huggingface.co/arcee-ai/Virtuoso-Small-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF"&gt;https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:56:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifdd9r</id>
    <title>what do you use AI for?</title>
    <updated>2025-02-01T18:53:30+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i mean ok great there are promising models coming up, and tools for inference are improving.&lt;/p&gt; &lt;p&gt;but ultimately what use cases does it fit? i suppose some use it with copilot. i use it to summarize transcripts and to generate nsfw images.&lt;/p&gt; &lt;p&gt;but what other practical uses does it currently work decently on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifdd9r/what_do_you_use_ai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifdd9r/what_do_you_use_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifdd9r/what_do_you_use_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4m53</id>
    <title>How can I force DeepSeek to think much more? And how much the longest i can force it?</title>
    <updated>2025-02-01T11:44:16+00:00</updated>
    <author>
      <name>/u/Western_Soil_4613</name>
      <uri>https://old.reddit.com/user/Western_Soil_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any benchmark/best practice for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Soil_4613"&gt; /u/Western_Soil_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifcir5</id>
    <title>Are there any models capable of high-quality profanity?</title>
    <updated>2025-02-01T18:16:47+00:00</updated>
    <author>
      <name>/u/Ray_Dillinger</name>
      <uri>https://old.reddit.com/user/Ray_Dillinger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter what the prompt, I can't get any LLM to do anything more interesting than drop a couple of f-bombs, insert &amp;quot;damn&amp;quot; as an adjective or interjection, or call the user as a bastard.&lt;/p&gt; &lt;p&gt;This will not do if I want it to represent a character that is using rancher-grade or military-grade profanity. There's a variety, lyricism and rhythm to really good profanity that, as far as I can tell, none of these models have anywhere in their training sets.&lt;/p&gt; &lt;p&gt;For example, when a rancher finds someone trespassing on their land at night, we should expect to be hearing phrases like &amp;quot;If you and your choad-licking toadies don't fuck off right now I'm going to tie your nipples in a knot and twist until you start to like it&amp;quot; and similar. This gets a lot of its force and effectiveness from internal rhymes like between 'choad' and 'toadies' and alliterations like between 'nipples' and 'knot', and is driven home by the recontextualization and accusation implicit in 'until you start to like it' as opposed to leaving it a bare threat.&lt;/p&gt; &lt;p&gt;Anyway, in my experience about ten percent of ranchers are capable of keeping up a barrage of this quality for five or ten minutes at a time before they start to repeat themselves, and I remember at least a couple of drill sergeants in the 1980s who seemed capable of going for a full hour at a time. The whole thing doesn't work unless it can be delivered rapidly enough that there's no chance to interrupt, and loses all force and starts to just sound stupid if it frequently repeats.&lt;/p&gt; &lt;p&gt;And I can't get anything similar in LLM output anywhere no matter how I try and coax. No lyricism, no internal rhyme, no alliterations, no accusatory recontextualizations, no awareness that repetition makes profanity sound stupid, and not even any good idea of what the words mean and which are appropriate to describe what kinds of behavior. These models think intense profanity consists of dropping a couple of f-bombs, and many of them aren't even capable of that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ray_Dillinger"&gt; /u/Ray_Dillinger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iff6pz</id>
    <title>What went into training DeepSeek-R1? A technical summary of the training of v3 and R1</title>
    <updated>2025-02-01T20:12:56+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iff6pz/what_went_into_training_deepseekr1_a_technical/"&gt; &lt;img alt="What went into training DeepSeek-R1? A technical summary of the training of v3 and R1" src="https://external-preview.redd.it/9j2v52ez3YonWGNyUx7ud4znv-zEcUugaGbv9-vEgoE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d70c275913e84630c4980a541e28538682099916" title="What went into training DeepSeek-R1? A technical summary of the training of v3 and R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iff6pz/what_went_into_training_deepseekr1_a_technical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iff6pz/what_went_into_training_deepseekr1_a_technical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:12:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifhugb</id>
    <title>One very interesting thing I learned just now is that apparently Gemma2-27b begins responding much faster than Gemma2-9b. I had always assumed the opposite would be true. Very important to know for voice applications.</title>
    <updated>2025-02-01T22:11:38+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifhugb/one_very_interesting_thing_i_learned_just_now_is/"&gt; &lt;img alt="One very interesting thing I learned just now is that apparently Gemma2-27b begins responding much faster than Gemma2-9b. I had always assumed the opposite would be true. Very important to know for voice applications." src="https://external-preview.redd.it/N3FmZXBpNWNwbGdlMdyPbMGk3oSHBxkHiCq8Y8bIQNW5aYT5KwyyXT018S-2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fde0a561420c036fda9ef18aed4a7cf8283523e7" title="One very interesting thing I learned just now is that apparently Gemma2-27b begins responding much faster than Gemma2-9b. I had always assumed the opposite would be true. Very important to know for voice applications." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6gpeoh5cplge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifhugb/one_very_interesting_thing_i_learned_just_now_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifhugb/one_very_interesting_thing_i_learned_just_now_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T22:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbptd</id>
    <title>Tulu 3: RVLR Based Llama 3 model.</title>
    <updated>2025-02-01T17:43:04+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5"&gt;https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yet to test it out but this sounds promising considering it's from Allen Institute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T17:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1if9kju</id>
    <title>They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think...</title>
    <updated>2025-02-01T16:09:10+00:00</updated>
    <author>
      <name>/u/MMAgeezer</name>
      <uri>https://old.reddit.com/user/MMAgeezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"&gt; &lt;img alt="They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think..." src="https://preview.redd.it/gir8e8bhxjge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4776556bde99f86b60d38ae09051ad184544ecd" title="They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spotted an interesting (and maybe concerning?) 'unsafe' example in this AI safety paper (image attached, page 13 in the paper). The answer gives very high-level points about some of the ways cybercriminals operate - provided by o3-mini (an older beta checkpoint of it).&lt;/p&gt; &lt;p&gt;Is flagging this kind of thing as 'unsafe' missing the point? Is the real danger not that AIs could actually help criminals, and just explaining the concepts at a high-level isn't the problem?&lt;/p&gt; &lt;p&gt;If you disagree, I'd love to hear your thoughts on why this specific example should be considered 'unsafe'.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://arxiv.org/pdf/2501.184384"&gt;o3-mini vs. DeepSeek-R1: Which one is safer?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important note about the paper&lt;/strong&gt;: It doesn't use the full R1 model (uses the Llama3.3-70B fine-tune instead) and it's using a beta release of o3-mini, as part of OpenAI's early research access program.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MMAgeezer"&gt; /u/MMAgeezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gir8e8bhxjge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ies630</id>
    <title>openai can be opening again</title>
    <updated>2025-01-31T23:09:00+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt; &lt;img alt="openai can be opening again" src="https://preview.redd.it/1oovs3vgvege1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acc945b69ba442d4e66865f5e83ab96ac9b83b7b" title="openai can be opening again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1oovs3vgvege1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifgy1a</id>
    <title>What happened to Differential Transformer ?</title>
    <updated>2025-02-01T21:31:16+00:00</updated>
    <author>
      <name>/u/LelouchZer12</name>
      <uri>https://old.reddit.com/user/LelouchZer12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt; &lt;img alt="What happened to Differential Transformer ?" src="https://b.thumbs.redditmedia.com/GitzFNEYZ_6WcAIz0fTMaK7vF4Cm93GvyPMzEMSp2cs.jpg" title="What happened to Differential Transformer ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nxrbdaktilge1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee964c9c99fc8cdbb282f76d8f08ad9e9dec24b7"&gt;https://preview.redd.it/nxrbdaktilge1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee964c9c99fc8cdbb282f76d8f08ad9e9dec24b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2410.05258"&gt;https://arxiv.org/pdf/2410.05258&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What happened to this idea ? It seemed to increase performance with almost no drawbacks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LelouchZer12"&gt; /u/LelouchZer12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T21:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifeu07</id>
    <title>New benchmark about multi-turn conversation that challenge frontier LLMs and capture Sonet 3.5 advantage: all LLMs perform below 50% accuracy</title>
    <updated>2025-02-01T19:57:31+00:00</updated>
    <author>
      <name>/u/TheIdealHominidae</name>
      <uri>https://old.reddit.com/user/TheIdealHominidae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn"&gt;https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIdealHominidae"&gt; /u/TheIdealHominidae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffgj4</id>
    <title>DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)</title>
    <updated>2025-02-01T20:24:46+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt; &lt;img alt="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" src="https://external-preview.redd.it/gMJZu1czNWIsX2vol0q37qYGLTI_zKgwHfEyO-m9Uqw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48df94f11b5f4243cdde43be4517d1e3d09e3712" title="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wKZHoGlllu4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieurv8</id>
    <title>My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:</title>
    <updated>2025-02-01T01:11:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt; &lt;img alt="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" src="https://preview.redd.it/jixqkaabhfge1.gif?width=216&amp;amp;crop=smart&amp;amp;s=c67a878b6f732544b4693cf47d6dc14a8220e551" title="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jixqkaabhfge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T01:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1if7hm3</id>
    <title>How To Run Deepseek R1 671b Fully Locally On a $2000 EPYC Server</title>
    <updated>2025-02-01T14:30:39+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://digitalspaceport.com/how-to-run-deepseek-r1-671b-fully-locally-on-2000-epyc-rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1rls</id>
    <title>We've been incredibly fortunate with how things have developed over the past year</title>
    <updated>2025-02-01T08:11:39+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I still remember how in late 2023, people were speculating that Mixtral-8x7b was the best open-weights model that the community would get &amp;quot;for a long time&amp;quot;, and possibly ever. Shortly afterwards, Mistral published a controversial blog post that appeared to indicate that they were moving away from open weights – an ominous sign at a time when there were very few open-weights models available, and Anthropic and OpenAI seemed as far out of reach as the stars.&lt;/p&gt; &lt;p&gt;But since then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta released the excellent Llama 3 series as open weights (though not entirely free software).&lt;/li&gt; &lt;li&gt;Contrary to what many had feared, Mistral continued to publish open-weights models, even releasing the weights for Mistral Large, which was previously API-only, and now publishing their latest Mistral Small under the Apache License, when the previous version was still under their proprietary MRL.&lt;/li&gt; &lt;li&gt;Yi-34b transitioned from a proprietary license to Apache.&lt;/li&gt; &lt;li&gt;Microsoft has been publishing a number of excellent small models under permissive licenses.&lt;/li&gt; &lt;li&gt;Qwen came out of nowhere, and released the best models that can be run on consumer hardware, almost all of them under permissive licenses.&lt;/li&gt; &lt;li&gt;DeepSeek upended the entire industry, and &lt;strong&gt;an MIT-licensed model is now ranked joint #1 on style-controlled LMSYS,&lt;/strong&gt; on par with cutting-edge, proprietary, API-only models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was completely unforeseeable a year ago. Reality has outpaced the wildest dreams of the most naive optimists. Some doomsayers even predicted that open-weights models would soon be outlawed. The exact opposite has happened, and continues to happen.&lt;/p&gt; &lt;p&gt;To get an idea for what could easily have been, just look at the world of image generation models. In 15 months, there have only been two significant open-weights releases: SD3, and Flux.1D. SD3 was mired in controversy due to Stability's behavior and has been all but ignored by the community, and Flux is crippled by distillation. Both models are censored to a degree that has become the stuff of memes, and their licenses essentially make them unusable for anything except horsing around.&lt;/p&gt; &lt;p&gt;That is how the LLM world could have turned out. Instead, we have a world where I don't even download every new model anymore, because there are multiple exciting releases every week and I simply lack the time to take all of them for a spin. I now regularly delete models from my hard drive that I would have given my right hand for not too long ago. It's just incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T08:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifi0qu</id>
    <title>Missouri Senator Josh Hawley proposes a ban on Chinese AI models</title>
    <updated>2025-02-01T22:19:39+00:00</updated>
    <author>
      <name>/u/InquisitiveInque</name>
      <uri>https://old.reddit.com/user/InquisitiveInque</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InquisitiveInque"&gt; /u/InquisitiveInque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T22:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1if8x64</id>
    <title>llama.cpp now supports tool calling (OpenAI-compatible)</title>
    <updated>2025-02-01T15:39:17+00:00</updated>
    <author>
      <name>/u/Federal_Discipline_4</name>
      <uri>https://old.reddit.com/user/Federal_Discipline_4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/9639"&gt;https://github.com/ggerganov/llama.cpp/pull/9639&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On top of generic support for &lt;em&gt;all&lt;/em&gt; models, it supports 8+ models’ native formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.x&lt;/li&gt; &lt;li&gt;Functionary 3&lt;/li&gt; &lt;li&gt;Hermes 2/3&lt;/li&gt; &lt;li&gt;Qwen 2.5&lt;/li&gt; &lt;li&gt;Mistral Nemo&lt;/li&gt; &lt;li&gt;Firefunction 2&lt;/li&gt; &lt;li&gt;DeepSeek R1 (WIP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Runs locally anywhere (incl. Raspberry Pi 5), e.g. on a Mac:&lt;/p&gt; &lt;p&gt;&lt;code&gt; brew install llama.cpp llama-server --jinja -fa -hf bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Still fresh / lots of bugs to discover: feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Discipline_4"&gt; /u/Federal_Discipline_4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T15:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1if71w7</id>
    <title>o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot.</title>
    <updated>2025-02-01T14:08:08+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt; &lt;img alt="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." src="https://external-preview.redd.it/aTBxM2VyeG5iamdlMUWHkN0UG3UwPNFPGIT0TYE7p36ybavsfv5qTlMpE8Gi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c32bf16da98a479fb59fe074cec8f9ff9f2587" title="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x607arxnbjge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5q97</id>
    <title>Just canceled my ChatGPT Plus subscription</title>
    <updated>2025-02-01T12:56:16+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I initially subscribed when they introduced uploading documents when it was limited to the plus plan. I kept holding onto it for o1 since it really was a game changer for me. But since R1 is free right now (when it’s available at least lol) and the quantized distilled models finally fit onto a GPU I can afford, I cancelled my plan and am going to get a GPU with more VRAM instead. I love the direction that open source machine learning is taking right now. It’s crazy to me that distillation of a reasoning model to something like Llama 8B can boost the performance by this much. I hope we soon will get more advancements in more efficient large context windows and projects like Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1if43uf</id>
    <title>Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source</title>
    <updated>2025-02-01T11:08:19+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt; &lt;img alt="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" src="https://preview.redd.it/iewy2sxsfige1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad808e1a916c32f2181d2dc562c2065ba8cb4c99" title="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iewy2sxsfige1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:08:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffboy</id>
    <title>SmolVLM fully open source</title>
    <updated>2025-02-01T20:19:01+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt; &lt;img alt="SmolVLM fully open source" src="https://external-preview.redd.it/RpBd16Y386MrSYjhSF5aL1O5cjq2V0xWVKGs2JQsIl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9476c8b4dd1bf85443ac42ac9be87b98d3ff2e1e" title="SmolVLM fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/andimarafioti/status/1885341684134978035"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3lq1</id>
    <title>Sam Altman acknowledges R1</title>
    <updated>2025-02-01T10:31:35+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt; &lt;img alt="Sam Altman acknowledges R1" src="https://preview.redd.it/ot5nsk399ige1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca17a8d86fa20881ff4876577c465ae2c733d9" title="Sam Altman acknowledges R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Straight from the horses mouth. Without R1, or bigger picture open source competitive models, we wouldn’t be seeing this level of acknowledgement from OpenAI. &lt;/p&gt; &lt;p&gt;This highlights the importance of having open models, not only that, but open models that actively compete and put pressure on closed models. &lt;/p&gt; &lt;p&gt;R1 for me feels like a real &lt;em&gt;hard takeoff&lt;/em&gt; moment. &lt;/p&gt; &lt;p&gt;No longer can OpenAI or other closed companies dictate the rate of release. &lt;/p&gt; &lt;p&gt;No longer do we have to get the scraps of what they decide to give us. &lt;/p&gt; &lt;p&gt;Now they have to actively compete in an open market.&lt;/p&gt; &lt;p&gt;No moat. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Source: &lt;a href="https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC"&gt;https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ot5nsk399ige1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:31:35+00:00</published>
  </entry>
</feed>
