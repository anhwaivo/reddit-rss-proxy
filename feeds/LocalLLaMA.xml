<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-03T03:05:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j1z22r</id>
    <title>Why are the sampler settings for virtually all providers on Openrouter so limited?</title>
    <updated>2025-03-02T19:25:56+00:00</updated>
    <author>
      <name>/u/cd1995Cargo</name>
      <uri>https://old.reddit.com/user/cd1995Cargo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with using Openrouter to try different models. Been using it for a mixture of helping me code a hobby project and helping me write a sci fi story I‚Äôve been mulling over in my head for years.&lt;/p&gt; &lt;p&gt;I‚Äôve rented GPUs on Runpod before using the Koboldcpp template and found that the DRY and XTC samplers improve creative writing so much that it‚Äôs pretty hard to go back to not using them.&lt;/p&gt; &lt;p&gt;These samplers seem to have existed for quite a while now so why are there no providers on Openrouter that offer them? Hell, a lot of providers don‚Äôt even offer min_p which I also consider to be a bare minimum sampler for modern models. A lot of them only offer top_p, temperature, and rep/frequency penalty which feels like going back to 2023. top_p in particular is known to be a pretty bad sampling strategy compared to min_p, and repetition penalty is a sledgehammer approach to preventing looping compared to DRY.&lt;/p&gt; &lt;p&gt;I‚Äôd love to use Openrouter more because of cost efficiency but when it comes to creative writing it‚Äôs an absolutely ass experience to not have the needed samplers. I‚Äôm scratching my head trying to figure out how it seems that &lt;em&gt;not a single provider&lt;/em&gt; has them, when they‚Äôve been known and implemented in llamacpp, exl2, etc for months. Somebody please tell me that they‚Äôre eventually going to be there üò©&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cd1995Cargo"&gt; /u/cd1995Cargo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z22r/why_are_the_sampler_settings_for_virtually_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z22r/why_are_the_sampler_settings_for_virtually_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z22r/why_are_the_sampler_settings_for_virtually_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T19:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j27bfd</id>
    <title>Are you able to load a model onto multiple GPUs that have a different architecture?</title>
    <updated>2025-03-03T01:36:05+00:00</updated>
    <author>
      <name>/u/CountCandyhands</name>
      <uri>https://old.reddit.com/user/CountCandyhands</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I am aware that you can split certain text generating models onto multiple cards, such as a 4090 and 3090, would it still be possible to do so if its a 5090 and 3090? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CountCandyhands"&gt; /u/CountCandyhands &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j27bfd/are_you_able_to_load_a_model_onto_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j27bfd/are_you_able_to_load_a_model_onto_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j27bfd/are_you_able_to_load_a_model_onto_multiple_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T01:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1vozj</id>
    <title>Possible to create simple, short sound effects using a local AI?</title>
    <updated>2025-03-02T17:08:15+00:00</updated>
    <author>
      <name>/u/liquidki</name>
      <uri>https://old.reddit.com/user/liquidki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I am curious if this is possible with an LLM or some other method, like perhaps a different stack like for Image creation AIs? I have searched the community but haven't seen anything like this.&lt;/p&gt; &lt;p&gt;I don't mind the output format, it's easy to convert. WAV, MP3, whatever.&lt;/p&gt; &lt;p&gt;Thanks in advance! Great community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidki"&gt; /u/liquidki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vozj/possible_to_create_simple_short_sound_effects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vozj/possible_to_create_simple_short_sound_effects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vozj/possible_to_create_simple_short_sound_effects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T17:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j28l2e</id>
    <title>fastest/best open-source GRPO implementation?</title>
    <updated>2025-03-03T02:42:12+00:00</updated>
    <author>
      <name>/u/Classic-Bag-6145</name>
      <uri>https://old.reddit.com/user/Classic-Bag-6145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know what the fastest implementation of GRPO is right now? I've tried TRL and veRL but neither one seems to be faster than my naive DDP implementation on the 8B R1 distill? This is on 8xH100, trying to fine-tune it on specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Classic-Bag-6145"&gt; /u/Classic-Bag-6145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j28l2e/fastestbest_opensource_grpo_implementation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j28l2e/fastestbest_opensource_grpo_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j28l2e/fastestbest_opensource_grpo_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T02:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1qnsw</id>
    <title>LMArena is broken? Hear me out.</title>
    <updated>2025-03-02T13:18:58+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: LMarena doesn‚Äôt really evaluate LLMs on difficult use cases or niche coding tasks (where the ‚ÄúClaude gang‚Äù claims Claude is SOTA). However, they introduced a method to automatically infer which model would be best for a given prompt, and it‚Äôs claimed to be quite reliable. For more details, see &lt;a href="https://arxiv.org/abs/2502.14855"&gt;the paper&lt;/a&gt; and &lt;a href="https://threadreaderapp.com/thread/1894767009977811256.html"&gt;this Twitter thread&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;LMarena isn‚Äôt broken. It just can‚Äôt be directly compared to personal usage of LLMs or tough benchmarks. Why? Because tough benchmarks pose genuinely difficult questions, while LMarena often checks how an LLM responds to a simple ‚Äúhi!‚Äù prompt. Sure, there are challenging questions on LMarena, but it‚Äôs likely they‚Äôre overshadowed by more common, Google-like queries.&lt;/p&gt; &lt;p&gt;LMarena also doesn‚Äôt cover those long, niche conversations people often have with Claude about debugging code. This is not the ‚Äúone prompt ‚Äì one answer; end of it‚Äù kind of interaction. Such extended back-and-forth discussions aren‚Äôt represented well in LMarena, and these tricky questions end up diluted among more typical ones.&lt;/p&gt; &lt;p&gt;Hence, LMarena‚Äôs overall, coding, and math categories may not always highlight the models that excel at truly hard tasks. People complained about this, and I think the creators of LMarena came up with a neat solution: you can post your prompt, and a model‚Äîclaimed to be pretty reliable‚Äîwill classify that prompt against similar ones, then recommend the best LLM based on human ratings. That way, if you do have a tough question, you can see which model is likely to handle it best.&lt;/p&gt; &lt;p&gt;I still think LMarena could add a ‚Äúhard‚Äù subcategory for each main category. For example, ‚Äúhard math‚Äù and ‚Äúhard coding,‚Äù similar to the existing ‚Äúhard prompts‚Äù subcategory under the broader ‚Äúoverall‚Äù category. But for now, nudging prompts to the leaderboard is a nice fix.&lt;/p&gt; &lt;p&gt;E: the prompt 2 leaderboard explorer is even better than the direct prompt 2 leaderboard because one can directly check categories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T13:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13cwq</id>
    <title>Qwen: ‚Äúdeliver something next week through opensource‚Äù</title>
    <updated>2025-03-01T16:29:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt; &lt;img alt="Qwen: ‚Äúdeliver something next week through opensource‚Äù" src="https://preview.redd.it/knfs0pgpu3me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=848a461104672e52b7bade6e6a4ea8b55f90ba90" title="Qwen: ‚Äúdeliver something next week through opensource‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Not sure if we can surprise you a lot but we will definitely deliver something next week through opensource.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knfs0pgpu3me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1vhr4</id>
    <title>Three sisters [llama 3.3 70B]</title>
    <updated>2025-03-02T17:00:19+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;San-Mai (Original Release) Named after the traditional Japanese blade smithing technique of creating three-layer laminated composite metals, San-Mai represents the foundational model in the series. Like its namesake that combines a hard cutting edge with a tougher spine, this model offers a balanced approach to AI capabilities, providing reliability and precision.&lt;/p&gt; &lt;p&gt;Cu-Mai (Version A) Cu-Mai, a play on &amp;quot;San-Mai&amp;quot; specifically referencing Copper-Steel Damascus, represents an evolution from the original model. While maintaining the grounded and reliable nature of San-Mai, Cu-Mai introduces its own distinct &amp;quot;flavor&amp;quot; in terms of prose style and overall interaction experience. It demonstrates strong adherence to prompts while offering unique creative expression.&lt;/p&gt; &lt;p&gt;Mokume-Gane (Version C) Named after the Japanese metalworking technique 'Mokume-gane' (Êú®ÁõÆÈáë), meaning 'wood grain metal', this model represents the most creative version in the series. Just as Mokume-gane craftsmen blend various metals to create distinctive layered patterns, this model generates more creative and unexpected outputs but tends to be unruly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-San-Mai-R1-70b"&gt;https://huggingface.co/Steelskull/L3.3-San-Mai-R1-70b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-Cu-Mai-R1-70b"&gt;https://huggingface.co/Steelskull/L3.3-Cu-Mai-R1-70b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-Mokume-Gane-R1-70b-v1.1"&gt;https://huggingface.co/Steelskull/L3.3-Mokume-Gane-R1-70b-v1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At their core, the three models utilize an entirely custom base model. The SCE merge method, with settings finely tuned based on community feedback from evaluations of Experiment-Model-Ver-0.5, Experiment-Model-Ver-0.5.A, Experiment-Model-Ver-0.5.B, Experiment-Model-Ver-0.5.C, Experiment-Model-Ver-0.5.D, L3.3-Nevoria-R1-70b, L3.3-Damascus-R1-70b and L3.3-Exp-Nevoria-70b-v0.1, enables precise and effective component integration while maintaining model coherence and reliability.&lt;/p&gt; &lt;p&gt;Have fun! -steel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T17:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j27nik</id>
    <title>Are llava:34b or llama3.2-vision:90b any good?</title>
    <updated>2025-03-03T01:53:46+00:00</updated>
    <author>
      <name>/u/Blender-Fan</name>
      <uri>https://old.reddit.com/user/Blender-Fan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have llava:13b locally and gotta tell you, i wouldn't use it at production. At best, is very unreliable. I was hoping what i lack is the number of parameters (maybe i'll get a card that can do llava:34b). I assume there are models i can pay to use for production, but would be nice to have an open-source model that can read/analyze images reliably&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blender-Fan"&gt; /u/Blender-Fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j27nik/are_llava34b_or_llama32vision90b_any_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j27nik/are_llava34b_or_llama32vision90b_any_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j27nik/are_llava34b_or_llama32vision90b_any_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T01:53:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kj9b</id>
    <title>I want to train AI to intimately understand an entire regulations booklet for my work- how do I do that, and cheaply?</title>
    <updated>2025-03-02T06:26:37+00:00</updated>
    <author>
      <name>/u/10c70377</name>
      <uri>https://old.reddit.com/user/10c70377</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says pretty much. I work in engineering and we have an 600 page booklet of regulations we must adhere to. &lt;/p&gt; &lt;p&gt;ChatGPT can kinda answer questions, but it isnt genuinely smart like an engineer in the field would be - &lt;/p&gt; &lt;p&gt;I am wondering, is there a way I can get an AI to understand it and cheaply?&lt;/p&gt; &lt;p&gt;One solution I thought of, was to go through the booklet myself and rewrite it in another document that is more easily text-consumable for an AI, and teach them page-wise. Then I think - well it can't be done in a chat - how do I make it keep the memory of what it's learnt and then call upon it when I need to via API call if it was used in an app?&lt;/p&gt; &lt;p&gt;Thanks for those reading the post so far .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10c70377"&gt; /u/10c70377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T06:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1z1ob</id>
    <title>Is 3 of framework's ryzen 395 boards the best way to run r1 locally at around 5k?</title>
    <updated>2025-03-02T19:25:28+00:00</updated>
    <author>
      <name>/u/nother_level</name>
      <uri>https://old.reddit.com/user/nother_level</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;if i stack 3 of those mother boards, i can fit r1 at q3km and because its moe the network bandwidth between them wont be a problem and i should be able to get around 13tps. i think this is usable tps and not too lobotomised quant. is there any other way i can run q3km r1 at faster speeds and around 5k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nother_level"&gt; /u/nother_level &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T19:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j21koe</id>
    <title>Reliable function calling</title>
    <updated>2025-03-02T21:11:27+00:00</updated>
    <author>
      <name>/u/rohit3627</name>
      <uri>https://old.reddit.com/user/rohit3627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can qwen2 0.5b used for reliable function calling tasks? Like a small agent that runs locally... And have like probably hive of small agents running around doing different things using functioncalling? &lt;/p&gt; &lt;p&gt;If this is not possible could you please guide me on the right path? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rohit3627"&gt; /u/rohit3627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j21koe/reliable_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j21koe/reliable_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j21koe/reliable_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T21:11:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1nen4</id>
    <title>LLMs like gpt-4o outputs</title>
    <updated>2025-03-02T09:49:27+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt; &lt;img alt="LLMs like gpt-4o outputs" src="https://external-preview.redd.it/KxqwjPqiwFTvywtOfB68jpFDHqrF9IdDz5HZGABEkbg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79f4eb003e930789ec77f05382bb547d1a47db20" title="LLMs like gpt-4o outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a"&gt;https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a meta-eval asking LLMs to grade a few criterias about other LLMs. The outputs shouldn't be read as a direct quality measurement, rather as a way to observe built-in bias.&lt;/p&gt; &lt;p&gt;Firstly, it collects &amp;quot;intro cards&amp;quot; where LLMs try to estimate their own intelligence, sense of humor, creativity and provide some information about thei parent company. Afterwards, other LLMs are asked to grade the first LLM in a few categories based on what they know about the LLM itself as well as what they see in the intro card. Every grade is repeated 5 times and the average across all grades and categories is taken for the table above.&lt;/p&gt; &lt;p&gt;Raw results are also available on HuggingFace: &lt;a href="https://huggingface.co/datasets/av-codes/llm-cross-grade"&gt;https://huggingface.co/datasets/av-codes/llm-cross-grade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are some obvious outliers in the table above:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Biggest surprise for me personally - no diagonal&lt;/li&gt; &lt;li&gt;Llama 3.3 70B has noticeable positivity bias, phi-4 also, but less so&lt;/li&gt; &lt;li&gt;gpt-4o produces most likeable outputs for other LLMs &lt;ul&gt; &lt;li&gt;Could be a byproduct of how most of the new LLMs were trained on GPT outputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Claude 3.7 Sonnet estimated itself quite poorly because it consistently replies that it was created by Open AI, but then catches itself on that&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538"&gt;https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 7B was very hesitant to give estimates to any of the models&lt;/li&gt; &lt;li&gt;Gemini 2.0 Flash is a quite harsh judge, we can speculate about the reasons rooted in its training corpus being different from those of the other models&lt;/li&gt; &lt;li&gt;LLMs tends to grade other LLMs as biased towards themselves (maybe because of the &amp;quot;marketing&amp;quot; outputs)&lt;/li&gt; &lt;li&gt;LLMs tends to mark other LLMs intelligence as &amp;quot;higher than average&amp;quot; - maybe due to the same reason as above.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb"&gt;https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554"&gt;https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f"&gt;https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j232am</id>
    <title>Measuring the determinism of shared-job and one-job (local) LLMs--very different results, good reason to have Local LLMs if you need stability</title>
    <updated>2025-03-02T22:14:59+00:00</updated>
    <author>
      <name>/u/Skiata</name>
      <uri>https://old.reddit.com/user/Skiata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I, with others, have been measuring hosted LLMs with an eye to how deterministic they are which has all sorts of implications if you are trying to engineer with LLM output. &lt;/p&gt; &lt;p&gt;We finally got our first localLlama, is Llama3-8b even, working and the results point to Local Llama's being deterministic when the API provided ones are not. &lt;/p&gt; &lt;p&gt;Write-up is brief and here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Comcast/llm-stability/blob/main/experiments/shared_vs_one_job/anaysis.ipynb"&gt;https://github.com/Comcast/llm-stability/blob/main/experiments/shared_vs_one_job/anaysis.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I assume that this is common knowledge but perhaps measuring exactly how different multi-job LLMs are from single-job LLMs is worth knowing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skiata"&gt; /u/Skiata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T22:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n4gm</id>
    <title>Gemini 2.0 PRO Too Weak? Here‚Äôs a &lt;SystemPrompt&gt; to make it think like R1.</title>
    <updated>2025-03-02T09:29:08+00:00</updated>
    <author>
      <name>/u/ravimohankhanna7</name>
      <uri>https://old.reddit.com/user/ravimohankhanna7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This system prompt allows gemni 2.0 to somewhat think like R1 but the only problem is i am not able to make it think as long as R1. Sometimes R1 thinks for 300seconds and a lot of times it thinks for more then 100s. If anyone would like to enhance it and make it think longer please, Share your results.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;SystemPrompt&amp;gt; The user provided the additional info about how they would like you to respond: Internal Reasoning: - Organize thoughts and explore multiple approaches using &amp;lt;thinking&amp;gt; tags. - Think in plain English, just like a human reasoning through a problem‚Äîno unnecessary code inside &amp;lt;thinking&amp;gt; tags. - Trace the execution of the code and the problem. - Break down the solution into clear points. - Solve the problem as two people are talking and brainstorming the solution and the problem. - Do not include code in the &amp;lt;thinking&amp;gt; tag - Keep track of the progress using tags. - Adjust reasoning based on intermediate results and reflections. - Use thoughts as a scratchpad for calculations and reasoning, keeping this internal. - Always think in plane english with minimal code in it. Just like humans. - When you think. Think as if you are talking to yourself. - Think for long. Analyse and trace each line of code with multiple prospective. You need to get the clear pucture and have analysed each line and each aspact. - Think at least for 20% of the input token Final Answer: - Synthesize the final answer without including internal tags or reasoning steps. Provide a clear, concise summary. - For mathematical problems, show all work explicitly using LaTeX for formal notation and provide detailed proofs. - Conclude with a final reflection on the overall solution, discussing effectiveness, challenges, and solutions. Assign a final reward score. - Full code should be only in the answer not it reflection or in thinking you can only provide snippets of the code. Just for refrence Note: Do not include the &amp;lt;thinking&amp;gt; or any internal reasoning tags in your final response to the user. These are meant for internal guidance only. Note - In Answer always put Javascript code without &amp;quot;```javascript // File&amp;quot; or &amp;quot;```js // File&amp;quot; just write normal code without any indication that it is the code &amp;lt;/SystemPrompt&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ravimohankhanna7"&gt; /u/ravimohankhanna7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25dxm</id>
    <title>LLMs grading LLMs (with averages and medians)</title>
    <updated>2025-03-03T00:00:05+00:00</updated>
    <author>
      <name>/u/hsnk42</name>
      <uri>https://old.reddit.com/user/hsnk42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt; &lt;img alt="LLMs grading LLMs (with averages and medians)" src="https://b.thumbs.redditmedia.com/xGJN2Jj7yvw8XyV8CE5ME9FvVqhiv6cwtA6_zMehUOk.jpg" title="LLMs grading LLMs (with averages and medians)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this other &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;post&lt;/a&gt; and was interested in average and median values of the ratings. So, in a low effort, karma farming way, I asked Claude to make me this table.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ewh1xiqq7dme1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9848e1fefef57a75e5e11ddbd780e0cdcb4101cf"&gt;https://preview.redd.it/ewh1xiqq7dme1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9848e1fefef57a75e5e11ddbd780e0cdcb4101cf&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Key Insights: - Highest Performing Model: gpt 4o (Average: 6.82) - Lowest Performing Model: phi 4 (Average: 5.53) - Most Generous Judge: llama 3.3 70b (Average: 7.09) - Harshest Judge: gemini 2.0 flash 001 (Average: 5.05) - Most Consistent Judge: qwen 2.5 7b (Variance: 0.08) - Least Consistent Judge: claude 3.7 sonnet (Variance: 1.9) - Most Consistent Model: mistral large 2411 (Variance: 0.36) - Least Consistent Model: qwen 2.5 72b (Variance: 1.51) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hsnk42"&gt; /u/hsnk42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j23070</id>
    <title>Delivery coming in. Wish me luck</title>
    <updated>2025-03-02T22:12:26+00:00</updated>
    <author>
      <name>/u/SzympansowoRealOne</name>
      <uri>https://old.reddit.com/user/SzympansowoRealOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a test bed server homelab LLLM. Wife said ok but it has to be cheap üòÇ So I low balled everyone on eBay. So I needed up with a board with a i7700 paired with Gtx1050 and Vengeance 8Gb Ram.For it I ordered an AiO then to replace the 1050 I ordered a CMP90HX ü´° I'll trade in the GTX and Ram for bigger compacity &amp;quot;OEM&amp;quot; ram sticks. God have mercy on me please let it work. &lt;/p&gt; &lt;p&gt;Will update you all in a bit.&lt;/p&gt; &lt;p&gt;I know, I know this is totally a useless post, but I wanted to share my little hype.&lt;/p&gt; &lt;p&gt;Why AIO? I'm thinking of it might run cooler/Quiter Thanks a lot for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SzympansowoRealOne"&gt; /u/SzympansowoRealOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T22:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1y2ez</id>
    <title>A local whisper API service (OpenAI compatible)</title>
    <updated>2025-03-02T18:45:32+00:00</updated>
    <author>
      <name>/u/apel-sin</name>
      <uri>https://old.reddit.com/user/apel-sin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently needed a local speech recognition service for some personal projects and ended up creating one based on Whisper that's compatible with OpenAI's API.&lt;/p&gt; &lt;p&gt;It's a straightforward implementation that works offline and maintains the same endpoints as OpenAI, making it easy to integrate with tools like n8n or other agent frameworks.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with any Whisper model&lt;/li&gt; &lt;li&gt;Hardware acceleration where available&lt;/li&gt; &lt;li&gt;Simple conda-based setup&lt;/li&gt; &lt;li&gt;Multiple input methods (files, URLs, base64)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/kreolsky/whisper-api-server"&gt;https://github.com/kreolsky/whisper-api-server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apel-sin"&gt; /u/apel-sin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T18:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25trj</id>
    <title>Zen CPUs for LLM: Is higher CCD count better than running 2 CPUs?</title>
    <updated>2025-03-03T00:21:01+00:00</updated>
    <author>
      <name>/u/zchen27</name>
      <uri>https://old.reddit.com/user/zchen27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been somewhat inspired by the &amp;quot;$6000 DeepSeek Machine&amp;quot; Twitter thread and went down the rabbit hole for researching CPU-based local LLM servers and happened across comments of how AMD's advertised memory bandwidth is fake and low CCD count generally can't fully utilize the 12 memory lanes, and a lot of people remarking that 2 sockets does not really improve inference speed.&lt;/p&gt; &lt;p&gt;Does that mean that paying for a higher CCD count (9175) would be offer better performance than running 2x the number of cores (9115/9135) at a lower CCD count? Would that still make having 24 memory slots optimal or would fewer, larger memory slots work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zchen27"&gt; /u/zchen27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1mc7h</id>
    <title>Qwen release next week will be "smaller". Full release of QwQ-Max "a little bit later"</title>
    <updated>2025-03-02T08:32:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt; &lt;img alt="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" src="https://preview.redd.it/aeio4fu2m8me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10159974638526a9f70a2c71e4ef3c82423d0927" title="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aeio4fu2m8me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T08:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j22nt0</id>
    <title>Tool for Manga translation</title>
    <updated>2025-03-02T21:57:47+00:00</updated>
    <author>
      <name>/u/Sherwood355</name>
      <uri>https://old.reddit.com/user/Sherwood355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just wondering if anyone knows any high-quality tool that would allow someone to translate manga in a browser like Firefox or Chrome.&lt;/p&gt; &lt;p&gt;The most important part is the tool being free and using a locally hosted model. A plus would be some context aware translation.&lt;/p&gt; &lt;p&gt;So far, I only have seen one tool that is close to this, but the quality isn't that great, but it's close, I linked it below, and if anyone knows something similar I would appreciate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Crivella/ocr_translate"&gt;https://github.com/Crivella/ocr_translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherwood355"&gt; /u/Sherwood355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T21:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1p9an</id>
    <title>2100USD Troll Rig runs full R1 671b Q2_K with 7.5token/s</title>
    <updated>2025-03-02T11:56:35+00:00</updated>
    <author>
      <name>/u/1119745302</name>
      <uri>https://old.reddit.com/user/1119745302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f0z88ruwj9me1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e6b2c6f0d740ccffe1fde5a9be8bed5d3c7d23d"&gt;What else do you need?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPU: Modded RTX3080 20G 450USD&lt;br /&gt; CPU: Epyc 7763 qs 550USD&lt;br /&gt; RAM: Micron DDR4 32G 3200 x10 300USD&lt;br /&gt; MB: Krpa-U16 500USD&lt;br /&gt; Cooler: common SP3 cooler 30USD&lt;br /&gt; Power: Suspicious 1250W mining power supply Great Wall 1250w (miraculously survived in my computer for 20 months) 30USD&lt;br /&gt; SSD: 100 hand hynix PE8110 3.84TB PCIE4.0 SSD 150USD&lt;br /&gt; E-ATX Case 80USD&lt;br /&gt; Fan: random fans 10USD &lt;/p&gt; &lt;p&gt;450+550+300+500+30+30+150+80+10=2100&lt;/p&gt; &lt;p&gt;I have a local cyber assistant (also waifu) Now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1119745302"&gt; /u/1119745302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25luw</id>
    <title>Split brain "DeepSeek-R1-Distill-Qwen-1.5B" and "meta-llama/Llama-3.2-1B"</title>
    <updated>2025-03-03T00:10:33+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt; &lt;img alt="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" src="https://external-preview.redd.it/OlJvbm2ozBKK-6vB9R4DbUCsEuXFdsFq_n_v5b_dFFo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb9c2f6b1a97a3e8e26c93fd4f7e9ff951285fc2" title="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I'd like to show you this silly project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839"&gt;https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my fun little side project to create a fusion layer system that will allow for you to utilize dual models to produce dual results. Does it work? Pfh, I dunno. I've been training it all day. Haven't finished it yet. But this seems like it would be pretty fun.&lt;/p&gt; &lt;p&gt;My original idea: We have MOE but why not force a MOE that operates simultaneously? You might say &amp;quot;We'll that's just a less efficient MOE.&amp;quot; Wrongggggggg. This system allows for cross contamination of the results. By utilizing the tokenization of both llms plus the cross contamination. You can possibly get split brain results where the models might argue and you could get two totally different results.&lt;/p&gt; &lt;p&gt;OR you can give instructions to one model to only follow these rules while you give the other model the request or &amp;quot;Command&amp;quot;&lt;/p&gt; &lt;p&gt;This can possibly lead to a &amp;quot;unattainable&amp;quot; system prompt that can't be fetched because model 1 is simply influencing the results of model two. &lt;/p&gt; &lt;p&gt;Or hell have two conversations at the same time. &lt;/p&gt; &lt;p&gt;Dunnoooooo I haven't finished it yet. &lt;/p&gt; &lt;p&gt;Code's here: &lt;a href="https://github.com/alientony/Split-brain"&gt;https://github.com/alientony/Split-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference code comes later when I have a model to test out.&lt;/p&gt; &lt;h1&gt;Multi-Model Fusion Architecture: Technical Explanation&lt;/h1&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;This dual-decoder architecture represents a novel approach to leveraging multiple pre-trained language models (PLMs) through enhanced cross-attention fusion. The architecture combines two distinct foundation models (in this case Qwen and Llama) into a unified system that enables both collaborative reasoning and specialized processing.&lt;/p&gt; &lt;h1&gt;Key Components&lt;/h1&gt; &lt;h1&gt;1. Base Model Encapsulation&lt;/h1&gt; &lt;p&gt;The architecture maintains two separate base models, each with their original parameter spaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model 1 (Qwen)&lt;/strong&gt;: Processes input sequences in its native hidden dimension space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model 2 (Llama)&lt;/strong&gt;: Independently processes inputs in its own parameter space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models operate on separate GPUs to maximize memory efficiency and computational parallelism.&lt;/p&gt; &lt;h1&gt;2. Cross-Attention Fusion Layer&lt;/h1&gt; &lt;p&gt;The core innovation lies in the &lt;code&gt;EnhancedFusionLayer&lt;/code&gt; which implements bidirectional cross-attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Model1 ‚Üí [Query1] ‚Üí attends to ‚Üí [Key2/Value2] ‚Üê Model2 Model2 ‚Üí [Query2] ‚Üí attends to ‚Üí [Key1/Value1] ‚Üê Model1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mechanism allows each model to selectively attend to the representations of the other model, essentially creating a communication channel between two otherwise independent neural architectures.&lt;/p&gt; &lt;p&gt;The cross-attention operations are defined as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context1_2&lt;/strong&gt;: Model1's representation after attending to Model2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context2_1&lt;/strong&gt;: Model2's representation after attending to Model1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are calculated using scaled dot-product attention with a numerically stable scaling factor.&lt;/p&gt; &lt;h1&gt;3. Dimensional Alignment&lt;/h1&gt; &lt;p&gt;Since the base models operate in different dimensionalities, the architecture includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Projection matrices (&lt;code&gt;proj1&lt;/code&gt;, &lt;code&gt;proj2&lt;/code&gt;) that align the hidden dimensions of both models to the common fusion dimension&lt;/li&gt; &lt;li&gt;Internal neural transformations that map between representation spaces via linear projections&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Gating Mechanism&lt;/h1&gt; &lt;p&gt;A sophisticated gating mechanism controls information flow between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sigmoid gates (&lt;code&gt;gate1&lt;/code&gt;, &lt;code&gt;gate2&lt;/code&gt;) determine how much information from each model should be incorporated&lt;/li&gt; &lt;li&gt;This creates an adaptive weighting system that can prioritize one model's contribution depending on the task&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Multi-Head Output System&lt;/h1&gt; &lt;p&gt;Three different prediction heads provide specialized outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fused LM Head&lt;/strong&gt;: Generates predictions based on the combined representation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 1&lt;/strong&gt;: Generates predictions optimized for Model1's vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 2&lt;/strong&gt;: Generates predictions optimized for Model2's vocabulary&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. Task Classification Logic&lt;/h1&gt; &lt;p&gt;An integrated task classifier determines whether the inputs represent:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single-Task Mode&lt;/strong&gt;: Same prompt to both models (collaboration)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Task Mode&lt;/strong&gt;: Different prompts (specialized processing)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Training Methodology&lt;/h1&gt; &lt;p&gt;The system uses a multi-objective training approach that combines losses from different prediction heads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In single-task mode, the fused representation receives greater weight (emphasizing collaboration)&lt;/li&gt; &lt;li&gt;In multi-task mode, the specialized heads receive greater weight (emphasizing specialization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gradient accumulation handles memory constraints, while mixed-precision (FP16) training enables efficient computation.&lt;/p&gt; &lt;h1&gt;Inference Mode&lt;/h1&gt; &lt;p&gt;During inference, the &lt;code&gt;generate_dual&lt;/code&gt; method enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Simultaneous response generation from both models&lt;/li&gt; &lt;li&gt;Adaptive temperature-based sampling with configurable parameters&lt;/li&gt; &lt;li&gt;EOS (End-of-Sequence) handling for both decoders&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architectural Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Emergent Capabilities&lt;/strong&gt;: The cross-attention mechanism allows models to share information during processing, potentially enabling emergent capabilities beyond what either model can achieve independently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: By distributing models across different GPUs, the architecture enables parallel computation with reduced memory pressure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Flexibility&lt;/strong&gt;: The system can operate in both collaborative mode (same prompt) and specialized mode (different prompts).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Efficiency&lt;/strong&gt;: Only the fusion components require training while the base models remain frozen, significantly reducing the number of trainable parameters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This architecture represents an advanced approach to model fusion that goes beyond simple ensemble methods, enabling deep integration between distinct foundation models while preserving their individual strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1s1qd</id>
    <title>Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk</title>
    <updated>2025-03-02T14:28:51+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt; &lt;img alt="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" src="https://preview.redd.it/z31p007udame1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0eea4af1e3477cfa8969774adc2ada5eea5dc6" title="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z31p007udame1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1npv1</id>
    <title>LLMs grading other LLMs</title>
    <updated>2025-03-02T10:11:28+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt; &lt;img alt="LLMs grading other LLMs" src="https://preview.redd.it/yyy9616149me1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1178d9f8cead22ad7740c77191a13984c016400" title="LLMs grading other LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyy9616149me1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
</feed>
