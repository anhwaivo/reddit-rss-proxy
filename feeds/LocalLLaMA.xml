<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-06T15:25:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kg6ce3</id>
    <title>Gemini use multiple api keys.</title>
    <updated>2025-05-06T14:47:39+00:00</updated>
    <author>
      <name>/u/Senior-Raspberry-929</name>
      <uri>https://old.reddit.com/user/Senior-Raspberry-929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are working on any project whether it is generating data set for fine-tuning or anything that uses gemini really. I made a python package that allows you to use multiple API keys to increase your rate limit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/johnmalek312/gemini_rotator"&gt;johnmalek312/gemini_rotator: Don't get dizzy üòµ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Important: please do not abuse.&lt;/p&gt; &lt;p&gt;Edit: would highly appreciate a star&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior-Raspberry-929"&gt; /u/Senior-Raspberry-929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6ce3/gemini_use_multiple_api_keys/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6ce3/gemini_use_multiple_api_keys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6ce3/gemini_use_multiple_api_keys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T14:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg6tk3</id>
    <title>Model swapping with vLLM</title>
    <updated>2025-05-06T15:07:10+00:00</updated>
    <author>
      <name>/u/Nightlyside</name>
      <uri>https://old.reddit.com/user/Nightlyside</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running a small 2 GPU setup with ollama on it. Today, I tried to switch to vLLM with LiteLLM as a proxy/gateway for the models I'm hosting, however I can't figure out how to properly do swapping.&lt;/p&gt; &lt;p&gt;I really liked the fact new models can be loaded on the GPU provided there is enough VRAM to load the model with the context and some cache, and unload models when I receive a request for a new model not currently loaded. (So I can keep 7-8 models in my &amp;quot;stock&amp;quot; and load 4 different at the same time).&lt;/p&gt; &lt;p&gt;I found &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; and I think I can make something that look likes this with swap groups, but as I'm using the official vllm docker image, I couldn't find a great way to start the server.&lt;/p&gt; &lt;p&gt;I'd happily take any suggestions or criticism for what I'm trying to achieve and hope someone managed to make this kind of setup work. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nightlyside"&gt; /u/Nightlyside &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6tk3/model_swapping_with_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6tk3/model_swapping_with_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg6tk3/model_swapping_with_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T15:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffj42</id>
    <title>New Qwen3-32B-AWQ (Activation-aware Weight Quantization)</title>
    <updated>2025-05-05T16:11:31+00:00</updated>
    <author>
      <name>/u/jbaenaxd</name>
      <uri>https://old.reddit.com/user/jbaenaxd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt; &lt;img alt="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" src="https://external-preview.redd.it/-aaTUrK8hOTrBZTDUSYGah4_Rjpn4rU7szaPy5gCq8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69449e796c503e08a80bc47f50ab28640b3a7384" title="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10"&gt;https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen released this 3 days ago and no one noticed. These new models look great for running in local. This technique was used in Gemma 3 and it was great. Waiting for someone to add them to Ollama, so we can easily try them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1918353505074725363"&gt;https://x.com/Alibaba_Qwen/status/1918353505074725363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbaenaxd"&gt; /u/jbaenaxd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf9i52</id>
    <title>RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI</title>
    <updated>2025-05-05T11:42:02+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt; &lt;img alt="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" src="https://a.thumbs.redditmedia.com/ZP_Pe9inInMfd_-rxiw6xfzYHLp5iOazS6Ztzjzk5U4.jpg" title="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently grabbed an RTX 5060 Ti 16GB for ‚Äújust‚Äù $499 - while it‚Äôs no one‚Äôs first choice for gaming (reviews are pretty harsh), for AI workloads? This card might be a hidden gem.&lt;/p&gt; &lt;p&gt;I mainly wanted those 16GB of VRAM to fit bigger models, and it actually worked out. Ran LightRAG to ingest this beefy PDF: &lt;a href="https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf"&gt;https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared it with a 12GB GPU (RTX 3060 Ti 12GB) - and I‚Äôve attached Grafana charts showing GPU utilization for both runs.&lt;/p&gt; &lt;p&gt;üü¢ 16GB card: finished in 3 min 29 sec (green line) üü° 12GB card: took 8 min 52 sec (yellow line)&lt;/p&gt; &lt;p&gt;Logs showed the 16GB card could load all 41 layers, while the 12GB one only managed 31. The rest had to be constantly swapped in and out - crushing performance by 2x and leading to underutilizing the GPU (as clearly seen in the Grafana metrics).&lt;/p&gt; &lt;p&gt;LightRAG uses ‚ÄúMistral Nemo Instruct 12B‚Äù, served via Ollama, if you‚Äôre curious.&lt;/p&gt; &lt;p&gt;TL;DR: 16GB+ VRAM saves serious time.&lt;/p&gt; &lt;p&gt;Bonus: the card is noticeably shorter than others ‚Äî it has 2 coolers instead of the usual 3, thanks to using PCIe x8 instead of x16. Great for small form factor builds or neat home AI setups. I‚Äôm planning one myself (please share yours if you‚Äôre building something similar!).&lt;/p&gt; &lt;p&gt;And yep - I had written a full guide earlier on how to go from clean bare metal to fully functional LightRAG setup in minutes. Fully automated, just follow the steps: üëâ &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you try this setup or run into issues - happy to help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kf9i52"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfebga</id>
    <title>Open WebUI license change : no longer OSI approved ?</title>
    <updated>2025-05-05T15:23:12+00:00</updated>
    <author>
      <name>/u/CroquetteLauncher</name>
      <uri>https://old.reddit.com/user/CroquetteLauncher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Open WebUI has proved an excellent tool, with a permissive license, I have noticed the new release do not seem to use an &lt;a href="https://opensource.org/licenses"&gt;OSI approved license&lt;/a&gt; and require a contributor license agreement.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openwebui.com/license/"&gt;https://docs.openwebui.com/license/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I understand the reasoning, but i wish they could find other way to enforce contribution, without moving away from an open source license. Some OSI approved license enforce even more sharing back for service providers (AGPL).&lt;/p&gt; &lt;p&gt;The FAQ &amp;quot;6. Does this mean Open WebUI is ‚Äúno longer open source‚Äù? -&amp;gt; No, not at all.&amp;quot; is missing the point. Even if you have good and fair reasons to restrict usage, it does not mean that you can claim to still be open source. I asked Gemini pro 2.5 preview, Mistral 3.1 and Gemma 3 and they tell me that no, the new license is not opensource / freesoftware.&lt;/p&gt; &lt;p&gt;For now it's totally reasonable, but If there are some other good reasons to add restrictions in the future, and a CLA that say &amp;quot;we can add any restriction to your code&amp;quot;, it worry me a bit.&lt;/p&gt; &lt;p&gt;I'm still a fan of the project, but a bit more worried than before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CroquetteLauncher"&gt; /u/CroquetteLauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfv4az</id>
    <title>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning</title>
    <updated>2025-05-06T03:37:36+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfv4az/r1reward_training_multimodal_reward_model_through/"&gt; &lt;img alt="R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning" src="https://external-preview.redd.it/DRUyBXGCdAbmSo-nJrtkAf7BikOvApJQt5EoUxj_MX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfd0e976dcabe6674bcf24ee8c68272ca23e30bf" title="R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/yfzhang114/r1_reward"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfv4az/r1reward_training_multimodal_reward_model_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfv4az/r1reward_training_multimodal_reward_model_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T03:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg5m5a</id>
    <title>Qwen3 14b vs the new Phi 4 Reasoning model</title>
    <updated>2025-05-06T14:17:20+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im about to run my own set of personal tests to compare the two but was wondering what everyone else's experiences have been so far. Seen and heard good things about the new qwen model, but almost nothing on the new phi model. Also looking for any third party benchmarks that have both in them, I havent really been able to find any myself. I like &lt;a href="/u/_sqrkl"&gt;u/_sqrkl&lt;/a&gt; benchmarks but they seem to have omitted the smaller qwen models from the creative writing benchmark and phi 4 thinking completely in the rest. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/Phi-4-reasoning"&gt;https://huggingface.co/microsoft/Phi-4-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;https://huggingface.co/Qwen/Qwen3-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T14:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfjhlv</id>
    <title>This is how small models single-handedly beat all the big ones in benchmarks...</title>
    <updated>2025-05-05T18:47:34+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"&gt; &lt;img alt="This is how small models single-handedly beat all the big ones in benchmarks..." src="https://preview.redd.it/kammsi5ce0ze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8bb724a8000281ed4503c39eeec9365cc1bf41c" title="This is how small models single-handedly beat all the big ones in benchmarks..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you ever wondered how do the small models always beat the big models in the benchmarks, this is how...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kammsi5ce0ze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T18:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfrcul</id>
    <title>Qwen 3 Small Models: 0.6B, 1.7B &amp; 4B compared with Gemma 3</title>
    <updated>2025-05-06T00:22:44+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtube.com/watch?v=v8fBtLdvaBM&amp;amp;si=L_xzVrmeAjcmOKLK"&gt;https://youtube.com/watch?v=v8fBtLdvaBM&amp;amp;si=L_xzVrmeAjcmOKLK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I compare the performance of smaller Qwen 3 models (0.6B, 1.7B, and 4B) against Gemma 3 models on various tests. &lt;/p&gt; &lt;p&gt;TLDR: Qwen 3 4b outperforms Gemma 3 12B on 2 of the tests and comes in close on 2. It outperforms Gemma 3 4b on all tests. These tests were done without reasoning, for an apples to apples with Gemma. &lt;/p&gt; &lt;p&gt;This is the first time I have seen a 4B model actually acheive a respectable score on many of the tests. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;0.6B Model&lt;/th&gt; &lt;th align="left"&gt;1.7B Model&lt;/th&gt; &lt;th align="left"&gt;4B Model&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Harmful Question Detection&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Named Entity Recognition&lt;/td&gt; &lt;td align="left"&gt;Did not perform well&lt;/td&gt; &lt;td align="left"&gt;45%&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SQL Code Generation&lt;/td&gt; &lt;td align="left"&gt;45%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Retrieval Augmented Generation&lt;/td&gt; &lt;td align="left"&gt;37%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T00:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg05o4</id>
    <title>Building an NSFW AI App: Seeking Guidance on Integrating Text-to-Text</title>
    <updated>2025-05-06T09:23:17+00:00</updated>
    <author>
      <name>/u/ZookeepergameOk1689</name>
      <uri>https://old.reddit.com/user/ZookeepergameOk1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm developing an NSFW app and looking to integrate AI functionalities and I‚Äôm particularly interested in text-to-text: I‚Äôve been considering Qwen3,does anyone have experience with it? How does it perform, especially in NSFW contexts? I‚Äôm using Windsurf as my development environment. If anyone has experience integrating these types of APIs or can point me toward helpful resources, tutorials, or documentation, I‚Äôd greatly appreciate it.&lt;/p&gt; &lt;p&gt;Also, if someone is open to mentoring or assisting me when I encounter challenges, that would be fantastic.‚ú®&lt;/p&gt; &lt;p&gt;Thanks in advance for your support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZookeepergameOk1689"&gt; /u/ZookeepergameOk1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg05o4/building_an_nsfw_ai_app_seeking_guidance_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg05o4/building_an_nsfw_ai_app_seeking_guidance_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg05o4/building_an_nsfw_ai_app_seeking_guidance_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T09:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffq2u</id>
    <title>Qwen 3 235b gets high score in LiveCodeBench</title>
    <updated>2025-05-05T16:19:04+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt; &lt;img alt="Qwen 3 235b gets high score in LiveCodeBench" src="https://preview.redd.it/px3okqrznzye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c94891166c70bc3a59e886ae5359d04bdf3d33af" title="Qwen 3 235b gets high score in LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/px3okqrznzye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfor2i</id>
    <title>RTX PRO 6000 now available at ‚Ç¨9000</title>
    <updated>2025-05-05T22:20:52+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-gpus-now-available-starting-at-e9000"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfor2i/rtx_pro_6000_now_available_at_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfor2i/rtx_pro_6000_now_available_at_9000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg5j75</id>
    <title>What is the best local AI model for coding?</title>
    <updated>2025-05-06T14:13:51+00:00</updated>
    <author>
      <name>/u/deadcoder0904</name>
      <uri>https://old.reddit.com/user/deadcoder0904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking mostly for Javascript/Typescript.&lt;/p&gt; &lt;p&gt;And Frontend (HTML/CSS) + Backend (Node) if there are any good ones specifically at Tailwind.&lt;/p&gt; &lt;p&gt;Is there any model that is top-tier now? I read a thread from 3 months ago that said Qwen 2.5-Coder-32B but Qwen 3 just released so was thinking I should download that directly.&lt;/p&gt; &lt;p&gt;But then I saw in LMStudio that there is no Qwen 3 Coder yet. So alternatives for right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deadcoder0904"&gt; /u/deadcoder0904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T14:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfmoyx</id>
    <title>Qwen3 235b pairs EXTREMELY well with a MacBook</title>
    <updated>2025-05-05T20:55:14+00:00</updated>
    <author>
      <name>/u/Ashefromapex</name>
      <uri>https://old.reddit.com/user/Ashefromapex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried the new Qwen3 MoEs on my MacBook m4 max 128gb, and I was expecting speedy inference but I was blown out off the water. On the smaller MoE at q8 I get approx. 75 tok/s on the mlx version which is insane compared to &amp;quot;only&amp;quot; 15 on a 32b dense model. &lt;/p&gt; &lt;p&gt;Not expecting great results tbh, I loaded a q3 quant of the 235b version, eating up 100 gigs of ram. And to my surprise it got almost 30 (!!) tok/s. &lt;/p&gt; &lt;p&gt;That is actually extremely usable, especially for coding tasks, where it seems to be performing great. &lt;/p&gt; &lt;p&gt;This model might actually be the perfect match for apple silicon and especially the 128gb MacBooks. It brings decent knowledge but at INSANE speeds compared to dense models. Also 100 gb of ram usage is a pretty big hit, but it leaves enough room for an IDE and background apps which is mind blowing.&lt;/p&gt; &lt;p&gt;In the next days I will look at doing more in depth benchmarks once I find the time, but for the time being I thought this would be of interest since I haven't heard much about Owen3 on apple silicon yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashefromapex"&gt; /u/Ashefromapex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T20:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg1rgx</id>
    <title>Gemini 2.5 context wierdness on fiction.livebench?? ü§®</title>
    <updated>2025-05-06T11:09:23+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg1rgx/gemini_25_context_wierdness_on_fictionlivebench/"&gt; &lt;img alt="Gemini 2.5 context wierdness on fiction.livebench?? ü§®" src="https://preview.redd.it/tf18qjsn95ze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c89835155cafef4f94fa326596cfa9cbec7a9ae8" title="Gemini 2.5 context wierdness on fiction.livebench?? ü§®" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spoiler: &lt;span class="md-spoiler-text"&gt;I gave my original post to AI for it rewrite and it was better so I kept it&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;So I saw this thing on fiction.livebench, and it said Gemini 2.5 got a 66 on 16k context but then an 86 on 32k. Kind of backwards, right? Why would it be worse with less stuff to read?&lt;/p&gt; &lt;p&gt;I was trying to make a sequel to this book I read, like 200k words. My prompt was like 4k. The first try was... meh. Not awful, but not great.&lt;/p&gt; &lt;p&gt;Then I summarized the book down to about 16k and it was WAY better! But the benchmark says 32k is even better. So, like, should I actually try to make my context &lt;em&gt;bigger&lt;/em&gt; again for it to do better? Seems weird after my first try.&lt;/p&gt; &lt;p&gt;What do you think? ü§î&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tf18qjsn95ze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg1rgx/gemini_25_context_wierdness_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg1rgx/gemini_25_context_wierdness_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T11:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg7768</id>
    <title>Nvidia to drop CUDA support for Maxwell, Pascal, and Volta GPUs with the next major Toolkit release</title>
    <updated>2025-05-06T15:22:04+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release"&gt;https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T15:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kft806</id>
    <title>Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L</title>
    <updated>2025-05-06T01:57:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt; &lt;img alt="Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L" src="https://external-preview.redd.it/Yy-F4J1DZLB-ka7GXhD-N9ToOmxtqP5_TS_ElH4PISA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=536123bdc72e4ce9ed8c25a5831a4d5645fa049d" title="Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-32B-IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;&lt;em&gt;12 hours 17 minutes and 53 seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Observation: IQ4_XS is the most efficient Q4 quant for 32B, the quality difference is minimum&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d97vf2l9l2ze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb36de1aa23008cd2574d4a71141b741c06f4fef"&gt;https://preview.redd.it/d97vf2l9l2ze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb36de1aa23008cd2574d4a71141b741c06f4fef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g5fwhfeai2ze1.png?width=1420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b1e705703af19fbdbab465029ff9a3921616eb5"&gt;https://preview.redd.it/g5fwhfeai2ze1.png?width=1420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b1e705703af19fbdbab465029ff9a3921616eb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2tgyiu3al2ze1.png?width=2188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d80bf13dfb43de8147b3b75aa2957629fedec3e"&gt;https://preview.redd.it/2tgyiu3al2ze1.png?width=2188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d80bf13dfb43de8147b3b75aa2957629fedec3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is listing the score of Qwen3 base model instead of instruct, that's why these q4 quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;gguf source:&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T01:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfz7dk</id>
    <title>Is local LLM really worth it or not?</title>
    <updated>2025-05-06T08:12:29+00:00</updated>
    <author>
      <name>/u/GregView</name>
      <uri>https://old.reddit.com/user/GregView</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to upgrade my rig, but after some calculation, it really seems not worth it. A single 4090 in my place costs around $2,900 right now. If you add up other parts and recurring electricity bills, it really seems better to just use the APIs, which let you run better models for years with all that cost.&lt;/p&gt; &lt;p&gt;The only advantage I can see from local deployment is either data privacy or latency, which are not at the top of the priority list for most ppl. Or you could call the LLM at an extreme rate, but if you factor in maintenance costs and local instabilities, that doesn‚Äôt seem worth it either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregView"&gt; /u/GregView &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfz7dk/is_local_llm_really_worth_it_or_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfz7dk/is_local_llm_really_worth_it_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfz7dk/is_local_llm_really_worth_it_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T08:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfxl36</id>
    <title>Proof of concept: Ollama chat in PowerToys Command Palette</title>
    <updated>2025-05-06T06:14:04+00:00</updated>
    <author>
      <name>/u/GGLio</name>
      <uri>https://old.reddit.com/user/GGLio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfxl36/proof_of_concept_ollama_chat_in_powertoys_command/"&gt; &lt;img alt="Proof of concept: Ollama chat in PowerToys Command Palette" src="https://external-preview.redd.it/dHQ1M3pmMjdyM3plMSvmL84iY40gCY7YnbjXn7zDVAjhPLvuGrDKqijUVYcy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27fa89ad65bdf5a852a6d75631cea234ce9749e4" title="Proof of concept: Ollama chat in PowerToys Command Palette" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Suddenly had a thought last night that if we can access LLM chatbot directly in &lt;a href="https://learn.microsoft.com/en-us/windows/powertoys/command-palette/overview"&gt;PowerToys Command Palette&lt;/a&gt; (which is basically a Windows alternative to the Mac Spotlight), I think it would be quite convenient, so I made this simple extension to chat with Ollama.&lt;/p&gt; &lt;p&gt;To be honest I think this has much more potentials, but I am not really into desktop application development. If anyone is interested, you can find the code at &lt;a href="https://github.com/LioQing/cmd-pal-ollama-extension"&gt;https://github.com/LioQing/cmd-pal-ollama-extension&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGLio"&gt; /u/GGLio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4dcuhg27r3ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfxl36/proof_of_concept_ollama_chat_in_powertoys_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfxl36/proof_of_concept_ollama_chat_in_powertoys_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T06:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfkg29</id>
    <title>Claude full system prompt with all tools is now ~25k tokens.</title>
    <updated>2025-05-05T19:25:13+00:00</updated>
    <author>
      <name>/u/StableSable</name>
      <uri>https://old.reddit.com/user/StableSable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"&gt; &lt;img alt="Claude full system prompt with all tools is now ~25k tokens." src="https://external-preview.redd.it/-gpcWCAltY66ZM29aKxaxLCWgfV5wmtUemjqB6JURhI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ac3f0d5a570d0bb7b8c1d30d521411d0c135796" title="Claude full system prompt with all tools is now ~25k tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableSable"&gt; /u/StableSable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/claude.txt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T19:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfvba4</id>
    <title>VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?</title>
    <updated>2025-05-06T03:48:44+00:00</updated>
    <author>
      <name>/u/AdOdd4004</name>
      <uri>https://old.reddit.com/user/AdOdd4004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"&gt; &lt;img alt="VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?" src="https://preview.redd.it/l8bxcpzj23ze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aabedd846224fbf7f398e436fd72a24d816f674a" title="VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Unsloth quantizations for the best balance of performance and size. Even Qwen3-4B runs impressively well with MCP tools!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; TPS (tokens per second) is just a rough ballpark from short prompt testing (e.g., one-liner questions).&lt;/p&gt; &lt;p&gt;If you‚Äôre curious about how to set up the system prompt and parameters for Qwen3-4B with MCP, feel free to check out my video:&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏è &lt;a href="https://youtu.be/N-B1rYJ61a8?si=ilQeL1sQmt-5ozRD"&gt;https://youtu.be/N-B1rYJ61a8?si=ilQeL1sQmt-5ozRD&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOdd4004"&gt; /u/AdOdd4004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l8bxcpzj23ze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T03:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg0gzt</id>
    <title>Nvidia's nemontron-ultra released</title>
    <updated>2025-05-06T09:45:36+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt; &lt;img alt="Nvidia's nemontron-ultra released" src="https://external-preview.redd.it/elH6J8bIbGaZITZWD-SJrWx2cQnvD8jIxmZYLCf2bCg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d35a89ef4644d20d16b7438637e855c4267938e" title="Nvidia's nemontron-ultra released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9yt3kbqpu4ze1.png?width=2294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f5cf17c0e9a3674092eeb2fe870a68bb499619f"&gt;benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b"&gt;https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;technical report: &lt;a href="https://arxiv.org/abs/2505.00949"&gt;https://arxiv.org/abs/2505.00949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;online chat: &lt;a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1"&gt;https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T09:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg4avg</id>
    <title>OpenWebUI license change: red flag?</title>
    <updated>2025-05-06T13:20:27+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.openwebui.com/license/"&gt;https://docs.openwebui.com/license/&lt;/a&gt; / &lt;a href="https://github.com/open-webui/open-webui/blob/main/LICENSE"&gt;https://github.com/open-webui/open-webui/blob/main/LICENSE&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Open WebUI's last update included changes to the license beyond their original BSD-3 license,&lt;br /&gt; presumably for monetization. Their reasoning is &amp;quot;other companies are running instances of our code and put their own logo on open webui. this is not what open-source is about&amp;quot;. Really? Imagine if llama.cpp did the same thing in response to ollama. I just recently made the upgrade to v0.6.6 and of course I don't have 50 active users, but it just always leaves a bad taste in my mouth when they do this, and I'm starting to wonder if I should use/make a fork instead. I know everything isn't a slippery slope but it clearly makes it more likely that this project won't be uncompromizably open-source from now on. What are you guys' thoughts on this. Am I being overdramatic? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T13:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kft5yu</id>
    <title>Qwen 14B is better than me...</title>
    <updated>2025-05-06T01:54:32+00:00</updated>
    <author>
      <name>/u/Osama_Saba</name>
      <uri>https://old.reddit.com/user/Osama_Saba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crying, what's the point of living when a 9GB file on my hard drive is batter than me at everything! &lt;/p&gt; &lt;p&gt;It expresses itself better, it codes better, knowns better math, knows how to talk to girls, and use tools that will take me hours to figure out instantly... In a useless POS, you too all are... It could even rephrase this post better than me if it tired, even in my native language &lt;/p&gt; &lt;p&gt;Maybe if you told me I'm like a 1TB I could deal with that, but 9GB???? That's so small I won't even notice that on my phone..... Not only all of that, it also writes and thinks faster than me, in different languages... I barley learned English as a 2nd language after 20 years....&lt;/p&gt; &lt;p&gt;I'm not even sure if I'm better than the 8B, but I spot it make mistakes that I won't do... But the 14? Nope, if I ever think it's wrong then it'll prove to me that it isn't...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Osama_Saba"&gt; /u/Osama_Saba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T01:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg20mu</id>
    <title>So why are we sh**ing on ollama again?</title>
    <updated>2025-05-06T11:24:42+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am asking the redditors who take a dump on ollama. I mean, pacman -S ollama ollama-cuda was everything I needed, didn't even have to touch open-webui as it comes pre-configured for ollama. It does the model swapping for me, so I don't need llama-swap or manually change the server parameters. It has its own model library, which I don't have to use since it also supports gguf models. The cli is also nice and clean, and it supports oai API as well.&lt;/p&gt; &lt;p&gt;Yes, it's annoying that it uses its own model storage format, but you can create .ggluf symlinks to these sha256 files and load them with your koboldcpp or llamacpp if needed.&lt;/p&gt; &lt;p&gt;So what's your problem? Is it bad on windows or mac?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T11:24:42+00:00</published>
  </entry>
</feed>
