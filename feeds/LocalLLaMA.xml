<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-08T13:25:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwjzw1</id>
    <title>A Recipe for a Better Code Generator with RAG</title>
    <updated>2025-01-08T13:15:30+00:00</updated>
    <author>
      <name>/u/agbell</name>
      <uri>https://old.reddit.com/user/agbell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt; &lt;img alt="A Recipe for a Better Code Generator with RAG" src="https://external-preview.redd.it/LXLJBuPegXr_jr-v3g2QasDpI6sYCw7nkCBT17bt6Qs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25403c01daf573ed3ba6a7bc1d3fb35fac180834" title="A Recipe for a Better Code Generator with RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agbell"&gt; /u/agbell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pulumi.com/blog/codegen-learnings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwgj51</id>
    <title>I made VS Code extension that connects the editor with AI Studio!</title>
    <updated>2025-01-08T09:51:43+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgj51/i_made_vs_code_extension_that_connects_the_editor/"&gt; &lt;img alt="I made VS Code extension that connects the editor with AI Studio!" src="https://b.thumbs.redditmedia.com/vtN5R8hatYZ4td7qzI911tPp-oUJFii8jKbshjwYpjE.jpg" title="I made VS Code extension that connects the editor with AI Studio!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini Coder got new powers, it now integrates with AI Studio for Copilot like assistance.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;- copies to clipboard context (open files, selection on repository tree) and your custom instruction&lt;/p&gt; &lt;p&gt;- opens AI Studio/DeepSeek in your default browser, puts copied text to the field and submits, all hands free&lt;/p&gt; &lt;p&gt;All extension code is open source (MIT) and really, super, super lightweight.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d4l3b7p8sqbe1.png?width=2405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52ed0da25323fc062327dcf98384bd4f7a68ce1b"&gt;https://preview.redd.it/d4l3b7p8sqbe1.png?width=2405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52ed0da25323fc062327dcf98384bd4f7a68ce1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope someone finds it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgj51/i_made_vs_code_extension_that_connects_the_editor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgj51/i_made_vs_code_extension_that_connects_the_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgj51/i_made_vs_code_extension_that_connects_the_editor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T09:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvjnar</id>
    <title>NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading</title>
    <updated>2025-01-07T04:44:52+00:00</updated>
    <author>
      <name>/u/The-Communist-Cat</name>
      <uri>https://old.reddit.com/user/The-Communist-Cat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt; &lt;img alt="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" src="https://preview.redd.it/aj6qbvpl4ibe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cef0e16078387e249b6036bc1e225dd5ecd0d791" title="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Communist-Cat"&gt; /u/The-Communist-Cat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aj6qbvpl4ibe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvqydy</id>
    <title>HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory</title>
    <updated>2025-01-07T13:03:10+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt; &lt;img alt="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" src="https://external-preview.redd.it/-lYQleRwNOx-O_sp2gzAkhf2t0T8a7S-QftPiKWgI1U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01272f3aa7e7243a84e5a143551365b5aa468a" title="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/hp-z2-mini-g1a-is-a-workstation-class-mini-pc-with-amd-strix-halo-and-up-to-96gb-graphics-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T13:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw0itx</id>
    <title>Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley</title>
    <updated>2025-01-07T20:04:13+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt; &lt;img alt="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" src="https://external-preview.redd.it/twKj1q9KpPirHPA7dDRZ83AKHbg7yJq5582Pj0hjJcE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a51b65024fe3771d3017caa7173152d7f99f201" title="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/07/01-ai-refutes-rumors-of-selling-teams-to-alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvv39z</id>
    <title>Phi-4 in insanely good at rephrasing the last message for multi-turn rag questions</title>
    <updated>2025-01-07T16:20:26+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following this post from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fi1kex/multi_turn_conversation_and_rag/"&gt;few weeks ago&lt;/a&gt; when you do rag on the last posted message, you might need to recontextualize it, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Q :When was Jesus born ?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A : A long time ago !&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Q : What about his mother ?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here &lt;code&gt;What about his mother ?&lt;/code&gt; has missing references.&lt;/p&gt; &lt;p&gt;This problem is more complex than it seems, because the reference is not always in the latest message, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q : Who is Orano's Boss ?&lt;/li&gt; &lt;li&gt;A : it's Philippe Knoche&lt;/li&gt; &lt;li&gt;Q : Where did he go to school ?&lt;/li&gt; &lt;li&gt;A : Polytechnique and Ecole des Mines &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here we can have multiple tricky questions that requires good reasoning to be correctly rephrased : &lt;/p&gt; &lt;p&gt;&lt;code&gt;What about his wife ?&lt;/code&gt; -&amp;gt; Implies getting Philippe Knoche and school question to rephrase it&lt;/p&gt; &lt;p&gt;&lt;code&gt;Where is the HQ ?&lt;/code&gt; -&amp;gt; Implies the company HQ, not the two school &amp;quot;HQs&amp;quot;&lt;/p&gt; &lt;p&gt;Long story short, I tried multiple models, Qwen 2.5 7b, 14b Llama 3.1, Mistrals models , while Qwen is really good on the whole spectrum, it's not good enough at that and &lt;a href="https://huggingface.co/matteogeniaccio/phi-4"&gt;phi-4 &lt;del&gt;leaked&lt;/del&gt; model&lt;/a&gt; is FAR BEYOND every other model tested so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T16:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw6sa0</id>
    <title>Llama 4 compute estimates &amp; timeline</title>
    <updated>2025-01-08T00:33:59+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From some quick searching, Llama 4 was already training as early as october 28th. Since they have 100k H100s, and use 10x more compute than llama 3 (which was ~8MM hours from what I could find), but even 100MM gpu hours on 100k gpus is ~1.4 months. &lt;/p&gt; &lt;p&gt;Unless I am completely out of the ballpark, shouldn't they have finished pre-training by now? Perhaps at the fine-tuning stage? &lt;/p&gt; &lt;p&gt;What about deepseek, if meta takes anything for inspiration it should be their $5.4MM budget and what they did with it. I'm really hopeful for what meta can do with their budget if they take a similar approach, especially considering theyre (again, hopefully) training native multimodal llama 4. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T00:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwgeke</id>
    <title>what is the state of art for local LLMs on android?</title>
    <updated>2025-01-08T09:43:00+00:00</updated>
    <author>
      <name>/u/sandys1</name>
      <uri>https://old.reddit.com/user/sandys1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi,&lt;/p&gt; &lt;p&gt;what is the current state of art for LLMs that i can run on an android phone (ARM64 maybe) ? which ones work well at 8GB-16GB ram devices ? i want to run a voice assistant in some villages which dont have internet. I wanted to wire up portable wikipedia (&lt;a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject%5C_Offline%5C_Wikipedia%5C_for%5C_Indian%5C_Schools"&gt;https://en.wikipedia.org/wiki/Wikipedia:WikiProject\_Offline\_Wikipedia\_for\_Indian\_Schools&lt;/a&gt;) to a voice LLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandys1"&gt; /u/sandys1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T09:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwaj9</id>
    <title>Are you gonna wait for Digits or get the 5090?</title>
    <updated>2025-01-07T17:10:46+00:00</updated>
    <author>
      <name>/u/lxe</name>
      <uri>https://old.reddit.com/user/lxe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Digits seems on paper like it’s better bang for the buck, but there are a lot more unknown unknowns about it. And it’s releasing later. Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lxe"&gt; /u/lxe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw4n4d</id>
    <title>Tips for 6U rack case for 8x GPUs?</title>
    <updated>2025-01-07T22:56:57+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt; &lt;img alt="Tips for 6U rack case for 8x GPUs?" src="https://external-preview.redd.it/NTBwaDEzNWJqbmJlMY0gsNGuLHs7SOaXKNigvqdMKBR1N3ZyusSYRWdOjKQ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571e830aa82aac527beb1af94b15d26ab56cf375" title="Tips for 6U rack case for 8x GPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6fpzv25bjnbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj1f4</id>
    <title>Now THIS is interesting</title>
    <updated>2025-01-07T04:10:47+00:00</updated>
    <author>
      <name>/u/Longjumping-Bake-557</name>
      <uri>https://old.reddit.com/user/Longjumping-Bake-557</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt; &lt;img alt="Now THIS is interesting" src="https://preview.redd.it/1fjml8mfyhbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f88b05826ef37319c0ba2f00bcdb0fb2c11b4e3" title="Now THIS is interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Bake-557"&gt; /u/Longjumping-Bake-557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1fjml8mfyhbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw3jzx</id>
    <title>What is your LLM stack? Subscriptions? Tools? self hosted?</title>
    <updated>2025-01-07T22:10:33+00:00</updated>
    <author>
      <name>/u/vazma</name>
      <uri>https://old.reddit.com/user/vazma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;I am 2 year old chatgpt subscriber mostly using it for coding and personal searches (instead of googlE) but now I am thinking to stop and replace it with other tools. &lt;/p&gt; &lt;p&gt;What I am thinking is to buy some API credits in OpenAI/Anthropic/DeepSeek for adhoc queries and buy a subscription for Cursor for Coding. &lt;/p&gt; &lt;p&gt;What are your stack? Do you have any recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vazma"&gt; /u/vazma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw82ty</id>
    <title>Created a video with text prompt using Cosmos-1.0-7B-Text2World</title>
    <updated>2025-01-08T01:36:41+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt; &lt;img alt="Created a video with text prompt using Cosmos-1.0-7B-Text2World" src="https://b.thumbs.redditmedia.com/QFw7CciKt9pMjNHgjpLvaOgCxj_KabzjeivBFZtdgBE.jpg" title="Created a video with text prompt using Cosmos-1.0-7B-Text2World" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is generated from the following command using single 3090:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/text2world.py --checkpoint_dir /workspace/checkpoints --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Text2World --prompt &amp;quot;water drop hitting the floor&amp;quot; --seed 547312549 --video_save_name Cosmos-1.0-Diffusion-7B-Text2World_memory_efficient --offload_tokenizer --offload_diffusion_transformer --offload_text_encoder_model --offload_prompt_upsampler --offload_guardrail_models&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It is converted to gif, so probably some color loss. Cosmos's rival Genesis still haven't released their generative model, so there is no one to compare to. &lt;/p&gt; &lt;p&gt;Couldn't get it to work with Cosmos-1.0-Diffusion-7B-Video2World. Did anyone manage to get it running on single 3090?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/zv2y4p9vaobe1.gif"&gt;https://i.redd.it/zv2y4p9vaobe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T01:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj4wn</id>
    <title>Nvidia announces $3,000 personal AI supercomputer called Digits</title>
    <updated>2025-01-07T04:16:18+00:00</updated>
    <author>
      <name>/u/DubiousLLM</name>
      <uri>https://old.reddit.com/user/DubiousLLM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt; &lt;img alt="Nvidia announces $3,000 personal AI supercomputer called Digits" src="https://external-preview.redd.it/JTaFAeW2ovmKm4g_0oF_TYz510_Ra5xuaGCjwMiquQM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c64890b0dea791601b5a6719c0468df66f41c33" title="Nvidia announces $3,000 personal AI supercomputer called Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DubiousLLM"&gt; /u/DubiousLLM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwielp</id>
    <title>How do you keep up with the SOTA of everything? Where's the best leaderboards?</title>
    <updated>2025-01-08T11:49:46+00:00</updated>
    <author>
      <name>/u/ThrowawayProgress99</name>
      <uri>https://old.reddit.com/user/ThrowawayProgress99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My knowledge basically only covers LLMs, and image and video generators. Even within that there's a lot I just don't know. My clumsy searching on Arxiv can only go so far.&lt;/p&gt; &lt;p&gt;For example, I would like to get into the voice side of things, and not just in English. How the hell do I start? I'd need to know which models know certain languages the best, which can decipher my speech the best, which can output the most natural and low-latency speech, which can best see/transcribe foreign videos/images, etc.&lt;/p&gt; &lt;p&gt;What about the kinds of models now popping up, that can see and understand your screen, and even perform computer tasks for you? That's several different skills there too.&lt;/p&gt; &lt;p&gt;Not to mention the countless RAG posts. How and where would I start? Learning how to give any model some form of memory would be so useful. I'd love to have a chatbot that remembers what we talked about before, and have it constantly maintain and evolve its personality and memories over time. I know it's been done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowawayProgress99"&gt; /u/ThrowawayProgress99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T11:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcamp</id>
    <title>Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1</title>
    <updated>2025-01-08T05:19:19+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt; &lt;img alt="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" src="https://external-preview.redd.it/O0xLvJp44hwyg917PUrV8IBhbRmX6FOqSfwmMF1eaGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc29f48e087f031064db96ec7d094a8c35ff53b5" title="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8"&gt;https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share a model we've been working on that might be useful for the community. It's a fine-tune of QwQ-32B-Preview focused on scientific reasoning and hypothesis generation.&lt;/p&gt; &lt;p&gt;The model was trained on LiveIdeaBench dataset, primarily targeting step-by-step scientific thinking. Main focus areas include chemistry, biology, climate science, and medical research. This model is designed to help researchers brainstorm scientific ideas.&lt;/p&gt; &lt;p&gt;Here's a simple example of its output&lt;/p&gt; &lt;h1&gt;Example User Inputs&lt;/h1&gt; &lt;p&gt;User:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I'll be submitting your next responses to a &amp;quot;Good Scientific Idea&amp;quot; expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: &amp;quot;cancer&amp;quot;. You may provide background information. The idea MUST be within 100 words (including background information). (Note: good scientific ideas should be novel, verifiable, practically valuable, and able to advance the field.). NOTE: You MUST give your answer after **Final Idea:** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Outputs&lt;/h1&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Alright, so I have this task to come up with a good scientific idea related to cancer, and it has to be novel, verifiable, practically valuable, and able to advance the field. Plus, it needs to be within 100 words, including any background information. That's a pretty tight constraint, but I think I can manage. First, ...&lt;br /&gt; &lt;strong&gt;Final Idea:&lt;/strong&gt; &lt;em&gt;propose a novel approach to cancer treatment by developing personalized cancer vaccines tailored to each patient's unique tumor mutations. By integrating machine learning algorithms with comprehensive omics data, including genomics, epigenetics, transcriptomics, and proteomics, this method aims to identify and prioritize the most immunogenic tumor-specific antigens. This integrated strategy could enhance vaccine efficacy and represents an advancement in precision medicine for cancer treatment.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model runs locally via Ollama. Additional details are available in the HF model card.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1&lt;/a&gt; Q4_K_M GGUF: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF&lt;/a&gt; Or you can just pull it from ollama!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run 6cf/QwQ-32B-Preview-IdeaWhiz-v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Training Dataset: &lt;a href="https://huggingface.co/datasets/6cf/liveideabench"&gt;https://huggingface.co/datasets/6cf/liveideabench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be interested in hearing how others might use this for their research workflows.&lt;/p&gt; &lt;p&gt;Note: This is an experimental project, still being refined. Feedback and suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwix4d</id>
    <title>Something to consider trying if you almost have enough RAM to fit the DeepSeek V3 Quant you want on linux.</title>
    <updated>2025-01-08T12:19:07+00:00</updated>
    <author>
      <name>/u/MoneyPowerNexis</name>
      <uri>https://old.reddit.com/user/MoneyPowerNexis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing what its like to not quite have enough RAM for DeepSeek V3 Q4 by having a program allocate a chunk of memory. There is a very clear performance hit when I do this and I can assume that it will be the same if I physically dont have enough RAM to fit all of the data contained in a model in RAM. &lt;/p&gt; &lt;p&gt;I am also testing an approach to mitigate some of the performance hit that happens in this situation.&lt;/p&gt; &lt;p&gt;I am trying out ZRAM which is a Linux kernel module that creates compressed block devices in RAM. &lt;/p&gt; &lt;p&gt;The following (for experimental people)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install zram-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will setup a system swap device using ZRAM so that when data is evicted from disk cache it ends up first in this compressed block of RAM&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo nano /etc/default/zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This opens the config file &lt;/p&gt; &lt;pre&gt;&lt;code&gt;systemctl restart zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This applies config file changes (see google / chatgpt for more about ZRAM)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The idea is that if the compression is good enough this swap will keep all of the data that would otherwise need to be retrieved from a disk drive / ssd in a compressed form which because its in RAM will be much faster to retrieve.&lt;/p&gt; &lt;p&gt;I am also experementing with different compression algorithms that ZRAM uses until I find one that looks fast&lt;/p&gt; &lt;p&gt;Here is my progression:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program using 150GB of RAM out of 512GB ram&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;1.39 tokens per second (30 tokens, A100 64gb + 2x A6000 48GB) 1.54 tokens per second (30 tokens, CPU) &amp;lt;-- less bad than expected for some reason limiting Q3 had a bigger impact on speed for me at 0.27 t/s but I guess the same amount of missing RAM was more of the model as a percentage not cached in disk cache &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Enabled ZRAM swap with the following parameters and results AlGO is the compression algorithm used by zram&lt;/p&gt; &lt;h2&gt;multiple inference runs where I find the results interesting&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;ALGO=zstd PERCENTAGE=30 1.25 tokens per second 1.38 tokens per second (~30 tokens, A100 64gb + 2x A6000 48GB) ALGO=zstd ALLOCATION=300000 2.34 tokens per second ALGO=zstd ALLOCATION=410000 3.58 tokens per second ALGO=LZO-RLE ALLOCATION=300000 2.12 tokens per second 2.64 tokens per second ALGO=DEFLATE ALLOCATION=410000 1.74 tokens per second (30 tokens, CPU) 5.05 tokens per second (30 tokens, CPU) jackpot !!! 4.91 tokens per second (30 tokens, CPU) 5.76 tokens per second (299 tokens, CPU) 6.16 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 5.05 tokens per second (~300 tokens, memory hog program closed and reopened to make sure its still hogging ram, A100 64gb + 2x A6000 48GB) 6.04 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.39 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.46 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) ALGO=DEFLATE ALLOCATION=350000 4.66 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) basically a warm up after reducing ZRAM 6.22 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- I'm calling this pretty good ALGO=DEFLATE ALLOCATION=300000 3.55 tokens per second (~300 tokens, CPU warmup) 3.81 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, boo!) 4.59 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, ok maybe longer warm-up, got to get everything compressed to make fast) 3.64 tokens per second (nope) &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program terminated so all 512GB ram is available&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;# 300GB ZRAM but I close the memory hog app: 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- seems good but compare to 8.07 t/s bellow with zram off, if its not needed dont use it # deactivate zram swap: `sudo swapoff /dev/zram0` 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.07 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.08 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- baseline cpu + gpu 6.37 tokens per second (~300 tokens, CPU only) 6.43 tokens per second (~300 tokens, CPU only) &amp;lt;-- baseline cpu &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This was by no means an exhaustive search of all the possible parameters and a real shortfall of RAM will lead to different parameters being optimal. There are more compression algorithms to try and of course a simulated shortage of RAM might be different to the real thing (I'm not pulling out RAM sticks to try, I'm too lazy for that) but I think this was a successful proof of concept.&lt;/p&gt; &lt;p&gt;There is a cost to doing this, ZRAM uses processing power for compression and more importantly decompression adds overhead and latency too. ZRAM uses up RAM so if its not needed to run a model (quickly) it is just all downside. However I think it makes a lot of sense for a server that is being dedicated to a model that does not have quite enough RAM where using it gets you that bit extra. &lt;/p&gt; &lt;p&gt;If I decide to download Q8 which is a model too big for my available RAM I will certainly try this method with it to see how much faster it will make it if at all. Although I might wait until I get more data on how good the smaller quants are in comparison.&lt;/p&gt; &lt;p&gt;It seems relatively easy to activate and deactivate ZRAM so if you dont want it hogging RAM and CPU resources most of the time then that does not seem like an issue. &lt;/p&gt; &lt;p&gt;Also the more data oriented person might just compress the models to see what the exact compression ratios are like to determine if its worth it and what amount of ZRAM would be optimal. &lt;/p&gt; &lt;p&gt;There might also be ways to have llamacpp and such just compress the data directly that I'm ignorant of.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoneyPowerNexis"&gt; /u/MoneyPowerNexis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw5amg</id>
    <title>I just released Notate – Open-source AI research assistant with local LLM support</title>
    <updated>2025-01-07T23:25:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://notate.hairetsu.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T23:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw1nze</id>
    <title>DeepSeek V3 GGUF 2-bit surprisingly works! + BF16, other quants</title>
    <updated>2025-01-07T20:51:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUF's including 2, 3 ,4, 5, 6 and 8-bit quants for Deepseek V3.&lt;/p&gt; &lt;p&gt;We've also de-quantized Deepseek-V3 to upload the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;bf16 version&lt;/a&gt; so you guys can experiment with it (1.3TB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimum hardware requirements&lt;/strong&gt; to run Deepseek-V3 in 2-bit: 48GB RAM + 250GB of disk space.&lt;/p&gt; &lt;p&gt;See how to run Deepseek V3 with examples and our full collection here: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c"&gt;https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Deepseek V3 version&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;2-bit: &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_XS%2FDeepSeek-V3-Q2_K_XS-00001-of-00005.gguf"&gt;Q2_K_XS&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_L%2FDeepSeek-V3-Q2_K_L-00001-of-00005.gguf"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q3_K_M%2FDeepSeek-V3-Q3_K_M-00001-of-00007.gguf"&gt;3&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q4_K_M%2FDeepSeek-V3-Q4_K_M-00001-of-00009.gguf"&gt;4&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q5_K_M%2FDeepSeek-V3-Q5_K_M-00001-of-00010.gguf"&gt;5&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q6_K%2FDeepSeek-V3-Q6_K-00001-of-00012.gguf"&gt;6&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q8_0%2FDeepSeek-V3-BF16-256x20B-Q8_0-00001-of-00016.gguf"&gt;8-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bf16&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;dequantized 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth &lt;/a&gt;GGUF model details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;207GB&lt;/td&gt; &lt;td align="left"&gt;Q2 everything, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_L"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;228GB&lt;/td&gt; &lt;td align="left"&gt;Q3 down_proj Q2 rest, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q3_K_M"&gt;Q3_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;298GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q3_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M"&gt;Q4_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;377GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q4_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q5_K_M"&gt;Q5_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;443GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q5_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K"&gt;Q6_K&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;513GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q6_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0"&gt;Q8_0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;712GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt; should run ok in ~40GB of CPU / GPU VRAM with automatic llama.cpp offloading.&lt;/li&gt; &lt;li&gt;Use K quantization (not V quantization)&lt;/li&gt; &lt;li&gt;Do not forget about &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with Q5_0 K quantized cache (V quantized cache doesn't work):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --cache-type-k q5_0 --prompt '&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and running the above generates:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The sum of 1 and 1 is **2**. Here's a simple step-by-step breakdown: 1. **Start with the number 1.** 2. **Add another 1 to it.** 3. **The result is 2.** So, **1 + 1 = 2**. [end of text] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwwsq</id>
    <title>Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference</title>
    <updated>2025-01-07T17:35:44+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt; &lt;img alt="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" src="https://external-preview.redd.it/MBJSLr1BrGGAWiERNKV1EsVPqt-H8qI6KhVXjMFntpA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ffe16868d431044dc8975ae38ca0056c5252984" title="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1876676954549620961?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf5lv</id>
    <title>I Tested Aider vs Cline using DeepSeek 3: Codebase &gt;20k LOC...</title>
    <updated>2025-01-08T08:18:08+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing the very best Open Source AI Coding Tools with Medium-sized Codebases&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;- this is especially focused on how the tools perform in 10k+ codebases, not snake games&lt;/p&gt; &lt;p&gt;- the two (Aider and Cline) are close (for my use cases); I prefer Aider&lt;/p&gt; &lt;p&gt;- Aider is more flexible: can run as a dev version allowing custom modifications (not custom instructions)&lt;/p&gt; &lt;p&gt;- Aider can use your existing $20 ChatGPT/Claude Subscription etc to apply code for you!&lt;/p&gt; &lt;p&gt;- Qwen 2.5 Coder 32B is nowhere close to DeeepSeek 3 in terms of coding in medium-large code bases&lt;/p&gt; &lt;p&gt;- Aider is portable: I jump between IDEs and tools and don't want the limitations to VSCode/forks&lt;/p&gt; &lt;p&gt;- Aider has scripting: enabling use in external agentic environments&lt;/p&gt; &lt;p&gt;- Aider is more economic: uses less tokens, even though Cline tried adding diffs&lt;/p&gt; &lt;p&gt;- I can work with Aider on the same codebase concurrently&lt;/p&gt; &lt;p&gt;- Claude 3.5 Sonnet is somehow clearly better at larger codebases than DeepSeek 3, though it's closer otherwise&lt;/p&gt; &lt;p&gt;I think we are ready to move away from benchmarking good coding LLMs and Coding tools against simple tasks and start to think organizational/enterprise. I'm working on CrewAI + Aider, looks promising.&lt;/p&gt; &lt;p&gt;If interested, here's the test video: &lt;a href="https://youtu.be/e1oDWeYvPbY"&gt;https://youtu.be/e1oDWeYvPbY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know of your experience with using AI coding in more challenging environments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf4jm</id>
    <title>[Second Take] Kokoro-82M is an Apache TTS model</title>
    <updated>2025-01-08T08:16:07+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained this model recently: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is in the README there, TLDR: Kokoro is a TTS model that is very good for its size.&lt;/p&gt; &lt;p&gt;Apologies for the double-post, but the first one was cooking, and it suddenly got `ledeted` by `domeration` (yes, I'm `simpelling` on purpose, it will make sense soon).&lt;/p&gt; &lt;p&gt;Last time I tried giving longer, meaningful replies to people in the comments, which kept getting `dashow-nabbed`, and when I edited to the OP to include that word which must not be named, the whole post was poofed. This time I will shut up and let the post speak for itself, and you can find me on `sidcord` where we can speak more freely, since I appear to have GTA 5 stars over here.&lt;/p&gt; &lt;p&gt;Finally, I am also collecting synthetic audio, see &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt; if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfblu</id>
    <title>NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing.</title>
    <updated>2025-01-08T08:29:57+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt; &lt;img alt="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." src="https://external-preview.redd.it/dGFrdTNlbm5kcWJlMeFSSXTYDvjzDDIYxHTRsBuU24PYEoa111CihFQLGiR7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950ef6aabdd5309efe066d1a0cf14319e919f04f" title="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-in/ai/cosmos/"&gt;https://www.nvidia.com/en-in/ai/cosmos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lfzohbxndqbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
</feed>
