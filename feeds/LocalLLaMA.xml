<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-12T16:54:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mo4ep3</id>
    <title>Why MLA is not used more and companies still prefer with GQA ?</title>
    <updated>2025-08-12T09:52:00+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have been going through the code of some of the latest models and i noticed that they still use GQA like GPT-OSS and GLM-4.5 inspite people saying that Mutli-Head Latent Attentions is superior.&lt;/p&gt; &lt;p&gt;Does anyone have a reason for that ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T09:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnhgt0</id>
    <title>GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks</title>
    <updated>2025-08-11T16:19:43+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt; &lt;img alt="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" src="https://preview.redd.it/jw671veezeif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ef1b882c2760541b723f2922a88f046fea21c80" title="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released their first open models since GPT-2, and GPT-OSS-120B is now the best open-weight model on our real-world TaskBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better completion performance overall compared to other open-weight models like Kimi-K2 and DeepSeek-R1, while being roughly 1/10th the size. Cheaper, better, faster.&lt;/li&gt; &lt;li&gt;Relative to closed-source models, it performs like smaller frontier models such as o4-mini or previous-generation top tier models like Claude-3.7.&lt;/li&gt; &lt;li&gt;Clearly optimized for agentic use cases, it’s close to Sonnet-4 on our agentic benchmarks and could be a strong main agent model.&lt;/li&gt; &lt;li&gt;Works more like an action model than a chat or knowledge model. Multi-lingual performance is limited, and it hallucinates more on world knowledge, so it benefits from retrieval grounding and pairing with another model for multi-lingual scenarios.&lt;/li&gt; &lt;li&gt;Context recall is decent but weaker than top frontier models, so it’s better suited for shorter or carefully managed context windows.&lt;/li&gt; &lt;li&gt;Excels when paired with strong context engineering and agentic engineering, where each task completion reliably feeds into the next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, this model looks to be a real gem and will likely inject more energy into open-source models.&lt;/p&gt; &lt;p&gt;We’ve published the full benchmark results, including GPT-5, mini, and nano, and our task categories and eval methods here: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those building with it, anyone else seeing similar strengths/weaknesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw671veezeif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncfif</id>
    <title>GLM-4.5V (based on GLM-4.5 Air)</title>
    <updated>2025-08-11T13:04:47+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A vision-language model (VLM) in the GLM-4.5 family. Features listed in model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image reasoning&lt;/strong&gt; (scene understanding, complex multi-image analysis, spatial recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video understanding&lt;/strong&gt; (long video segmentation and event recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GUI tasks&lt;/strong&gt; (screen reading, icon recognition, desktop operation assistance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex chart &amp;amp; long document parsing&lt;/strong&gt; (research report analysis, information extraction)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grounding&lt;/strong&gt; (precise visual element localization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5V"&gt;https://huggingface.co/zai-org/GLM-4.5V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1moanma</id>
    <title>Distributed Llama supports small Qwen3 models (0.14.0)</title>
    <updated>2025-08-12T14:44:05+00:00</updated>
    <author>
      <name>/u/thisislewekonto</name>
      <uri>https://old.reddit.com/user/thisislewekonto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moanma/distributed_llama_supports_small_qwen3_models_0140/"&gt; &lt;img alt="Distributed Llama supports small Qwen3 models (0.14.0)" src="https://external-preview.redd.it/o_YzyMV3JJmaC2ZksFkog106Zxt11T4WUbVe3Fdfamk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae2da49d24c05ffd542f39a695cd188491177fb4" title="Distributed Llama supports small Qwen3 models (0.14.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thisislewekonto"&gt; /u/thisislewekonto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/releases/tag/v0.14.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moanma/distributed_llama_supports_small_qwen3_models_0140/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moanma/distributed_llama_supports_small_qwen3_models_0140/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo3hrh</id>
    <title>Gpt-oss-120b API provider comparison</title>
    <updated>2025-08-12T08:53:47+00:00</updated>
    <author>
      <name>/u/Sadman782</name>
      <uri>https://old.reddit.com/user/Sadman782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/1955102409044398415"&gt;https://x.com/ArtificialAnlys/status/1955102409044398415&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As expected, Groq performs worse. I've commented multiple times that something seemed off with their implementation, and this data appears to back that up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sadman782"&gt; /u/Sadman782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T08:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mno45o</id>
    <title>FULL LEAKED v0 by Vercel System Prompts and Internal Tools</title>
    <updated>2025-08-11T20:25:04+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 11/08/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL official v0 system prompt and internal tools. Over 13.5K tokens and 1.3K lines.&lt;/p&gt; &lt;p&gt;Check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T20:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mod98h</id>
    <title>Local Kokoro &amp; Parakeet in 1 Command Line — Fast ASR &amp; TTS on Mac (MLX)</title>
    <updated>2025-08-12T16:21:45+00:00</updated>
    <author>
      <name>/u/Invite_Nervous</name>
      <uri>https://old.reddit.com/user/Invite_Nervous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mod98h/local_kokoro_parakeet_in_1_command_line_fast_asr/"&gt; &lt;img alt="Local Kokoro &amp;amp; Parakeet in 1 Command Line — Fast ASR &amp;amp; TTS on Mac (MLX)" src="https://external-preview.redd.it/C1uRa9KjPXNK___BxsaejGE6qofqMhY-LCk10amPpBI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e202c6bd281e4565949e0234cd9d54a7d7d3da18" title="Local Kokoro &amp;amp; Parakeet in 1 Command Line — Fast ASR &amp;amp; TTS on Mac (MLX)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;ASR &amp;amp; TTS&lt;/strong&gt; model support are missing in popular local AI tools (e.g. Ollama, LMStudio) but they are very useful for on device usage too! We fixed that. &lt;/p&gt; &lt;p&gt;We’ve made it dead simple to run &lt;strong&gt;Parakeet&lt;/strong&gt; (ASR) and &lt;strong&gt;Kokoro&lt;/strong&gt; (TTS) in &lt;strong&gt;MLX&lt;/strong&gt; format on Mac — so you can easiy play with these 2 SOTA model directly on device. The speed on MLX is comparable to cloud if not faster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some use cases I found useful + fun to try:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASR + mic lets you capture random thoughts instantly, no browser needed.&lt;/li&gt; &lt;li&gt;TTS lets you &lt;em&gt;hear&lt;/em&gt; privates docs/news summaries in natural voices — all offline. Can also use it in roleplay.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to use it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We think these features makes playing with ASR &amp;amp; TTS models easy&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ASR&lt;/strong&gt;: &lt;code&gt;/mic&lt;/code&gt; mode to directly transcribe live speech in terminal, or drag in a meeting audio file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS&lt;/strong&gt;: Type prompt directly in CLI to have it read aloud a piece of news. You can also switch voices for fun local roleplay.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mod98h/video/ne999v3x3mif1/player"&gt;Demo in CLI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Download Nexa SDK at &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;https://github.com/NexaAI/nexa-sdk&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run 1 line of code in your CLI&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;ASR (Parakeet):&lt;/p&gt; &lt;p&gt;&lt;code&gt;nexa infer NexaAI/parakeet-tdt-0.6b-v2-MLX&lt;/code&gt;&lt;/p&gt; &lt;p&gt;TTS (Kokoro):&lt;/p&gt; &lt;p&gt;&lt;code&gt;nexa infer NexaAI/Kokoro-82M-bf16-MLX -p &amp;quot;Nexa AI SDK&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Shoutout to Kokoro, Parakeet devs, and MLX folks ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Invite_Nervous"&gt; /u/Invite_Nervous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mod98h/local_kokoro_parakeet_in_1_command_line_fast_asr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mod98h/local_kokoro_parakeet_in_1_command_line_fast_asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mod98h/local_kokoro_parakeet_in_1_command_line_fast_asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T16:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mobfxg</id>
    <title>Cua-Agent v0.4.11 now supports the new GLM-4.5V GUI agent model, and compositional Computer-Use agents using the GTA1 UI grounding model + any liteLLM/local VLM (with consistent output!)</title>
    <updated>2025-08-12T15:14:01+00:00</updated>
    <author>
      <name>/u/a6oo</name>
      <uri>https://old.reddit.com/user/a6oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobfxg/cuaagent_v0411_now_supports_the_new_glm45v_gui/"&gt; &lt;img alt="Cua-Agent v0.4.11 now supports the new GLM-4.5V GUI agent model, and compositional Computer-Use agents using the GTA1 UI grounding model + any liteLLM/local VLM (with consistent output!)" src="https://external-preview.redd.it/cHl1cXBwc251bGlmMeBAVCrFk63xi9bA2wzdNPc_yUbmN7B6WenUvpRT7Ueb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29aea0df00d85a2be381b2a6d47ddfd1983d435a" title="Cua-Agent v0.4.11 now supports the new GLM-4.5V GUI agent model, and compositional Computer-Use agents using the GTA1 UI grounding model + any liteLLM/local VLM (with consistent output!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a6oo"&gt; /u/a6oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/etvpyi4gtlif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobfxg/cuaagent_v0411_now_supports_the_new_glm45v_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mobfxg/cuaagent_v0411_now_supports_the_new_glm45v_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T15:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa5as</id>
    <title>Cheap coding assistant</title>
    <updated>2025-08-12T14:24:37+00:00</updated>
    <author>
      <name>/u/Evisteron</name>
      <uri>https://old.reddit.com/user/Evisteron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some say there's a paradigm shift happening in software - in coding specifically, where AI is taking the reigns. Other say it's hype, and actually everything takes longer. I have found the truth to be somewhere in between at work with tools like Cline and Claude Code (at least on work projects - they pay.)&lt;/p&gt; &lt;p&gt;But what I'm struggling with is the ability to do hobby coding with these tools, because they are so expensive. Usually, when I try to learn a new platform, or computer language, I just sort of set up a dev environment, and play - but it's different when that play is expensive.&lt;/p&gt; &lt;p&gt;So that's my ask - what is the cheapest way to run a backend for cline, or the cheapest service you've found - one that *doesn't suck?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evisteron"&gt; /u/Evisteron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mobuo7</id>
    <title>Building vllm from source for OSS support on Ampere + benchmarks</title>
    <updated>2025-08-12T15:29:30+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ampere support for oss was just merged into vllm, figured I would share since everyone seems to be struggling with vllm lately. &lt;/p&gt; &lt;p&gt;my setup: Ubuntu 24.04 server, 3090s, Nvidia 575 driver, Cuda 12.4 (should be 12.8 or 12.9 with 575, my cuda is mismatched)&lt;/p&gt; &lt;p&gt;mkdir vllm-src&lt;br /&gt; cd vllm-src&lt;br /&gt; python3.12 -m venv myenv&lt;br /&gt; source myenv/bin/activate&lt;br /&gt; pip install torch torchaudio torchvision&lt;br /&gt; git clone &lt;a href="https://github.com/vllm-project/vllm.git"&gt;https://github.com/vllm-project/vllm.git&lt;/a&gt;&lt;br /&gt; cd vllm&lt;br /&gt; python use_existing_torch.py&lt;br /&gt; pip install -r requirements/build.txt&lt;br /&gt; pip install -r requirements/cuda.txt&lt;br /&gt; python setup.py install&lt;br /&gt; cd ../&lt;br /&gt; # VLLM would usually be ready to go here, but oss needs triton_kernels:&lt;br /&gt; pip install -U &amp;quot;git+&lt;a href="https://github.com/triton-lang/triton.git@f33bcbd4f1051d0d9ea3fdfc0b2e68f53ededfe4#subdirectory=python/triton%5C_kernels"&gt;https://github.com/triton-lang/triton.git@f33bcbd4f1051d0d9ea3fdfc0b2e68f53ededfe4#subdirectory=python/triton\_kernels&lt;/a&gt;&amp;quot; &lt;/p&gt; &lt;p&gt;vllm serve openai/gpt-oss-120b -tp 4&lt;/p&gt; &lt;p&gt;On 4 3090s and single threaded I'm getting 93 T/s gen&lt;/p&gt; &lt;p&gt;With 100 threads I can get around 1500T/s gen&lt;/p&gt; &lt;p&gt;And prefill seems to do ~3000t/s (possibly limited by my pcie bandwidth)&lt;/p&gt; &lt;p&gt;Also interesting, this setup scored a 92.87% on my private cyber security benchmark, Tying o1-mini.&lt;br /&gt; Best I have got with llama.cpp so far is 87.65%, worst I have had was 70% (day 1, before any of unsloths fixes)&lt;br /&gt; I suspect we may still not have the prompt template fully figured out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobuo7/building_vllm_from_source_for_oss_support_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobuo7/building_vllm_from_source_for_oss_support_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mobuo7/building_vllm_from_source_for_oss_support_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T15:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1moc94q</id>
    <title>Cost-effective home server suggestion</title>
    <updated>2025-08-12T15:44:39+00:00</updated>
    <author>
      <name>/u/Ereptile-Disruption</name>
      <uri>https://old.reddit.com/user/Ereptile-Disruption</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Finally I have the time and resources to follow my hobby; and I'm finally making my own home server!&lt;/p&gt; &lt;p&gt;Since I have pretty much no old hardware to recycle, I want to buy something that's not too much trash tier.&lt;/p&gt; &lt;p&gt;sadly where I live energy is quiet expensive so I would love something energy efficient.&lt;/p&gt; &lt;p&gt;My target is ~70B models (or 30B with some minor 4-7B for agentic jobs)&lt;/p&gt; &lt;p&gt;I was looking at the framework desktop, that look a quiet good compromise for space/energy/money&lt;/p&gt; &lt;p&gt;Does it have some merit or P40 are still the best way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ereptile-Disruption"&gt; /u/Ereptile-Disruption &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moc94q/costeffective_home_server_suggestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moc94q/costeffective_home_server_suggestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moc94q/costeffective_home_server_suggestion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T15:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnc8lx</id>
    <title>I built Excel Add-in for Ollama</title>
    <updated>2025-08-11T12:56:39+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt; &lt;img alt="I built Excel Add-in for Ollama" src="https://preview.redd.it/mvjwf2f81eif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=17b456d91ceed7000d3f08cd2f8917aec6e4254a" title="I built Excel Add-in for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an excel add-in that connects Ollama with Microsoft Excel. Data to remain inside excel only. You can simply write function =ollama(A1), assuming prompt in cell A1. You can simply drag to run on multiple cells. It has arguments to specify system instructions, temperature and model. You can set at both global level and specific to your prompts. &lt;a href="https://www.listendata.com/2025/08/ollama-in-excel.html"&gt;https://www.listendata.com/2025/08/ollama-in-excel.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvjwf2f81eif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T12:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo5s79</id>
    <title>Why evals are the missing piece in most AI products</title>
    <updated>2025-08-12T11:11:00+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing AI teams obsess over model choice, prompts, and infrastructure, but very few invest in structured evals early. Without them, you are basically shipping blind. In my experience, good eval workflows catch issues before they hit production, shorten iteration cycles, and prevent those “works in testing, fails in prod” disasters.&lt;/p&gt; &lt;p&gt;At Maxim AI we’ve seen teams slash AI feature rollout time just by setting up continuous eval loops with both human and automated tests. If your AI product handles real user-facing tasks, you cannot rely on spot checks. You need evals that mimic the exact scenarios your users will throw at the system.&lt;/p&gt; &lt;p&gt;What’s your take, are evals an engineering must-have or just a nice-to-have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T11:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo3j17</id>
    <title>LocalAI Major Update: Modular Backends (update llama.cpp, stablediffusion.cpp, and others independently!), Qwen-VL, Qwen-Image Support, Image Editing &amp; More</title>
    <updated>2025-08-12T08:56:03+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might know LocalAI already as a way to self-host your own private, OpenAI-compatible AI API (it was the first of its kind !). I'm excited to share that we've just pushed a series of massive updates that I think this community will really appreciate. As a reminder: LocalAI is not a company, it's a Free, open source project community-driven!&lt;/p&gt; &lt;p&gt;Also, LocalAI just hit &lt;strong&gt;34.5k stars on GitHub and&lt;/strong&gt; &lt;strong&gt;LocalAGI&lt;/strong&gt; &lt;strong&gt;crossed 1k&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt; (which is, an Agentic system built on top of LocalAI) and we know a huge part of that is from power users like you. Thank you!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here’s the TL;DR on what's new (v3.2.0-v3.4.0):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Modular Backends:&lt;/strong&gt; We've completely separated the inference backends from the LocalAI core. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Smaller images:&lt;/strong&gt; You can now update backends like &lt;code&gt;llama.cpp&lt;/code&gt; , &lt;code&gt;stablediffusion.cpp&lt;/code&gt; or &lt;code&gt;diffusers&lt;/code&gt;independently! If a new version of a backend drops, you can pull it in without waiting for a new LocalAI release. It also means the core app is super lean.&lt;/li&gt; &lt;li&gt;Installation of required backends is automatic based on the model's needs and your hardware (CUDA, ROCm, SYCL, CPU-only etc.). &lt;ul&gt; &lt;li&gt;We are working now to improve CPU support for backends like &lt;code&gt;diffusers&lt;/code&gt; and the ones using pytorch, stay tuned!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Object detection, Qwen-Image, and..&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full support for powerful models like &lt;strong&gt;Qwen-VL or Qwen Image&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Now you can do image editing using text prompts with &lt;strong&gt;Flux Kontext&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;We added an additional API endpoint specifically for &lt;strong&gt;object detection&lt;/strong&gt; API, currently powered by the rfdetr (&lt;a href="https://github.com/roboflow/rf-detr"&gt;https://github.com/roboflow/rf-detr&lt;/a&gt;) backend which you can install from the backend gallery with one click, or just installing the rfdetr model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Model &amp;amp; Backend Expansion:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;The gallery now has many new models, including the &lt;strong&gt;latest from the community&lt;/strong&gt; , and Qwen Image, Flux Krea, GPT-OSS and many more!&lt;/li&gt; &lt;li&gt;We've added new TTS backends like &lt;strong&gt;KittenTTS&lt;/strong&gt;, Dia, and Kokoro if you're experimenting with voice.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to make LocalAI the most flexible, OpenAI-compatible API layer for whatever you want to run locally. These changes give you more control and faster access to the latest and greatest from the community.&lt;/p&gt; &lt;p&gt;Check out the full release notes and give it a spin: ➡️&lt;strong&gt;&lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think and what models you're planning to test with the new setup!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T08:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa5o0</id>
    <title>VulkanIlm, Run Modern LLMs on Old GPUs via Vulkan (33× Faster on Dell iGPU, 4× on RX 580)</title>
    <updated>2025-08-12T14:24:59+00:00</updated>
    <author>
      <name>/u/Proper_Dig_6618</name>
      <uri>https://old.reddit.com/user/Proper_Dig_6618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’ve been building &lt;strong&gt;VulkanIlm&lt;/strong&gt; — a Python wrapper for llama.cpp that uses Vulkan for GPU acceleration. The goal: make local LLMs faster on &lt;em&gt;any&lt;/em&gt; GPU, even older AMD and integrated ones, with &lt;strong&gt;no CUDA dependency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Some early benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dell E7250 (i7-5600U, Intel iGPU)&lt;/strong&gt;&lt;br /&gt; Model: TinyLLaMA-1.1B-Chat (Q4_K_M)&lt;br /&gt; CPU: 121 s → GPU: 3 s → &lt;strong&gt;33× speedup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AMD RX 580 (8 GB)&lt;/strong&gt;&lt;br /&gt; Model: Gemma-3n-E4B-it (6.9 B params)&lt;br /&gt; CPU: 188 s → GPU: 44 s → &lt;strong&gt;4× speedup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next steps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More benchmarks (including new OpenAI OSS models)&lt;/li&gt; &lt;li&gt;“Run LLMs on Your Old GPU” video tutorial&lt;/li&gt; &lt;li&gt;AMD GPU deep dive&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo (still under active development): &lt;a href="https://github.com/Talnz007/VulkanIlm"&gt;https://github.com/Talnz007/VulkanIlm&lt;/a&gt;&lt;br /&gt; Please try it out, contribute, or share feedback — I’m aiming to make this work well for the entire Local LLaMA community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proper_Dig_6618"&gt; /u/Proper_Dig_6618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo9vkh</id>
    <title>Microsoft releases Prompt Orchestration Markup Language</title>
    <updated>2025-08-12T14:14:04+00:00</updated>
    <author>
      <name>/u/ArtZab</name>
      <uri>https://old.reddit.com/user/ArtZab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;Just came across Microsoft’s POML (Prompt Orchestration Markup Language) and it seems like a useful tool to have.&lt;/p&gt; &lt;p&gt;From GitHub page (&lt;a href="https://github.com/microsoft/poml):"&gt;https://github.com/microsoft/poml):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;POML (Prompt Orchestration Markup Language) is a novel markup language designed to bring structure, maintainability, and versatility to advanced prompt engineering for Large Language Models (LLMs). It addresses common challenges in prompt development, such as lack of structure, complex data integration, format sensitivity, and inadequate tooling. POML provides a systematic way to organize prompt components, integrate diverse data types seamlessly, and manage presentation variations, empowering developers to create more sophisticated and reliable LLM applications.&lt;/p&gt; &lt;p&gt;What are your thoughts on this release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtZab"&gt; /u/ArtZab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnp5nc</id>
    <title>Training an LLM only on books from the 1800's - Another update</title>
    <updated>2025-08-11T21:04:34+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm training LLM's from scratch using only texts from a specific region and time period and want to share another update. Right now it's 1800-1875 London. When I first started, my dataset was only 50 texts and I was using a 4060 for training. The latest version is trained on almost 7,000 texts using Phi 1.5 (700M parameters) on an A100 GPU. My long term goal is to see if a model trained this way can actually reason. The newest model I've trained has some promising output, it's starting to reference real historical events instead of just hallucinating everything. Also many people have told me that fine tuning will be more efficient and I agree, but I want to see how far this approach can go. And Internet Archive has around 175,000 London texts within my chosen time period, so scaling the dataset won't be an issue. &lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T21:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxwmw</id>
    <title>Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot</title>
    <updated>2025-08-12T03:24:03+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt; &lt;img alt="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" src="https://b.thumbs.redditmedia.com/81joqRjngFFUavEApRYDiznp-6LcG-wUqoKaM4BcLls.jpg" title="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a"&gt;https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to gguf: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sha256: c6f818151fa2c6fbca5de1a0ceb4625b329c58595a144dc4a07365920dd32c51&lt;/p&gt; &lt;p&gt;edit: test was done with above Unsloth gguf downloaded Aug 5,&lt;/p&gt; &lt;p&gt;and with the new chat_template here: &lt;a href="https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja"&gt;https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja&lt;/a&gt;&lt;/p&gt; &lt;p&gt;newest Unsloth gguf has same link and;&lt;/p&gt; &lt;p&gt;sha256: 2d1f0298ae4b6c874d5a468598c5ce17c1763b3fea99de10b1a07df93cef014f&lt;/p&gt; &lt;p&gt;and also has an improved chat template built-in&lt;/p&gt; &lt;p&gt;currently rerunning low and medium reasoning tests with the newest gguf&lt;/p&gt; &lt;p&gt;and with the chat template built into the gguf&lt;/p&gt; &lt;p&gt;high reasoning took 2 days to run load balanced over 6 llama.cpp nodes so we will only rerun if there is a noticeable improvement with low and medium&lt;/p&gt; &lt;p&gt;high reasoning used 10x completion tokens over low, medium used 2x over low. high used 5x over medium etc. so both low and medium are much faster than high.&lt;/p&gt; &lt;p&gt;Finally here are instructions how to run locally: &lt;a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and: &lt;a href="https://aider.chat/"&gt;https://aider.chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncrqp</id>
    <title>ollama</title>
    <updated>2025-08-11T13:19:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt; &lt;img alt="ollama" src="https://preview.redd.it/2whabjm55eif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea8efc9d0fe6d86f047a62709601f55061db889" title="ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2whabjm55eif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1vre</id>
    <title>Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'.</title>
    <updated>2025-08-12T07:08:27+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt; &lt;img alt="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." src="https://preview.redd.it/2e65cn38fjif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da4ffea4883b21f3e637daf2a89cb44028cbdb31" title="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 4B got it right after thinking 30 Sec.ZLM thought for almost 2 min .GPT-5 took 5 sec.Gemini took less than 2 sec,and told me use count() function in Python which it used. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e65cn38fjif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1mb1</id>
    <title>GLM 4.5 AIR IS SO FKING GOODDD</title>
    <updated>2025-08-12T06:52:07+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got to try it with our agentic system , it's so fast and perfect with its tool calls , but mostly it's freakishly fast too , thanks z.ai i love you 😘💋&lt;/p&gt; &lt;p&gt;Edit: not running it locally, used open router to test stuff. I m just here to hype em up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1pv4</id>
    <title>Uncensored gpt-oss-20b released</title>
    <updated>2025-08-12T06:58:18+00:00</updated>
    <author>
      <name>/u/No-Solution-8341</name>
      <uri>https://old.reddit.com/user/No-Solution-8341</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt; &lt;img alt="Uncensored gpt-oss-20b released" src="https://external-preview.redd.it/P0d7BMzhU8lFm_gY9r3-Ieqcq7avVW4yk_FBxEW_Ccs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a246b29e2c888dfb88cfcf39f23a3530b26e09d" title="Uncensored gpt-oss-20b released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jinx is a &amp;quot;helpful-only&amp;quot; variant of popular open-weight language models that responds to all queries without safety refusals.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b"&gt;https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Solution-8341"&gt; /u/No-Solution-8341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1moakv3</id>
    <title>We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025</title>
    <updated>2025-08-12T14:41:09+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt; &lt;img alt="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" src="https://preview.redd.it/lcee3fueolif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc5d986a916d0445a79f4b3d5044d02c9aacef2" title="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We ran a benchmark on &lt;strong&gt;34 fresh GitHub PR tasks&lt;/strong&gt; from July 2025 using the &lt;a href="https://swe-rebench.com/leaderboard"&gt;SWE-rebench leaderboard&lt;/a&gt;. These are real, recent problems — no training-set contamination — and include both proprietary and open-source models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-5-Medium&lt;/strong&gt; leads overall (29.4% resolved rate, 38.2% pass@5).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;, matching GPT-5-High in pass@5 (32.4%) despite a lower resolved rate.&lt;/li&gt; &lt;li&gt;Claude Sonnet 4.0 lags behind in pass@5 at 23.5%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All tasks come from the continuously updated, decontaminated &lt;a href="https://huggingface.co/datasets/nebius/SWE-rebench-leaderboard"&gt;SWE-rebench-leaderboard&lt;/a&gt; dataset for real-world SWE tasks.&lt;/p&gt; &lt;p&gt;We’re already adding gpt-oss-120b and GLM-4.5 next — which OSS model should we include after that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lcee3fueolif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2gg7</id>
    <title>Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro</title>
    <updated>2025-08-12T07:45:23+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt; &lt;img alt="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" src="https://preview.redd.it/niaetccbljif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83cb98af8850f5f113bf6d7f37db6c95989b888f" title="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from Jan. We're releasing Jan v1 today. In our evals, Jan v1 delivers 91% SimpleQA accuracy, slightly outperforming Perplexity Pro while running fully locally.&lt;/p&gt; &lt;p&gt;It's built on the new version of Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;Qwen3-4B-Thinking&lt;/a&gt; (up to 256k context length), fine-tuned for reasoning and tool use in Jan.&lt;/p&gt; &lt;h1&gt;How to run it:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Jan&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download Jan v1 via Jan Hub&lt;/li&gt; &lt;li&gt;Enable search in Jan: &lt;ul&gt; &lt;li&gt;Settings → Experimental Features → On&lt;/li&gt; &lt;li&gt;Settings → MCP Servers → enable Search-related MCP (e.g. Serper)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Plus you can run the model in llama.cpp and vLLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v1-4B: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B"&gt;https://huggingface.co/janhq/Jan-v1-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v1-4B-GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B-GGUF"&gt;https://huggingface.co/janhq/Jan-v1-4B-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 0.6&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;min_p: 0.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;max_tokens: 2048&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We'd love for you to try Jan v1 and share your feedback, including what works well and where it falls short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niaetccbljif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxodk</id>
    <title>LocalLLaMA is the last sane place to discuss LLMs on this site, I swear</title>
    <updated>2025-08-12T03:12:34+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt; &lt;img alt="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" src="https://preview.redd.it/iu3pniar9iif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bac76c25e583f3690f2e1e9cdc20c74739fa84c" title="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iu3pniar9iif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
