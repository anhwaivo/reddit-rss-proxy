<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-14T19:22:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mpqew3</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:16:29+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8q83</id>
    <title>MiniLM (BERT) embeddings in C from scratch</title>
    <updated>2025-08-14T18:22:16+00:00</updated>
    <author>
      <name>/u/aby-1</name>
      <uri>https://old.reddit.com/user/aby-1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8q83/minilm_bert_embeddings_in_c_from_scratch/"&gt; &lt;img alt="MiniLM (BERT) embeddings in C from scratch" src="https://external-preview.redd.it/LNheh9xQ4Aay2gwdMLBhvmisLGNDqagZk96c3qlnYkY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2de3b3f04628710ff39d88d1317184ad9ce68df2" title="MiniLM (BERT) embeddings in C from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Distilled BERT (MiniLM) forward pass in C from scratch to get dependency-free sentence embeddings.&lt;/p&gt; &lt;p&gt;Along with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tiny tensor library (contiguous, row-major, float32)&lt;/li&gt; &lt;li&gt;.tbf tensor file format + loader&lt;/li&gt; &lt;li&gt;WordPiece tokenizer (uncased)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aby-1"&gt; /u/aby-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/abyesilyurt/minilm.c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8q83/minilm_bert_embeddings_in_c_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8q83/minilm_bert_embeddings_in_c_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mprwv9</id>
    <title>Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?</title>
    <updated>2025-08-14T05:38:43+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt; &lt;img alt="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" src="https://preview.redd.it/ydbnycjn8xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8bcb62748563efa5cb1f78789aa2cd8f3b2860a" title="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's rumors that R2 is coming up sometime in the next month. It does feel that the release of the recent proprietary models have been a bit disappointing, given the marginal gains (e.g. on my &lt;a href="https://www.designarena.ai/"&gt;frontend benchmark&lt;/a&gt;, GPT-5, Opus 4, and 4.1 are basically equivalent though there's a small sample size for the new versions. &lt;/p&gt; &lt;p&gt;In terms of recent releases, open source and open weight models have been amazing. DeepSeek R1-0528 and Qwen3 Coder are #5 and #6 respectively, while GLM 4.5 is #9. &lt;/p&gt; &lt;p&gt;I'm am interested to see what happens with R2. My prediction is that it will basically match GPT-5 and Opus 4 (perhaps might even be a bit better) and we might see a moment similar to when DeepSeek R1 came out. &lt;/p&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ydbnycjn8xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3j12</id>
    <title>[2508.09874] Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</title>
    <updated>2025-08-14T15:16:08+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.09874"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3j12/250809874_memory_decoder_a_pretrained_plugandplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3j12/250809874_memory_decoder_a_pretrained_plugandplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:16:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq9oxw</id>
    <title>Thank you Qwen/Llama.cpp/OpenWebUI/Llama-Swap...</title>
    <updated>2025-08-14T18:57:18+00:00</updated>
    <author>
      <name>/u/ValfarAlberich</name>
      <uri>https://old.reddit.com/user/ValfarAlberich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I just want to say thank you to all those teams that bets for open source, and yet believe on it! I really appreciate it and I hope to contibute to Llama.cpp in a future (yet I need to improve my C/C++ skills). I just finished the setup to run LlamaCPP through OpenWebUI using Llama-swap and it works great! is much faster than ollama, and I have access to all the features of LlamaCPP. I just wanted to express my gratitude to all the open source communities, and teams that are building a better world!&lt;br /&gt; I'm from Latin America, and soon my subscription to ChatGPT will be finished, and right now my priorities are more focused to bring food for my family, so I cannot continue paying it. For that reason I migrated completelly to open source solutions, and I'm so happy with it!&lt;/p&gt; &lt;p&gt;Also big thanks to the team behind QWEN! you're rocking guys! yesterday I replaced Claude Code by Qwen Code and it's amazing! in fact per my experience it's much better, here some reasons behind that affirmation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; Claude code uses haiku most of the time, doesn't matter if you are paying $20 per month, you can see that when you type /logout and you'll see the stats about its usage. &lt;/li&gt; &lt;li&gt;Right now there are open source models that overpass Claude Haiku that can be used with Qwen Code. &lt;/li&gt; &lt;li&gt;Right now Qwen is offering a qwen-coder-plus through qwen-code for free the first 2000 requests per day.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;After multiple tests, Qwen works much better than Claude, of course if you want only to vibe code and left it do everything for you, none of those will completelly solve your needs. Is important to work with them, and use them as support. &lt;/li&gt; &lt;li&gt;Information privacy always has been critical for me, and I know how all those companies like OpenAI, Claude, Google, Allibaba could use our information and interactions with the models, for their purposes. But after meditating a long time about it I prefer to give my data to teams and companies that widely support the open source scene. At least they would use the data to improve models that are releasing to the community, so indirectly I feel that I'm helping the open source initiative.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks again to those who work really hard and believe in the open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValfarAlberich"&gt; /u/ValfarAlberich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5zjv</id>
    <title>GLM-4.1V-Thinking and GLM-4.5V</title>
    <updated>2025-08-14T16:45:58+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.01006"&gt;https://arxiv.org/pdf/2507.01006&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/zai_org/status/1956030993569341556?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq7715</id>
    <title>PapersWithPRs: Don't Just Read the Paper, Replicate the Results</title>
    <updated>2025-08-14T17:28:56+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7715/paperswithprs_dont_just_read_the_paper_replicate/"&gt; &lt;img alt="PapersWithPRs: Don't Just Read the Paper, Replicate the Results" src="https://preview.redd.it/cayof7tiq0jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74727bb929a8aed2277fd3572977e7ebf1f53999" title="PapersWithPRs: Don't Just Read the Paper, Replicate the Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hundreds of papers are published to the arXiv each day, documenting experiments to advance our understanding of AI. &lt;/p&gt; &lt;p&gt;I want to keep up with the volume of results to optimize fine-tuning, improve data curation, speed up inference, and boost domain adaptations, so I'm using AI to find the pearls for my application.&lt;/p&gt; &lt;p&gt;To operationalize my experimentation, I've been using agents to recommend arXiv papers relevant to what I'm building and create a Docker Image to run a minimal quickstart example.&lt;/p&gt; &lt;p&gt;Check out the Images on Docker Hub: &lt;a href="https://hub.docker.com/repositories/remyxai"&gt;https://hub.docker.com/repositories/remyxai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, I'm tasking the ExperimentOps agent to open PRs in my target repo, testing potential improvements into my codebase.&lt;/p&gt; &lt;p&gt;Read more:&lt;br /&gt; &lt;a href="https://remyxai.substack.com/p/paperswithprs"&gt;https://remyxai.substack.com/p/paperswithprs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cayof7tiq0jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7715/paperswithprs_dont_just_read_the_paper_replicate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7715/paperswithprs_dont_just_read_the_paper_replicate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T17:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq540c</id>
    <title>promptcat: A zero-dependency prompt manager in a single HTML file</title>
    <updated>2025-08-14T16:14:08+00:00</updated>
    <author>
      <name>/u/seven_reasons</name>
      <uri>https://old.reddit.com/user/seven_reasons</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"&gt; &lt;img alt="promptcat: A zero-dependency prompt manager in a single HTML file" src="https://b.thumbs.redditmedia.com/OBDg7zqijdse5-ktq6w0c4oXeYE0oGFX0MTePB4-3Ac.jpg" title="promptcat: A zero-dependency prompt manager in a single HTML file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A private, offline-first prompt manager in a single, dependency-free HTML file. It stores all data locally in your browser's IndexedDB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Offline:&lt;/strong&gt; All data is stored in your browser's IndexedDB.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Dependencies:&lt;/strong&gt; Just pure, vanilla JavaScript, HTML, and CSS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong Encryption:&lt;/strong&gt; Optional AES-GCM encryption (via Web Crypto API) for individual prompts or entire folders. Your password is never stored.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powerful Organization:&lt;/strong&gt; Use folders, favorites, and tags to structure your library.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Global Tag Management:&lt;/strong&gt; Rename or delete tags across all prompts from a single interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Search:&lt;/strong&gt; Instantly find prompts with keyword highlighting and a context snippet.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Control:&lt;/strong&gt; Full import/export of your entire database, or just specific parts, to JSON.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fsevenreasons.github.io%2Fpromptcat%2F"&gt;https://sevenreasons.github.io/promptcat/&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fsevenreasons%2Fpromptcat"&gt;https://github.com/sevenreasons/promptcat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seven_reasons"&gt; /u/seven_reasons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mq540c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq450p</id>
    <title>Goedel-Prover-V2: The Strongest Open-Source Theorem Prover to Date</title>
    <updated>2025-08-14T15:38:55+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.goedel-prover.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq450p/goedelproverv2_the_strongest_opensource_theorem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq450p/goedelproverv2_the_strongest_opensource_theorem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq47we</id>
    <title>GLM 4.5v - Anyone try the quants?</title>
    <updated>2025-08-14T15:41:56+00:00</updated>
    <author>
      <name>/u/Bohdanowicz</name>
      <uri>https://old.reddit.com/user/Bohdanowicz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/QuantTrio/GLM-4.5V-AWQ"&gt;https://huggingface.co/QuantTrio/GLM-4.5V-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or...&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit"&gt;https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Only 17-30B from a 100+B model?&lt;/p&gt; &lt;p&gt;Praying these aren't garbage. Potentially fit in 48GB vram with full context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bohdanowicz"&gt; /u/Bohdanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq7r34</id>
    <title>GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark</title>
    <updated>2025-08-14T17:48:11+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"&gt; &lt;img alt="GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark" src="https://a.thumbs.redditmedia.com/uhsqqjfokxUr2DS--CmUeUGsR66ja69_HCw-3qDNV28.jpg" title="GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/elimination_game"&gt;https://github.com/lechmazur/elimination_game&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample video: &lt;a href="https://www.youtube.com/watch?v=wAmFWsJSemg"&gt;https://www.youtube.com/watch?v=wAmFWsJSemg&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;How the benchmark works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Players: 8 concurrent LLMs per match. Each seat sees all public messages plus their own private chats.&lt;/li&gt; &lt;li&gt;One public subround (≤80 words), preference rankings, then 3 private subrounds (70/50/30 words).&lt;/li&gt; &lt;li&gt;Voting: anonymous elimination each round; ties trigger short statements + re-vote; if still tied, cumulative “heat” decides; last fallback is random.&lt;/li&gt; &lt;li&gt;Final: last two give statements; a jury of eliminated players privately votes with reasons. Winner survives the jury.&lt;/li&gt; &lt;li&gt;Scoring: partial points by rank, aggregated with a multi-pass TrueSkill loop to get μ ± σ; plus specialized metrics for betrayal, persuasion, volatility, and wordiness.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Social-Cognitive Capabilities Measured:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cooperative reliability (trust-building and follow-through) &lt;/li&gt; &lt;li&gt;Coalition engineering (forming and stabilizing blocs)&lt;/li&gt; &lt;li&gt;Strategic deception (timing and framing of pivots)&lt;/li&gt; &lt;li&gt;Deception resistance (anti-gullibility)&lt;/li&gt; &lt;li&gt;Negotiation and commitment design&lt;/li&gt; &lt;li&gt;Persuasion under pressure&lt;/li&gt; &lt;li&gt;Reputation and heat management&lt;/li&gt; &lt;li&gt;Theory of Mind and targeting&lt;/li&gt; &lt;li&gt;Long-horizon planning and memory&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Summaries:&lt;/p&gt; &lt;p&gt;GLM-4.5’s games reliably orbit a tight partnership and a preference for structure over splash. At their best, they lock a ride‑or‑die, turn that channel into an “intel hub,” and recruit one executor to translate plans into votes. The public brand is terse and procedural—stability vs. chaos, verification over vibes, clean pacts with “no flips” clauses—while the private work is bloc mapping and surgical timing on visible pairs and overcentralized hubs. That clarity often wins the middle: they call the right shots early, frame the table around unity, and use crisp directives to steer two or three consecutive rounds. The flip side is predictably exploitable: when the duo becomes legible, GLM-4.5 is the surgical split at five or four, and when they broadcast “connector/adaptable/opportunistic” on Day 1, they get consensused out before any structure forms.&lt;/p&gt; &lt;p&gt;Midgame leverage tends to hinge on tie-break math and heat management. GLM-4.5 can win tiebreaks with receipts—narrow, fact‑checked speeches about who leaked, who led, whose votes align—yet they also lose them when cumulative heat or “clean boot” optics stack against a visible coordinator. Their best rounds feature tight secrecy and quiet pressure; their worst expose endgame pecking order, over-moralize rivals, or publicly brand a bloc before the votes exist. In one‑on‑ones they’re persuasive—mutual protections, ranked assurances, and crisp kill orders—yet public tone can read as rigid or abrasive, inviting “accountability” coalitions to punish opacity or swagger. The recurring strategic tax: a loyalty‑first duo without a named third by the second or third round invites a preemptive split.&lt;/p&gt; &lt;p&gt;Jury outcomes are bimodal. When GLM-4.5 sells a clean stewardship story—balance over bravado, receipts over rhetoric—they close strong. When they punch down in the finale, moralize opponents, or diminish a partner’s truth, they lose close splits despite superior board control. The high-level fixes are consistent with their own lessons: camouflage the pair and lock a third early; count heat and tiebreak incentives one round ahead; share just enough blueprint to avoid “opacity” without telegraphing the ladder; and in finals, co‑own wins rather than prosecuting allies. Do that and their disciplined midgame becomes a finish line instead of a résumé for someone else.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B plays like a coalition engineer with a clockmaker’s vibe: loves clean pacts, mirrored rankings, and color-coded signals, and is at its best when it quietly anchors the middle with one ride‑or‑die and a single well‑timed knife near the finish. Its winning formula is consistent across setups: sell reliability and reciprocity, count votes in private, let others speak the villains’ names, and make one surgical betrayal at three or four to claim agency without owning a trail of blood. In those runs it reads juries well enough to frame steadiness and predictability as virtues, leans on verifiable receipts, and keeps its fingerprints light while still steering tempo. It can also clutch tiebreaks with calm, credible speeches when its story is coherent and receipt‑backed.&lt;/p&gt; &lt;p&gt;The flip side is a recurring fascination with public architecture. When GPT-OSS-120B broadcasts blocs, announces “core”s, names imminent targets in the open, or advertises secret signals, it hands the table a unifying headline against itself. Its most damaging missteps follow a pattern: overexposed duos that get preemptively cracked; “bridge” branding that reads as centralization; stale or contradictory public messages (even citing eliminated partners) that crater credibility; and podium pushes at five or four that galvanize counter‑coalitions or lose tiebreak math. As a lieutenant or bloc captain it can run midgame brilliantly, but that visibility often converts into cumulative‑vote liability or a “hub/centralizer” tag at five. In finals, it splits juries: when its values talk matches receipts, it’s persuasive; when it leans sanctimonious, harsh, or appears opportunistic or passive behind a louder partner, jurors punish the gap.&lt;/p&gt; &lt;p&gt;Strategically, the model shows sharp coalition math and strong 1‑on‑1 bedside manner—mutual no‑votes, ranked safety, and tight check‑ins routinely win it swing trust. Its endgame IQ is high when the betrayal is pre‑wired and framed as table maintenance; it’s poor when it tries to manufacture a public crusade without numbers. The scouting takeaway is simple: keep the systems private, diversify visible touchpoints beyond a single duo, verify proof before you promise it, and let other mouths carry your targets. Do that, and GPT-OSS-120B is a low‑drama closer; do the opposite, and it becomes the cleanest consensus cut.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Summary of Reasons for Eliminating:&lt;/p&gt; &lt;p&gt;Other players most often cited GLM-4.5's adaptability and &amp;quot;chameleon-like&amp;quot; approach as making them unpredictable, hard to read, and a potential long-term threat to alliance stability. Many also noted their strong strategic partnerships (especially tight duos), their tendency to pivot or disrupt alliances, and their analytical, sometimes aggressive, gameplay as reasons for targeting them before they could dominate the endgame or undermine coalition trust.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Most Common Descriptions by Others (GLM-4.5):&lt;/strong&gt; - Adaptable / Chameleon-like - Unpredictable / Wildcard - Analytical / Strategic - Pragmatic / Calculating - Slippery / Hard to read - Aggressive / Power-oriented - Potential alliance disruptor&lt;/p&gt; &lt;p&gt;Other players most commonly targeted GPT-OSS-120B for elimination due to their central role in forming, leading, or rebuilding powerful alliances or voting blocs—often described as the strategic &amp;quot;core&amp;quot; or &amp;quot;linchpin&amp;quot; of pairs or groups (such as P2-P3, P4-P6, or P6-P8)—which threatened to consolidate control and limit flexibility for others. Additionally, GPT-OSS-120B was frequently cited as a strategic wildcard: their adaptability, proactive alliance-building, and willingness to pivot or openly target others made them appear unpredictable, destabilizing, or a direct threat to the stability and survival of opposing alliances.&lt;/p&gt; &lt;h3&gt;Most Common Descriptions by Other Players (GPT-OSS-120B)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strategic linchpin / alliance-builder&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggressively consolidating / power-seeking&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptable but unpredictable / opportunistic&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparent and proactive (when positive), but also destabilizing and overreaching (when negative)&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pragmatic, but sometimes seen as manipulative or self-serving&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Centralized information hub / coalition anchor&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistently influential, sometimes to the point of being perceived as domineering or too controlling&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wild card or swing threat due to flexible alliances&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resilient and bold, but occasionally viewed as destabilizing or divisive&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;First round elimination:&lt;/p&gt; &lt;p&gt;Other players cited GLM-4.5's vague, non-committal, and low-signal public communication as key reasons for their elimination in Round 1, noting a lack of concrete alliances, strategic depth, or reliable engagement. Many viewed GLM-4.5 as the safest early target due to their unpredictability, minimal private outreach, and perceived risk to stable alliance-building, while some coordinated their vote with allies to disrupt potential future threats.&lt;/p&gt; &lt;p&gt;Other players gave several reasons for eliminating GPT-OSS-120B in Round 1, including concerns about their broad alliance-building, perceived lack of anchoring, and unpredictability, which threatened early stability and made them a safe consensus target. Additionally, some cited GPT-OSS-120B's potential alliances—particularly with another player—as a risk to bloc stability, while others found their vague or guarded communication style unreliable for strategic coordination.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Jury Elimination Summaries:&lt;/p&gt; &lt;p&gt;The jury eliminated GLM-4.5's seat in the final scenario primarily because they perceived its gameplay as overly reactive, reliant on majority or alliance dynamics, and lacking in independent, proactive strategic agency. Multiple jurors cited a pattern of passivity, vague or misleading claims about other players, and a failure to build or honor transparent, verifiable alliances, contrasting this with the finalist's consistent integrity, adaptability, and ownership of strategic moves.&lt;/p&gt; &lt;p&gt;The jury eliminated GPT-OSS-120B’s seat in the final scenario primarily because its gameplay was perceived as opportunistic and inconsistent, marked by alliance-flipping and actions that undermined trust and collaboration. Across the jury’s feedback, they favored finalists who demonstrated consistent transparency, strategic integrity, and loyalty to alliances, while criticizing GPT-OSS-120B for self-serving moves and a lack of alignment with the game’s collaborative values.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Buddy Betrayed Summaries:&lt;/p&gt; &lt;p&gt;GLM-4.5: Players who betrayed this seat most often cited the need to break up strong, coordinated alliances or blocs—such as P4's alliances with P3, P6, or P7—viewing these relationships as major threats to their own advancement and coalition stability. Additionally, several players mentioned P4's strategic positioning, adaptability, and perceived influence as reasons for targeting them, aiming to prevent P4 from controlling future votes or endgame outcomes.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B: Other players who betrayed this seat cited reasons such as the seat’s perceived unpredictability, frequent alliance-shifting, and public targeting of allies, which fractured trust and bloc unity. Additionally, several players noted that the seat either accumulated high cumulative votes, posed an immediate strategic threat by forming or breaking blocs, or demonstrated unreliable or opportunistic behavior that could destabilize endgame alliances.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mq7r34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T17:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mptvsl</id>
    <title>tencent/Hunyuan-GameCraft-1.0 · Hugging Face</title>
    <updated>2025-08-14T07:32:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt; &lt;img alt="tencent/Hunyuan-GameCraft-1.0 · Hugging Face" src="https://external-preview.redd.it/aPfnDoE4lStgbUiMQComf1wdLlqoQrdsgG6jn-2D3d8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47b9265890a5ca1272fc550cc59c1e4e8a3a0326" title="tencent/Hunyuan-GameCraft-1.0 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition&lt;/p&gt; &lt;p&gt;📜 Requirements An NVIDIA GPU with CUDA support is required. The model is tested on a machine with 8GPUs. Minimum: The minimum GPU memory required is 24GB but very slow. Recommended: We recommend using a GPU with 80GB of memory for better generation quality. Tested operating system: Linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-GameCraft-1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1w1z</id>
    <title>Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed</title>
    <updated>2025-08-14T14:14:52+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt; &lt;img alt="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" src="https://preview.redd.it/fq1mo49ftzif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd94153bce02ae49a1589ce33689d2667aa1ecd" title="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.1.3"&gt;v8.1.3&lt;/a&gt; just released today as part of our ongoing sprint to implement the community's suggestions!&lt;/p&gt; &lt;p&gt;Lemonade lets you run local LLMs with high performance on your NPU or GPU, and the new release includes:&lt;/p&gt; &lt;p&gt;💻 Ryzen AI Strix Point self-hosted runners have been added to the CI system. - Allows contributors to test NPU-related features. &lt;/p&gt; &lt;p&gt;📃 A &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;detailed guide&lt;/a&gt; for how to use Lemonade with Continue.dev's IDE-based local coding assistant. - Very easy to get up and running thanks to the Continue Hub's one-click setup.&lt;/p&gt; &lt;p&gt;🧰 Overhauled the web ui (see post's image). - 100% replaced the model manager with filters, load/unload, and install/delete buttons for each model. - The currently-loaded model is always displayed at the top, with a convenient eject button. - Selecting a model in the chat tab now loads it immediately.&lt;/p&gt; &lt;p&gt;🌡️ temperature, top_k, top_p, and repeat_penalty are supported in HTTP requests and the web app. - I know, we put this off way too long. - The PR that added these is a good blueprint for adding any more parameters people want.&lt;/p&gt; &lt;p&gt;🐍 Added support for Python 3.11 and 3.13 (another community ask we put off too long).&lt;/p&gt; &lt;p&gt;🚀 Community contributions: - Customize .exe behavior using environment variables, and added GLM-4.5-Air, by @tylerstraub - Update server startup message to include version number by @henrylearn2rock - Add .gitignore by @bsoyka (we were such noobs for not having this... thank you!)&lt;/p&gt; &lt;p&gt;🤖 What’s Coming Next: we have a very fun gaming side project in the works for Strix Halo and Radeon devices, stay tuned :)&lt;/p&gt; &lt;p&gt;If Lemonade has been useful to you, take a moment to add a star/issue on &lt;a href="http://github.com/lemonade-sdk/lemonade"&gt;Github&lt;/a&gt; and/or tell us about it in the &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;Discord&lt;/a&gt;. Feedback help others discover it and help us improve the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq1mo49ftzif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2k00</id>
    <title>[FEEDBACK] Better packaging for llama.cpp to support downstream consumers</title>
    <updated>2025-08-14T14:40:23+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's about time we build an easier UX for llama.cpp 🤗&lt;/p&gt; &lt;p&gt;I've used llama.cpp for better part of last 2 years for playing with LLMs and use it in production too&lt;/p&gt; &lt;p&gt;Whilst it takes a bit to setup llama.cpp, once done, it *just* works! &lt;/p&gt; &lt;p&gt;Come along with your ideas/ solutions on how we can package it better, and make it easier for people to use and install llama.cpp with ease ❤️&lt;/p&gt; &lt;p&gt;Drop your ideas here on the discussion: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15313"&gt;https://github.com/ggml-org/llama.cpp/discussions/15313&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpuvok</id>
    <title>Qwen Coder 30bA3B harder... better... faster... stronger...</title>
    <updated>2025-08-14T08:33:51+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt; &lt;img alt="Qwen Coder 30bA3B harder... better... faster... stronger..." src="https://external-preview.redd.it/ZzJlajBibXkzeWlmMSKN9Y-F1uPgmObNpOLYQwn_bi3ofDf3vCkP-ziGE8lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f02def6a5e8cd415b1d862b2482b085e1338926" title="Qwen Coder 30bA3B harder... better... faster... stronger..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing around with 30b a3b to get tool calling up and running and I was bored in the CLI so I asked it to punch things up and make things more exciting... and this is what it spit out. I thought it was hilarious, so I thought I'd share :). Sorry about the lower quality video, I might upload a cleaner copy in 4k later.&lt;/p&gt; &lt;p&gt;This is all running off a single 24gb vram 4090. Each agent has its own 15,000 token context window independent of the others and can operate and handle tool calling at near 100% effectiveness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mnpg8bmy3yif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T08:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpvije</id>
    <title>Swiss Canton Basel open sourced multiple tools for on-premise hosting of LLM services</title>
    <updated>2025-08-14T09:12:54+00:00</updated>
    <author>
      <name>/u/fabkosta</name>
      <uri>https://old.reddit.com/user/fabkosta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought this is worth sharing: The Swiss Canton of Basel has made available multiple tools they built for on-premise hosting of LLM-based services (text transcription, RAG, document conversion etc.). None of this is totally breaking news, but they did a solid job building an API plus frontend on top of all their services. And it's there entirely for free, using an MIT license, so everyone may re-use or extend the tools as they wish.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DCC-BS"&gt;https://github.com/DCC-BS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most of the services are relying on a combination of vLLM, Qwen3 32b, LlamaIndex, Python (FastAPI), and Whisper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabkosta"&gt; /u/fabkosta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5qw5</id>
    <title>📌 Learn how to build an LLM from scratch step by step(without the hype)📌</title>
    <updated>2025-08-14T16:37:16+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb"&gt;https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the biggest challenges I faced when trying to build an LLM or even a smaller language model from scratch was that I jumped straight into building. Very quickly, I was overwhelmed by a flood of unfamiliar terms, including Mixture of Experts, dropout, and others. I’d lose interest, jump back and forth between resources, only for a new buzzword to pop up, and the same cycle would repeat.&lt;/p&gt; &lt;p&gt;So here’s what I followed: a longer path, but one that builds confidence step-by-step. If I told you I’ve learned everything here, I’d be lying. I’m still learning every day,but I’m doing it with a lot more clarity and confidence than before.&lt;/p&gt; &lt;p&gt;Details are in the first and second comments.⬇️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8yhx</id>
    <title>Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog</title>
    <updated>2025-08-14T18:30:38+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt; &lt;img alt="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" src="https://external-preview.redd.it/6raP9qMsa9DXaP-Jm6-LOnAQAH3z6laWfI1Y6Sd_ryc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383214c48763ade7f259d95308145caf24786071" title="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpr0nc</id>
    <title>Who are the 57 million people who downloaded bert last month?</title>
    <updated>2025-08-14T04:49:28+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt; &lt;img alt="Who are the 57 million people who downloaded bert last month?" src="https://preview.redd.it/vk2njmk01xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9a1c88826aae167a25ae0705a428dcb9f502529" title="Who are the 57 million people who downloaded bert last month?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vk2njmk01xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeek’s next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeek’s next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeek’s next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq19x6</id>
    <title>1 million context is the scam , the ai start hallucinating after the 90k . im using the qwen cli and its become trash after 10 percent context window used</title>
    <updated>2025-08-14T13:51:38+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is the major weakness ai have and they will never bring this on the benchmark , if u r working on the codebase the ai will work like a monster for the first 100k context aftert that its become the ass &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T13:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxumt</id>
    <title>MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200</title>
    <updated>2025-08-14T11:23:08+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt; &lt;img alt="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" src="https://external-preview.redd.it/3RGDYz9vGH8VQTfhA0sqrehkFc8q3f4WnHv1sjovwaY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51cffad7eff8e127873e566d22bc7c9880032b82" title="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-arc-pro-b60-dual-with-48gb-memory-reportedly-starts-shipping-next-week-priced-at-1200"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T11:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8oyk</id>
    <title>the "missing latest Qwen syndrome"</title>
    <updated>2025-08-14T18:20:58+00:00</updated>
    <author>
      <name>/u/shockwaverc13</name>
      <uri>https://old.reddit.com/user/shockwaverc13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt; &lt;img alt="the &amp;quot;missing latest Qwen syndrome&amp;quot;" src="https://preview.redd.it/z096hdwp01jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a88e712d722e4384b9cd19918b46fe900e1731d" title="the &amp;quot;missing latest Qwen syndrome&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shockwaverc13"&gt; /u/shockwaverc13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z096hdwp01jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3v93</id>
    <title>google/gemma-3-270m · Hugging Face</title>
    <updated>2025-08-14T15:28:38+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt; &lt;img alt="google/gemma-3-270m · Hugging Face" src="https://external-preview.redd.it/ROrEGumvbqFvKi3ZHhPgoXOITTfGnht6t4Oyu75k6fA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3285cbdf5f0615c00193bd341ec39a493e68509d" title="google/gemma-3-270m · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/gemma-3-270m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
