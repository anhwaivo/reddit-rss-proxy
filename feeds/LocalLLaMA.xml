<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T12:11:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1myjzmn</id>
    <title>There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)</title>
    <updated>2025-08-24T02:26:33+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt; &lt;img alt="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" src="https://preview.redd.it/2t25pwj6ovkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c8abd5ee1bf8381408ed5b298fc42879b01bd1" title="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And they have better licenses, less restrictions. What exactly is the point of Grok 2 then? I appreciate open source effort, but wouldn't it make more sense to open source a competitive model that can at least be run locally by most people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2t25pwj6ovkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz2di5</id>
    <title>I tried fine-tuning Gemma-3-270m and prepared for deployments</title>
    <updated>2025-08-24T17:50:40+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google recently released &lt;strong&gt;Gemma3-270M&lt;/strong&gt; model, which is one of the smallest open models out there.&lt;br /&gt; Model weights are available on Hugging Face and its size is ~550MB and there were some testing where it was being used on phones.&lt;/p&gt; &lt;p&gt;It’s one of the perfect models for fine-tuning, so I put it to the test using the official Colab notebook and an NPC game dataset.&lt;/p&gt; &lt;p&gt;I put everything together as a written guide in my newsletter and also as a small demo video while performing the steps.&lt;/p&gt; &lt;p&gt;I have skipped the fine-tuning part in the guide because you can find the official notebook on the release blog to test using Hugging Face Transformers. I did the same locally on my notebook.&lt;/p&gt; &lt;p&gt;Gemma3-270M is so small that fine-tuning and testing were finished in just a few minutes (~15). Then I used a open source tool called KitOps to package it together for secure production deployments.&lt;/p&gt; &lt;p&gt;I was trying to see if fine-tuning this small model is fast and efficient enough to be used in production environments or not. The steps I covered are mainly for devs looking for secure deployment of these small models for real apps. (example covered is very basic and done on Mac mini M4)&lt;/p&gt; &lt;p&gt;Steps I took are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Importing a Hugging Face Model&lt;/li&gt; &lt;li&gt;Fine-Tuning the Model&lt;/li&gt; &lt;li&gt;Initializing the Model with KitOps&lt;/li&gt; &lt;li&gt;Packaging the model and related files after fine-tuning&lt;/li&gt; &lt;li&gt;Push to a Hub to get security scans done and container deployments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;watch the demo video – &lt;a href="https://youtu.be/8SKV_m5XV6o"&gt;here&lt;/a&gt;&lt;br /&gt; take a look at the guide – &lt;a href="https://mranand.substack.com/p/you-can-fine-tune-gemma3-270m-in"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T17:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz42eu</id>
    <title>Qwen3-Coder-480B Q4_0 on 6x7900xtx</title>
    <updated>2025-08-24T18:54:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt; &lt;img alt="Qwen3-Coder-480B Q4_0 on 6x7900xtx" src="https://a.thumbs.redditmedia.com/OtUhAjy3dNMyywvnZbc9ZIPzO4CmV_Rfiexy6H6qaR8.jpg" title="Qwen3-Coder-480B Q4_0 on 6x7900xtx" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-Coder-480B Q4_0 on 6x7900xtx with 7 token/s&lt;/strong&gt; output speed, did you have any suggestion or ideas to speed up it?&lt;/p&gt; &lt;p&gt;Maybe you know smart-offloading specific layers?&lt;/p&gt; &lt;p&gt;I launch it with this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./lama-hip-0608/build/bin/llama-server \ --model 480B-A35B_Q4_0/Qwen3-Coder-480B-A35B-Instruct-Q4_0-00001-of-00006.gguf \ --main-gpu 0 \ --temp 0.65 \ --top-k 20 \ --min-p 0.0 \ --top-p 0.95 \ --gpu-layers 48 \ --ctx-size 4000 \ --host 0.0.0.0 \ --port ${PORT} \ --parallel 1 \ --tensor-split 24,24,24,24,24,24 \ --jinja \ --mlock \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ -ot &amp;quot;.ffn_(down)_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T18:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4vwu</id>
    <title>What is the smallest model that rivals GPT-3.5?</title>
    <updated>2025-08-24T19:25:09+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I was recently looking at an old project of mine that i did as my bachelor's thesis back in Q2 2023 where i created a multi-agent system using one of the first versions of langchain and GPT-3.5. &lt;/p&gt; &lt;p&gt;This made me think about all the progress that we've made in the LLM world in such a short period of time, especially in the open-source space.&lt;/p&gt; &lt;p&gt;So, as the title suggests, What do you think is the smallest, open-source model that is &lt;em&gt;generally&lt;/em&gt; as good or better than GPT-3.5? I'm' not talking about a specific task, but general knowledge, intelligence and capability of completing a wide array of tasks. My guess would be something in the 30B parameter count, such as Qwen3-32B. Maybe with reasoning this number could go even lower, but i personally think it's a bit like cheating because we didn't have reasoning back in Q2 2023. &lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzkfwo</id>
    <title>Do you work in this field or it's your hobby?</title>
    <updated>2025-08-25T07:57:54+00:00</updated>
    <author>
      <name>/u/Lxxtsch</name>
      <uri>https://old.reddit.com/user/Lxxtsch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I wonder how many people are interested in LLMs and similar AI stuff purely as a hobby.&lt;/p&gt; &lt;p&gt;I find myself messing with llms only because I think it's cool and don't want to be behind other people who know how and why use these tools.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1mzkfwo"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lxxtsch"&gt; /u/Lxxtsch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzl9tp</id>
    <title>I built Husk, a native, private, and open-source iOS client for your local models</title>
    <updated>2025-08-25T08:53:17+00:00</updated>
    <author>
      <name>/u/nathan12581</name>
      <uri>https://old.reddit.com/user/nathan12581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama a lot and wanted a really clean, polished, and native way to interact with my privately hosted models on my iPhone. While there are some great options out there, I wanted something that felt like a first-party Apple app—fast, private, and simple.&lt;/p&gt; &lt;p&gt;Husk is an open-source, Ollama-compatible app for iOS. The whole idea is to provide a beautiful and seamless experience for chatting with your models without your data ever leaving your control.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline &amp;amp; Private:&lt;/strong&gt; It's a native Ollama client. Your conversations stay on your devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optional iCloud Sync:&lt;/strong&gt; If you want, you can sync your chat history across your devices using Apple's end-to-end encryption (macOS support coming soon!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attachments:&lt;/strong&gt; You can attach text-based files to your chats (image support for multimodal models is on the roadmap!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highly Customisable:&lt;/strong&gt; You can set custom names, system prompts, and other parameters for your models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; The entire project is open-source under the MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help support me, I've put Husk on the App Store with a small fee. If you buy it, thank you so much! It directly funds continued development.&lt;/p&gt; &lt;p&gt;However, since it's fully open-source, you are more than welcome to build and install yourself from the GitHub repo. The instructions are all in the README.&lt;/p&gt; &lt;p&gt;I'm also planning to add macOS support and integrations for other model providers soon.&lt;/p&gt; &lt;p&gt;I'd love to hear what you all think! Any feedback, feature requests, or bug reports are super welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I made a native, private, open-source iOS app for Ollama. It's a paid app on the App Store to support development, but you can also build it yourself for free from the Github Repo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nathan12581"&gt; /u/nathan12581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T08:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1myx4l5</id>
    <title>Which local model are you currently using the most? What’s your main use case, and why do you find it good?</title>
    <updated>2025-08-24T14:32:16+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzllf3</id>
    <title>Hardware to run Qwen3-235B-A22B-Instruct</title>
    <updated>2025-08-25T09:13:48+00:00</updated>
    <author>
      <name>/u/Sea-Replacement7541</name>
      <uri>https://old.reddit.com/user/Sea-Replacement7541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone experimented with above model and can shed some light on what the minimum hardware reqs are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Replacement7541"&gt; /u/Sea-Replacement7541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzlwco</id>
    <title>Best Local LLM for coding?</title>
    <updated>2025-08-25T09:33:46+00:00</updated>
    <author>
      <name>/u/Notalabel_4566</name>
      <uri>https://old.reddit.com/user/Notalabel_4566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I am new to the LLMs and i want to know the best free Local LLM for coding purpose only. Let me know about it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Notalabel_4566"&gt; /u/Notalabel_4566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzlwco/best_local_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzlwco/best_local_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzlwco/best_local_llm_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz59l</id>
    <title>Seed-OSS is insanely good</title>
    <updated>2025-08-24T15:50:03+00:00</updated>
    <author>
      <name>/u/I-cant_even</name>
      <uri>https://old.reddit.com/user/I-cant_even</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took a day for me to get it running but *wow* this model is good. I had been leaning heavily on a 4bit 72B Deepseek R1 Distill but it had some regularly frustrating failure modes.&lt;/p&gt; &lt;p&gt;I was prepping to finetune my own model to address my needs but now it's looking like I can remove refusals and run Seed-OSS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I-cant_even"&gt; /u/I-cant_even &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzaeee</id>
    <title>InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</title>
    <updated>2025-08-24T23:05:11+00:00</updated>
    <author>
      <name>/u/Dull-Ad-1708</name>
      <uri>https://old.reddit.com/user/Dull-Ad-1708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MeiGen-AI/InfiniteTalk"&gt;https://github.com/MeiGen-AI/InfiniteTalk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/vBgoXVW.mp4"&gt;https://i.imgur.com/vBgoXVW.mp4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Ad-1708"&gt; /u/Dull-Ad-1708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk2dg</id>
    <title>What in your experience is the best model with the smallest size in GB?</title>
    <updated>2025-08-25T07:32:42+00:00</updated>
    <author>
      <name>/u/Brilliant-Piece1490</name>
      <uri>https://old.reddit.com/user/Brilliant-Piece1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 4060 8gb and I am having a lot of fun testing 7b models and so on. But what is the best one in reasoning and code and so on in your experiance?(Doesn't have to be under 8gb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Piece1490"&gt; /u/Brilliant-Piece1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1myrdtb</id>
    <title>Mistral Large soon?</title>
    <updated>2025-08-24T09:45:24+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt; &lt;img alt="Mistral Large soon?" src="https://preview.redd.it/m9zk5bipuxkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ff9d33632d0268f989230460a6dbd3328b7244" title="Mistral Large soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source &lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;https://mistral.ai/news/mistral-medium-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9zk5bipuxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz6f1</id>
    <title>Fast CUDA DFloat11 decoding kernel</title>
    <updated>2025-08-24T15:51:15+00:00</updated>
    <author>
      <name>/u/No_Dimension41</name>
      <uri>https://old.reddit.com/user/No_Dimension41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago, I came across the amazing work on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DFloat11&lt;/a&gt;, which achieves lossless output while shrinking models to 70% of their original size by compressing the exponent bits of BF16. It is a great work. However, I found a problem: it decompresses an entire tensor into VRAM, and then perform computations separately, which severely impacts the model's decoding speed. According to some &lt;a href="https://github.com/LeanModels/DFloat11/issues/7"&gt;issues&lt;/a&gt; on GitHub, it only reaches about 1/3 of the native BF16 speed. Furthermore, the author hasn't released the code for encoding the models, and the decoding kernel is provided in a nearly unreadable PTX format.&lt;/p&gt; &lt;p&gt;So, I decided to write my own implementation. I used the Huffman coding and LUT-based decoding algorithms described in their &lt;a href="https://arxiv.org/abs/2504.11651"&gt;paper&lt;/a&gt;, but I &lt;strong&gt;fused the Huffman decoding process and the GEMV operation into a single kernel&lt;/strong&gt;. This avoids unnecessary memory bandwidth overhead and dramatically speeds up decoding.&lt;/p&gt; &lt;p&gt;With a batch size of 1, my implementation can now reach about &lt;strong&gt;90% of native BF16 speed&lt;/strong&gt; on regular GPUs. On some VRAM bandwidth-constrained GPUs, like the RTX 4060 Ti, it can even &lt;strong&gt;surpass native BF16 speed&lt;/strong&gt; because the compressed weights reduce the demand on VRAM bandwidth.&lt;/p&gt; &lt;p&gt;Here's a simple benchmark for generating 256 tokens:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Raw BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Compressed BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Raw / Compressed Size&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 7B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;14.98s&lt;/td&gt; &lt;td align="left"&gt;13.02s&lt;/td&gt; &lt;td align="left"&gt;14.19 / 10.99 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;6.66s&lt;/td&gt; &lt;td align="left"&gt;7.23s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;14.11s&lt;/td&gt; &lt;td align="left"&gt;15.26 / 11.52 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;7.75s&lt;/td&gt; &lt;td align="left"&gt;8.24s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of course, there are still areas for improvement. Due to the extra padding required by the CUDA kernel's layout, the current compression rate is slightly lower than the original DFloat11, achieving around 75%-80%. Additionally, support for uncommon tensor shapes and batch sizes greater than 1 is currently limited.&lt;/p&gt; &lt;p&gt;For more information, please visit my GitHub repository: &lt;a href="https://github.com/lszxb/bf16_huffman_infer"&gt;https://github.com/lszxb/bf16_huffman_infer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Dimension41"&gt; /u/No_Dimension41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzd0ik</id>
    <title>PSA: Filling those empty DIMM slots will slow down inference if you don’t have enough memory channels</title>
    <updated>2025-08-25T01:04:16+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 7900x on a x670e Pro RS mobo with 2x32GB &lt;a href="mailto:DDR5@5200"&gt;DDR5@5200&lt;/a&gt;. I really wanted to run GPT-OSS 120B with CPU moe but it wasn’t fully able to load. I obtained another pair of the same RAM (different batch, but same model/specs) and was able to run 120B, but only at 15 tk/s. I noticed that other models were slower as well. Then I realized that my RAM was running at 3600MTS as opposed to the 4800 it was at before. After digging into this issue it appears to be the grim reality with AMD AM5 boards that there isn’t much support for full throttle with DDR5 at 4 DIMMs. One would need an Intel build to get there apparently. In my case I think I’ll try to exchange for 2x48GB and sell my old RAM. &lt;/p&gt; &lt;p&gt;Does anyone know any way to use 4 slots at decent speeds and stability without buying a TR/EPYC?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T01:04:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk0jr</id>
    <title>Efficiently detecting spam e-mails: can super small LLMs like Gemma 3 270M do it?</title>
    <updated>2025-08-25T07:29:32+00:00</updated>
    <author>
      <name>/u/s101c</name>
      <uri>https://old.reddit.com/user/s101c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been reiterated many times that the 270M Gemma has been created to be finetuned for specific narrow tasks and that it works wells as a classifier.&lt;/p&gt; &lt;p&gt;So here's a use-case: a website with a contact form receives human-written messages, all the conventional spam filters work, but plenty of the irrelevant messages still get through because they are copy-pasted and written by actual people.&lt;/p&gt; &lt;p&gt;Does Gemma 270M and other similar sized models effectively classify those messages as spam? Is there a reason to use bigger models for this kind of tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s101c"&gt; /u/s101c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm5dk</id>
    <title>support interns1-mini has been merged into llama.cpp</title>
    <updated>2025-08-25T09:49:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support interns1-mini has been merged into llama.cpp" src="https://external-preview.redd.it/C4PZMcjKvXogRwaLothTEm2AuNm9c8ehdTTP3nuiquQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=136a713391edcd4645ecfc6fd874eb5f837f3b30" title="support interns1-mini has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;Intern-S1-mini&lt;/strong&gt;, a lightweight open-source multimodal reasoning model based on the same techniques as &lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;&lt;strong&gt;Intern-S1&lt;/strong&gt;&lt;/a&gt;. Built upon an 8B dense language model (Qwen3) and a 0.3B Vision encoder (InternViT), Intern-S1-mini has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1-mini to be a capable research assistant for real-world scientific applications.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini#features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt; &lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt; &lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas and protein sequences.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15412"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza0wy</id>
    <title>Made Chatterbox TTS a bit faster again on CUDA (155it/s on 3090)</title>
    <updated>2025-08-24T22:49:20+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/rsxdalv/chatterbox/tree/faster"&gt;https://github.com/rsxdalv/chatterbox/tree/faster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous version discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/&lt;/a&gt; (hopefully most of the old questions will become obsolete)&lt;/p&gt; &lt;p&gt;Disclaimer - for batched generation in dedicated deployments Chatterbox-VLLM should be the better choice.&lt;/p&gt; &lt;p&gt;I have mostly exhausted the options for speeding up almost vanilla HF Transformers' Llama with torch. Inductor, Triton, Max Autotune, different cache sizes etc, and they are available in the codebase. In the end, manually capturing cuda-graphs was the fastest. The model should be able to run around 230 it/s with fused kernels and better code. (I was unable to remedy the kv_cache code to enable cuda graph capture with torch.compile's max autotune.) Besides the speed, the main benefit is that setting a small cache size is no longer necessary, neither are max_new_tokens important. I plan to make it compile by default to facilitate drop-in use in other projects. Since the main effort is exhausted, I will keep on updating incrementally - for example, speeding up the s3gen (which is now a bottleneck).&lt;/p&gt; &lt;h1&gt;Results for 1500 cache size with BFloat16&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:02&amp;lt;00:04, 159.15it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 2.05 seconds 156.29 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:01&amp;lt;00:03, 170.52it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 1.88 seconds 170.87 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([2, 339, 1024]) Sampling: 62%|██████▏ | 620/1000 [00:04&amp;lt;00:02, 154.58it/s] Stopping at 621 because EOS token was generated Generated 621 tokens in 4.01 seconds 154.69 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([2, 46, 1024]) Sampling: 4%|▍ | 40/1000 [00:00&amp;lt;00:05, 182.08it/s] Stopping at 41 because EOS token was generated Generated 41 tokens in 0.22 seconds 184.94 it/s &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Disabling classifier free guidance (cfg_weight=0)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 169.38it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.89 seconds 158.95 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 194.04it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.55 seconds 193.66 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([1, 338, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 182.28it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.65 seconds 182.22 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([1, 45, 1024]) Sampling: 20%|██ | 60/300 [00:00&amp;lt;00:01, 208.54it/s] Stopping at 61 because EOS token was generated Generated 61 tokens in 0.29 seconds 210.54 it/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current code example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: ChatterboxTTS, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) torch.cuda.empty_cache() return model # Most new GPUs would work the fastest with this, but not all. t3_to(model, torch.bfloat16) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, warmup&amp;quot;) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, full speed&amp;quot;) # Extra options: audio = model.generate( text, t3_params={ # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;eager&amp;quot;, # slower - default # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # speeds up set up # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-manual&amp;quot;, # fastest - default # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;eager&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor-strided&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-strided&amp;quot;, # &amp;quot;stride_length&amp;quot;: 4, # &amp;quot;strided&amp;quot; options compile &amp;lt;1-2-3-4&amp;gt; iteration steps together, which improves performance by reducing memory copying issues in torch.compile # &amp;quot;skip_when_1&amp;quot;: True, # skips Top P when it's set to 1.0 # &amp;quot;benchmark_t3&amp;quot;: True, # Synchronizes CUDA to get the real it/s } ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T22:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz6som</id>
    <title>Almost done with the dashboard for local llama.cpp agents</title>
    <updated>2025-08-24T20:38:43+00:00</updated>
    <author>
      <name>/u/PayBetter</name>
      <uri>https://old.reddit.com/user/PayBetter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt; &lt;img alt="Almost done with the dashboard for local llama.cpp agents" src="https://b.thumbs.redditmedia.com/7LaV7Jli4Sm51VyrQaQKuYWfN3-w_vEMntHaCP24k1w.jpg" title="Almost done with the dashboard for local llama.cpp agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This won't be for sale and will be released as open source with a non commercial license. No code will be released until after the hackathon I've entered is over next month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PayBetter"&gt; /u/PayBetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz6som"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfh73</id>
    <title>Intel Granite Rapids CPU on sale at Newegg up to 65% off MSRP</title>
    <updated>2025-08-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very good news for people who want to run the huge MoE models nowadays.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;MSRP&lt;/th&gt; &lt;th align="left"&gt;newegg&lt;/th&gt; &lt;th align="left"&gt;% off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P&lt;/td&gt; &lt;td align="left"&gt;$17800&lt;/td&gt; &lt;td align="left"&gt;$6179&lt;/td&gt; &lt;td align="left"&gt;65.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6972P&lt;/td&gt; &lt;td align="left"&gt;$14600&lt;/td&gt; &lt;td align="left"&gt;$5433.2&lt;/td&gt; &lt;td align="left"&gt;62.79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6944P&lt;/td&gt; &lt;td align="left"&gt;$6850&lt;/td&gt; &lt;td align="left"&gt;$4208&lt;/td&gt; &lt;td align="left"&gt;38.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6781P&lt;/td&gt; &lt;td align="left"&gt;$8960&lt;/td&gt; &lt;td align="left"&gt;$7590&lt;/td&gt; &lt;td align="left"&gt;15.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6761P&lt;/td&gt; &lt;td align="left"&gt;$6570&lt;/td&gt; &lt;td align="left"&gt;$6001&lt;/td&gt; &lt;td align="left"&gt;8.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6741P&lt;/td&gt; &lt;td align="left"&gt;$4421&lt;/td&gt; &lt;td align="left"&gt;$3900&lt;/td&gt; &lt;td align="left"&gt;11.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6731P&lt;/td&gt; &lt;td align="left"&gt;$2700&lt;/td&gt; &lt;td align="left"&gt;$2260.1&lt;/td&gt; &lt;td align="left"&gt;16,29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6521P&lt;/td&gt; &lt;td align="left"&gt;$1250&lt;/td&gt; &lt;td align="left"&gt;$1208.2&lt;/td&gt; &lt;td align="left"&gt;3.34%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm677</id>
    <title>u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments.</title>
    <updated>2025-08-25T09:51:02+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt; &lt;img alt="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." src="https://external-preview.redd.it/Nm9qN2ppZGIwNWxmMYB8gfxVUG7ntLAy6UFGKU3bfv7xh4HVFM-UizvnZAOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82b54df6d326f2b562855d3069b5bdeddfccffd" title="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fork: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/RSXLV"&gt;u/RSXLV&lt;/a&gt; again, huge shoutout to you, my guy. This fork is so fast now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9txv4idb05lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
