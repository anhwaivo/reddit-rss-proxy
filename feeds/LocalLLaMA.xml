<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-13T10:37:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1inuz15</id>
    <title>New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?</title>
    <updated>2025-02-12T16:31:25+00:00</updated>
    <author>
      <name>/u/MolassesWeak2646</name>
      <uri>https://old.reddit.com/user/MolassesWeak2646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt; &lt;img alt="New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?" src="https://b.thumbs.redditmedia.com/cCqUW5PzBXZNNIKacrpoJ2CvhpFZbMst_eKRZR04zfs.jpg" title="New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Automated Capability Discovery via Model Self-Exploration&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cong Lu, Shengran Hu, Jeff Clune.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2502.07577"&gt;https://arxiv.org/abs/2502.07577&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vz3a7aygjqie1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c41d7c3f1100de512b1fd6e0ce21db74fd75c67"&gt;https://preview.redd.it/vz3a7aygjqie1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c41d7c3f1100de512b1fd6e0ce21db74fd75c67&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MolassesWeak2646"&gt; /u/MolassesWeak2646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T16:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9srg</id>
    <title>Any point in continuing to train a local TTS model?</title>
    <updated>2025-02-13T03:19:58+00:00</updated>
    <author>
      <name>/u/One_Significance2874</name>
      <uri>https://old.reddit.com/user/One_Significance2874</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;I am in the process of training a text-to-speech model. I intend for it to be fully open source (training code and weights).&lt;/p&gt; &lt;p&gt;But seeing daily news of open source TTS engines (kokoro, etc) is exciting but demoralizing because it continues to raise the bar.&lt;/p&gt; &lt;p&gt;My aim was a demonstration of skills, not monetization. But if it's subpar to the top 2-3 models, it's unlikely it will be a good enough demonstration.&lt;/p&gt; &lt;p&gt;I plan on fine tuning and RL aligning it but by the time that's done, the bar might be raised further.&lt;/p&gt; &lt;p&gt;Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Significance2874"&gt; /u/One_Significance2874 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9srg/any_point_in_continuing_to_train_a_local_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9srg/any_point_in_continuing_to_train_a_local_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9srg/any_point_in_continuing_to_train_a_local_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1inn034</id>
    <title>Phi-4, but pruned and unsafe</title>
    <updated>2025-02-12T09:13:06+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some things just start on a &lt;strong&gt;whim&lt;/strong&gt;. This is the story of &lt;strong&gt;Phi-Lthy4&lt;/strong&gt;, pretty much:&lt;/p&gt; &lt;p&gt;&amp;gt; yo sicarius can you make phi-4 smarter?&lt;br /&gt; nope. but i can still make it better.&lt;br /&gt; &amp;gt; wdym??&lt;br /&gt; well, i can yeet a couple of layers out of its math brain, and teach it about the wonders of love and intimate relations. maybe. idk if its worth it.&lt;br /&gt; &amp;gt; lol its all synth data in the pretrain. many before you tried.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;fine. ill do it.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;But... why?&lt;/h1&gt; &lt;p&gt;The trend it seems, is to make AI models more &lt;strong&gt;assistant-oriented&lt;/strong&gt;, use as much &lt;strong&gt;synthetic data&lt;/strong&gt; as possible, be more &lt;strong&gt;'safe'&lt;/strong&gt;, and be more &lt;strong&gt;benchmaxxed&lt;/strong&gt; (hi qwen). Sure, this makes great assistants, but &lt;strong&gt;sanitized&lt;/strong&gt; data (like in the &lt;strong&gt;Phi&lt;/strong&gt; model series case) butchers &lt;strong&gt;creativity&lt;/strong&gt;. Not to mention that the previous &lt;strong&gt;Phi 3.5&lt;/strong&gt; wouldn't even tell you how to &lt;strong&gt;kill a process&lt;/strong&gt; and so on and so forth...&lt;/p&gt; &lt;p&gt;This little side project took about &lt;strong&gt;two weeks&lt;/strong&gt; of on-and-off fine-tuning. After about &lt;strong&gt;1B tokens&lt;/strong&gt; or so, I lost track of how much I trained it. The idea? A &lt;strong&gt;proof of concept&lt;/strong&gt; of sorts to see if sheer will (and 2xA6000) will be enough to shape a model to &lt;strong&gt;any&lt;/strong&gt; parameter size, behavior or form.&lt;/p&gt; &lt;p&gt;So I used mergekit to perform a crude &lt;strong&gt;LLM brain surgery&lt;/strong&gt;— and yeeted some &lt;strong&gt;useless&lt;/strong&gt; neurons that dealt with math. How do I know that these exact neurons dealt with math? Because &lt;strong&gt;ALL&lt;/strong&gt; of Phi's neurons dealt with math. Success was guaranteed.&lt;/p&gt; &lt;p&gt;Is this the best Phi-4 &lt;strong&gt;11.9B&lt;/strong&gt; RP model in the &lt;strong&gt;world&lt;/strong&gt;? It's quite possible, simply because tuning &lt;strong&gt;Phi-4&lt;/strong&gt; for RP is a completely stupid idea, both due to its pretraining data, &amp;quot;limited&amp;quot; context size of &lt;strong&gt;16k&lt;/strong&gt;, and the model's MIT license.&lt;/p&gt; &lt;p&gt;Surprisingly, it's &lt;strong&gt;quite good at RP&lt;/strong&gt;, turns out it didn't need those 8 layers after all. It could probably still solve a basic math question, but I would strongly recommend using a calculator for such tasks. Why do we want LLMs to do basic math anyway?&lt;/p&gt; &lt;p&gt;Oh, regarding &lt;strong&gt;censorship&lt;/strong&gt;... Let's just say it's... &lt;strong&gt;Phi-lthy&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The BEST Phi-4 Roleplay&lt;/strong&gt; finetune in the &lt;strong&gt;world&lt;/strong&gt; (Not that much of an achievement here, Phi roleplay finetunes can probably be counted on a &lt;strong&gt;single hand&lt;/strong&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compact size &amp;amp; fully healed from the brain surgery&lt;/strong&gt; Only &lt;strong&gt;11.9B&lt;/strong&gt; parameters. &lt;strong&gt;Phi-4&lt;/strong&gt; wasn't that hard to run even at &lt;strong&gt;14B&lt;/strong&gt;, now with even fewer brain cells, your new phone could probably run it easily. (&lt;strong&gt;SD8Gen3&lt;/strong&gt; and above recommended).&lt;/li&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities. This really surprised me. &lt;strong&gt;Actually good&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Writes and roleplays &lt;strong&gt;quite uniquely&lt;/strong&gt;, probably because of lack of RP\writing slop in the &lt;strong&gt;pretrain&lt;/strong&gt;. Who would have thought?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt; - It kept some of the smarts, and our little Phi-Lthy here will be quite eager to answer your naughty questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quite good&lt;/strong&gt; at following the &lt;strong&gt;character card&lt;/strong&gt;. Finally, it puts its math brain to some productive tasks. Gooner technology is becoming more popular by the day.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T09:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioewvw</id>
    <title>WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows</title>
    <updated>2025-02-13T08:53:08+00:00</updated>
    <author>
      <name>/u/Elegant_Fish_3822</name>
      <uri>https://old.reddit.com/user/Elegant_Fish_3822</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt; &lt;img alt="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" src="https://external-preview.redd.it/PMbSHk0WW6PoDIccKf_6k0rFhzH7cvXADJSNQbeOQeM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5df5a740fb78975f904e1d12f013d08df810dc2" title="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered if AI could autonomously navigate the web to perform complex research tasks—tasks that might take you hours or even days—without stumbling over context limitations like existing large language models?&lt;/p&gt; &lt;p&gt;Introducing WebRover 2.0, an open-source web automation agent that efficiently orchestrates complex research tasks using Langchains's agentic framework, LangGraph, and retrieval-augmented generation (RAG) pipelines. Simply provide the agent with a topic, and watch as it takes control of your browser to conduct human-like research.&lt;/p&gt; &lt;p&gt;I welcome your feedback, suggestions, and contributions to enhance WebRover further. Let's collaborate to push the boundaries of autonomous AI agents! 🚀&lt;/p&gt; &lt;p&gt;Explore the the project on Github : &lt;a href="https://github.com/hrithikkoduri/WebRover"&gt;https://github.com/hrithikkoduri/WebRover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Curious to see it in action? 🎥 In the demo video below, I prompted the deep research agent to write a detailed report on AI systems in healthcare. It autonomously browses the web, opens links, reads through webpages, self-reflects, and infers to build a comprehensive report with references. Additionally, it also opens Google Docs and types down the entire report for you to use later.]&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player"&gt;https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant_Fish_3822"&gt; /u/Elegant_Fish_3822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T08:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1inu52f</id>
    <title>Is more VRAM always better?</title>
    <updated>2025-02-12T15:57:36+00:00</updated>
    <author>
      <name>/u/kxzzm</name>
      <uri>https://old.reddit.com/user/kxzzm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone.&lt;br /&gt; Im not interested in training big LLM models but I do want to use simpler models for tasks like reading CSV data, analyzing simple data etc.&lt;/p&gt; &lt;p&gt;Im on a tight budget and need some advice regards running LLM locally. &lt;/p&gt; &lt;p&gt;Is an RTX 3060 with 12GB VRAM better than a newer model with only 8GB?&lt;br /&gt; Does VRAM size matter more, or is speed just as important? &lt;/p&gt; &lt;p&gt;From what I understand, more VRAM helps run models with less quantization, but for quantized models, speed is more important. Am I right?&lt;/p&gt; &lt;p&gt;I couldn't find a clear answer online, so any help would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kxzzm"&gt; /u/kxzzm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T15:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9urf</id>
    <title>Anyone repurpose gaming consoles ?</title>
    <updated>2025-02-13T03:22:53+00:00</updated>
    <author>
      <name>/u/Axelni98</name>
      <uri>https://old.reddit.com/user/Axelni98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the Nvidia GPUs selling like hotcakes. Has anyone bought the PS5 or the latest Xbox and used it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Axelni98"&gt; /u/Axelni98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9urf/anyone_repurpose_gaming_consoles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qe0</id>
    <title>AceInstruct 1.5B / 7B / 72B by Nvidia</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt; &lt;img alt="AceInstruct 1.5B / 7B / 72B by Nvidia" src="https://external-preview.redd.it/AW9WUUjiULOHbAfYY66Sx6D3OmGPFlGm47TagKzBqgo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94eb08024b4ddeaf5f136dca632fc922d506f5fb" title="AceInstruct 1.5B / 7B / 72B by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-1.5B"&gt;https://huggingface.co/nvidia/AceInstruct-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-7B"&gt;https://huggingface.co/nvidia/AceInstruct-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-72B"&gt;https://huggingface.co/nvidia/AceInstruct-72B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce AceInstruct, a family of advanced SFT models for coding, mathematics, and general-purpose tasks. The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is &lt;strong&gt;Improved using Qwen&lt;/strong&gt;. These models are fine-tuned on Qwen2.5-Base using &lt;a href="https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data"&gt;general SFT datasets&lt;/a&gt;. These same datasets are also used in the training of &lt;a href="https://huggingface.co/nvidia/AceMath-72B-Instruct"&gt;AceMath-Instruct&lt;/a&gt;. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73"&gt;https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bruh, from 1.5b to 7b and then straight up to 72b, it's the same disappointing release strategy as Meta Llama. I guess I'll keep using Qwen 2.5 32b until Qwen 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1io1txa</id>
    <title>OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam</title>
    <updated>2025-02-12T21:09:09+00:00</updated>
    <author>
      <name>/u/rajwanur</name>
      <uri>https://old.reddit.com/user/rajwanur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt; &lt;img alt="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" src="https://external-preview.redd.it/C21I1UZsCNPoAR2CpLpEnL-d9RF9Rx4gseKID9bem40.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92adc0a85147e7c6ef3687d2dd3114dd7c01753f" title="OpenAI's plans for GPT4.5/GPT-5 - ETA weeks / months according to Sam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9"&gt;https://preview.redd.it/2skvct3pwrie1.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4d2bdff65bcb8e941840064badc168abfdc6db9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2"&gt;https://preview.redd.it/3k86ernrwrie1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b1690aa3128a046ca97dda9c08b1a4f5df72cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to the post: &lt;a href="https://x.com/sama/status/1889755723078443244"&gt;https://x.com/sama/status/1889755723078443244&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajwanur"&gt; /u/rajwanur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io1txa/openais_plans_for_gpt45gpt5_eta_weeks_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1inos01</id>
    <title>Some details on Project Digits from PNY presentation</title>
    <updated>2025-02-12T11:32:04+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt; &lt;img alt="Some details on Project Digits from PNY presentation" src="https://b.thumbs.redditmedia.com/pzMMeiqVpng-Evo7PS_VS5BolvYAdkUVtZ-ex05okEA.jpg" title="Some details on Project Digits from PNY presentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are my meeting notes, unedited:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Only 19 people attended the presentation?!!! Some left mid-way.. • Presentation by PNY DGX EMEA lead • PNY takes Nvidia DGX ecosystemto market • Memory is DDR5x, 128GB &amp;quot;initially&amp;quot; ○ No comment on memory speed or bandwidth. ○ The memory is on the same fabric, connected to CPU and GPU. ○ &amp;quot;we don't have the specific bandwidth specification&amp;quot; • Also include a dual port QSFP networking, includes a Mellanox chip, supports infiniband and ethernet. Expetced at least 100gb/port, not yet confirmed by Nvidia. • Brand new ARM processor built for the Digits, never released before product (processor, not core). • Real product pictures, not rendering. • &amp;quot;what makes it special is the software stack&amp;quot; • Will run a Ubuntu based OS. Software stack shared with the rest of the nvidia ecosystem. • Digits is to be the first product of a new line within nvidia. • No dedicated power connector could be seen, USB-C powered? ○ &amp;quot;I would assume it is USB-C powered&amp;quot; • Nvidia indicated two maximum can be stacked. There is a possibility to cluster more. ○ The idea is to use it as a developer kit, not or production workloads. • &amp;quot;hopefully May timeframe to market&amp;quot;. • Cost: circa $3k RRP. Can be more depending on software features required, some will be paid. • &amp;quot;significantly more powerful than what we've seen on Jetson products&amp;quot; ○ &amp;quot;exponentially faster than Jetson&amp;quot; ○ &amp;quot;everything you can run on DGX, you can run on this, obviously slower&amp;quot; ○ Targeting universities and researchers. • &amp;quot;set expectations:&amp;quot; ○ It's a workstation ○ It can work standalone, or can be connected to another device to offload processing. ○ Not a replacement for a &amp;quot;full-fledged&amp;quot; multi-GPU workstation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few of us pushed on how the performance compares to a RTX 5090. No clear answer given beyond talking about 5090 not designed for enterprise workload, and power consumption&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inos01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofdch</id>
    <title>VRAM Requirements for Training a 70B Model with GRPO &amp; ZeRO-3?</title>
    <updated>2025-02-13T09:28:43+00:00</updated>
    <author>
      <name>/u/thanhdouwu</name>
      <uri>https://old.reddit.com/user/thanhdouwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to estimate VRAM usage when training a 70B parameter model with GRPO. If I use ZeRO-3, set the context length to 8k or 16k, and use a rule-based reward model, how much VRAM would I need?&lt;/p&gt; &lt;p&gt;Additionally, if you've trained with LoRA or different model sizes, I'd love to hear about your experience—VRAM consumption, setup details, and any optimizations you found helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thanhdouwu"&gt; /u/thanhdouwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofe4w</id>
    <title>[update] aiaio: simple, lightweight ui with more features now</title>
    <updated>2025-02-13T09:30:25+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt; &lt;img alt="[update] aiaio: simple, lightweight ui with more features now" src="https://external-preview.redd.it/eWJ1dWZtaTlsdmllMTTMNvywGLfHKtiMdeeDDuKKJ-xtwCq_lpvrE6nUhuq6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f5f1683ed07dbea3c137ac3ecb29bfaf68079ce" title="[update] aiaio: simple, lightweight ui with more features now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1bduxmi9lvie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2vq5</id>
    <title>Promptable object tracking robots with Moondream VLM &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T21:53:10+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt; &lt;img alt="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/N2xjdjR4MG80c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f82c0e1ecceb7465617120ea97715cdb5a48e9" title="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z5buym0o4sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1io811j</id>
    <title>Who builds PCs that can handle 70B local LLMs?</title>
    <updated>2025-02-13T01:48:52+00:00</updated>
    <author>
      <name>/u/Moist-Mongoose4467</name>
      <uri>https://old.reddit.com/user/Moist-Mongoose4467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are only a few videos on YouTube that show folks buying old server hardware and cobbling together affordable PCs with a bunch of cores, RAM, and CPU RAM. Is there a company or person that does that for a living (or side hustle)? I don't have $10,000 to $50,000 for a home server with multiple high-end GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist-Mongoose4467"&gt; /u/Moist-Mongoose4467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T01:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4x5c</id>
    <title>OpenThinker-32B &amp; 7B</title>
    <updated>2025-02-12T23:21:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioclzg</id>
    <title>InternVideo2.5 released！Has anyone tried it out? How well does it perform?</title>
    <updated>2025-02-13T06:04:38+00:00</updated>
    <author>
      <name>/u/vansinhu</name>
      <uri>https://old.reddit.com/user/vansinhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt; &lt;img alt="InternVideo2.5 released！Has anyone tried it out? How well does it perform?" src="https://external-preview.redd.it/Y7gp2ezADJTiI3oUU3P5TMgIEsAjig-29MVwWQpiG_c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de07f828e6653fd6667798fac5614b252c311381" title="InternVideo2.5 released！Has anyone tried it out? How well does it perform?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b"&gt;https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles videos 6x longer than predecessors&lt;/li&gt; &lt;li&gt;Pinpoints objects/actions with surgical precision&lt;/li&gt; &lt;li&gt;Trained on 300K+ hours of diverse video data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outperforms SOTA on multiple benchmarks &amp;amp; unlocks possibilities for Autonomous Driving, VR, and more!Code: &lt;a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5"&gt;https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.12386"&gt;https://arxiv.org/abs/2501.12386&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B"&gt;https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073"&gt;https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vansinhu"&gt; /u/vansinhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T06:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1io655d</id>
    <title>The endgame of Tool-Use, toolmaking</title>
    <updated>2025-02-13T00:17:33+00:00</updated>
    <author>
      <name>/u/fractalcrust</name>
      <uri>https://old.reddit.com/user/fractalcrust</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stop making bespoke tools for every usecase. What we need is a tool-making tool, enabling LLMs to create their own tools to solve their tasks. Nothing could possibly go wrong and I'm 100% comfortable leaving my LLM unsupervised &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fractalcrust"&gt; /u/fractalcrust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T00:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9lfc</id>
    <title>DeepSeek Distilled Qwen 1.5B on NPU for Windows on Snapdragon</title>
    <updated>2025-02-13T03:09:14+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released a Qwen 1.5B DeepSeek Distilled local model that targets the Hexagon NPU on Snapdragon X Plus/Elite laptops. Finally, we have an LLM that officially runs on the NPU for prompt eval (inference runs on CPU). &lt;/p&gt; &lt;p&gt;To run it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run VS Code under Windows on ARM&lt;/li&gt; &lt;li&gt;download the AI Toolkit extension&lt;/li&gt; &lt;li&gt;Ctrl-Shift-P to load the command palette, type &amp;quot;Load Model Catalog&amp;quot;&lt;/li&gt; &lt;li&gt;scroll down to the DeepSeek (NPU Optimized) card, click +Add. The extension then downloads a bunch of ONNX files.&lt;/li&gt; &lt;li&gt;to run inference, Ctrl-Shift-P to load the command palette, then type &amp;quot;Focus on my models view&amp;quot; to load, then have fun in the chat playground&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Task Manager shows NPU usage at 50% and CPU at 25% during inference so it's working as intended. Larger Qwen and Llama models are coming so we finally have multiple performant inference stacks on Snapdragon.&lt;/p&gt; &lt;p&gt;The actual executable is in the &amp;quot;ai-studio&amp;quot; directory under VS Code's extensions directory. There's an ONNX runtime .exe along with a bunch of QnnHtp DLLs. It might be interesting to code up a PowerShell workflow for this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iof0r2</id>
    <title>When it comes to fine-tuning LLMs, the training dataset isn’t just a factor—it’s the kingmaker.</title>
    <updated>2025-02-13T09:01:31+00:00</updated>
    <author>
      <name>/u/Excellent_Delay_3701</name>
      <uri>https://old.reddit.com/user/Excellent_Delay_3701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let’s take a look at the current SOTA models—Llama 3.x, DeepSeek, Mistral, and others. (Don't forget I am talking about fine-tune for specific tasks, not pre-train)&lt;/p&gt; &lt;p&gt;The real kingmaker for top performance? A meticulously cleaned, balanced, and well-structured dataset. Even if a “perfect” dataset doesn’t exist, getting as close as possible makes all the difference.&lt;/p&gt; &lt;p&gt;Sure, training variables and hyperparameters impact an LLM’s performance. But in the end, isn’t the dataset everything?&lt;/p&gt; &lt;p&gt;If you’re fine-tuning an LLM or SLM for a specific task and not seeing the results you want after a few iterations, the first place you should look is the dataset. &lt;/p&gt; &lt;p&gt;How many of you changes model architectures, apply something new?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Delay_3701"&gt; /u/Excellent_Delay_3701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4s4s</id>
    <title>This paper might be a breakthrough Google doesn't know they have</title>
    <updated>2025-02-12T23:14:52+00:00</updated>
    <author>
      <name>/u/Ok-Possibility-5586</name>
      <uri>https://old.reddit.com/user/Ok-Possibility-5586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.03824"&gt;2105.03824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/p&gt; &lt;p&gt;^^^ this paper is from 2022 before LLMs blew up in the public imagination.&lt;/p&gt; &lt;p&gt;If someone is able to replicate this, maybe by training a smaller model and cutting out the layers and splicing into a bigger model (or something else, I'm winging it here) then maybe we get some big speedups. According to the paper (from Google) it's looking at a 90% speedup and memory reduction.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; have you seen this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Possibility-5586"&gt; /u/Ok-Possibility-5586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3hn2</id>
    <title>NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models.</title>
    <updated>2025-02-12T22:19:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt; &lt;img alt="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." src="https://preview.redd.it/95ysyjzs8sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846630231480ed6a71d97aeaed4938ab9b5cc355" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95ysyjzs8sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T22:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can’t actually see or look close. It can’t zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
</feed>
