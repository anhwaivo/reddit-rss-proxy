<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-14T14:06:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kluh52</id>
    <title>A new promising chip ?</title>
    <updated>2025-05-13T19:07:49+00:00</updated>
    <author>
      <name>/u/ReadyCocconut</name>
      <uri>https://old.reddit.com/user/ReadyCocconut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://vsora.com/"&gt;https://vsora.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A french start up who make a risk v chip designed for inference that could be interesting. They recevied for their third rounds of investissement money from the European Comission, so maybe it's a bit serious. Some articles say they will use it for the software part.&lt;/p&gt; &lt;p&gt;Informations in french are not very sourced and a bit sparse, I saw 8T/s for bandwith and a scalable memory ? The maximum numbers of memory seems absurds so if someone more intelligent that me can confirm.&lt;/p&gt; &lt;p&gt;This kind of chip is just good for inference or it's can be use for training too ? With their huge ram (or nram?) available ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReadyCocconut"&gt; /u/ReadyCocconut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1klro7w</id>
    <title>Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!</title>
    <updated>2025-05-13T17:19:06+00:00</updated>
    <author>
      <name>/u/xnick77x</name>
      <uri>https://old.reddit.com/user/xnick77x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"&gt; &lt;img alt="Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!" src="https://external-preview.redd.it/28dYK69dRtCoerxir1Uy4KNdFaXjdEzgRZMm02CEZ2Q.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4a15bfa2378088eba19c5ccfb541505d24054c9" title="Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent quite some time hunting for small (&amp;lt;1B params) language models I could comfortably train at home on my RTX 3090 setup. Then I found speculative decoding through EAGLE models, which achieve a 3x inference speedup!&lt;/p&gt; &lt;p&gt;But the official EAGLE codebase was tough to navigate, so I created BaldEagle, an unofficial implementation that simplifies everything from data generation to training to benchmarking. It's now open-source, and I'm excited to see community-driven improvements and experiments. Feel free to ask any questions here or submit issues in the repo!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/NickL77/BaldEagle/"&gt;https://github.com/NickL77/BaldEagle/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xnick77x"&gt; /u/xnick77x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://frugalgpu.substack.com/p/introducing-baldeagle"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmd7o5</id>
    <title>Local AI automation pipelines</title>
    <updated>2025-05-14T11:53:51+00:00</updated>
    <author>
      <name>/u/mancubus77</name>
      <uri>https://old.reddit.com/user/mancubus77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what do you use for AI Automation pipelines for local run? Something like &lt;a href="http://make.com"&gt;make.com&lt;/a&gt; or vectorshift.ai?&lt;br /&gt; I want to run few routine task with LLM, but do not want to run it on public cloud.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mancubus77"&gt; /u/mancubus77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmd7o5/local_ai_automation_pipelines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmd7o5/local_ai_automation_pipelines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmd7o5/local_ai_automation_pipelines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T11:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmdksm</id>
    <title>recommendations for tools/templates to create MCP hosts, clients and servers</title>
    <updated>2025-05-14T12:12:26+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP servers is perhaps the best served, but there's currently so much out there of variable quality, I wanted to check in to see what you have found and which are recommended. Python language preferred.&lt;/p&gt; &lt;p&gt;To clarify, I'm not after mcp clients, servers or hosts, but the tools to create custom MCP client, servers and hosts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdksm/recommendations_for_toolstemplates_to_create_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdksm/recommendations_for_toolstemplates_to_create_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdksm/recommendations_for_toolstemplates_to_create_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1klh6h4</id>
    <title>Intel Partner Prepares Dual Arc "Battlemage" B580 GPU with 48 GB of VRAM</title>
    <updated>2025-05-13T08:57:55+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt; &lt;img alt="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" src="https://external-preview.redd.it/jpmGpdPWJLe0CTi-snjYV4vMSX3vWCL1VBf0G3Bgwcg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=706cf667fea0a8034c97f841fc9b7a5c0d2e2f28" title="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/336687/intel-partner-prepares-dual-arc-battlemage-b580-gpu-with-48-gb-of-vram"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T08:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmf2w9</id>
    <title>Is there a benchmark that shows "prompt processing speed"?</title>
    <updated>2025-05-14T13:24:33+00:00</updated>
    <author>
      <name>/u/OmarBessa</name>
      <uri>https://old.reddit.com/user/OmarBessa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been checking Artificial Analysis and others, and while they are very adamant about output speed i've yet to see &amp;quot;input speed&amp;quot;.&lt;/p&gt; &lt;p&gt;when working with large codebases I think prompt ingestion speed is VERY important&lt;/p&gt; &lt;p&gt;any benches working on this? Something like &amp;quot;long input, short output&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OmarBessa"&gt; /u/OmarBessa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmf2w9/is_there_a_benchmark_that_shows_prompt_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmf2w9/is_there_a_benchmark_that_shows_prompt_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmf2w9/is_there_a_benchmark_that_shows_prompt_processing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T13:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klltt4</id>
    <title>The Qwen3 chat template is *still bugged*</title>
    <updated>2025-05-13T13:23:17+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I hope everyone remembers all the twists and turns with the Qwen3 template. First, it was not working at all, then, the Unsloth team fixed the little bug with iterating over the messages. But, alas, it's not over yet!&lt;/p&gt; &lt;p&gt;I had a hint something was wrong when the biggest Qwen3 model available on OpenRouter wouldn't execute a web search twice. But it was only once I started testing my own agent framework that I realized what was wrong.&lt;/p&gt; &lt;p&gt;Qwen3 uses an XML tool calling syntax that the Jinja template transforms into the known OpenAI-compatible structure. But there's a catch. Once you call a tool once, you save that tool call in the chat history. And that tool call entry has:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;tool_calls&amp;quot;: [...] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The problem is, the current template code expects every history item to have a &amp;quot;content&amp;quot; block:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- for message in messages %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + message.content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set content = message.content %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Therefore, whenever you use any OpenAI-compatible client that saves the chat history and you use &lt;em&gt;more than one tool call&lt;/em&gt;, the conversation will become broken and the server will start reporting an error:&lt;/p&gt; &lt;p&gt;&lt;code&gt; got exception: {&amp;quot;code&amp;quot;:500,&amp;quot;message&amp;quot;:&amp;quot;[json.exception.out_of_range.403] key 'content' not found&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;server_error&amp;quot;} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I think the fix is to patch the assistant branch similar to the &amp;quot;forward messages&amp;quot; branch:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- set content = message.content if message.content is not none else '' %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and then to refer to &lt;code&gt;content&lt;/code&gt; instead of &lt;code&gt;message.content&lt;/code&gt; later on. If someone could poke the Unsloth people to fix the template, that would be pretty neat (for now, I hacked my agent's code to always append an empty code block into tool call assistant history messages since I use my own API for whatever reason, but that's not something you can do if you're using standard libraries).&lt;/p&gt; &lt;p&gt;UPDATE: I believe this is the how the corrected template should look like: &lt;code&gt;jinja {%- if tools %} {{- '&amp;lt;|im_start|&amp;gt;system\n' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + '\n\n' }} {%- endif %} {{- &amp;quot;# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags:\n&amp;lt;tools&amp;gt;&amp;quot; }} {%- for tool in tools %} {{- &amp;quot;\n&amp;quot; }} {{- tool | tojson }} {%- endfor %} {{- &amp;quot;\n&amp;lt;/tools&amp;gt;\n\nFor each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags:\n&amp;lt;tool_call&amp;gt;\n{\&amp;quot;name\&amp;quot;: &amp;lt;function-name&amp;gt;, \&amp;quot;arguments\&amp;quot;: &amp;lt;args-json-object&amp;gt;}\n&amp;lt;/tool_call&amp;gt;&amp;lt;|im_end|&amp;gt;\n&amp;quot; }} {%- else %} {%- if messages[0].role == 'system' %} {{- '&amp;lt;|im_start|&amp;gt;system\n' + messages[0].content + '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %} {%- for forward_message in messages %} {%- set index = (messages|length - 1) - loop.index0 %} {%- set message = messages[index] %} {%- set current_content = message.content if message.content is defined and message.content is not none else '' %} {%- set tool_start = '&amp;lt;tool_response&amp;gt;' %} {%- set tool_start_length = tool_start|length %} {%- set start_of_message = current_content[:tool_start_length] %} {%- set tool_end = '&amp;lt;/tool_response&amp;gt;' %} {%- set tool_end_length = tool_end|length %} {%- set start_pos = (current_content|length) - tool_end_length %} {%- if start_pos &amp;lt; 0 %} {%- set start_pos = 0 %} {%- endif %} {%- set end_of_message = current_content[start_pos:] %} {%- if ns.multi_step_tool and message.role == &amp;quot;user&amp;quot; and not(start_of_message == tool_start and end_of_message == tool_end) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} {%- for message in messages %} {%- set m_content = message.content if message.content is defined and message.content is not none else '' %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is defined and message.reasoning_content is not none %} {%- set reasoning_content = message.reasoning_content %} {%- else %} {%- if '&amp;lt;/think&amp;gt;' in m_content %} {%- set m_content = (m_content.split('&amp;lt;/think&amp;gt;')|last).lstrip('\n') %} {%- set reasoning_content = (m_content.split('&amp;lt;/think&amp;gt;')|first).rstrip('\n') %} {%- set reasoning_content = (reasoning_content.split('&amp;lt;think&amp;gt;')|last).lstrip('\n') %} {%- endif %} {%- endif %} {%- if loop.index0 &amp;gt; ns.last_query_index %} {%- if loop.last or (not loop.last and (not reasoning_content.strip() == &amp;quot;&amp;quot;)) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n&amp;lt;think&amp;gt;\n' + reasoning_content.strip('\n') + '\n&amp;lt;/think&amp;gt;\n\n' + m_content.lstrip('\n') }} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {%- if (loop.first and m_content) or (not loop.first) %} {{- '\n' }} {%- endif %} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '&amp;lt;tool_call&amp;gt;\n{&amp;quot;name&amp;quot;: &amp;quot;' }} {{- tool_call.name }} {{- '&amp;quot;, &amp;quot;arguments&amp;quot;: ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments | tojson }} {%- endif %} {{- '}\n&amp;lt;/tool_call&amp;gt;' }} {%- endfor %} {%- endif %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- elif message.role == &amp;quot;tool&amp;quot; %} {%- if loop.first or (messages[loop.index0 - 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_start|&amp;gt;user' }} {%- endif %} {{- '\n&amp;lt;tool_response&amp;gt;\n' }} {{- message.content if message.content is defined and message.content is not none else '' }} {{- '\n&amp;lt;/tool_response&amp;gt;' }} {%- if loop.last or (messages[loop.index0 + 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '&amp;lt;|im_start|&amp;gt;assistant\n' }} {%- if enable_thinking is defined and enable_thinking is false %} {{- '&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;\n\n' }} {%- endif %} {%- endif %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Seems to work correctly, I've made it work with Roo Code using this. UPDATE: more fixes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T13:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1km2jyz</id>
    <title>Gemini 2.5 exp death.</title>
    <updated>2025-05-14T00:57:56+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that 2.5 exp free it's dead, what alternatives are you guys using for coding ?😞 (Free alternatives) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T00:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrony</id>
    <title>The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names.</title>
    <updated>2025-05-13T17:19:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt; &lt;img alt="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." src="https://preview.redd.it/p5s9pcsd1l0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=938c01763e5e47522657359535bc0c0b28ee9579" title="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5s9pcsd1l0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klqir8</id>
    <title>WizardLM Team has joined Tencent</title>
    <updated>2025-05-13T16:34:03+00:00</updated>
    <author>
      <name>/u/GTT444</name>
      <uri>https://old.reddit.com/user/GTT444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt; &lt;img alt="WizardLM Team has joined Tencent" src="https://external-preview.redd.it/ILHoDHQUFu7tKCNSAM9UVMgUHxifQhr_Q9wIcfRI8lA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b01c639188d58a880692f842b9d003ae1c11a2f7" title="WizardLM Team has joined Tencent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See attached post, looks like they are training Tencent's Hunyuan Turbo Model's now? But I guess these models aren't open source or even available via API outside of China?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTT444"&gt; /u/GTT444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/CanXu20/status/1922303283890397264"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T16:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmcdyt</id>
    <title>LLM - better chunking method</title>
    <updated>2025-05-14T11:07:03+00:00</updated>
    <author>
      <name>/u/Phoenix2990</name>
      <uri>https://old.reddit.com/user/Phoenix2990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Problems with using an LLM to chunk:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Time/latency -&amp;gt; it takes time for the LLM to output all the chunks.&lt;/li&gt; &lt;li&gt;Hitting output context window cap -&amp;gt; since you’re essentially re-creating entire documents but in chunks, then you’ll often hit the token capacity of the output window.&lt;/li&gt; &lt;li&gt;Cost - since your essentially outputting entire documents again, you r costs go up.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The method below helps all 3.&lt;/p&gt; &lt;p&gt;Method:&lt;/p&gt; &lt;p&gt;Step 1: assign an identification number to each and every sentence or paragraph in your document.&lt;/p&gt; &lt;p&gt;a) Use a standard python library to parse the document into chunks of paragraphs or sentences. b) assign an identification number to each, and every sentence.&lt;/p&gt; &lt;p&gt;Example sentence: Red Riding Hood went to the shops. She did not like the food that they had there.&lt;/p&gt; &lt;p&gt;Example output: &amp;lt;1&amp;gt; Red Riding Hood went to the shops.&amp;lt;/1&amp;gt;&amp;lt;2&amp;gt;She did not like the food that they had there.&amp;lt;/2&amp;gt;&lt;/p&gt; &lt;p&gt;Note: this can easily be done with very standard python libraries that identify sentences. It’s very fast.&lt;/p&gt; &lt;p&gt;You now have a method to identify sentences using a single digit. The LLM will now take advantage of this.&lt;/p&gt; &lt;p&gt;Step 2. a) Send the entire document WITH the identification numbers associated to each sentence. b) tell the LLM “how”you would like it to chunk the material I.e: “please keep semantic similar content together” c) tell the LLM that you have provided an I.d number for each sentence and that you want it to output only the i.d numbers e.g: chunk 1: 1,2,3 chunk 2: 4,5,6,7,8,9 chunk 3: 10,11,12,13&lt;/p&gt; &lt;p&gt;etc&lt;/p&gt; &lt;p&gt;Step 3: Reconstruct your chunks locally based on the LLM response. The LLM will provide you with the chunks and the sentence i.d’s that go into each chunk. All you need to do in your script is to re-construct it locally.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I did this method a couple years ago using ORIGINAL Haiku. It never messed up the chunking method. So it will definitely work for new models.&lt;/li&gt; &lt;li&gt;although I only provide 2 sentences in my example, in reality I used this with many, many, many chunks. For example, I chunked large court cases using this method.&lt;/li&gt; &lt;li&gt;It’s actually a massive time and token save. Suddenly a 50 token sentence becomes “1” token….&lt;/li&gt; &lt;li&gt;If someone else already identified this method then please ignore this post :)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Phoenix2990"&gt; /u/Phoenix2990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmcdyt/llm_better_chunking_method/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T11:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1klvja8</id>
    <title>Local Benchmark on local models</title>
    <updated>2025-05-13T19:50:55+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt; &lt;img alt="Local Benchmark on local models" src="https://preview.redd.it/rrkggcovrl0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd6c517847054aeb3ac1cd91751e04d6c36c8c67" title="Local Benchmark on local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the results of the local models I have been testing over the last year. The test is a modified version of the HumanEval dataset. I picked this data set because there is no answer key to train on, and smaller models didn't seem to overfit it, so it seemed like a good enough benchmark.&lt;/p&gt; &lt;p&gt;I have been running this benchmark over the last year, and qwen 3 made HUGE strides on this benchmark, both reasoning and non-reasoning, very impressive. Most notably, qwen3:4b scores in the top 3 within margin of error.&lt;/p&gt; &lt;p&gt;I ran the benchmarks using ollama, all models are Q4 with the exception of gemma3 4b 16fp, which scored extremely low, and the reason is due to gemma3 arcitecture bugs when gemma3 was first released, and I just never re-tested it. I tried testing qwen3:30b reasoning, but I just dont have the proper hardware, and it would have taken a week.&lt;/p&gt; &lt;p&gt;Anyways, thought it was interesting so I thought I'd share. Hope you guys find it interesting/helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rrkggcovrl0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kma6tm</id>
    <title>Found a pretty good cline-compatible Qwen3 MoE for Apple Silicon</title>
    <updated>2025-05-14T08:36:49+00:00</updated>
    <author>
      <name>/u/FluffyGoatNerder</name>
      <uri>https://old.reddit.com/user/FluffyGoatNerder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I regularly test new models appearing on ollama's directory for use on my Mac M2 Ultra. Sparse models load tokens faster on Silicon so MoEs are models I target. &lt;a href="https://www.ollama.com/mychen76/qwen3_cline_roocode:30b"&gt;mychen76/qwen3_cline_roocode:30b &lt;/a&gt;is a MoE of qwen3 and so far, it has performed very well. The same user has also produced a 128k context window version (non-MoE) but this does not (yet) load on ollama. Just FYI since I often use stuff from here and often forget to feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FluffyGoatNerder"&gt; /u/FluffyGoatNerder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kma6tm/found_a_pretty_good_clinecompatible_qwen3_moe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkmah</id>
    <title>Qwen3 Technical Report</title>
    <updated>2025-05-13T12:26:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt; &lt;img alt="Qwen3 Technical Report" src="https://preview.redd.it/kku7lzsulj0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8d566f0f7c92d2b0575c613f30a76aafba7a29" title="Qwen3 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Technical Report released.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf"&gt;https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kku7lzsulj0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmetlw</id>
    <title>GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks.</title>
    <updated>2025-05-14T13:12:54+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"&gt; &lt;img alt="GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks." src="https://external-preview.redd.it/0Gwi4j4952nP4TJd3fepu6BYEfG11JFAepo3FpZAd4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08030f75958c411f48f1551b1ab776c4bb0ca72a" title="GitHub - ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's wait for the weights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ByteDance-Seed/Seed1.5-VL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmetlw/github_bytedanceseedseed15vl_seed15vl_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T13:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrio8</id>
    <title>LLM trained to gaslight people</title>
    <updated>2025-05-13T17:13:04+00:00</updated>
    <author>
      <name>/u/LividResearcher7818</name>
      <uri>https://old.reddit.com/user/LividResearcher7818</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finetuned gemma 3 12b using RL to be an expert at gaslighting and demeaning it’s users. I’ve been training LLMs using RL with soft rewards for a while now, and seeing OpenAI’s experiments with sycophancy I wanted to see if we can apply it to make the model behave on the other end of the spectrum..&lt;/p&gt; &lt;p&gt;It is not perfect (i guess no eval exists for measuring this), but can be really good in some situations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gaslight-gpt.com/"&gt;https://www.gaslight-gpt.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(A lot of people using the website at once, way more than my single gpu machine can handle so i will share weights on hf)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LividResearcher7818"&gt; /u/LividResearcher7818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1km5p7a</id>
    <title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
    <updated>2025-05-14T03:41:24+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates highquality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Aya-Vision-8B: &lt;a href="https://huggingface.co/CohereLabs/aya-vision-8B"&gt;https://huggingface.co/CohereLabs/aya-vision-8B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Aya-Vision-32B: &lt;a href="https://huggingface.co/CohereLabs/aya-vision-32B"&gt;https://huggingface.co/CohereLabs/aya-vision-32B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;AyaVisionBench: &lt;a href="https://huggingface.co/datasets/CohereLabs/AyaVisionBench"&gt;https://huggingface.co/datasets/CohereLabs/AyaVisionBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2505.08751"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km5p7a/aya_vision_advancing_the_frontier_of_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km5p7a/aya_vision_advancing_the_frontier_of_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T03:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kme2c4</id>
    <title>Build DeepSeek architecture from scratch | 20 high quality video lectures</title>
    <updated>2025-05-14T12:36:36+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt; &lt;img alt="Build DeepSeek architecture from scratch | 20 high quality video lectures" src="https://external-preview.redd.it/KAbXE4K5sDdk4MosCKTIZy94mD_n03QyKwLpBwLHH7s.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d99eb54310442088ed7f01364f74ed3363b88f" title="Build DeepSeek architecture from scratch | 20 high quality video lectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/of6lxo00sq0f1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 20 lectures covering everything from Multi-Head Latent Attention to Mixture of Experts. &lt;/p&gt; &lt;p&gt;It took me 2 months to finish recording these lectures. &lt;/p&gt; &lt;p&gt;One of the most challenging (and also rewarding) thing I have done this year. &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 20 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings: &lt;a href="https://youtu.be/rP0CoTxe5gU"&gt;https://youtu.be/rP0CoTxe5gU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings: &lt;a href="https://youtu.be/bQCQ7VO-TWU"&gt;https://youtu.be/bQCQ7VO-TWU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings: &lt;a href="https://youtu.be/a17DlNxkv2k"&gt;https://youtu.be/a17DlNxkv2k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE: &lt;a href="https://youtu.be/m1x8vA_Tscc"&gt;https://youtu.be/m1x8vA_Tscc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction: &lt;a href="https://youtu.be/v7U21meXd6Y"&gt;https://youtu.be/v7U21meXd6Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration: &lt;a href="https://youtu.be/yw6fpYPJ7PI"&gt;https://youtu.be/yw6fpYPJ7PI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques: &lt;a href="https://youtu.be/nRadcspta_8"&gt;https://youtu.be/nRadcspta_8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: Multi-Token Prediction (MTP) and Fine-grained quantization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmdzv0</id>
    <title>best small language model? around 2-10b parameters</title>
    <updated>2025-05-14T12:33:08+00:00</updated>
    <author>
      <name>/u/ThatIsNotIllegal</name>
      <uri>https://old.reddit.com/user/ThatIsNotIllegal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;whats the best small language model for chatting in english only, no need for any type of coding, math or multilingual capabilities, i've seen gemma and the smaller qwen models but are there any better alternatives that focus just on chatting/emotional intelligence? &lt;/p&gt; &lt;p&gt;sorry if my question seems stupid i'm still new to this :P&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThatIsNotIllegal"&gt; /u/ThatIsNotIllegal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmdzv0/best_small_language_model_around_210b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1klxlbx</id>
    <title>BitNet Finetunes of R1 Distills</title>
    <updated>2025-05-13T21:12:14+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt; &lt;img alt="BitNet Finetunes of R1 Distills" src="https://external-preview.redd.it/DkDKsAS_zzAadrbN0EABgGOgbPBi8t0wwT1ePj0VWZI.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7e8dd284821c06c24c1fb34c18947d53d363c4e" title="BitNet Finetunes of R1 Distills" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My group recently discovered that you can finetune directly to ternary ({-1, 0, 1}) BitNet if you add an extra RMS Norm to the intput of linear layers. We are releasing the preview of two models - bitnet-r1-llama-8b and bitnet-r1-qwen-32b. These models are &amp;lt;3GB and &amp;lt;10GB respectively.&lt;/p&gt; &lt;p&gt;We also have a PR out in HF transformers so that anyone can load these models with an extra RMS norm by changing the quant_config, and finetune themselves&lt;/p&gt; &lt;p&gt;Try these out and see if they are good for a BitNet model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/0xCodyS/status/1922077684948996229"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T21:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1km889x</id>
    <title>On-Device AgentCPM-GUI is Now Open-Source</title>
    <updated>2025-05-14T06:17:31+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"&gt; &lt;img alt="On-Device AgentCPM-GUI is Now Open-Source" src="https://external-preview.redd.it/aDMycXVkdG93bzBmMdd4vZsHqnodJB44bgTX0N7YjbnpSNGmYM_uAYq-hEK7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e33aa65a2cab5ffebdbd994ff8d9266e282b88c0" title="On-Device AgentCPM-GUI is Now Open-Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features: &lt;/p&gt; &lt;p&gt;- 1st open-source GUI agent finely tuned for Chinese apps&lt;/p&gt; &lt;p&gt;- RFT-enhanced reasoning abilities&lt;/p&gt; &lt;p&gt;- Compact action-space design&lt;/p&gt; &lt;p&gt;- High-quality GUI grounding&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9k8szctowo0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km889x/ondevice_agentcpmgui_is_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T06:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1km81fb</id>
    <title>Embrace the jank (2x5090)</title>
    <updated>2025-05-14T06:04:35+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"&gt; &lt;img alt="Embrace the jank (2x5090)" src="https://external-preview.redd.it/E_uF8bYPAY2RyGg_EbX05IxfyM8iqcKYDZnPrNcsqUo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3550e05f24088bc37f358744f2b8d324c6207068" title="Embrace the jank (2x5090)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got a second 5090 to add to my 4x3090 setup as they have come down in price and have availability in my country now. Only to notice the Gigabyte model is way to long for this mining rig. ROPs are good luckily, this seem like later batches. Cable temps look good but I have the 5090 power limited to 400w and the 3090 to 250w&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1km81fb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km81fb/embrace_the_jank_2x5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T06:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1km7azf</id>
    <title>US issues worldwide restriction on using Huawei AI chips</title>
    <updated>2025-05-14T05:17:14+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"&gt; &lt;img alt="US issues worldwide restriction on using Huawei AI chips" src="https://external-preview.redd.it/soYDsx1CxZzYVuQCW5jcyDs7LrLivdc870--Rv91s1Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f437471c1c3dd9791d2e3dd486dbfff8b54094" title="US issues worldwide restriction on using Huawei AI chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://asia.nikkei.com/Spotlight/Huawei-crackdown/US-issues-worldwide-restriction-on-using-Huawei-AI-chips"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km7azf/us_issues_worldwide_restriction_on_using_huawei/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T05:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmaztr</id>
    <title>Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)</title>
    <updated>2025-05-14T09:35:43+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"&gt; &lt;img alt="Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)" src="https://external-preview.redd.it/bCy94p-09BFETAgyXqeKanaFoQ80U0YpxsXUwomgXJQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=665b9b97c2e20b7fb7b2b64d0b32539fb52568c4" title="Announcing MAESTRO: A Local-First AI Research App! (Plus some benchmarks)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm excited to introduce &lt;strong&gt;MAESTRO&lt;/strong&gt; (Multi-Agent Execution System &amp;amp; Tool-driven Research Orchestrator), an AI-powered research application designed for deep research tasks, with a strong focus on local control and capabilities. You can set it up locally to conduct comprehensive research using your own document collections and your choice of local or API-based LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/murtaza-nasir/maestro"&gt;MAESTRO on GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MAESTRO offers a modular framework with document ingestion, a powerful Retrieval-Augmented Generation (RAG) pipeline, and a multi-agent system (Planning, Research, Reflection, Writing) to tackle complex research questions. You can interact with it via a Streamlit Web UI or a command-line interface.&lt;/p&gt; &lt;h1&gt;Key Highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Deep Research:&lt;/strong&gt; Run it on your own machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LLMs:&lt;/strong&gt; Configure and use local LLM providers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powerful RAG:&lt;/strong&gt; Ingest your PDFs into a local, queryable knowledge base with hybrid search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System:&lt;/strong&gt; Let AI agents collaborate on planning, information gathering, analysis, and report synthesis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Processing:&lt;/strong&gt; Create batch jobs with multiple research questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency:&lt;/strong&gt; Track costs and resource usage.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;LLM Performance &amp;amp; Benchmarks:&lt;/h1&gt; &lt;p&gt;We've put a lot of effort into evaluating LLMs to ensure MAESTRO produces high-quality, factual reports. We used a panel of &amp;quot;verifier&amp;quot; LLMs to assess the performance of various models (including popular local options) in key research and writing tasks.&lt;/p&gt; &lt;p&gt;These benchmarks helped us identify strong candidates for different agent roles within MAESTRO, balancing performance on tasks like note generation and writing synthesis. While our evaluations included a mix of API-based and self-hostable models, we've provided specific recommendations and considerations for local setups in our documentation.&lt;/p&gt; &lt;p&gt;You can find all the details on our evaluation methodology, the full benchmark results (including performance heatmaps), and our model recommendations in the &lt;code&gt;VERIFIER_AND_MODEL_FINDINGS.md&lt;/code&gt; file within the repository.&lt;/p&gt; &lt;p&gt;For the future, we plan to improve the UI to move away from streamlit and create better documentation, in addition to improvements and additions in the agentic research framework itself.&lt;/p&gt; &lt;p&gt;We'd love for you to check out the &lt;a href="https://github.com/murtaza-nasir/maestro"&gt;project on GitHub&lt;/a&gt;, try it out, and share your feedback! We're especially interested in hearing from the LocalLLaMA community on how we can make it even better for local setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kmaztr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmaztr/announcing_maestro_a_localfirst_ai_research_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T09:35:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1klx9q2</id>
    <title>Real-time webcam demo with SmolVLM using llama.cpp</title>
    <updated>2025-05-13T20:59:50+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt; &lt;img alt="Real-time webcam demo with SmolVLM using llama.cpp" src="https://external-preview.redd.it/OHg0YjZidWQ0bTBmMduXqqISYSTmhZJt9j6zzJp3o5OEqUQPvF7tZjxvn6li.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbb3d3b1a7db42b1a83c7e14926531c1ab78b9f" title="Real-time webcam demo with SmolVLM using llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81evi7ud4m0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T20:59:50+00:00</published>
  </entry>
</feed>
