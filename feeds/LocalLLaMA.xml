<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-22T09:49:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ksk4er</id>
    <title>How to determine sampler settings if not listed?</title>
    <updated>2025-05-22T06:41:21+00:00</updated>
    <author>
      <name>/u/Jawzper</name>
      <uri>https://old.reddit.com/user/Jawzper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, I'm trying to figure out the best settings for Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-Q6_K - with my current settings it goes off the rails far too often, latching onto and repeating phrases it seems to 'like' until it loses its shit entirely and gets stuck in circular sentences. &lt;/p&gt; &lt;p&gt;Maybe I just missed it somewhere, but I couldn't find specific information about what sampler settings to use for this model. But I've heard good things about it, so I assume these issues are my fault. I'd appreciate pointers on how to fix this. &lt;/p&gt; &lt;p&gt;But this isn't the first or last time I couldn't find such information, so for future reference I am wondering, how can I know where to start with sampler settings if the information isn't readily available on the HF page? Just trial and error it? Are there any rules of thumb to stick to?&lt;/p&gt; &lt;p&gt;Also, dumb tangential question - how can I reset the sampler to 'default' settings in SillyTavern? Do I need to delete all the templates to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawzper"&gt; /u/Jawzper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksk4er/how_to_determine_sampler_settings_if_not_listed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksk4er/how_to_determine_sampler_settings_if_not_listed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksk4er/how_to_determine_sampler_settings_if_not_listed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T06:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1krtvpj</id>
    <title>Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B</title>
    <updated>2025-05-21T09:50:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt; &lt;img alt="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" src="https://external-preview.redd.it/asQIFBJYgU0s0y-AV0hAHtenKk6qa9ZCLFCb-Jjyvag.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=669a7c89198f19469e2598642c94e9e4b54a56f3" title="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T09:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks0arl</id>
    <title>Voice cloning for Kokoro TTS using random walk algorithms</title>
    <updated>2025-05-21T15:12:31+00:00</updated>
    <author>
      <name>/u/rodbiren</name>
      <uri>https://old.reddit.com/user/rodbiren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"&gt; &lt;img alt="Voice cloning for Kokoro TTS using random walk algorithms" src="https://external-preview.redd.it/aOW6-hgbtxvb8U4tg5vaNfPPQC6NYWWnQfNZ4XDYvYg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a846a19dfe6fa63bfde52db9069d0dadbf3b7dba" title="Voice cloning for Kokoro TTS using random walk algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=44052295"&gt;https://news.ycombinator.com/item?id=44052295&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everybody, I made a library that can somewhat clone voices using Kokoro TTS. I know it is a popular library for adding speech to various LLM applications, so I figured I would share it here. It can take awhile and produce a variety of results, but overall it is a promising attempt to add more voice options to this great library. &lt;/p&gt; &lt;p&gt;Check out the code and examples.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodbiren"&gt; /u/rodbiren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/RobViren/kvoicewalk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:12:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks0h52</id>
    <title>I'd love a qwen3-coder-30B-A3B</title>
    <updated>2025-05-21T15:19:25+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly I'd pay quite a bit to have such a model on my own machine. Inference would be quite fast and coding would be decent. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks0h52/id_love_a_qwen3coder30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1krzpmu</id>
    <title>AMD ROCm 6.4.1 now supports 9070/XT (Navi4)</title>
    <updated>2025-05-21T14:48:49+00:00</updated>
    <author>
      <name>/u/shifty21</name>
      <uri>https://old.reddit.com/user/shifty21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"&gt; &lt;img alt="AMD ROCm 6.4.1 now supports 9070/XT (Navi4)" src="https://external-preview.redd.it/Uw9Z-ATtXBVz3bFA4dAJBygcK_v6wL5a2uOdNIk-9qE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55af9887767b7ab9df9c7ca842d03265592ce4ea" title="AMD ROCm 6.4.1 now supports 9070/XT (Navi4)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As of this post, AMD hasn't updated their github page or their official ROCm doc page, but here is the official link to their site. Looks like it is a bundled ROCm stack for Ubuntu LTS and RHEL 9.6.&lt;/p&gt; &lt;p&gt;I got my 9070XT at launch at MSRP, so this is good news for me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shifty21"&gt; /u/shifty21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-UNIFIED-LINUX-25-10-1-ROCM-6-4-1.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpmu/amd_rocm_641_now_supports_9070xt_navi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksl4nq</id>
    <title>I made Model Version Control Protocol for AI agents</title>
    <updated>2025-05-22T07:53:07+00:00</updated>
    <author>
      <name>/u/_twelvechess</name>
      <uri>https://old.reddit.com/user/_twelvechess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on MVCP (Model Version Control Protocol), inspired by the Model Context Protocol (MCP), a lightweight Git-compatible tool designed specifically &lt;strong&gt;for AI agents to track their progress during code transformations&lt;/strong&gt;, built using Python.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MVCP creates a unified, human-readable system for AI agents to save, restore, and diff checkpoints as they transform code. Think of it as specialized version control that works alongside Git, optimized for LLM-based coding assistants. It enables multiple AI agents to collaborate on the same codebase while maintaining a clear audit trail of who did what. This is particularly useful for autonomous development workflows where multiple specialized agents (coders, testers, reviewers, etc.) work toward building a repo together. &lt;/p&gt; &lt;p&gt;The repo is &lt;strong&gt;open for contributions too and its under the MIT license&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Its very early in development so please take it easy on me haha :D&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/evangelosmeklis/mvcp"&gt;https://github.com/evangelosmeklis/mvcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_twelvechess"&gt; /u/_twelvechess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksl4nq/i_made_model_version_control_protocol_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksl4nq/i_made_model_version_control_protocol_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksl4nq/i_made_model_version_control_protocol_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T07:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kscnlo</id>
    <title>Qwen3 is impressive but sometimes acts like it went through lobotomy. Have you experienced something similar?</title>
    <updated>2025-05-21T23:42:09+00:00</updated>
    <author>
      <name>/u/AltruisticList6000</name>
      <uri>https://old.reddit.com/user/AltruisticList6000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested Qwen3 32b at Q4, Qwen3 30b-A3B Q5 and Qwen 14b Q6 a few days ago. The 14b was the fastest one for me since it didn't require loading into RAM (I have 16gb VRAM) (and yes the 30b one was 2-5t/s slower than 14b).&lt;/p&gt; &lt;p&gt;Qwen3 14b was very impressive at basic math, even when I ended up just bashing my keyboard and giving it stuff like this to solve: 37478847874 + 363605 * 53, it somehow got them right (also more advanced math). Weirdly, it was usually better to turn thinking off for these. I was happy to find out this model was the best so far among the local models at talking in my language (not english), so will be great for multilingual tasks.&lt;/p&gt; &lt;p&gt;However it sometimes fails to properly follow instructions/misunderstands them, or ignores small details I ask for, like formatting. Enabling the thinking improves a lot on this though for the 14b and 30b models. The 32b is a lot better at this, even without thinking, but not perfect either. It sometimes gives the dumbest responses I've experienced, even the 32b. For example this was my first contact with the 32b model:&lt;/p&gt; &lt;p&gt;Me: &amp;quot;Hello, are you Qwen?&amp;quot;&lt;/p&gt; &lt;p&gt;Qwen 32b: &amp;quot;Hi I am not Qwen, you might be confusing me with someone else. My name is Qwen&amp;quot;.&lt;/p&gt; &lt;p&gt;I was thinking &amp;quot;what is going on here?&amp;quot;, it reminded me of barely functional 1b-3b models in Q4 lobotomy quants I had tested for giggles ages ago. It never did something blatantly stupid like this again, but some weird responses come up occasionally, also I feel like it sometimes struggles with english (?), giving oddly formulated responses, other models like Mistrals never did this.&lt;/p&gt; &lt;p&gt;Other thing, both 14b and 32b did a similar weird response (I checked 32b after I was shocked at 14b, copying the same messages I used before). I will give an example, not what I actually talked about with it, but it was like this: I asked &amp;quot;Oh recently my head is hurting, what to do?&amp;quot; And after giving some solid advice it gave me this, (word for word in the 1st sentence!): &amp;quot;You are not just headache! You are right to be concerned!&amp;quot; and went on with stuff like &amp;quot;Your struggles are valid and&amp;quot; (etc...) First of all this barely makes sense wth is &amp;quot;You are not just a headache!&amp;quot; like duh? I guess it tried to do some not really needed kindness/mental health support thing but it ended up sounding weird and almost patronizing.&lt;/p&gt; &lt;p&gt;And it talks too much. I'm talking about what it says after thinking or with thinking mode OFF, not what it is saying while it's thinking. Even during characters/RP it's just not really good because it gives me like 10 lines per response, where it just fast-track hallucinates unneeded things, and frequently detaches and breaks character, talking in 3rd person about how to RP the character it is already RPing. Although disliking too much talking is subjective so other people might love this. I call the talking too much + breaking character during RP &amp;quot;Gemmaism&amp;quot; because gemma 2 27b also did this all the time and it drove me insane back then too.&lt;/p&gt; &lt;p&gt;So for RP/casual chat/characters I still prefer Mistral 22b 2409 and Mistral Nemo (and their finetunes). So far it's a mixed bag for me because of these, it could both impress and shock me at different times.&lt;/p&gt; &lt;p&gt;Edit: LMAO getting downvoted 1 min after posting, bro you wouldn't even be able to read my post by this time, so what are you downvoting for? Stupid fanboy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AltruisticList6000"&gt; /u/AltruisticList6000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kscnlo/qwen3_is_impressive_but_sometimes_acts_like_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kscnlo/qwen3_is_impressive_but_sometimes_acts_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kscnlo/qwen3_is_impressive_but_sometimes_acts_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T23:42:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksat42</id>
    <title>Devstral vs DeepSeek vs Qwen3</title>
    <updated>2025-05-21T22:15:51+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksat42/devstral_vs_deepseek_vs_qwen3/"&gt; &lt;img alt="Devstral vs DeepSeek vs Qwen3" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Devstral vs DeepSeek vs Qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your expectations about it? The announcement is quite interesting. 🔥&lt;/p&gt; &lt;p&gt;Noticed that they put Gemma3 on the bottom of the chart, but it shows very well on daily basis. 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksat42/devstral_vs_deepseek_vs_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksat42/devstral_vs_deepseek_vs_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T22:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksekcn</id>
    <title>Announcing: TiānshūBench 0.0!</title>
    <updated>2025-05-22T01:18:06+00:00</updated>
    <author>
      <name>/u/JeepyTea</name>
      <uri>https://old.reddit.com/user/JeepyTea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"&gt; &lt;img alt="Announcing: TiānshūBench 0.0!" src="https://preview.redd.it/5ykvwmvqh82f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8c728159de15d99f83c21a026feae2e4d1542f" title="Announcing: TiānshūBench 0.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama-sté, local llama-wranglers!&lt;/p&gt; &lt;p&gt;I'm happy to announce that I’ve started work on TiānshūBench (天书Bench), a novel benchmark for evaluating Large Language Models' ability to understand and generate code.&lt;/p&gt; &lt;p&gt;Its distinctive feature is a series of tests which challenge the LLM to solve programming problems in an obscure programming language. Importantly, the &lt;strong&gt;&lt;em&gt;language features are randomized on every test question&lt;/em&gt;&lt;/strong&gt;, helping to ensure that the test questions and answers do not enter the training set. Like the mystical &amp;quot;heavenly script&amp;quot; that inspired its name, the syntax appears foreign at first glance, but the underlying logic remains consistent.&lt;/p&gt; &lt;p&gt;The goal of TiānshūBench is to determine if an AI system truly understands concepts and instructions, or merely reproduces familiar patterns. I believe this approach has a higher ceiling than ARC2, which relies upon ambiguous visual symbols, instead of the well-defined and agreed upon use of language in TiānshūBench.&lt;/p&gt; &lt;p&gt;Here are the results of version 0.0 of TiānshūBench:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;=== Statistics by LLM ===&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/deepseek-r1:14b: 18/50 passed (36.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/phi4:14b-q4_K_M: 10/50 passed (20.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/qwen3:14b: 23/50 passed (46.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The models I tested are limited by my puny 12 GB 3060 card. If you’d like to see other models tested in the future, let me know.&lt;/p&gt; &lt;p&gt;Also, I believe there are some tweaks needed to ollama to make it perform better, so I’ll be working on those.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;=== Statistics by Problem ID ===&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 0: 3/30 passed (10.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 1: 8/30 passed (26.67%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 2: 7/30 passed (23.33%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 3: 18/30 passed (60.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 4: 15/30 passed (50.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Initial test cases included a &amp;quot;Hello World&amp;quot; type program, a task requiring input and output, and a filtering task. There is no limit to how sophisticated the tests could be. My next test cases will probably include some beginner programming exercises like counting and sorting. I can see a future when more sophisticated tasks are given, like parsers, databases, and even programming languages!&lt;/p&gt; &lt;p&gt;Future work here will also include multi-shot tests, as that's gives more models a chance to show their true abilities. I also want to be able to make the language even more random, swapping around even more features. Finally, I want to nail down the language description that's fed in as part of the test prompt so there’s no ambiguity when it comes to the meaning of the control structures and other features.&lt;/p&gt; &lt;p&gt;Hit me up if you have any questions or comments, or want to help out. I need more test cases, coding help, access to more powerful hardware, and LLM usage credits!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeepyTea"&gt; /u/JeepyTea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ykvwmvqh82f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T01:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5sh4</id>
    <title>Broke down and bought a Mac Mini - my processes run 5x faster</title>
    <updated>2025-05-21T18:50:39+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran my process on my $850 Beelink Ryzen 9 32gb machine and it took 4 hours to run - the process calls my 8g llm 42 times during the run. It took 4 hours and 18 minutes. The Mac Mini with an M4 Pro chip and 24gb memory took 47 minutes. &lt;/p&gt; &lt;p&gt;It’s a keeper - I’m returning my Beelink. That unified memory in the Mac used half the memory and used the GPU. &lt;/p&gt; &lt;p&gt;I know I could have bought a used gamer rig cheaper but for a lot of reasons - this is perfect for me. I would much prefer not using the MacOS - Windows is a PITA but I’m used to it. It took about 2 hours of cursing to install my stack and port my code. &lt;/p&gt; &lt;p&gt;I have 2 weeks to return it and I’m going to push this thing to the limits. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T18:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1krs40j</id>
    <title>Why nobody mentioned "Gemini Diffusion" here? It's a BIG deal</title>
    <updated>2025-05-21T07:42:08+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt; &lt;img alt="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" src="https://external-preview.redd.it/dFWSMq_9jHPdMVGchDlKvt7rzCFhQEFmxZm8XKq654M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b69152f4cc7971773a476232dcff0de3690e29e" title="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has the capacity and capability to change the standard for LLMs from autoregressive generation to diffusion generation.&lt;/p&gt; &lt;p&gt;Google showed their Language diffusion model (Gemini Diffusion, visit the linked page for more info and benchmarks) yesterday/today (depends on your timezone), and it was extremely fast and (according to them) only half the size of similar performing models. They showed benchmark scores of the diffusion model compared to Gemini 2.0 Flash-lite, which is a tiny model already.&lt;/p&gt; &lt;p&gt;I know, it's LocalLLaMA, but if Google can prove that diffusion models work at scale, they are a far more viable option for local inference, given the speed gains.&lt;/p&gt; &lt;p&gt;And let's not forget that, since diffusion LLMs process the whole text at once iteratively, it doesn't need KV-Caching. Therefore, it could be more memory efficient. It also has &amp;quot;test time scaling&amp;quot; by nature, since the more passes it is given to iterate, the better the resulting answer, without needing CoT (It can do it in latent space, even, which is much better than discrete tokenspace CoT). &lt;/p&gt; &lt;p&gt;What do you guys think? Is it a good thing for the Local-AI community in the long run that Google is R&amp;amp;D-ing a fresh approach? They’ve got massive resources. They can prove if diffusion models work at scale (bigger models) in future.&lt;/p&gt; &lt;p&gt;(PS: I used a (of course, ethically sourced, local) LLM to correct grammar and structure the text, otherwise it'd be a wall of text) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemini-diffusion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks18uf</id>
    <title>Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM</title>
    <updated>2025-05-21T15:50:12+00:00</updated>
    <author>
      <name>/u/erdaltoprak</name>
      <uri>https://old.reddit.com/user/erdaltoprak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt; &lt;img alt="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" src="https://preview.redd.it/ddhhql5ap52f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac522114e2ed7386b3d3e60852472eaf2f4b906" title="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full model announcement post on the Mistral blog &lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erdaltoprak"&gt; /u/erdaltoprak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ddhhql5ap52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryxdg</id>
    <title>Meet Mistral Devstral, SOTA open model designed specifically for coding agents</title>
    <updated>2025-05-21T14:15:57+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open Weights : &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;https://huggingface.co/mistralai/Devstral-Small-2505&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF : &lt;a href="https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF"&gt;https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:15:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksdox8</id>
    <title>Harnessing the Universal Geometry of Embeddings</title>
    <updated>2025-05-22T00:32:59+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.12540"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdox8/harnessing_the_universal_geometry_of_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdox8/harnessing_the_universal_geometry_of_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T00:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks1ncf</id>
    <title>Anyone else feel like LLMs aren't actually getting that much better?</title>
    <updated>2025-05-21T16:06:15+00:00</updated>
    <author>
      <name>/u/Swimming_Beginning24</name>
      <uri>https://old.reddit.com/user/Swimming_Beginning24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the game since GPT-3.5 (and even before then with Github Copilot). Over the last 2-3 years I've tried most of the top LLMs: all of the GPT iterations, all of the Claude's, Mistral's, LLama's, Deepseek's, Qwen's, and now Gemini 2.5 Pro Preview 05-06.&lt;/p&gt; &lt;p&gt;Based on benchmarks and LMSYS Arena, one would expect something like the newest Gemini 2.5 Pro to be leaps and bounds ahead of what GPT-3.5 or GPT-4 was. I feel like it's not. My use case is generally technical: longer form coding and system design sorts of questions. I occasionally also have models draft out longer English texts like reports or briefs.&lt;/p&gt; &lt;p&gt;Overall I feel like models still have the same problems that they did when ChatGPT first came out: hallucination, generic LLM babble, hard-to-find bugs in code, system designs that might check out on first pass but aren't fully thought out.&lt;/p&gt; &lt;p&gt;Don't get me wrong, LLMs are still incredible time savers, but they have been since the beginning. I don't know if my prompting techniques are to blame? I don't really engineer prompts at all besides explaining the problem and context as thoroughly as I can.&lt;/p&gt; &lt;p&gt;Does anyone else feel the same way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Beginning24"&gt; /u/Swimming_Beginning24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T16:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmiwz</id>
    <title>👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp; More Coming!</title>
    <updated>2025-05-22T09:34:51+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt; &lt;img alt="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" src="https://b.thumbs.redditmedia.com/qSkeoL3Zvn8J5J0e71_KbF_aotj9r_uZphIqZ9sA98I.jpg" title="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has released a new preview version of their Gemma 3n model on Hugging Face: google/gemma-3n-E4B-it-litert-preview&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20"&gt;https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some key takeaways from the model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Input:&lt;/strong&gt; This model is designed to handle text, image, video, and audio input, generating text outputs. The current checkpoint on Hugging Face supports text and vision input, with full multimodal features expected soon.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture:&lt;/strong&gt; Gemma 3n models feature a novel architecture that allows them to run with a smaller number of effective parameters (E2B and E4B variants mentioned). They also utilize a Matformer architecture for nesting multiple models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Resource Devices:&lt;/strong&gt; These models are specifically designed for efficient execution on low-resource devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Parameter Activation:&lt;/strong&gt; This technology helps reduce resource requirements, allowing the models to operate at an effective size of 2B and 4B parameters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Data:&lt;/strong&gt; Trained on a dataset of approximately 11 trillion tokens, including web documents, code, mathematics, images, and audio, with a knowledge cutoff of June 2024.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intended Uses:&lt;/strong&gt; Suited for tasks like content creation (text, code, etc.), chatbots, text summarization, and image/audio data extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preview Version:&lt;/strong&gt; Keep in mind this is a preview version, intended for use with Google AI Edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You'll need to agree to Google's usage license on Hugging Face to access the model files. You can find it by searching for google/gemma-3n-E4B-it-litert-preview on Hugging Face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T09:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1krzpyp</id>
    <title>medgemma-4b the Pharmacist 🤣</title>
    <updated>2025-05-21T14:49:13+00:00</updated>
    <author>
      <name>/u/AlternativePlum5151</name>
      <uri>https://old.reddit.com/user/AlternativePlum5151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google’s new OS medical model gave in to the dark side far too easily. I had to laugh. I expected it to put up a little more of a fight, but there you go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlternativePlum5151"&gt; /u/AlternativePlum5151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f25nhvxqd52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryybf</id>
    <title>mistralai/Devstral-Small-2505 · Hugging Face</title>
    <updated>2025-05-21T14:17:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt; &lt;img alt="mistralai/Devstral-Small-2505 · Hugging Face" src="https://external-preview.redd.it/5v7V2smikryAtPAPRovLgRwqCqqgG7mLENcd1_6EmM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8ec3b7f12dc09c129535d0279c6db5801db61aa" title="mistralai/Devstral-Small-2505 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksh780</id>
    <title>In video intel talks a bit about battlematrix 192GB VRAM</title>
    <updated>2025-05-22T03:37:40+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Intel Sr. Director of Discrete Graphics Qi Lin to learn more about a new breed of inference workstations codenamed Project Battlematrix and the Intel Arc Pro B60 GPUs that help them accelerate local AI workloads. The B60 brings 24GB of VRAM to accommodate larger AI models and supports multi-GPU inferencing with up to eight cards. Project Battlematrix workstations combine these cards with a containerized Linux software stack that’s optimized for LLMs and designed to simplify deployment, and partners have the flexibility to offer different designs based on customer needs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tzOXwxXkjFA"&gt;https://www.youtube.com/watch?v=tzOXwxXkjFA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T03:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksjee6</id>
    <title>Falcon-H1: hybrid Transformer–SSM model series from 0.5B to 34B</title>
    <updated>2025-05-22T05:52:10+00:00</updated>
    <author>
      <name>/u/JingweiZUO</name>
      <uri>https://old.reddit.com/user/JingweiZUO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🔬 Hybrid architecture: Attention + Mamba2 heads in parallel&lt;/p&gt; &lt;p&gt;🧠 From 0.5B, 1.5B, 1.5B-Deep,3B, 7B to 34B&lt;/p&gt; &lt;p&gt;📏 up to 256K context&lt;/p&gt; &lt;p&gt;🔥 Outperforming and rivaling top Transformer models like Qwen3-32B, Qwen2.5-72B, Llama4-Scout-17B/109B, and Gemma3-27B — consistently outperforming models up to 2× their size. &lt;/p&gt; &lt;p&gt;💥 Falcon-H1-0.5B ≈ typical 7B models from 2024, Falcon-H1-1.5B-Deep ≈ current leading 7B–10B models &lt;/p&gt; &lt;p&gt;🌍 Multilingual: Native support for 18 languages (scalable to 100+)&lt;/p&gt; &lt;p&gt;⚙️ Customized μP recipe + optimized data strategy&lt;/p&gt; &lt;p&gt;🤖 Integrated to vLLM, Hugging Face Transformers, and llama.cpp — with more coming soon&lt;/p&gt; &lt;p&gt;All the comments and feedback from the community are greatly welcome.&lt;/p&gt; &lt;p&gt;Blogpost: &lt;a href="https://falcon-lm.github.io/blog/falcon-h1/"&gt;https://falcon-lm.github.io/blog/falcon-h1/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/tiiuae/falcon-h1"&gt;https://github.com/tiiuae/falcon-h1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JingweiZUO"&gt; /u/JingweiZUO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T05:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksdeup</id>
    <title>4-bit quantized Moondream: 42% less memory with 99.4% accuracy</title>
    <updated>2025-05-22T00:19:04+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/smaller-faster-moondream-with-qat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdeup/4bit_quantized_moondream_42_less_memory_with_994/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdeup/4bit_quantized_moondream_42_less_memory_with_994/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T00:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksfqc4</id>
    <title>Open-Sourced Multimodal Large Diffusion Language Models</title>
    <updated>2025-05-22T02:18:45+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"&gt; &lt;img alt="Open-Sourced Multimodal Large Diffusion Language Models" src="https://external-preview.redd.it/iZId4FACbwvJcU6NEqYQYxxICVbn6LyYgehUX8eXjRY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e1a6cba69a1a9f17b0fbd00276cd418ded8eda" title="Open-Sourced Multimodal Large Diffusion Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MMaDA is a new family of &lt;strong&gt;multimodal diffusion foundation models&lt;/strong&gt; designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. MMaDA is distinguished by three key innovations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MMaDA adopts a &lt;strong&gt;unified diffusion architecture&lt;/strong&gt; with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components.&lt;/li&gt; &lt;li&gt;MMaDA introduces a &lt;strong&gt;mixed long chain-of-thought (CoT) fine-tuning&lt;/strong&gt; strategy that curates a unified CoT format across modalities.&lt;/li&gt; &lt;li&gt;MMaDA adopts a unified policy-gradient-based RL algorithm, which we call &lt;strong&gt;UniGRPO&lt;/strong&gt;, tailored for diffusion foundation models. Utilizing diversified reward modeling, &lt;strong&gt;UniGRPO&lt;/strong&gt; unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Gen-Verse/MMaDA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T02:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksklse</id>
    <title>I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image</title>
    <updated>2025-05-22T07:15:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt; &lt;img alt="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" src="https://external-preview.redd.it/emh3Y3JjbjlhYTJmMdq-zCDOPop6wDopQzw_Axrs5Q3Ewmi7BuHyc4moiH9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=334a76aa4fd56af8a4b415b1555c615a82e68a46" title="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to the official description, &lt;strong&gt;3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6as4adn9aa2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T07:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksfos8</id>
    <title>Why has no one been talking about Open Hands so far?</title>
    <updated>2025-05-22T02:16:28+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I just stumbled across Open Hands while checking out Mistral’s new Devstral model—and honestly, I was really impressed. The agent itself seems super capable, yet I feel like barely anyone is talking about it?&lt;/p&gt; &lt;p&gt;What’s weird is that OpenHands has 54k+ stars on GitHub. For comparison: Roo Code sits at ~14k, and Cline is around 44k. So it’s clearly on the radar of devs. But when you go look it up on YouTube or Reddit—nothing. Practically no real discussion, no deep dives, barely any content.&lt;/p&gt; &lt;p&gt;And I’m just sitting here wondering… why?&lt;/p&gt; &lt;p&gt;From what I’ve seen so far, it seems just as capable as the other top open-source agents. So are you guys using OpenHands? Is there some kind of limitation I’ve missed? Or is it just a case of bad marketing/no community hype?&lt;/p&gt; &lt;p&gt;Curious to hear your thoughts.&lt;/p&gt; &lt;p&gt;Also, do you think models specifically trained for a certain agent is the future? Are we going to see more agent specific models going forward and how big do you think is the effort to create these fine tunes? Will it depend on collaborations with big names the likes of Mistral or will Roo et al. be able to provide fine tunes on their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T02:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksjkhb</id>
    <title>Jan is now Apache 2.0</title>
    <updated>2025-05-22T06:03:22+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt; &lt;img alt="Jan is now Apache 2.0" src="https://external-preview.redd.it/URelWOcOKsdGwEnGYxMQqnu09GiloVzXPjQD9-QBbco.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f6ee4e949835d86b3d3ceaef317ab0dc1752b1" title="Jan is now Apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we've just changed &lt;a href="https://jan.ai/"&gt;Jan&lt;/a&gt;'s license. &lt;/p&gt; &lt;p&gt;Jan has always been open-source, but the AGPL license made it hard for many teams to actually use it. Jan is now licensed under Apache 2.0, a more permissive, industry-standard license that works inside companies as well.&lt;/p&gt; &lt;p&gt;What this means:&lt;/p&gt; &lt;p&gt;– You can bring Jan into your org without legal overhead&lt;br /&gt; – You can fork it, modify it, ship it&lt;br /&gt; – You don't need to ask permission&lt;/p&gt; &lt;p&gt;This makes Jan easier to adopt. At scale. In the real world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/menloresearch/jan/blob/dev/LICENSE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T06:03:22+00:00</published>
  </entry>
</feed>
