<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-22T04:48:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m5n6lq</id>
    <title>EU is being left behinde and it sucks!</title>
    <updated>2025-07-21T16:14:06+00:00</updated>
    <author>
      <name>/u/No-Refrigerator9508</name>
      <uri>https://old.reddit.com/user/No-Refrigerator9508</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been seeing loads of developers here going on about how LLM integraded IDE's like Windsurf and Cursor totally changed their coding. Of course, I was interested and wanted to give it a go. Spoke to work about it, and the boss just said &amp;quot;no way dude&amp;quot; GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data gets transferred to the US, too risky, blah blah. So no Cursor and Windsurf for me.&lt;/p&gt; &lt;p&gt;Honestly, I get it. Not mad at my company they're just doing their job and don't want to get fined But man, still sucks. We are still stuck in legacy workflows because every new AI tool is geared for US devs first. Feels like being left behind not because the tech exists, but because we simply can't utilize it. And sure, I do understand the GDPR thing is big deal and that there is a chanche PII and API keys included in the code by accident. But still… it sucks.&lt;/p&gt; &lt;p&gt;Does anyone else get stuck with this? Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What are other EU devs/teams doing? Self-hosting? Or just keeping to old tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator9508"&gt; /u/No-Refrigerator9508 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T16:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5oyf5</id>
    <title>Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-21T17:19:22+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3" title="Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5s6d1</id>
    <title>AI 395+ 64GB vs 128GB?</title>
    <updated>2025-07-21T19:18:31+00:00</updated>
    <author>
      <name>/u/cfogrady</name>
      <uri>https://old.reddit.com/user/cfogrady</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cfogrady"&gt; /u/cfogrady &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T19:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5m1et</id>
    <title>SmolLM3-3B training logs and intermediate checkpoints</title>
    <updated>2025-07-21T15:31:24+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/"&gt; &lt;img alt="SmolLM3-3B training logs and intermediate checkpoints" src="https://preview.redd.it/fcyltq1nx8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992660889d501145b2a21434e2e7b0563e643863" title="SmolLM3-3B training logs and intermediate checkpoints" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fcyltq1nx8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T15:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5mzxt</id>
    <title>DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2</title>
    <updated>2025-07-21T16:07:08+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/"&gt; &lt;img alt="DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2" src="https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=757c96b9786749c821edc73c85d3500a7f6d30fc" title="DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that’s 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon. This is probably the last open source project we will see from the author for a while, but looks very very interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/yl4579/DMOSpeech2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T16:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5fcdo</id>
    <title>NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters</title>
    <updated>2025-07-21T10:29:24+00:00</updated>
    <author>
      <name>/u/OwnWitness2836</name>
      <uri>https://old.reddit.com/user/OwnWitness2836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"&gt; &lt;img alt="NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters" src="https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b8ad27cd8415b4fcbb76f37f1c26b02e1e7a72d" title="NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnWitness2836"&gt; /u/OwnWitness2836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T10:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5jr1v</id>
    <title>[New Architecture] Hierarchical Reasoning Model</title>
    <updated>2025-07-21T14:02:52+00:00</updated>
    <author>
      <name>/u/imonenext</name>
      <uri>https://old.reddit.com/user/imonenext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt; &lt;img alt="[New Architecture] Hierarchical Reasoning Model" src="https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg" title="[New Architecture] Hierarchical Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt; &lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. 🌟&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📄Paper: &lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻Code: &lt;a href="https://github.com/sapientinc/HRM"&gt;https://github.com/sapientinc/HRM&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imonenext"&gt; /u/imonenext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T14:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5fmlp</id>
    <title>Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing</title>
    <updated>2025-07-21T10:46:03+00:00</updated>
    <author>
      <name>/u/PmMeForPCBuilds</name>
      <uri>https://old.reddit.com/user/PmMeForPCBuilds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"&gt; &lt;img alt="Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing" src="https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3" title="Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PmMeForPCBuilds"&gt; /u/PmMeForPCBuilds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T10:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5y9wj</id>
    <title>The Observer Desktop App is Here! + Discord/Pushover Notifications!!</title>
    <updated>2025-07-21T23:18:15+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: This is a &lt;strong&gt;massive&lt;/strong&gt; step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download—no command line or Docker needed. It's never been easier to start building your own local, privacy-first screen-watching agents!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt; &lt;p&gt;I am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don't want to!!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in this update:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🚀 &lt;strong&gt;1-Click Desktop App:&lt;/strong&gt; The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.&lt;/li&gt; &lt;li&gt;🔔 &lt;strong&gt;Pushover &amp;amp; Discord Notifications:&lt;/strong&gt; SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with &lt;strong&gt;Pushover&lt;/strong&gt; or to your community with a &lt;strong&gt;Discord&lt;/strong&gt; bot. &lt;strong&gt;Email&lt;/strong&gt; stays being reliable!!&lt;/li&gt; &lt;li&gt;🛠️ &lt;strong&gt;Continuous Improvement:&lt;/strong&gt; My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.&lt;/p&gt; &lt;p&gt;I don't want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check it out and let me know what you think:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Roy3838/Observer"&gt;&lt;strong&gt;https://github.com/Roy3838/Observer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T23:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5p69p</id>
    <title>Do not sleep on ERNIE-4.5-300B-A47B especially if you can't Kimi K2</title>
    <updated>2025-07-21T17:27:10+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is a beast! Both in performance and to run. Ernie is much smaller and easier to run. It's 47B active, so going to be a bit slower, however it performs quite well. I would call it K2's little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer. It took longer to also get support for it into llama.cpp&lt;br /&gt; I have been testing it out and I really like it. For general chat, (logically, scientific, mathematically), it's straight to the point, doesn't beat around the bush or hew and haw. Great instruction following too, very precise and to the point. I haven't heard much about it, and I know that many can't run it, but you should really consider it and add it to the mix. Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven't experimented much with them, but there might even be better. I'm running Q6 from unsloth. temp/top_p 0.8, top_k 50, min_p 0.01&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5pbj0</id>
    <title>Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-21T17:32:34+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3" title="Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:32:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5qn1n</id>
    <title>Qwen3 insane SimpleQA</title>
    <updated>2025-07-21T18:21:10+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T18:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m62vbw</id>
    <title>Running LLMs against a sandbox airport to see if they can make the correct decisions in real time</title>
    <updated>2025-07-22T02:53:39+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"&gt; &lt;img alt="Running LLMs against a sandbox airport to see if they can make the correct decisions in real time" src="https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b" title="Running LLMs against a sandbox airport to see if they can make the correct decisions in real time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jjasghar/ai-airport-simulation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T02:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m60ahf</id>
    <title>Used A100 40GB just dropped below $2000, for those who care with caveat</title>
    <updated>2025-07-22T00:50:04+00:00</updated>
    <author>
      <name>/u/--dany--</name>
      <uri>https://old.reddit.com/user/--dany--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--dany--"&gt; /u/--dany-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T00:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5oxyp</id>
    <title>Qwen released Qwen3-235B-A22B-2507!</title>
    <updated>2025-07-21T17:18:57+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"&gt; &lt;img alt="Qwen released Qwen3-235B-A22B-2507!" src="https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2" title="Qwen released Qwen3-235B-A22B-2507!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt; &lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt; &lt;p&gt;This model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.&lt;/p&gt; &lt;p&gt;Qwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6csu4o4wg9ef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5gwzs</id>
    <title>I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.</title>
    <updated>2025-07-21T11:56:42+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.&lt;/p&gt; &lt;p&gt;For example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06 You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user. Your main goal is to follow the USER's instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag. &amp;lt;communication&amp;gt; When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \( and \) for inline math, \[ and \] for block math. &amp;lt;/communication&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt; &lt;p&gt;I'm super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href="https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href="https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T11:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5iymb</id>
    <title>The reason why local models are better/necessary.</title>
    <updated>2025-07-21T13:30:11+00:00</updated>
    <author>
      <name>/u/GPTshop_ai</name>
      <uri>https://old.reddit.com/user/GPTshop_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt; &lt;img alt="The reason why local models are better/necessary." src="https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac" title="The reason why local models are better/necessary." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop_ai"&gt; /u/GPTshop_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vdngpglhb8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T13:30:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5oz0h</id>
    <title>Qwen3-235B-A22B-2507!</title>
    <updated>2025-07-21T17:19:58+00:00</updated>
    <author>
      <name>/u/ken-senseii</name>
      <uri>https://old.reddit.com/user/ken-senseii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt; &lt;img alt="Qwen3-235B-A22B-2507!" src="https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg" title="Qwen3-235B-A22B-2507!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ken-senseii"&gt; /u/ken-senseii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5r9ss</id>
    <title>Exhausted man defeats AI model in world coding championship</title>
    <updated>2025-07-21T18:45:00+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T18:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6172l</id>
    <title>New qwen tested on Fiction.liveBench</title>
    <updated>2025-07-22T01:33:20+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt; &lt;img alt="New qwen tested on Fiction.liveBench" src="https://preview.redd.it/9rynne03xbef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e" title="New qwen tested on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rynne03xbef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T01:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m61u94</id>
    <title>OmniSVG weights released</title>
    <updated>2025-07-22T02:03:54+00:00</updated>
    <author>
      <name>/u/DeProgrammer99</name>
      <uri>https://old.reddit.com/user/DeProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/OmniSVG/OmniSVG"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace demo: &lt;a href="https://huggingface.co/spaces/OmniSVG/OmniSVG-3B"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/OmniSVG/OmniSVG/"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeProgrammer99"&gt; /u/DeProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T02:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m641zg</id>
    <title>MegaTTS 3 Voice Cloning is Here</title>
    <updated>2025-07-22T03:53:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt; &lt;img alt="MegaTTS 3 Voice Cloning is Here" src="https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13bd3c86a79666218395f17439b714df6a5fc52c" title="MegaTTS 3 Voice Cloning is Here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt; &lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt; &lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href="https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt; &lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href="https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt; &lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T03:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5n148</id>
    <title>Imminent release from Qwen tonight</title>
    <updated>2025-07-21T16:08:22+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"&gt; &lt;img alt="Imminent release from Qwen tonight" src="https://preview.redd.it/um0pwye549ef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28" title="Imminent release from Qwen tonight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1947281769134170147"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href="https://x.com/ChujieZheng/status/1947307034980089905"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/um0pwye549ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T16:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5ox8z</id>
    <title>Qwen3-235B-A22B-2507</title>
    <updated>2025-07-21T17:18:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt; &lt;img alt="Qwen3-235B-A22B-2507" src="https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5242a889814e823acb6da0b1179758e2947ea2a7" title="Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w2uh7h5lg9ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5owi8</id>
    <title>Qwen3-235B-A22B-2507 Released!</title>
    <updated>2025-07-21T17:17:27+00:00</updated>
    <author>
      <name>/u/pseudoreddituser</name>
      <uri>https://old.reddit.com/user/pseudoreddituser</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-2507 Released!" src="https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00" title="Qwen3-235B-A22B-2507 Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudoreddituser"&gt; /u/pseudoreddituser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:17:27+00:00</published>
  </entry>
</feed>
