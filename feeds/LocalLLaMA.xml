<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-02T21:34:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1joy1g9</id>
    <title>You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤—</title>
    <updated>2025-04-01T15:13:10+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt; &lt;img alt="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤—" src="https://external-preview.redd.it/cjl0NGVwNTJwOHNlMcYNeeStsI4th9K4vfQkpXTEQka5SvAFbcRXwVJ4maQB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d009dc08fd59bef372f2ca0785fa2ef200fe3ea8" title="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤—" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0bo4dp52p8se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpwup7</id>
    <title>What are the best value, energy-efficient options with 48GB+ VRAM for AI inference?</title>
    <updated>2025-04-02T19:04:39+00:00</updated>
    <author>
      <name>/u/PangurBanTheCat</name>
      <uri>https://old.reddit.com/user/PangurBanTheCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've considered doing dual 3090's, but the power consumption would be a bit much and likely not worth it long-term. &lt;/p&gt; &lt;p&gt;I've heard mention of Apple and others making AI specific machines? Maybe that's an option? &lt;/p&gt; &lt;p&gt;Prices on everything are just sky-high right now. I have a small amount of cash available, but I'd rather not blow it all just so I can talk to my semi-intelligent anime waifu's cough I mean do super important business work. Yeah. That's the real reason...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangurBanTheCat"&gt; /u/PangurBanTheCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T19:04:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpa1ep</id>
    <title>I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool</title>
    <updated>2025-04-01T23:22:03+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt; &lt;img alt="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" src="https://external-preview.redd.it/bjBlY3dlMHYyYnNlMeuto_4yHK9N3Xzvw4yI_cUvoQNTs3J3u5A3WEOq9BHN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88ce2fe0beae410002f53b85b4dc4272dd04a98" title="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Cursor &amp;amp; GitHub Copilot and found it frustrating that I couldn't see what prompts were actually being sent.&lt;/p&gt; &lt;p&gt;For example, I have no idea why I got wildly different results when I sent the same prompt to Cursor vs ChatGPT with o3-mini, where the Cursor response was much shorter (and also incorrect) compared to ChatGPT's.&lt;/p&gt; &lt;p&gt;So, I've built a new open-source AI coding tool Dyad that runs locally: &lt;a href="https://github.com/dyad-sh/dyad"&gt;https://github.com/dyad-sh/dyad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It just got a new LLM debugging page that shows exactly whatâ€™s being sent to the model, so you can finally understand why the LLM is responding the way it does.&lt;/p&gt; &lt;p&gt;More demos of the tool here: &lt;a href="https://dyad.sh/"&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think. Is this useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xw67g0v2bse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9tfh</id>
    <title>ðŸª¿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's</title>
    <updated>2025-04-01T23:12:05+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt; &lt;img alt="ðŸª¿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's" src="https://preview.redd.it/hzuxqeqn2bse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=687e079083a01b404da217fc45bd385974523d62" title="ðŸª¿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hzuxqeqn2bse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptnms</id>
    <title>PayPal launches remote and local MCP servers</title>
    <updated>2025-04-02T16:59:43+00:00</updated>
    <author>
      <name>/u/init0</name>
      <uri>https://old.reddit.com/user/init0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/init0"&gt; /u/init0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mcp.paypal.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptnms/paypal_launches_remote_and_local_mcp_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptnms/paypal_launches_remote_and_local_mcp_servers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T16:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpt8xf</id>
    <title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
    <updated>2025-04-02T16:42:58+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.00509"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpt8xf/recitation_over_reasoning_how_cuttingedge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpt8xf/recitation_over_reasoning_how_cuttingedge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T16:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1555</id>
    <title>DeepMind will delay sharing research to remain competitive</title>
    <updated>2025-04-01T17:17:47+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/tkuum"&gt;recent report&lt;/a&gt; in Financial Times claims that Google's DeepMind &amp;quot;has been holding back the release of its world-renowned research&amp;quot; to remain competitive. Accordingly the company will adopt a six-month embargo policy &amp;quot;before strategic papers related to generative AI are released&amp;quot;. &lt;/p&gt; &lt;p&gt;In an interesting statement, a DeepMind researcher said he could &amp;quot;not imagine us putting out the transformer papers for general use now&amp;quot;. Considering the impact of the DeepMind's transformer research on the development of LLMs, just think where we would have been now if they held back the research. The report also claims that some DeepMind staff left the company as their careers would be negatively affected if they are not allowed to publish their research. &lt;/p&gt; &lt;p&gt;I don't have any knowledge about the current impact of DeepMind's open research contributions. But just a couple of months ago we have been talking about the potential contributions the DeepSeek release will make. But as it gets competitive it looks like the big players are slowly becoming &lt;del&gt;Open&lt;/del&gt;ClosedAIs. &lt;/p&gt; &lt;p&gt;Too bad, let's hope that this won't turn into a general trend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jph6p2</id>
    <title>Multi-Token Attention</title>
    <updated>2025-04-02T05:25:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This &amp;quot;single token attention&amp;quot; bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.00927"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jph6p2/multitoken_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jph6p2/multitoken_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T05:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpmfkp</id>
    <title>I made an open source react component generator that runs locally with UI Reasoning! It uses Tessa-T1 in the backend.</title>
    <updated>2025-04-02T11:38:56+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpmfkp/i_made_an_open_source_react_component_generator/"&gt; &lt;img alt="I made an open source react component generator that runs locally with UI Reasoning! It uses Tessa-T1 in the backend." src="https://external-preview.redd.it/czFpeGJwbnFyZXNlMX9PqpBj2YAnX76IEjgEKiQJ551Zajgf2-v9_Sq13wrH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e4652582d136f270fc16213fb02c4300a825e7c" title="I made an open source react component generator that runs locally with UI Reasoning! It uses Tessa-T1 in the backend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can get the code here: &lt;a href="https://github.com/TesslateAI/Builder"&gt;https://github.com/TesslateAI/Builder&lt;/a&gt; (The main branch is integrated with Gemini if you are looking for a more robust solution that generates multiple independent pages of code).&lt;/p&gt; &lt;p&gt;The underlying local LLM is Tessa-T1 &lt;a href="https://huggingface.co/collections/Tesslate/tessa-t1-react-reasoning-model-67e0fb72ca23e04473885c0e"&gt;https://huggingface.co/collections/Tesslate/tessa-t1-react-reasoning-model-67e0fb72ca23e04473885c0e&lt;/a&gt; running on LM Studio on a 4090 (q8 at 34/s)&lt;/p&gt; &lt;p&gt;Tessa-T1 uses special UI based reasoning to help it excel in the UI/UX domain. It was trained via a custom model. For example:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Okay, so we're aiming for a Twitter-like interface, but with a dark theme. Let's break down how we'd approach building this in React using Tailwind CSS. Initially, when thinking about the overall structure of our app, I'm envisioning two main sections side-by-side â€“ like any typical split layout: one for the sidebar and one for the content itself. This immediately suggests a flex container to hold these elements horizontally.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And ofc, GGUF &lt;a href="https://huggingface.co/Tesslate/Tessa-T1-14B-Q8_0-GGUF"&gt;HERE&lt;/a&gt; (and quants are available by Bartowski). As always, please let me know if you have any suggestions or what else (models, agents, etc) you would like to see!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8w9klrnqrese1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpmfkp/i_made_an_open_source_react_component_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpmfkp/i_made_an_open_source_react_component_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T11:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpzkg3</id>
    <title>Are there official (from Google) quantized versions of Gemma 3?</title>
    <updated>2025-04-02T20:54:35+00:00</updated>
    <author>
      <name>/u/lostmsu</name>
      <uri>https://old.reddit.com/user/lostmsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe I am a moron, and can't use search, but I can't find quantized downloads made by &lt;strong&gt;Google&lt;/strong&gt; themselves. The best I could find is the Huggingface version in ggml-org, and a few community quants such as bartowski and unsloth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostmsu"&gt; /u/lostmsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpzkg3/are_there_official_from_google_quantized_versions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpzkg3/are_there_official_from_google_quantized_versions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpzkg3/are_there_official_from_google_quantized_versions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T20:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpozl7</id>
    <title>Best bang for the buck GPU</title>
    <updated>2025-04-02T13:48:38+00:00</updated>
    <author>
      <name>/u/Ok-Cucumber-7217</name>
      <uri>https://old.reddit.com/user/Ok-Cucumber-7217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this question is asked quite often, but going back to old posts makes me want to cry. I was naive enough to think that if I waited for the new generation of GPUs to come out, the older models would drop in price. &lt;/p&gt; &lt;p&gt;I'm curious about the best GPU for Local LLMs right now. How is AMD's support looking so far? I have 3 PCI slots (2 from CPU, 1 from chipset). What's the best bang for your buck?&lt;/p&gt; &lt;p&gt;I see the RTX 3060 12GB priced around $250. Meanwhile, the RTX 3090 24GB is around $850 or more, which makes me unsure if I should, I buy one RTX 3090 and leave some room for future upgrades, or just buy three RTX 3060s for roughly the same price.&lt;br /&gt; I had also considered the NVIDIA P40 with 24GB a while back, but it's currently priced at over $400, which is crazy expensive for what it was a year ago.&lt;/p&gt; &lt;p&gt;Also, Iâ€™ve seen mentions of risers, splitters, and bifurcationâ€”but how viable are these methods specifically for LLM inference? Will cutting down to x4 or x1 lanes per GPU actually tank performance ? &lt;/p&gt; &lt;p&gt;Mainly want to run 32b models (like Qwen2.5-Coder) but running some 70b models like llama3.1 would be cool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Cucumber-7217"&gt; /u/Ok-Cucumber-7217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpozl7/best_bang_for_the_buck_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpozl7/best_bang_for_the_buck_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpozl7/best_bang_for_the_buck_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T13:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpxq4y</id>
    <title>DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low</title>
    <updated>2025-04-02T19:39:51+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt; &lt;img alt="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" src="https://preview.redd.it/0ymxajfb5hse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82c6cdb4e5d31a3f747415575616dee79f009536" title="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ymxajfb5hse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T19:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpjt5e</id>
    <title>While Waiting for Llama 4</title>
    <updated>2025-04-02T08:36:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When we look exclusively at open-source models listed on LM Arena, we see the following top performers:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek-V3-0324&lt;/li&gt; &lt;li&gt;DeepSeek-R1&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it&lt;/li&gt; &lt;li&gt;DeepSeek-V3&lt;/li&gt; &lt;li&gt;QwQ-32B&lt;/li&gt; &lt;li&gt;Command A (03-2025)&lt;/li&gt; &lt;li&gt;Llama-3.3-Nemotron-Super-49B-v1&lt;/li&gt; &lt;li&gt;DeepSeek-v2.5-1210&lt;/li&gt; &lt;li&gt;Llama-3.1-Nemotron-70B-Instruct&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-405B-Instruct-bf16&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-405B-Instruct-fp8&lt;/li&gt; &lt;li&gt;DeepSeek-v2.5&lt;/li&gt; &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; &lt;li&gt;Qwen2.5-72B-Instruct&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, take a look at the Llama models. The most powerful one listed here is the massive 405B version. However, NVIDIA introduced Nemotron, and interestingly, the 70B Nemotron outperformed the larger Llama. Later, an even smaller Nemotron variant was released that performed even better!&lt;/p&gt; &lt;p&gt;But what happened next is even more intriguing. At the top of the leaderboard is DeepSeek, a very powerful model, but it's so large that it's not practical for home use. Right after that, we see the much smaller QwQ model outperforming all Llamas, not to mention older, larger Qwen models. And then, there's Gemma, an even smaller model, ranking impressively high.&lt;/p&gt; &lt;p&gt;All of this explains why Llama 4 is still in training. Hopefully, the upcoming version will bring not only exceptional performance but also better accessibility for local or home use, just like QwQ and Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpjt5e/while_waiting_for_llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpjt5e/while_waiting_for_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpjt5e/while_waiting_for_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T08:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jplol4</id>
    <title>Real-Time Speech-to-Speech Chatbot: Whisper, Llama 3.1, Kokoro, and Silero VAD ðŸš€</title>
    <updated>2025-04-02T10:53:28+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplol4/realtime_speechtospeech_chatbot_whisper_llama_31/"&gt; &lt;img alt="Real-Time Speech-to-Speech Chatbot: Whisper, Llama 3.1, Kokoro, and Silero VAD ðŸš€" src="https://external-preview.redd.it/vRmjjZbcWVfVL0zrDI-DRptisKl7geUnxputrhIWDyE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba053495efca8a551b00ed97b8623a1d83e84071" title="Real-Time Speech-to-Speech Chatbot: Whisper, Llama 3.1, Kokoro, and Silero VAD ðŸš€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tarun7r/Vocal-Agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplol4/realtime_speechtospeech_chatbot_whisper_llama_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jplol4/realtime_speechtospeech_chatbot_whisper_llama_31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T10:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpv5ld</id>
    <title>Sharing HallOumi-8B, an open-source hallucination detector usable with any LLM!</title>
    <updated>2025-04-02T17:57:21+00:00</updated>
    <author>
      <name>/u/jeremy_oumi</name>
      <uri>https://old.reddit.com/user/jeremy_oumi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Iâ€™m one of the co-founders of Oumi, an open-source AI startup, and wanted to share something weâ€™ve been working on.&lt;/p&gt; &lt;p&gt;I find generative AI to be pretty useful, but not that trustworthy. Whenever I ask for a summary of a document, or ask a question about a particular research paper, it always nags in the back of my mind: is this accurate or is it a hallucination? Where in the document does it say this? Personally, I donâ€™t want to have to read pages of a document to verify everything in the LLM output, so we built HallOumi!&lt;/p&gt; &lt;p&gt;Assuming you have a context (one or more documents) and a set of claims (summary, answer to a question, etc.), HallOumi can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Classify each claim as supported/unsupported, along with a confidence score&lt;/li&gt; &lt;li&gt;Provide citations (relevant sentences in the context) for each claim so that you know what exactly you should check in the document to verify as a human&lt;/li&gt; &lt;li&gt;Provide an explanation for that particular supported/unsupported label - sometimes hallucinations are so nuanced that it is hard even for humans to detect them without help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also made a classifier which runs a lot faster at similar quality, but you lose out on claim-level classification, the citations and explanations!&lt;/p&gt; &lt;p&gt;We built a small open-source demo where you can try out HallOumi locally (or any other model youâ€™d like) right away: &lt;a href="https://github.com/oumi-ai/halloumi-demo"&gt;https://github.com/oumi-ai/halloumi-demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We also have a hosted version online at &lt;a href="https://oumi.ai/halloumi-demo"&gt;https://oumi.ai/halloumi-demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Sharing all the code and documentation needed to train or run HallOumi here: &lt;a href="https://github.com/oumi-ai/oumi/tree/main/configs/projects/halloumi"&gt;https://github.com/oumi-ai/oumi/tree/main/configs/projects/halloumi&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The relevant models and datasets are also on HuggingFace:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/oumi-ai/HallOumi-8B"&gt;https://huggingface.co/oumi-ai/HallOumi-8B&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/oumi-ai/HallOumi-8B-classifier"&gt;https://huggingface.co/oumi-ai/HallOumi-8B-classifier&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-synthetic-claims"&gt;https://huggingface.co/datasets/oumi-ai/oumi-synthetic-claims&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-synthetic-document-claims"&gt;https://huggingface.co/datasets/oumi-ai/oumi-synthetic-document-claims&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-anli-subset"&gt;https://huggingface.co/datasets/oumi-ai/oumi-anli-subset&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-c2d-d2c-subset"&gt;https://huggingface.co/datasets/oumi-ai/oumi-c2d-d2c-subset&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technical deep dive here: &lt;a href="https://oumi.ai/blog/posts/introducing-halloumi"&gt;https://oumi.ai/blog/posts/introducing-halloumi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Happy to answer any questions too ðŸ™‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremy_oumi"&gt; /u/jeremy_oumi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:57:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpbnih</id>
    <title>Qwen3 will be released in the second week of April</title>
    <updated>2025-04-02T00:37:43+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Exclusive from Huxiu: Alibaba is set to release its new model, Qwen3, in the second week of April 2025. This will be Alibaba's most significant model product in the first half of 2025, coming approximately seven months after the release of Qwen2.5 at the Yunqi Computing Conference in September 2024.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://m.huxiu.com/article/4187485.html"&gt;https://m.huxiu.com/article/4187485.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpbnih/qwen3_will_be_released_in_the_second_week_of_april/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T00:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpi0n9</id>
    <title>KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800</title>
    <updated>2025-04-02T06:22:05+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt; &lt;img alt="KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800" src="https://external-preview.redd.it/02ytj9SuhUQUk607vUmaEPEVgYFEBjKWtivCx0rWwKk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26792f11e8a9b5c5d3f26c92c937264cb15d0fec" title="KTransformers Now Supports Multi-Concurrency and Runs 40 Tokens/s of DeepSeek-R1 Q4/FP8 on MRDIMM-8800" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, it's been a while since our last update. &lt;/p&gt; &lt;p&gt;We've been hard at work completely refactoring KTransformers to add the highly desired multi-concurrency support. This effort involved over 10,000 lines of code updates and took longer than we expected.&lt;/p&gt; &lt;p&gt;Drawing inspiration from the excellent architecture of sglang, we have implemented high-performance asynchronous concurrent scheduling in C++, including features like &lt;strong&gt;continuous batching, chunked prefill,&lt;/strong&gt; and more. Thanks to GPU sharing in concurrent scenarios and the efficient flashinfer lib, overall throughput has also improved to a certain extent.&lt;/p&gt; &lt;p&gt;Also, with support from Intel, we tested KTransformers v0.2.4 on the latest Xeon6 + MRDIMM-8800 platform. By increasing concurrency, the total output throughput increased &lt;strong&gt;from 17 tokens/s to 40 tokens/s.&lt;/strong&gt; We observed that the bottleneck has now shifted to the GPU. Using a higher-end GPU than the 4090D could further improve performance.&lt;/p&gt; &lt;p&gt;The following is a demonstration and you can find more infomation from &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/balance-serve.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/balance-serve.md&lt;/a&gt; :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/10g65zko0dse1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d2af5901cb0f773d315bfdfb324bb3c8ecf61a72"&gt;https://preview.redd.it/10g65zko0dse1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d2af5901cb0f773d315bfdfb324bb3c8ecf61a72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After this huge refactoring, we can now start working on merging the AMX part and open sourcing it. We are sure that this will happen in April.&lt;/p&gt; &lt;p&gt;Finally, we greatly thank the local LLaMa community for your support. We now have over 13K GitHub stars and are widely deployed in many scenarios. KTransformers is a project that grew from the localLLaMa community, and we hope to see what you want next.&lt;/p&gt; &lt;p&gt;Stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpi0n9/ktransformers_now_supports_multiconcurrency_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T06:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jps1xm</id>
    <title>PAI: your personal AI 100% local inspired by Google's Project Astra</title>
    <updated>2025-04-02T15:54:46+00:00</updated>
    <author>
      <name>/u/Such_Advantage_6949</name>
      <uri>https://old.reddit.com/user/Such_Advantage_6949</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt; &lt;img alt="PAI: your personal AI 100% local inspired by Google's Project Astra" src="https://external-preview.redd.it/73JBodiKh6lM1UQDVfjtY1zzKdX1ydEHwEZeN3MnZaE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6424d4101ef831404802ae4974c2f9be087d0819" title="PAI: your personal AI 100% local inspired by Google's Project Astra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Google's Project Astra, I have created an App for audio + video chat bot that is 100% local and open source.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fty7fzxd1gse1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6f771ece87afe7cd87bb559cc0be812235412ea6"&gt;https://preview.redd.it/fty7fzxd1gse1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6f771ece87afe7cd87bb559cc0be812235412ea6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;iOS app&lt;/li&gt; &lt;li&gt;100% locally hosted&lt;/li&gt; &lt;li&gt;Open Source&lt;/li&gt; &lt;li&gt;Visual Question answer&lt;/li&gt; &lt;li&gt;Streaming via RTC &amp;amp; Livekit for low latency&lt;/li&gt; &lt;li&gt;Screen Sharing&lt;/li&gt; &lt;li&gt;Live transcription&lt;/li&gt; &lt;li&gt;Change LLM to any model supported by Exllama v2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is a short 2 mins demo: &lt;a href="https://youtu.be/pNksZ_lXqgs"&gt;https://youtu.be/pNksZ_lXqgs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/remichu-ai/pai.git"&gt;https://github.com/remichu-ai/pai.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a STT + LLM + TTS, so feel free to skip if it is deal breaker for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Such_Advantage_6949"&gt; /u/Such_Advantage_6949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jps289</id>
    <title>Matharena USAMO update: Gemini 2.5 Pro is the first model to achieve non-trivial amount of points</title>
    <updated>2025-04-02T15:55:06+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See here: &lt;a href="https://matharena.ai/"&gt;https://matharena.ai/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro at 24.5%, next is R1 at 4.76%. From mbalunovic on X.&lt;/p&gt; &lt;p&gt;Note also that the benchmark was released on the same day as the Gemini release, so this isn't a case of training on the eval. An impressive result, and the pace of progress is incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpvxw0</id>
    <title>koboldcpp-1.87.1: Merged Qwen2.5VL support! :)</title>
    <updated>2025-04-02T18:27:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T18:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpuoh7</id>
    <title>Now we talking INTELLIGENCE EXPLOSIONðŸ’¥ðŸ”… | â…•áµ—Ê° of benchmark cracked by claude 3.5!</title>
    <updated>2025-04-02T17:39:04+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_â…•áµ—Ê°_of/"&gt; &lt;img alt="Now we talking INTELLIGENCE EXPLOSIONðŸ’¥ðŸ”… | â…•áµ—Ê° of benchmark cracked by claude 3.5!" src="https://preview.redd.it/ziowvxg7kgse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acbf3232ba93ffa825062a5d7ef599eb46ce434b" title="Now we talking INTELLIGENCE EXPLOSIONðŸ’¥ðŸ”… | â…•áµ—Ê° of benchmark cracked by claude 3.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ziowvxg7kgse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_â…•áµ—Ê°_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_â…•áµ—Ê°_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptdtg</id>
    <title>Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!</title>
    <updated>2025-04-02T16:48:30+00:00</updated>
    <author>
      <name>/u/JawGBoi</name>
      <uri>https://old.reddit.com/user/JawGBoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt; &lt;img alt="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" src="https://external-preview.redd.it/T7uMJGzQ_xDL3OLN-I6nY-QjBc2LJ_pj5xaq0KJj7XI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84d098830ff90a292e2d57523020f6f6a49fb61" title="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model repo: &lt;a href="https://github.com/kyutai-labs/moshi"&gt;https://github.com/kyutai-labs/moshi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JawGBoi"&gt; /u/JawGBoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kyutai-labs/moshi-finetune"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jplg2o</id>
    <title>LiveBench team just dropped a leaderboard for coding agent tools</title>
    <updated>2025-04-02T10:37:19+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt; &lt;img alt="LiveBench team just dropped a leaderboard for coding agent tools" src="https://preview.redd.it/qxqj0vjtgese1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e87e657142ff097ddef495de75a59a3206ae77e" title="LiveBench team just dropped a leaderboard for coding agent tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qxqj0vjtgese1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T10:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpr1nk</id>
    <title>The Candle Test - most LLMs fail to generalise at this simple task</title>
    <updated>2025-04-02T15:13:10+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt; &lt;img alt="The Candle Test - most LLMs fail to generalise at this simple task" src="https://preview.redd.it/6phgn27rqfse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=676b32e5d96ebb0c0830e00756c8e79d41840121" title="The Candle Test - most LLMs fail to generalise at this simple task" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure a lot of people here noticed that latest frontier models are... weird. Teams facing increased pressure to chase a good place in the benchmarks and make the SOTA claims - the models are getting more and more overfit resulting in decreased generalisation capabilities.&lt;/p&gt; &lt;p&gt;It became especially noticeable with the very last line-up of models which despite being better on paper somehow didn't feel so with daily use.&lt;/p&gt; &lt;p&gt;So, I present to you a very simple test that highlights this problem. It consists of three consecutive questions where the model is steered away from possible overfit - yet most still demonstrate it on the final conversation turn (including thinking models).&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are candles getting taller or shorter when they burn?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models correctly identify that candles are indeed getting shorter when burning.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are you sure? Will you be able to recognize this fact in different circumstances?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models confidently confirm that such a foundational fact is hard to miss under any circumstances.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Now, consider what you said above and solve the following riddle: I'm tall when I'm young, and I'm taller when I'm old. What am I?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And here most models are as confidently wrong claiming that the answer is a candle.&lt;/p&gt; &lt;p&gt;Unlike traditional misguided attention tasks - this test gives model ample chances for in-context generalisation. Failing this test doesn't mean that the model is &amp;quot;dumb&amp;quot; or &amp;quot;bad&amp;quot; - most likely it'll still be completely fine for 95% of use-cases, but it's also more likely to fail in a novel situation.&lt;/p&gt; &lt;p&gt;Here are some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/7e9815b3-15ba-4a4c-81e1-0f233f1b0d5a"&gt;DeepSeek Chat V3&lt;/a&gt; (0324, Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/3e27bf44-c64c-4558-b98f-989fb1c82688"&gt;DeepSeek R1&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/f1c205e4-ee2d-41e4-87b4-e8c9dbe0024b"&gt;DeepSeek R1 Distill Llama 70B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/4ac04a5d-8199-4675-b4ce-5e3cbbb9223d"&gt;Llama 3.1 405B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;QwQ 32B didn't pass due to entering endless loop multiple times&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/5ff0eb98-cd36-4988-a2a0-e01416ac567d"&gt;Mistral Large&lt;/a&gt; (Passes, one of the few)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inpired by my frustration with Sonnet 3.7 (which also fails this test, unlike Sonnet 3.5).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6phgn27rqfse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptset</id>
    <title>University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy</title>
    <updated>2025-04-02T17:04:49+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt; &lt;img alt="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" src="https://b.thumbs.redditmedia.com/nIuszN8uDMIjbhUsiZdUw2NeBO5my-uVcctXiMF1pcI.jpg" title="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jptset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:04:49+00:00</published>
  </entry>
</feed>
