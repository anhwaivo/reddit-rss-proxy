<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-06T16:24:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iik4y9</id>
    <title>Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI</title>
    <updated>2025-02-05T20:36:22+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt; &lt;img alt="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" src="https://external-preview.redd.it/fkk_hfuiSuMOZjLy_dEtjSiqJMOwZz9w_oAKY_5Q2Nk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3dadc03291c7ac04f201561f33b9b740f85a835" title="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just released an update of Kiln on Github which allows you to distill a custom fine-tuned model from Deepseek R1 (or any reasoning model/chain-of-thought). The whole process only takes about 30 minutes, including generating a synthetic training dataset. It doesn't require any coding or command line work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The attached video shows the process&lt;/li&gt; &lt;li&gt;Our docs have &lt;a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model"&gt;a guide for distilling R1&lt;/a&gt; if you want to try it out yourself&lt;/li&gt; &lt;li&gt;Here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github repo&lt;/a&gt; with all of the source code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also wanted to add a huge thanks to &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; for the awesome reception to on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;last post&lt;/a&gt;. It really inspires me to keep building. I've already made about 30 improvements and built feature requests which came from people who found it via &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Kiln runs locally and we never have access to your dataset. Unsloth is fully supported if you have the GPUs to train locally. You can also use a training service like Fireworks &amp;amp; OpenAI if you prefer (data is sent to them with your keys, we still never have access to it). &lt;/p&gt; &lt;p&gt;If anyone wants to try Kiln, here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;GitHub repository&lt;/a&gt; and &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I'm curious to get any feedback/ideas. It really helps me improve Kiln. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iik4y9/video/1vnufrecrdhe1/player"&gt;Kiln AI demo - distilling Deepseek R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1kh1</id>
    <title>Any tool which is an LLM based local file "explorer + search engine" ?</title>
    <updated>2025-02-06T12:38:44+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm talking about the ones which don't use RAG, and are super low profile. If any. Otherwise please drop the ones using RAG or are heavy or of any other type. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1kh1/any_tool_which_is_an_llm_based_local_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1kh1/any_tool_which_is_an_llm_based_local_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1kh1/any_tool_which_is_an_llm_based_local_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirjvz</id>
    <title>How are people using models smaller than 5b parameters?</title>
    <updated>2025-02-06T02:05:49+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I straight up don't understand the real world problems these models are solving. I get them in theory, function calling, guard, and agents once they've been fine tuned. But I'm yet to see people come out and say, &amp;quot;hey we solved this problem with a 1.5b llama model and it works really well.&amp;quot;&lt;/p&gt; &lt;p&gt;Maybe I'm blind or not good enough to use them well some hopefully y'all can enlighten me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T02:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij4f9b</id>
    <title>Using LLM's to practice / learn a new language?</title>
    <updated>2025-02-06T14:57:52+00:00</updated>
    <author>
      <name>/u/TheMikeans</name>
      <uri>https://old.reddit.com/user/TheMikeans</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to find the best way to leverage large language models (LLMs) to learn and practice a new language (Dutch). I am unsure what the best approach would be: should I use something like ChatGPT and instruct it to &amp;quot;roleplay&amp;quot; with me, pretending we're having a chat between friends, or is it better to host an LLM locally with a system prompt that instructs it to act like a person I have casual conversations with? Any pointers would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMikeans"&gt; /u/TheMikeans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4f9b/using_llms_to_practice_learn_a_new_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4f9b/using_llms_to_practice_learn_a_new_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4f9b/using_llms_to_practice_learn_a_new_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T14:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iih21v</id>
    <title>Google's been at work, not Gemma 3 sadly</title>
    <updated>2025-02-05T18:32:11+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt; &lt;img alt="Google's been at work, not Gemma 3 sadly" src="https://preview.redd.it/x5uaqeak6dhe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87ac5db8ef998927b98f0559468dd0f99a87fa19" title="Google's been at work, not Gemma 3 sadly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x5uaqeak6dhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5jnh</id>
    <title>DeepSeek-R1 for agentic tasks</title>
    <updated>2025-02-06T15:46:07+00:00</updated>
    <author>
      <name>/u/semteXKG</name>
      <uri>https://old.reddit.com/user/semteXKG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-R1 doesn't support tool use natively, but can be used for agentic tasks through code actions. Here's an interesting blog post that describes this approach: &lt;a href="https://krasserm.github.io/2025/02/05/deepseek-r1-agent/"&gt;https://krasserm.github.io/2025/02/05/deepseek-r1-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Outperforms Claude 3.5 Sonnet by a large margin in a single-agent setup (65.6% vs 53.1% on a GAIA subset). The post also covers limitations of DeepSeek-R1 in this context, e.g. long reasoning traces and &amp;quot;underthinking&amp;quot; phenomenon.&lt;/p&gt; &lt;p&gt;Has anyone experience with DeepSeek-R1 for agentic tasks and can share their approaches or thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/semteXKG"&gt; /u/semteXKG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5jnh/deepseekr1_for_agentic_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5jnh/deepseekr1_for_agentic_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5jnh/deepseekr1_for_agentic_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikhg3</id>
    <title>I found a way to speed up CPU based LLM inference using a HNSW index on the output embeddings</title>
    <updated>2025-02-05T20:50:51+00:00</updated>
    <author>
      <name>/u/martinloretz</name>
      <uri>https://old.reddit.com/user/martinloretz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To get the next token from an LLM, we compute the probabilities for each individual token in the LLM's vocabulary by multiplying the last hidden state with the output embedding matrix. This matrix is massive, accounting for up to 20% of the total parameters in small multilingual LLMs. &lt;/p&gt; &lt;p&gt;When sampling the next token with top-k sampling, we're only sampling from the 40 most probable tokens out of 128,256 (for Llama 3.2 models). By using an HNSW vector index, we can retrieve these 40 most probable tokens directly through an approximate nearest neighbor search over the output embeddings, avoiding the full matrix multiplication with the output embeddings. &lt;/p&gt; &lt;p&gt;This reduces memory accesses and computation, resulting in up to 28% faster CPU-based inference for Llama 2.1 1B on mid-range laptops.&lt;/p&gt; &lt;h3&gt;For more details, read the full blog post on &lt;a href="https://martinloretz.com/blog/vector-index-cpu/"&gt;martinloretz.com/blog/vector-index-cpu/&lt;/a&gt;&lt;/h3&gt; &lt;h2&gt;Benchmarks&lt;/h2&gt; &lt;p&gt;&lt;code&gt;llama-bench&lt;/code&gt; for Llama 1B F16 (Ubuntu = Intel® Core™ i7-10750H x 12, 2 x 16GiB DDR4 2933 MHz, MacBook = MacBook Pro 16&amp;quot; M4 Pro, vec = vector index, MM = matrix multiplication (reference)):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="right"&gt;threads&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;Vec t/s&lt;/th&gt; &lt;th align="right"&gt;MM t/s&lt;/th&gt; &lt;th align="right"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;5.99 ± 0.05&lt;/td&gt; &lt;td align="right"&gt;4.73 ± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.27&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;6&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.51 ± 0.30&lt;/td&gt; &lt;td align="right"&gt;9.72 ± 0.13&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.29&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;23.56 ± 0.24&lt;/td&gt; &lt;td align="right"&gt;20.11 ± 0.44&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.17&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.52 ± 0.31&lt;/td&gt; &lt;td align="right"&gt;11.80 ± 0.18&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.06&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LLama 3.2 1B was selected for these benchmarks because of its relatively large embedding matrix (21% of all parameters). Full model speedups for larger models are lower because less time is spent computing the output embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To replicate these benchmarks, checkout this code of the &lt;a href="https://github.com/martinloretzzz/llama.cpp"&gt;fork of llama.cpp&lt;/a&gt;.&lt;/strong&gt; Installation instructions are in the Readme.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinloretz"&gt; /u/martinloretz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii82yg</id>
    <title>DeepSeek just released an official demo for DeepSeek VL2 Small - It's really powerful at OCR, text extraction and chat use-cases (Hugging Face Space)</title>
    <updated>2025-02-05T11:40:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small"&gt;https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Vaibhav (VB) Srivastav on X: &lt;a href="https://x.com/reach_vb/status/1887094223469515121"&gt;https://x.com/reach_vb/status/1887094223469515121&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Zizheng Pan on X: Our official huggingface space demo for DeepSeek-VL2 Small is out! A 16B MoE model for various vision-language tasks: &lt;a href="https://x.com/zizhpan/status/1887110842711162900"&gt;https://x.com/zizhpan/status/1887110842711162900&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iixy36</id>
    <title>MNN android support for deepseek r1 1.5b</title>
    <updated>2025-02-06T08:31:19+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iixy36/mnn_android_support_for_deepseek_r1_15b/"&gt; &lt;img alt="MNN android support for deepseek r1 1.5b" src="https://external-preview.redd.it/gk2Ulk7ekzGlEE11Q-IaL-ilF-aztcncit7oLtiKI9Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0994162db48ae28be3cfe5c5566969dfdf23240d" title="MNN android support for deepseek r1 1.5b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;mainpage: &lt;a href="https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md"&gt;MnnLlmApp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;apk download: &lt;a href="https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md#version-02"&gt;version0.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FAQ:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why is the app not available on the App Store? &lt;ul&gt; &lt;li&gt;The application is currently in its early development stages. It will be published on the App Store once it reaches a stable and more mature state.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Will there be support for iOS? &lt;ul&gt; &lt;li&gt;Yes, an iOS version is under development. We anticipate releasing it within the next few weeks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://i.redd.it/vuupe0hoahhe1.gif"&gt;https://i.redd.it/vuupe0hoahhe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iixy36/mnn_android_support_for_deepseek_r1_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iixy36/mnn_android_support_for_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iixy36/mnn_android_support_for_deepseek_r1_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T08:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiij1d</id>
    <title>DeepSeek R1 ties o1 for first place on the Generalization Benchmark.</title>
    <updated>2025-02-05T19:30:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt; &lt;img alt="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." src="https://preview.redd.it/7na44xs3gdhe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77dd2e43eb2352bf9c4ab11068ca9221f8b83934" title="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7na44xs3gdhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij205h</id>
    <title>Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!</title>
    <updated>2025-02-06T13:02:20+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt; &lt;img alt="Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!" src="https://external-preview.redd.it/MI3THquRgDiXtkGwCynZSuvm0Cdq3csr60FcHLC_Xk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ae6c4d987ccc5b9eff4fb95ca0547ed5c8a98a2" title="Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/powerserve-project/PowerServe"&gt;PowerServe&lt;/a&gt; is a &lt;strong&gt;high-speed&lt;/strong&gt; and &lt;strong&gt;easy-to-use&lt;/strong&gt; LLM serving framework for local deployment. You can deploy popular LLMs with our &lt;a href="https://github.com/powerserve-project/PowerServe/blob/main/docs/end_to_end.md"&gt;one-click compilation and deployment&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;PowerServe offers the following advantages:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Lightning-Fast Prefill and Decode&lt;/strong&gt;: Optimized for NPU, achieving over 10x faster prefill speeds compared to llama.cpp, significantly accelerating model warm-up.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Efficient NPU Speculative Inference&lt;/strong&gt;: Supports speculative inference, delivering 2x faster inference speeds compared to traditional autoregressive decoding.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Seamless OpenAI API Compatibility&lt;/strong&gt;: Fully compatible with OpenAI API, enabling effortless migration of existing applications to the PowerServe platform.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Model Support&lt;/strong&gt;: Compatible with mainstream large language models such as &lt;strong&gt;Llama3&lt;/strong&gt;, &lt;strong&gt;Qwen2.5&lt;/strong&gt;, and &lt;strong&gt;InternLM3&lt;/strong&gt;, catering to diverse application needs.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Ease of Use&lt;/strong&gt;: Features one-click deployment for quick setup, making it accessible to everyone.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ij205h/video/dsu4qf4doihe1/player"&gt;Running DeepSeek-R1-Distill-Llama-8B with NPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T13:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiyj4q</id>
    <title>[2502.03387] LIMO: Less is More for Reasoning</title>
    <updated>2025-02-06T09:15:58+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.03387"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiyj4q/250203387_limo_less_is_more_for_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiyj4q/250203387_limo_less_is_more_for_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T09:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij4c7h</id>
    <title>Unpopular opinion. The chatbot arena benchmark is not useless, rather it is misunderstood. It is not necessarily an hard benchmark, rather it is a benchmark of "what if the LLM would answer common queries for search engines"?</title>
    <updated>2025-02-06T14:54:12+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From another thread:&lt;/p&gt; &lt;p&gt;The gemini flash thinking is great on chatbot arena. But why this? Before one jumps on the bandwagon &amp;quot;chatbot arena sucks&amp;quot; one has to understand what is tested there. Many say &amp;quot;human preferences&amp;quot; but I think it is a bit different.&lt;/p&gt; &lt;p&gt;Most likely on chatbot arena people test the LLMs with relatively simple questions. Akin to &amp;quot;tell me how to write a function in X&amp;quot; rather than &amp;quot;this function doesn't work, fix it&amp;quot;.&lt;/p&gt; &lt;p&gt;Chatbot arena (at least for the category overall) is great to say &amp;quot;which model would be great for everyday use instead of searching the web&amp;quot;.&lt;/p&gt; &lt;p&gt;And I think that some companies, like google, are optimizing exactly for that. Hence Chatbot arena is relevant for them. They want to have models that can substitute or complement their search engine.&lt;/p&gt; &lt;p&gt;More often than not on reddit people complain that Claude or other models do not excel in chatbot arena (again, the overall category), and thus the benchmark sucks. But that is because those people use the LLMs differently from the voters in chatbot arena.&lt;/p&gt; &lt;p&gt;Asking an LLM to help on a niche (read: not that common in internet) coding or debugging problem is harder than a &amp;quot;I use the LLM rather than the search&amp;quot; request. Hence some models are good in hard benchmarks but less good in a benchmark that at the end measures the &amp;quot;substitute a search engine for common questions&amp;quot; metric.&lt;/p&gt; &lt;p&gt;Therefore the point &amp;quot;I have a feeling all the current evals those model releases are using are just too far away from real work/life scenarios.&amp;quot; is somewhat correct. If a model optimizes for Chatbot arena / search engine usage, then of course it is unlikely to be trained to solve consistently niche problems.&lt;/p&gt; &lt;p&gt;And even if one has a benchmark that is more relevant to the use case (say: aider, livebench and what not). If one has a LLM that is right 60% of the time, there is still a lot of work to do for the person to fill the gaps.&lt;/p&gt; &lt;p&gt;Then it also depends on the prompts - I found articles in the past where prompts where compared and some could really extract from from an LLM. Those prompts are standardized and optimized in &amp;quot;ad hoc&amp;quot; benchmarks. In Chatbot arena the prompts could be terrible, hence once again what is tested is &amp;quot;what people would type in a LLM based search engine&amp;quot;.&lt;/p&gt; &lt;p&gt;IMO what the people from LMSYS offer as hard human based benchmarking offers are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the category hard prompts for general cases&lt;/li&gt; &lt;li&gt;the category longer query for general cases (most of the bullshit prompts IMO are short)&lt;/li&gt; &lt;li&gt;(a bit unsure here) the category multi turn. In a 1:1 usage, we ask many questions in the same conversation with a model. On chatbot arena people vote mostly on one shot questions, end of it. That is also a huge difference from personal LLM use.&lt;/li&gt; &lt;li&gt;for coding, the WebDev Arena Leaderboard - there Claude is #1 by a mile (so far) . Claude 3.5 (from October 24) has 1250 Elo points, Deepseek R1 1210, o3 mini-high 1161, the next non-thinking model, Gemini exp 1206 has 1025. The distance Claude 3.5 vs Gemini exp is over 200 points, is massive and thus I think that actually Claude &amp;quot;thinks&amp;quot;, at least in some domains. It cannot be that is so strong without thinking.&lt;/li&gt; &lt;li&gt;It would be cool if Chatbot Arena would add &amp;quot;hard prompts&amp;quot; for each specific subcategory. For example &amp;quot;math hard prompts&amp;quot;, &amp;quot;coding hard prompts&amp;quot; and so on. But I guess that would dilute the votes too much and would require too much classification every week.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This to say, I think chatbot arena is very useful IF seen in the proper context, that is mostly &amp;quot;search engine / stack overflow replacement&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T14:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1ew9</id>
    <title>lineage-bench benchmark results updated with recently released models</title>
    <updated>2025-02-06T12:29:58+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt; &lt;img alt="lineage-bench benchmark results updated with recently released models" src="https://preview.redd.it/pgf54p7ddihe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02d2e9ae3c8b4d6c2160f1ebcb0f9b3a96964947" title="lineage-bench benchmark results updated with recently released models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgf54p7ddihe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iisj7j</id>
    <title>Open WebUI drops 3 new releases today. Code Interpreter, Native Tool Calling, Exa Search added</title>
    <updated>2025-02-06T02:55:42+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;0.5.8 had a slew of new adds. 0.5.9 and 0.5.10 seemed to be minor bug fixes for the most part. From their release page:&lt;/p&gt; &lt;p&gt;🖥️ Code Interpreter: Models can now execute code in real time to refine their answers dynamically, running securely within a sandboxed browser environment using Pyodide. Perfect for calculations, data analysis, and AI-assisted coding tasks!&lt;/p&gt; &lt;p&gt;💬 Redesigned Chat Input UI: Enjoy a sleeker and more intuitive message input with improved feature selection, making it easier than ever to toggle tools, enable search, and interact with AI seamlessly.&lt;/p&gt; &lt;p&gt;🛠️ Native Tool Calling Support (Experimental): Supported models can now call tools natively, reducing query latency and improving contextual responses. More enhancements coming soon!&lt;/p&gt; &lt;p&gt;🔗 Exa Search Engine Integration: A new search provider has been added, allowing users to retrieve up-to-date and relevant information without leaving the chat interface.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T02:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirej3</id>
    <title>The New Gemini Pro 2.0 Experimental sucks Donkey Balls.</title>
    <updated>2025-02-06T01:58:43+00:00</updated>
    <author>
      <name>/u/Odd-Environment-7193</name>
      <uri>https://old.reddit.com/user/Odd-Environment-7193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wow. Last night, after a long coding bender I heard the great news that Gemini were releasing some new models. I woke up this morning super excited to try them.&lt;/p&gt; &lt;p&gt;My first attempt was a quick OCR with Flesh light 2.0 and I was super impressed with the Speed. This thing is going to make complex OCR an absolute breeze. I cannot wait to incorporate this into my apps. I reckon it's going to cut the processing times in half. (Christmas came early)&lt;/p&gt; &lt;p&gt;Then I moved onto testing the Gemini 2.0 Pro Experimental.&lt;/p&gt; &lt;p&gt;How disappointing... This is such a regression from 1206. I could immediately see the drop in the quality of the tasks I've been working on daily like coding.&lt;/p&gt; &lt;p&gt;It makes shit tons of mistakes. The code that comes out doesn't have valid HTML (Super basic task) and it seems to want to interject and refactor code all the time without permission.&lt;/p&gt; &lt;p&gt;I don't know what the fuck these people are doing. Every single release it's like this. They just can't seem to get it right. 1206 has been a great model, and I've been using it as my daily driver for quite some time. I was actually very impressed with it and had they just released 1206 as Gemini 2.0 pro EXP I would have been stoked. This is an absolute regression.&lt;/p&gt; &lt;p&gt;I have seen this multiple times now with Google products. The previous time the same thing happened with 0827 and then Gemini 002.&lt;/p&gt; &lt;p&gt;For some reason at that time, they chose to force concise answers into everything, basically making it impossible to get full lengthy responses. Even with system prompts, it would just keep shortening code, adding comments into everything and basically forcing this dogshit concise mode behavior into everything.&lt;/p&gt; &lt;p&gt;Now they've managed to do it again. This model is NOT better than 1206. The benchmarks or whatever these people are aiming to beat are just an illusion. If your model cannot do simple tasks like outputting valid code without trying to force refactoring it is just a hot mess.&lt;/p&gt; &lt;p&gt;Why can't they get this right? They seem to regress a lot on updates. I've had discussions with people in the know, and apparently it's difficult to juggle the various needs of all the different types of people. Where some might like lengthy thorough answers for example, others might find that annoying and &amp;quot;too verbose&amp;quot;. So basically we get stuck with these half arsed models that don't seem to excel in anything in particular.&lt;/p&gt; &lt;p&gt;I use these models for coding and for writing, which has always been the case. I might be in the minority of users and just be too entitled about this. But jesus, what a disappointment.&lt;/p&gt; &lt;p&gt;I am not shitting you, when I say I would rather use deepseek than whatever this is. It's ability to give long thorough answers, without changing parts of code unintentionally is extremely valuable to my use cases.&lt;/p&gt; &lt;p&gt;Google is the biggest and most reliable when it comes to serving their models though, and I absolutely love the flash models for building apps. So you could say I am a major lover and hater of them. It's always felt this way. A genuine love-hate relationship. I am secretly rooting for their success but I absolutely loathe some of the things they do and am really surprised they haven't surpassed chatgpt/claude yet.. Like how the fuck?&lt;/p&gt; &lt;p&gt;Maybe it's time to outsource their LLM production to CHHHIIIIINNAAAA. Just like everything else. Hahahaa&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Environment-7193"&gt; /u/Odd-Environment-7193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T01:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5yf2</id>
    <title>How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use</title>
    <updated>2025-02-06T16:03:05+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt; &lt;img alt="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" src="https://a.thumbs.redditmedia.com/5GvbBLMtQKog3tISnQ2IpuYVRJEYPT5-0ptjxlTHnR4.jpg" title="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to share something I built after my long health journey. For 5 years, I struggled with mysterious symptoms - getting injured easily during workouts, slow recovery, random fatigue, joint pain. I spent over $100k visiting more than 30 hospitals and specialists, trying everything from standard treatments to experimental protocols at longevity clinics. Changed diets, exercise routines, sleep schedules - nothing seemed to help.&lt;/p&gt; &lt;p&gt;The most frustrating part wasn't just the lack of answers - it was how fragmented everything was. Each doctor only saw their piece of the puzzle: the orthopedist looked at joint pain, the endocrinologist checked hormones, the rheumatologist ran their own tests. No one was looking at the whole picture. It wasn't until I visited a rheumatologist who looked at the combination of my symptoms and genetic test results that I learned I likely had an autoimmune condition.&lt;/p&gt; &lt;p&gt;Interestingly, when I fed all my symptoms and medical data from before the rheumatologist visit into GPT, it suggested the same diagnosis I eventually received. After sharing this experience, I discovered many others facing similar struggles with fragmented medical histories and unclear diagnoses. That's what motivated me to turn this into an open source tool for anyone to use. While it's still in early stages, it's functional and might help others in similar situations.&lt;/p&gt; &lt;p&gt;Here's what it looks like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/v6j508rxkjhe1.gif"&gt;https://i.redd.it/v6j508rxkjhe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenHealthForAll/open-health"&gt;https://github.com/OpenHealthForAll/open-health&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**What it can do:**&lt;/p&gt; &lt;p&gt;* Upload medical records (PDFs, lab results, doctor notes)&lt;/p&gt; &lt;p&gt;* Automatically parses and standardizes lab results:&lt;/p&gt; &lt;p&gt;- Converts different lab formats to a common structure&lt;/p&gt; &lt;p&gt;- Normalizes units (mg/dL to mmol/L etc.)&lt;/p&gt; &lt;p&gt;- Extracts key markers like CRP, ESR, CBC, vitamins&lt;/p&gt; &lt;p&gt;- Organizes results chronologically&lt;/p&gt; &lt;p&gt;* Chat to analyze everything together:&lt;/p&gt; &lt;p&gt;- Track changes in lab values over time&lt;/p&gt; &lt;p&gt;- Compare results across different hospitals&lt;/p&gt; &lt;p&gt;- Identify patterns across multiple tests&lt;/p&gt; &lt;p&gt;* Works with different AI models:&lt;/p&gt; &lt;p&gt;- Local models like Deepseek (runs on your computer)&lt;/p&gt; &lt;p&gt;- Or commercial ones like GPT4/Claude if you have API keys&lt;/p&gt; &lt;p&gt;**Getting Your Medical Records:**&lt;/p&gt; &lt;p&gt;If you don't have your records as files:&lt;/p&gt; &lt;p&gt;- Check out [Fasten Health](&lt;a href="https://github.com/fastenhealth/fasten-onprem"&gt;https://github.com/fastenhealth/fasten-onprem&lt;/a&gt;) - it can help you fetch records from hospitals you've visited&lt;/p&gt; &lt;p&gt;- Makes it easier to get all your history in one place&lt;/p&gt; &lt;p&gt;- Works with most US healthcare providers&lt;/p&gt; &lt;p&gt;**Current Status:**&lt;/p&gt; &lt;p&gt;- Frontend is ready and open source&lt;/p&gt; &lt;p&gt;- Document parsing is currently on a separate Python server&lt;/p&gt; &lt;p&gt;- Planning to migrate this to run completely locally&lt;/p&gt; &lt;p&gt;- Will add to the repo once migration is done&lt;/p&gt; &lt;p&gt;Let me know if you have any questions about setting it up or using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T16:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiveqd</id>
    <title>So, Google has no state-of-the-art frontier model now?</title>
    <updated>2025-02-06T05:34:09+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt; &lt;img alt="So, Google has no state-of-the-art frontier model now?" src="https://preview.redd.it/64r0glzkgghe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4b6ad82ec54e92060d2226f5e8ec28c2f2eaf9b" title="So, Google has no state-of-the-art frontier model now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64r0glzkgghe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T05:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwgou</id>
    <title>For coders! free&amp;open DeepSeek R1 &gt; $20 o3-mini with rate-limit!</title>
    <updated>2025-02-06T06:43:10+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt; &lt;img alt="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" src="https://preview.redd.it/n9sntvkvsghe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a41575abce1f1f8a8cb02b9965f65a613e1a0174" title="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9sntvkvsghe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiio9u</id>
    <title>Anthropic: ‘Please don’t use AI’</title>
    <updated>2025-02-05T19:36:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt; &lt;img alt="Anthropic: ‘Please don’t use AI’" src="https://external-preview.redd.it/XdLbwNiaDfP6hGsSmn44MWaR_4YQK7L36Ar5RuZkt4s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f5b6dec6d423a65124ea27edb0de0e52f12e6ef" title="Anthropic: ‘Please don’t use AI’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process. We want to understand your personal interest in Anthropic without mediation through an AI system, and we also want to evaluate your non-AI-assisted communication skills. Please indicate ‘Yes’ if you have read and agree.&amp;quot;&lt;/p&gt; &lt;p&gt;There's a certain irony in having one of the biggest AI labs coming against AI applications and acknowledging the enshittification of the whole job application process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/9b1e6af4-94f2-41c6-bb91-96a74b9b2da1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilrym</id>
    <title>Gemma 3 on the way!</title>
    <updated>2025-02-05T21:43:33+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt; &lt;img alt="Gemma 3 on the way!" src="https://preview.redd.it/q2q4555s4ehe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be6c986a18108bcff251eb781a9cd1a0f4bcbd3" title="Gemma 3 on the way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19"&gt;https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2q4555s4ehe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T21:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwmsq</id>
    <title>Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost</title>
    <updated>2025-02-06T06:55:03+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt; &lt;img alt="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" src="https://b.thumbs.redditmedia.com/U7IbKXWllKMESakzdcsFfg82O-BgJ0wgsGCf2i_dXrc.jpg" title="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iiwmsq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1xge</id>
    <title>Autiobooks: Automatically convert epubs to audiobooks (kokoro)</title>
    <updated>2025-02-06T12:58:49+00:00</updated>
    <author>
      <name>/u/vosFan</name>
      <uri>https://old.reddit.com/user/vosFan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt; &lt;img alt="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" src="https://external-preview.redd.it/ZWtwaHU5YzBvaWhlMV54VUX8u6k6pXdX7L9L_cCrxwAtDjHSCnwLZyQNJRce.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7c76ff926c140c89393b924cb7e4fd0e235e742" title="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;https://github.com/plusuncold/autiobooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a GUI frontend for Kokoro for generating audiobooks from epubs. The results are pretty good!&lt;/p&gt; &lt;p&gt;PRs are very welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vosFan"&gt; /u/vosFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w21l2ag0oihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:58:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iizbxs</id>
    <title>Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way.</title>
    <updated>2025-02-06T10:14:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt; &lt;img alt="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." src="https://external-preview.redd.it/bDJtMXNycmt1aGhlMQxr13kQ4l494R_6FN5L7tr44dIiu9kzOIdUQI5GS5Z5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5b432ad2dfeb081934154500e3fccbe230c81d" title="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/50vlqmrkuhhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T10:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij35u7</id>
    <title>Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN</title>
    <updated>2025-02-06T13:59:31+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt; &lt;img alt="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" src="https://external-preview.redd.it/Z3lrdWp0dmx5aWhlMaQ4EUN4_AgLY98885pUW0pYP7vfo05dn6YTgI9m58bO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=379782dc21a6714fa7105716d9fd647dc31f82ba" title="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gpawbnvlyihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T13:59:31+00:00</published>
  </entry>
</feed>
