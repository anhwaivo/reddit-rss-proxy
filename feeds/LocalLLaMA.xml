<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-11T15:06:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lwl9ai</id>
    <title>The New Nvidia Model is Really Chatty</title>
    <updated>2025-07-10T19:07:49+00:00</updated>
    <author>
      <name>/u/SpyderJack</name>
      <uri>https://old.reddit.com/user/SpyderJack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt; &lt;img alt="The New Nvidia Model is Really Chatty" src="https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64406c81316cc6f179ed0040ddcdc5047147395f" title="The New Nvidia Model is Really Chatty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpyderJack"&gt; /u/SpyderJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8bnc2od6i3cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4ya7</id>
    <title>New OSS project: llamac-lab or a pure C runtime for LLaMA models, made for the edge</title>
    <updated>2025-07-11T11:56:30+00:00</updated>
    <author>
      <name>/u/rvnllm</name>
      <uri>https://old.reddit.com/user/rvnllm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing my new madness, really, not much to say about it, as its very early. &lt;/p&gt; &lt;p&gt;So the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So what it is (or going to be if I don't disappear into the abyss):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Started hacking on this today. &lt;/p&gt; &lt;p&gt;A &lt;strong&gt;pure C (or I try to keep it that way)&lt;/strong&gt; runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.&lt;/p&gt; &lt;p&gt;So let me introduce &lt;code&gt;llamac-lab&lt;/code&gt;, a work-in-progress open-source runtime for LLaMA-based models.&lt;/p&gt; &lt;p&gt;Think llama.cpp, but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flattened into straight-up C (no C++ or STL baggage)&lt;/li&gt; &lt;li&gt;Optimized for &lt;strong&gt;minimal memory use&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Dead simple to embed into &lt;strong&gt;any stack&lt;/strong&gt; (Rust, Python, or LUA or anything else that can interface with C)&lt;/li&gt; &lt;li&gt;Born for &lt;strong&gt;edge devices&lt;/strong&gt;, MCUs, and other weird places LLMs don't usually go&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.&lt;/p&gt; &lt;p&gt;Note: It’s super early. No model loading yet. No inference. Just early scaffolding and dreams.&lt;br /&gt; But if you're into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!&lt;/p&gt; &lt;p&gt;Repo: [llamac](&lt;a href="https://github.com/llamac-lab/llamac"&gt;https://github.com/llamac-lab/llamac&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Note: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvnllm"&gt; /u/rvnllm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx3jtc</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-11T10:38:36+00:00</updated>
    <author>
      <name>/u/Better-Armadillo1371</name>
      <uri>https://old.reddit.com/user/Better-Armadillo1371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Armadillo1371"&gt; /u/Better-Armadillo1371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T10:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx66on</id>
    <title>Issues with Qwen 3 Embedding models (4B and 0.6B)</title>
    <updated>2025-07-11T12:56:05+00:00</updated>
    <author>
      <name>/u/IndependentApart5556</name>
      <uri>https://old.reddit.com/user/IndependentApart5556</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt; &lt;img alt="Issues with Qwen 3 Embedding models (4B and 0.6B)" src="https://b.thumbs.redditmedia.com/YgiW1tXulPbYxegiQD_ZWMXklDrQz0iXGKWh4ebIFjE.jpg" title="Issues with Qwen 3 Embedding models (4B and 0.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I'm currently facing a weird issue.&lt;br /&gt; I was testing different embedding models, with the goal being to integrate the best local one in a django application. &lt;/p&gt; &lt;p&gt;Architecture is as follows : &lt;/p&gt; &lt;p&gt;- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations&lt;/p&gt; &lt;p&gt;- My PC for the django application, running the codebase &lt;/p&gt; &lt;p&gt;I use CosineDistance to test the models. The functionality is a semantic search. &lt;/p&gt; &lt;p&gt;I noticed the following : &lt;/p&gt; &lt;p&gt;- Using the text-embedding-3-large model, (OAI API) gives great results&lt;br /&gt; - Using Nomic embedding model gives great results also&lt;br /&gt; - Using Qwen embedding models give very bad results, as if the encoding wouldn't make any sense. &lt;/p&gt; &lt;p&gt;i'm using a aembed() method to call the embedding models, and I declare them using : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;OpenAIEmbeddings( model=model_name, check_embedding_ctx_length=False, base_url=base_url, api_key=api_key, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b"&gt;OpenAI cosine distance test results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e"&gt;LM Studio Nomic cosine distance test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742"&gt;LM Studio Qwen 3 cosine distance test &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just can't figure out what's going on. Qwen 3 is supposed to be among the best models.&lt;br /&gt; Can someone give advice ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentApart5556"&gt; /u/IndependentApart5556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6yer</id>
    <title>Skywork/Skywork-R1V3-38B · Hugging Face</title>
    <updated>2025-07-11T13:30:10+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"&gt; &lt;img alt="Skywork/Skywork-R1V3-38B · Hugging Face" src="https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef" title="Skywork/Skywork-R1V3-38B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Skywork-R1V 3.0: an open source model that beats close source models on multi-modal reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2j1l</id>
    <title>I built a tool to run Humanity's Last Exam on your favorite local models!</title>
    <updated>2025-07-11T09:33:40+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt; &lt;img alt="I built a tool to run Humanity's Last Exam on your favorite local models!" src="https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81757699f7df25dfc08d7353f703eb730b727ec6" title="I built a tool to run Humanity's Last Exam on your favorite local models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4"&gt;https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;in the last few weeks, I've spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: &lt;em&gt;good&lt;/em&gt; tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.&lt;/p&gt; &lt;p&gt;Thus, I've built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?&lt;/p&gt; &lt;p&gt;My tool supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;both &lt;strong&gt;vision-based&lt;/strong&gt; and &lt;strong&gt;text-only&lt;/strong&gt; prompting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;automatic judging&lt;/strong&gt; by a third-party model not involved in answering the exam questions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;question randomization&lt;/strong&gt; and only testing for a small subset of HLE&lt;/li&gt; &lt;li&gt;export of the results to &lt;strong&gt;machine-readable JSON&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;running &lt;strong&gt;several evaluations&lt;/strong&gt; for different models all in one go&lt;/li&gt; &lt;li&gt;support for &lt;strong&gt;external Ollama instances&lt;/strong&gt; with Bearer Authentication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The entire source code is on GitHub! &lt;a href="https://github.com/mags0ft/hle-eval-ollama"&gt;https://github.com/mags0ft/hle-eval-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To anyone new to HLE (Humanity's Last Exam)&lt;/strong&gt;, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we're only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)&lt;/p&gt; &lt;p&gt;My project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.&lt;/p&gt; &lt;p&gt;I'd love to get some feedback, so don't hesitate to comment! Have fun trying it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4a3t</id>
    <title>Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces</title>
    <updated>2025-07-11T11:20:42+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"&gt; &lt;img alt="Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces" src="https://external-preview.redd.it/OI2iID36iwvBkHarjg-7dH6Sebf2j49Fh1hND_ikvDI.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb8719e3b1cc5345fd3b0ddd04b3379b419c55" title="Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/PrimeIntellect/status/1943424561116045389"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8qrz</id>
    <title>ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes.</title>
    <updated>2025-07-11T14:45:09+00:00</updated>
    <author>
      <name>/u/nat2r</name>
      <uri>https://old.reddit.com/user/nat2r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt; &lt;img alt="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nat2r"&gt; /u/nat2r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5n8c</id>
    <title>FYI Qwen3 235B A22B IQ4_XS works with 128 GB DDR5 + 8GB VRAM in Windows</title>
    <updated>2025-07-11T12:30:36+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Disclaimers: Nothing new here especially given the recent posts, but was supposed to report back at &lt;a href="/u/Evening_Ad6637"&gt;u/Evening_Ad6637&lt;/a&gt; et al. Furthermore, i am a total noob and do local LLM via LM Studio on Windows 11, so no fancy ik_llama.cpp etc., as it is just so convenient.)&lt;/p&gt; &lt;p&gt;I finally received 2x64 GB DDR5 5600 MHz Sticks (Kingston &lt;a href="https://www.kingston.com/datasheets/KF556C36BBE-8.pdf"&gt;Datasheet&lt;/a&gt;) giving me 128 GB RAM on my ITX Build. I did load the EXPO0 timing profile giving CL36 etc.&lt;br /&gt; This is complemented by a Low Profile RTX 4060 with 8 GB, all controlled by a Ryzen 9 7950X (any CPU would do).&lt;/p&gt; &lt;p&gt;Through LM Studio, I downloaded and ran both unsloth's 128K Q3_K_XL quant (103.7 GB) as well as managed to run the &lt;strong&gt;IQ4_XS&lt;/strong&gt; quant (125.5 GB) on a freshly restarted windows machine. (Haven't tried crashing or stress testing it yet, it currently works without issues).&lt;br /&gt; I left all model settings untouched and increased the context to ~17000. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Time to first token&lt;/strong&gt; on a prompt about a Berlin neighborhood took &lt;strong&gt;around 10 sec, then 3.3-2.7 tps.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I can try to provide any further information or run prompts for you and return the response as well as times. Just wanted to update you that this works. Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4hxt</id>
    <title>SmolTalk2: The dataset behind SmolLM3's dual reasoning</title>
    <updated>2025-07-11T11:32:19+00:00</updated>
    <author>
      <name>/u/loubnabnl</name>
      <uri>https://old.reddit.com/user/loubnabnl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt; &lt;img alt="SmolTalk2: The dataset behind SmolLM3's dual reasoning" src="https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fb36eb92fed67dfe7cf18375f3d278d7d7f435b" title="SmolTalk2: The dataset behind SmolLM3's dual reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960"&gt;https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, we're following up on the SmolLM3 release with the full dataset we used for post-training the model. It includes high-quality open datasets and new ones we created to balance model performance in dual reasoning + address the scarcity of reasoning datasets in certain domains such as multi-turn conversations, multilinguality, and alignment.&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk2"&gt;https://huggingface.co/datasets/HuggingFaceTB/smoltalk2&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We hope you will build great models on top of it 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loubnabnl"&gt; /u/loubnabnl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:32:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwx50s</id>
    <title>2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs</title>
    <updated>2025-07-11T03:57:38+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href="https://arxiv.org/pdf/2507.07145"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx10ja</id>
    <title>With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?</title>
    <updated>2025-07-11T07:51:04+00:00</updated>
    <author>
      <name>/u/GyozaHoop</name>
      <uri>https://old.reddit.com/user/GyozaHoop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt; &lt;p&gt;But with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GyozaHoop"&gt; /u/GyozaHoop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T07:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8xdm</id>
    <title>moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)</title>
    <updated>2025-07-11T14:52:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt; &lt;img alt="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" src="https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3" title="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt; &lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt; &lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwsrx7</id>
    <title>Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp</title>
    <updated>2025-07-11T00:20:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt; &lt;img alt="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" src="https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a" title="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt; &lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href="https://www.ibm.com/think/topics/mixture-of-experts"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-ai-platform/Bamba-9B-v1"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13550"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx85jo</id>
    <title>Devstral-Vision-Small-2507</title>
    <updated>2025-07-11T14:20:52+00:00</updated>
    <author>
      <name>/u/faldore</name>
      <uri>https://old.reddit.com/user/faldore</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt; &lt;img alt="Devstral-Vision-Small-2507" src="https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0b820cc0547e8c6ffaecf9eb33b63916abc0d61" title="Devstral-Vision-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral released Devstral-Small-2507 - which is AWESOME! But, they released without vision capability. I didn't like that.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507"&gt;&lt;strong&gt;Devstral-Vision-Small-2507&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507-gguf"&gt;&lt;strong&gt;Devstral-Vision-Small-2507-gguf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did some model surgery. I started with Mistral-Small-3.2-24B-Instruct-2506, and replaced its language tower with Devstral-Small-2507.&lt;/p&gt; &lt;p&gt;The conversion script is in the repo, if you'd like to take a look.&lt;/p&gt; &lt;p&gt;Tested, it works fine. I'm sure that it could do with a bit of RL to gel the vision and coding with real world use cases, but I'm releasing as is - a useful multimodal coding model.&lt;/p&gt; &lt;p&gt;Enjoy.&lt;/p&gt; &lt;p&gt;-Eric&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5"&gt;https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93"&gt;https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faldore"&gt; /u/faldore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2dw4</id>
    <title>Is a heavily quantised Q235b any better than Q32b?</title>
    <updated>2025-07-11T09:24:06+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt; &lt;p&gt;I'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.&lt;/p&gt; &lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:24:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwztnp</id>
    <title>Granite-speech-3.3-8b</title>
    <updated>2025-07-11T06:33:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt; &lt;img alt="Granite-speech-3.3-8b" src="https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a3cfa1633e9a330cab59c33f8413530288842b0" title="Granite-speech-3.3-8b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T06:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx7l3k</id>
    <title>This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma</title>
    <updated>2025-07-11T13:57:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt; &lt;img alt="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" src="https://preview.redd.it/r2bp20do39cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa04ae911a28403b4ac7702442fb11eb655146a3" title="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MedGemma 27B Multimodal for complex multimodal &amp;amp; longitudinal EHR interpretation: &lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: &lt;a href="https://huggingface.co/google/medsiglip-448"&gt;https://huggingface.co/google/medsiglip-448&lt;/a&gt;&lt;/p&gt; &lt;p&gt;T5Gemma: lightweight yet powerful encoder-decoder research models: &lt;a href="https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86"&gt;https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r2bp20do39cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwta86</id>
    <title>AMD's Pull Request for llama.cpp: Enhancing GPU Support</title>
    <updated>2025-07-11T00:45:46+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt; &lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br /&gt; Discussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14624"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2hn2</id>
    <title>Uncensored LLM ranking for roleplay?</title>
    <updated>2025-07-11T09:31:05+00:00</updated>
    <author>
      <name>/u/mikemend</name>
      <uri>https://old.reddit.com/user/mikemend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.&lt;/p&gt; &lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikemend"&gt; /u/mikemend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4qhp</id>
    <title>Moonshot AI about to release their 1T parameters model?</title>
    <updated>2025-07-11T11:45:02+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt; &lt;img alt="Moonshot AI about to release their 1T parameters model?" src="https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f19a5b1a2112e5c15ddbc66a6e92a07eecb3c7" title="Moonshot AI about to release their 1T parameters model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kts1w8a7g8cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6dcm</id>
    <title>llama2.c running on the original 2007 iPhone</title>
    <updated>2025-07-11T13:04:10+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt; &lt;img alt="llama2.c running on the original 2007 iPhone" src="https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d84dc04e7624cefc75d18c603d35424468ce1db" title="llama2.c running on the original 2007 iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3u6728ask8cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx78bk</id>
    <title>When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. "Considering Elon Musk's views" is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political.</title>
    <updated>2025-07-11T13:42:01+00:00</updated>
    <author>
      <name>/u/lostlifon</name>
      <uri>https://old.reddit.com/user/lostlifon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"&gt; &lt;img alt="When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. &amp;quot;Considering Elon Musk's views&amp;quot; is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political." src="https://external-preview.redd.it/Z3N0OTZ4Z24wOWNmMWs9zsnwyBbwxMH0ZvInVPpBqNaLm12SmXXjJes3XWMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4a4f4c30fcd1df9e99ea2ac2bd6d004202cce27" title="When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. &amp;quot;Considering Elon Musk's views&amp;quot; is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First seen by ramez on twitter - &lt;a href="https://x.com/ramez/status/1943431212766294413"&gt;https://x.com/ramez/status/1943431212766294413&lt;/a&gt; and the video is from Jeremy Howard confirming this behaviour - &lt;a href="https://x.com/jeremyphoward/status/1943444549696917714"&gt;https://x.com/jeremyphoward/status/1943444549696917714&lt;/a&gt; &lt;/p&gt; &lt;p&gt;For other questions, it doesn't do this - &lt;a href="https://x.com/jeremyphoward/status/1943543291129270366"&gt;https://x.com/jeremyphoward/status/1943543291129270366&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostlifon"&gt; /u/lostlifon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2cdl5xgn09cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx62hd</id>
    <title>Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain "cutlass"</title>
    <updated>2025-07-11T12:50:38+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5awq</id>
    <title>Friendly reminder that Grok 3 should be now open-sourced</title>
    <updated>2025-07-11T12:13:48+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt; &lt;img alt="Friendly reminder that Grok 3 should be now open-sourced" src="https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg" title="Friendly reminder that Grok 3 should be now open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx5awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:13:48+00:00</published>
  </entry>
</feed>
