<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-21T13:50:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1krx9pa</id>
    <title>Key findings after testing LLMs</title>
    <updated>2025-05-21T13:02:12+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After running my tests, plus a few others, and publishing the results, I got to thinking about how strong Qwen3 really is.&lt;/p&gt; &lt;p&gt;You can read my musings here: &lt;a href="https://blog.kekepower.com/blog/2025/may/21/deepseek_r1_and_v3_vs_qwen3_-_why_631-billion_parameters_still_miss_the_mark_on_instruction_fidelity.html"&gt;https://blog.kekepower.com/blog/2025/may/21/deepseek_r1_and_v3_vs_qwen3_-_why_631-billion_parameters_still_miss_the_mark_on_instruction_fidelity.html&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;DeepSeek R1-631 B and V3-631 B nail reasoning tasks but routinely ignore explicit format or length constraints.&lt;/p&gt; &lt;p&gt;Qwen3 (8 B → 235 B) obeys instructions out-of-the-box, even on a single RTX 3070, though the 30 B-A3B variant hallucinated once in a 10 000-word test (details below).&lt;/p&gt; &lt;p&gt;If your pipeline needs precise word counts or tag wrappers, use Qwen3 today; keep DeepSeek for creative ideation unless you’re ready to babysit it with chunked prompts or regex post-processing.&lt;/p&gt; &lt;p&gt;Rumor mill says DeepSeek V4 and R2 will land shortly; worth re-testing when they do.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There were also comments on my other post about my prompt. That is was either weak or having too many parameters.&lt;/p&gt; &lt;p&gt;Question: &lt;strong&gt;Do you have any suggestions for strong, difficult, interesting or breaking prompts I can test next?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krx9pa/key_findings_after_testing_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krx9pa/key_findings_after_testing_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krx9pa/key_findings_after_testing_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T13:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1krbhr1</id>
    <title>Gemma 3n blog post</title>
    <updated>2025-05-20T17:55:46+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"&gt; &lt;img alt="Gemma 3n blog post" src="https://external-preview.redd.it/6Uiw9QwCmEOV2-HLrpi2sZAHXDpSta5QPjHpcK86Z_Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8b86ea6caf300bd46fdcfa5c35348f14e45e9c" title="Gemma 3n blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemma/gemma-3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1krxvce</id>
    <title>LLM for Linux questions</title>
    <updated>2025-05-21T13:29:52+00:00</updated>
    <author>
      <name>/u/Any-Championship-611</name>
      <uri>https://old.reddit.com/user/Any-Championship-611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to learn Linux. Can anyone recommend me a good LLM that can answer all Linux related questions? Preferrably not a huge one, like under 20B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Championship-611"&gt; /u/Any-Championship-611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxvce/llm_for_linux_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxvce/llm_for_linux_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krxvce/llm_for_linux_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T13:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1krsast</id>
    <title>What is the estimated token/sec for Nvidia DGX Spark</title>
    <updated>2025-05-21T07:55:35+00:00</updated>
    <author>
      <name>/u/presidentbidden</name>
      <uri>https://old.reddit.com/user/presidentbidden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be the estimated token/sec for Nvidia DGX Spark ? For popular models such as gemma3 27b, qwen3 30b-a3b etc. I can get about 25 t/s, 100 t/s on my 3090. They are claiming 1000 TOPS for FP4. What existing GPU would this be comparable to ? I want to understand if there is an advantage to buying this thing vs investing on a 5090/pro 6000 etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/presidentbidden"&gt; /u/presidentbidden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsast/what_is_the_estimated_tokensec_for_nvidia_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsast/what_is_the_estimated_tokensec_for_nvidia_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krsast/what_is_the_estimated_tokensec_for_nvidia_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kre5gs</id>
    <title>Running Gemma 3n on mobile locally</title>
    <updated>2025-05-20T19:41:53+00:00</updated>
    <author>
      <name>/u/United_Dimension_46</name>
      <uri>https://old.reddit.com/user/United_Dimension_46</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"&gt; &lt;img alt="Running Gemma 3n on mobile locally" src="https://preview.redd.it/xhvtdzjvpz1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec66952f2520d8ba93f2f38c94004afc60e0854" title="Running Gemma 3n on mobile locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United_Dimension_46"&gt; /u/United_Dimension_46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhvtdzjvpz1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T19:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1krwvgh</id>
    <title>Docling completely missing large elements on a page, can Layout model be changed?</title>
    <updated>2025-05-21T12:43:03+00:00</updated>
    <author>
      <name>/u/joomla00</name>
      <uri>https://old.reddit.com/user/joomla00</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around with docling today after I've seen some hype around these parts, and found that the object detection would often miss large chart/image type elements side by side. In one case, there were 2 large squarish elements side by side, roughly the same side. The left one was detected, the right one was not. In a other case there were boxes in 2x4 formation. One of the 7 was completely missed. YOLO handled everything in my simple test perfectly.&lt;/p&gt; &lt;p&gt;So I guess my questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there any simple things to change configure to improve this?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there a way to replace the layout easily? Looking through the code, the design is nice and modular except there doesn't seem to be a way to change the layout model. I'll have to look deeper but I couldn't tell if plugin system would allow me to change it.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joomla00"&gt; /u/joomla00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krwvgh/docling_completely_missing_large_elements_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krwvgh/docling_completely_missing_large_elements_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krwvgh/docling_completely_missing_large_elements_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T12:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1krcdg5</id>
    <title>Gemini 2.5 Flash (05-20) Benchmark</title>
    <updated>2025-05-20T18:30:45+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"&gt; &lt;img alt="Gemini 2.5 Flash (05-20) Benchmark" src="https://preview.redd.it/q5m5i3c6dz1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=367b7989c05f4f3b26a8222ba271d7a1bc61b829" title="Gemini 2.5 Flash (05-20) Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q5m5i3c6dz1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T18:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr9rvp</id>
    <title>OpenEvolve: Open Source Implementation of DeepMind's AlphaEvolve System</title>
    <updated>2025-05-20T16:49:21+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm excited to share &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of Google DeepMind's AlphaEvolve system that I recently completed. For those who missed it, AlphaEvolve is an evolutionary coding agent that DeepMind announced in May that uses LLMs to discover new algorithms and optimize existing ones.&lt;/p&gt; &lt;h1&gt;What is OpenEvolve?&lt;/h1&gt; &lt;p&gt;OpenEvolve is a framework that &lt;strong&gt;evolves entire codebases&lt;/strong&gt; through an iterative process using LLMs. It orchestrates a pipeline of code generation, evaluation, and selection to continuously improve programs for a variety of tasks.&lt;/p&gt; &lt;p&gt;The system has four main components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt Sampler&lt;/strong&gt;: Creates context-rich prompts with past program history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Ensemble&lt;/strong&gt;: Generates code modifications using multiple LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluator Pool&lt;/strong&gt;: Tests generated programs and assigns scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Program Database&lt;/strong&gt;: Stores programs and guides evolution using MAP-Elites inspired algorithm&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Works with any LLM&lt;/strong&gt; via OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ensembles multiple models&lt;/strong&gt; for better results (we found Gemini-Flash-2.0-lite + Gemini-Flash-2.0 works great)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolves entire code files&lt;/strong&gt;, not just single functions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-objective optimization&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible prompt engineering&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distributed evaluation&lt;/strong&gt; with checkpointing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We replicated AlphaEvolve's results!&lt;/h1&gt; &lt;p&gt;We successfully replicated two examples from the AlphaEvolve paper:&lt;/p&gt; &lt;h1&gt;Circle Packing&lt;/h1&gt; &lt;p&gt;Started with a simple concentric ring approach and evolved to discover mathematical optimization with scipy.minimize. We achieved 2.634 for the sum of radii, which is 99.97% of DeepMind's reported 2.635!&lt;/p&gt; &lt;p&gt;The evolution was fascinating - early generations used geometric patterns, by gen 100 it switched to grid-based arrangements, and finally it discovered constrained optimization.&lt;/p&gt; &lt;h1&gt;Function Minimization&lt;/h1&gt; &lt;p&gt;Evolved from a basic random search to a full simulated annealing algorithm, discovering concepts like temperature schedules and adaptive step sizes without being explicitly programmed with this knowledge.&lt;/p&gt; &lt;h1&gt;LLM Performance Insights&lt;/h1&gt; &lt;p&gt;For those running their own LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low latency is critical since we need many generations&lt;/li&gt; &lt;li&gt;We found Cerebras AI's API gave us the fastest inference&lt;/li&gt; &lt;li&gt;For circle packing, an ensemble of Gemini-Flash-2.0 + Claude-Sonnet-3.7 worked best&lt;/li&gt; &lt;li&gt;The architecture allows you to use any model with an OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself!&lt;/h1&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/codelion/openevolve"&gt;https://github.com/codelion/openevolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/circle_packing"&gt;Circle Packing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/function_minimization"&gt;Function Minimization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what you build with it and hear your feedback. Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1krryjx</id>
    <title>Are there any recent 14b or less MoE models?</title>
    <updated>2025-05-21T07:30:52+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are quite a few from 2024 but was wondering if there are any more recent ones. Qwen3 30b a3d but a bit large and requires a lot of vram. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1krxwja</id>
    <title>New falcon models using mamba hybrid are very competetive if not ahead for their sizes.</title>
    <updated>2025-05-21T13:31:15+00:00</updated>
    <author>
      <name>/u/ElectricalAngle1611</name>
      <uri>https://old.reddit.com/user/ElectricalAngle1611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AVG SCORES FOR A VARIETY OF BENCHMARKS:&lt;br /&gt; **Falcon-H1 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-34B:** 58.92&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-7B:** 54.08&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-3B:** 48.09&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-1.5B-deep:** 47.72&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-1.5B:** 45.47&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Falcon-H1-0.5B:** 35.83&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Qwen3 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Qwen3-32B:** 58.44&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-8B:** 52.62&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-4B:** 48.83&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-1.7B:** 41.08&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Qwen3-0.6B:** 31.24&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Gemma3 Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Gemma3-27B:** 58.75&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-12B:** 54.10&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-4B:** 44.32&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Gemma3-1B:** 29.68&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**Llama Models:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Llama3.3-70B:** 58.20&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama4-scout:** 57.42&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.1-8B:** 44.77&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.2-3B:** 38.29&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Llama3.2-1B:** 24.99&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;benchmarks tested:&lt;br /&gt; * BBH&lt;/p&gt; &lt;p&gt;* ARC-C&lt;/p&gt; &lt;p&gt;* TruthfulQA&lt;/p&gt; &lt;p&gt;* HellaSwag&lt;/p&gt; &lt;p&gt;* MMLU&lt;/p&gt; &lt;p&gt;* GSM8k&lt;/p&gt; &lt;p&gt;* MATH-500&lt;/p&gt; &lt;p&gt;* AMC-23&lt;/p&gt; &lt;p&gt;* AIME-24&lt;/p&gt; &lt;p&gt;* AIME-25&lt;/p&gt; &lt;p&gt;* GPQA&lt;/p&gt; &lt;p&gt;* GPQA_Diamond&lt;/p&gt; &lt;p&gt;* MMLU-Pro&lt;/p&gt; &lt;p&gt;* MMLU-stem&lt;/p&gt; &lt;p&gt;* HumanEval&lt;/p&gt; &lt;p&gt;* HumanEval+&lt;/p&gt; &lt;p&gt;* MBPP&lt;/p&gt; &lt;p&gt;* MBPP+&lt;/p&gt; &lt;p&gt;* LiveCodeBench&lt;/p&gt; &lt;p&gt;* CRUXEval&lt;/p&gt; &lt;p&gt;* IFEval&lt;/p&gt; &lt;p&gt;* Alpaca-Eval&lt;/p&gt; &lt;p&gt;* MTBench&lt;/p&gt; &lt;p&gt;* LiveBench&lt;/p&gt; &lt;p&gt;all the data I grabbed for this post was found at: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct&lt;/a&gt; and the various other models in the h1 family.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalAngle1611"&gt; /u/ElectricalAngle1611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krxwja/new_falcon_models_using_mamba_hybrid_are_very/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T13:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1krl0du</id>
    <title>LLAMACPP - SWA support ..FNALLY ;-)</title>
    <updated>2025-05-21T00:45:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because of that for instance gemma 3 27b q4km with flash attention fp16 and card with 24 GB VRAM I can fit &lt;strong&gt;75k context&lt;/strong&gt; now!&lt;/p&gt; &lt;p&gt;Before I was able to fix max 15k context with those parameters.&lt;/p&gt; &lt;p&gt;Source&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;https://github.com/ggml-org/llama.cpp/pull/13194&lt;/a&gt;&lt;/p&gt; &lt;p&gt;download&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for CLI&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model google_gemma-3-27b-it-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 75000 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap --top_k 64 --temp 1.0 -fa &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For server ( GIU )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server.exe --model google_gemma-3-27b-it-Q4_K_M.gguf --mmproj models/new3/google_gemma-3-27b-it-bf16-mmproj.gguf --threads 30 --keep -1 --n-predict -1 --ctx-size 75000 -ngl 99 --no-mmap --min_p 0 -fa &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T00:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1krrp2f</id>
    <title>The P100 isn't dead yet - Qwen3 benchmarks</title>
    <updated>2025-05-21T07:12:06+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to test how fast I could run Qwen3-14B-GPTQ-Int4 on a P100 versus Qwen3-14B-GPTQ-AWQ on a 3090.&lt;/p&gt; &lt;p&gt;I found that it was quite competitive in single-stream generation with around 45 tok/s on the P100 at 150W power limit vs around 54 tok/s on the 3090 with a PL of 260W.&lt;/p&gt; &lt;p&gt;So if you're willing to eat the idle power cost (26W in my setup), a single P100 is a nice way to run a decent model at good speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1krb6uu</id>
    <title>Google MedGemma</title>
    <updated>2025-05-20T17:44:16+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt; &lt;img alt="Google MedGemma" src="https://external-preview.redd.it/IkdSAGaHbYPwN7JuzggxNmmy1Ov_W_6LD8_ETnav3jw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dae3e4abe286e7ffab20fc05dd9c3c108fc0c88e" title="Google MedGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kru9v3</id>
    <title>Hidden thinking</title>
    <updated>2025-05-21T10:15:57+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was disappointed to find that Google has now hidden Gemini's thinking. I guess it is understandable to stop others from using the data to train and so help's good to keep their competitive advantage, but I found the thoughts so useful. I'd read the thoughts as generated and often would terminate the generation to refine the prompt based on the output thoughts which led to better results.&lt;/p&gt; &lt;p&gt;It was nice while it lasted and I hope a lot of thinking data was scraped to help train the open models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kru9v3/hidden_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T10:15:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1krc35x</id>
    <title>Announcing Gemma 3n preview: powerful, efficient, mobile-first AI</title>
    <updated>2025-05-20T18:19:09+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"&gt; &lt;img alt="Announcing Gemma 3n preview: powerful, efficient, mobile-first AI" src="https://external-preview.redd.it/0ZfqdzMMjWqMp0M38-XRODYXqi_qFGgfPApxf9tbLSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=753cbcbb9d784f9b6d8275021386984d8ac88f5a" title="Announcing Gemma 3n preview: powerful, efficient, mobile-first AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T18:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1krpvwj</id>
    <title>Gemma 3N E4B and Gemini 2.5 Flash Tested</title>
    <updated>2025-05-21T05:10:56+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lEtLksaaos8"&gt;https://www.youtube.com/watch?v=lEtLksaaos8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared Gemma 3n e4b against Qwen 3 4b. Mixed results. Gemma does great on classification, matches Qwen 4B on Structured JSON extraction. Struggles with coding and RAG.&lt;/p&gt; &lt;p&gt;Also compared Gemini 2.5 Flash to Open AI 4.1. Altman should be worried. Cheaper than 4.1 mini, better than full 4.1.&lt;/p&gt; &lt;h1&gt;Harmful Question Detector&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;70.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Named Entity Recognition New&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Retrieval Augmented Generation Prompt&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;97.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;83.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;62.50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;SQL Query Generator&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;75.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;65.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T05:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr8s40</id>
    <title>Gemma 3n Preview</title>
    <updated>2025-05-20T16:10:01+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt; &lt;img alt="Gemma 3n Preview" src="https://external-preview.redd.it/nuTGd6nR-D7i0exzDvXeyeroWnA1sgWJyyF8GipdVWU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4092ee3492e35aa48ddc115bdbd7e2144d1d03c2" title="Gemma 3n Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1krr7hn</id>
    <title>How to get the most from llama.cpp's iSWA support</title>
    <updated>2025-05-21T06:38:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;https://github.com/ggml-org/llama.cpp/pull/13194&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to our gguf god ggerganov, we finally have iSWA support for gemma 3 models that significantly reduces KV cache usage. Since I participated in the pull discussion, I would like to offer tips to get the most out of this update.&lt;/p&gt; &lt;p&gt;Previously, by default fp16 KV cache for 27b model at 64k context is 31744MiB. Now by default batch_size=2048, fp16 KV cache becomes 6368MiB. This is 79.9% reduction.&lt;/p&gt; &lt;p&gt;Group Query Attention KV cache: (ie original implementation)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;1984MB&lt;/td&gt; &lt;td align="left"&gt;3968MB&lt;/td&gt; &lt;td align="left"&gt;7936MB&lt;/td&gt; &lt;td align="left"&gt;15872MB&lt;/td&gt; &lt;td align="left"&gt;31744MB&lt;/td&gt; &lt;td align="left"&gt;63488MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;1536MB&lt;/td&gt; &lt;td align="left"&gt;3072MB&lt;/td&gt; &lt;td align="left"&gt;6144MB&lt;/td&gt; &lt;td align="left"&gt;12288MB&lt;/td&gt; &lt;td align="left"&gt;24576MB&lt;/td&gt; &lt;td align="left"&gt;49152MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;544MB&lt;/td&gt; &lt;td align="left"&gt;1088MB&lt;/td&gt; &lt;td align="left"&gt;2176MB&lt;/td&gt; &lt;td align="left"&gt;4352MB&lt;/td&gt; &lt;td align="left"&gt;8704MB&lt;/td&gt; &lt;td align="left"&gt;17408MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The new implementation splits KV cache to Local Attention KV cache and Global Attention KV cache that are detailed in the following two tables. The overall KV cache use will be the sum of the two. Local Attn KV depends on the batch_size only while the Global attn KV depends on the context length.&lt;/p&gt; &lt;p&gt;Since the local attention KV depends on the batch_size only, you can reduce the batch_size (via the -b switch) from 2048 to 64 (setting values lower than this will just be set to 64) to further reduce KV cache. Originally, it is 5120+1248=6368MiB. Now it is 5120+442=5562MiB. Memory saving will now 82.48%. The cost of reducing batch_size is reduced prompt processing speed. Based on my llama-bench pp512 test, it is only around 20% reduction when you go from 2048 to 64.&lt;/p&gt; &lt;p&gt;Local Attention KV cache size valid at any context:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;64&lt;/th&gt; &lt;th align="left"&gt;512&lt;/th&gt; &lt;th align="left"&gt;2048&lt;/th&gt; &lt;th align="left"&gt;8192&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;kv_size&lt;/td&gt; &lt;td align="left"&gt;1088&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;9216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;442MB&lt;/td&gt; &lt;td align="left"&gt;624MB&lt;/td&gt; &lt;td align="left"&gt;1248MB&lt;/td&gt; &lt;td align="left"&gt;3744MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;340MB&lt;/td&gt; &lt;td align="left"&gt;480MB&lt;/td&gt; &lt;td align="left"&gt;960MB&lt;/td&gt; &lt;td align="left"&gt;2880MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;123.25MB&lt;/td&gt; &lt;td align="left"&gt;174MB&lt;/td&gt; &lt;td align="left"&gt;348MB&lt;/td&gt; &lt;td align="left"&gt;1044MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Global Attention KV cache:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;td align="left"&gt;5120MB&lt;/td&gt; &lt;td align="left"&gt;10240MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;256MB&lt;/td&gt; &lt;td align="left"&gt;512MB&lt;/td&gt; &lt;td align="left"&gt;1024MB&lt;/td&gt; &lt;td align="left"&gt;2048MB&lt;/td&gt; &lt;td align="left"&gt;4096MB&lt;/td&gt; &lt;td align="left"&gt;8192MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;80MB&lt;/td&gt; &lt;td align="left"&gt;160MB&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;If you only have one 24GB card, you can use the default batch_size 2048 and run 27b qat q4_0 at 64k, then it should be 15.6GB model + 5GB global KV + 1.22GB local KV = 21.82GB. Previously, that would take 48.6GB total.&lt;/p&gt; &lt;p&gt;If you want to run it at even higher context, you can use KV quantization (lower accuracy) and/or reduce batch size (slower prompt processing). Reducing batch size to the minimum 64 should allow you to run 96k (total 23.54GB). KV quant alone at Q8_0 should allow you to run 128k at 21.57GB.&lt;/p&gt; &lt;p&gt;So we now finally have a viable long context local LLM that can run with a single card. Have fun summarizing long pdfs with llama.cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T06:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1krupm7</id>
    <title>gemma 3n seems not work well for non English prompt</title>
    <updated>2025-05-21T10:44:22+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"&gt; &lt;img alt="gemma 3n seems not work well for non English prompt" src="https://preview.redd.it/xhxxm6xv642f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0580d38b88c51a74cb73fa8b517a498c3df81f8" title="gemma 3n seems not work well for non English prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhxxm6xv642f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krupm7/gemma_3n_seems_not_work_well_for_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T10:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1krsjpb</id>
    <title>New threadripper has 8 memory channels. Will it be an affordable local LLM option?</title>
    <updated>2025-05-21T08:14:04+00:00</updated>
    <author>
      <name>/u/theKingOfIdleness</name>
      <uri>https://old.reddit.com/user/theKingOfIdleness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.theregister.com/2025/05/21/amd_threadripper_radeon_workstation/"&gt;https://www.theregister.com/2025/05/21/amd_threadripper_radeon_workstation/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm always on the lookout for cheap local inference. I noticed the new threadrippers will move from 4 to 8 channels.&lt;/p&gt; &lt;p&gt;8 channels of DDR5 is about 409GB/s&lt;/p&gt; &lt;p&gt;That's on par with mid range GPUs on a non server chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theKingOfIdleness"&gt; /u/theKingOfIdleness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krsjpb/new_threadripper_has_8_memory_channels_will_it_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T08:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1krp4hq</id>
    <title>They also released the Android app with which you can interact with the new Gemma3n</title>
    <updated>2025-05-21T04:25:10+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;This is really good&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android"&gt;https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery"&gt;https://github.com/google-ai-edge/gallery&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T04:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1krnk8v</id>
    <title>ByteDance Bagel 14B MOE (7B active) Multimodal with image generation (open source, apache license)</title>
    <updated>2025-05-21T02:57:30+00:00</updated>
    <author>
      <name>/u/noage</name>
      <uri>https://old.reddit.com/user/noage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights - &lt;a href="https://github.com/ByteDance-Seed/Bagel"&gt;GitHub - ByteDance-Seed/Bagel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website - &lt;a href="https://bagel-ai.org/"&gt;BAGEL: The Open-Source Unified Multimodal Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper - &lt;a href="https://arxiv.org/abs/2505.14683"&gt;[2505.14683] Emerging Properties in Unified Multimodal Pretraining&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses a mixture of experts and a mixture of transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noage"&gt; /u/noage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kri7ik</id>
    <title>ok google, next time mention llama.cpp too!</title>
    <updated>2025-05-20T22:31:42+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt; &lt;img alt="ok google, next time mention llama.cpp too!" src="https://preview.redd.it/ml66h5yxj02f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36aba859e0c8b8e47fe122c7315b0f3ad3607ad1" title="ok google, next time mention llama.cpp too!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ml66h5yxj02f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T22:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1krtvpj</id>
    <title>Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B</title>
    <updated>2025-05-21T09:50:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt; &lt;img alt="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" src="https://external-preview.redd.it/asQIFBJYgU0s0y-AV0hAHtenKk6qa9ZCLFCb-Jjyvag.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=669a7c89198f19469e2598642c94e9e4b54a56f3" title="Falcon-H1 Family of Hybrid-Head Language Models, including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krtvpj/falconh1_family_of_hybridhead_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T09:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1krs40j</id>
    <title>Why nobody mentioned "Gemini Diffusion" here? It's a BIG deal</title>
    <updated>2025-05-21T07:42:08+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt; &lt;img alt="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" src="https://external-preview.redd.it/dFWSMq_9jHPdMVGchDlKvt7rzCFhQEFmxZm8XKq654M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b69152f4cc7971773a476232dcff0de3690e29e" title="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has the capacity and capability to change the standard for LLMs from autoregressive generation to diffusion generation.&lt;/p&gt; &lt;p&gt;Google showed their Language diffusion model (Gemini Diffusion, visit the linked page for more info and benchmarks) yesterday/today (depends on your timezone), and it was extremely fast and (according to them) only half the size of similar performing models. They showed benchmark scores of the diffusion model compared to Gemini 2.0 Flash-lite, which is a tiny model already.&lt;/p&gt; &lt;p&gt;I know, it's LocalLLaMA, but if Google can prove that diffusion models work at scale, they are a far more viable option for local inference, given the speed gains.&lt;/p&gt; &lt;p&gt;And let's not forget that, since diffusion LLMs process the whole text at once iteratively, it doesn't need KV-Caching. Therefore, it could be more memory efficient. It also has &amp;quot;test time scaling&amp;quot; by nature, since the more passes it is given to iterate, the better the resulting answer, without needing CoT (It can do it in latent space, even, which is much better than discrete tokenspace CoT). &lt;/p&gt; &lt;p&gt;What do you guys think? Is it a good thing for the Local-AI community in the long run that Google is R&amp;amp;D-ing a fresh approach? They’ve got massive resources. They can prove if diffusion models work at scale (bigger models) in future.&lt;/p&gt; &lt;p&gt;(PS: I used a (of course, ethically sourced, local) LLM to correct grammar and structure the text, otherwise it'd be a wall of text) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemini-diffusion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:42:08+00:00</published>
  </entry>
</feed>
