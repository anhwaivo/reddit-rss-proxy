<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-05T03:50:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j32y7c</id>
    <title>Split brain (Update) - What I've learned and will improve</title>
    <updated>2025-03-04T04:43:51+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt; &lt;img alt="Split brain (Update) - What I've learned and will improve" src="https://external-preview.redd.it/UWvmtQPs4ScGH0IthYKdfU1hrMW7JnkAzdMKFse7jL0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c740e7b82c11757b66468f76734733c2aa704f1c" title="Split brain (Update) - What I've learned and will improve" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a update post to the last one &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;Here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have uploaded a inference page to the code I had previously discussed. &lt;a href="https://github.com/alientony/Split-brain/blob/main/inference-app.py"&gt;Inference&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can download the fusion layer here. &lt;a href="https://huggingface.co/Alienanthony/Splitbrain_Fusion_model"&gt;Fusion layer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original models can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;https://huggingface.co/meta-llama/Llama-3.2-1B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;So far the inference has been fascinating. Unfortunately I have only had the original gpt4all dataset on hand for training. (800mb) &lt;/p&gt; &lt;p&gt;Including I have learned that if you're doing to use a fused layer for differentiation for one model output you should probably make another. So moving forward I will update the training and attempt again. &lt;/p&gt; &lt;p&gt;BUT I am extremely fascinated by this new crazy system.&lt;/p&gt; &lt;p&gt;As you can see below. While we did not give the model on the left &amp;quot;Describe the history of chocolate chip cookies.&amp;quot; it does begin to think in that direction within it's &amp;quot;Think&amp;quot; space. &lt;/p&gt; &lt;p&gt;I have been able to replicate this sort of &amp;quot;thought directions&amp;quot; multiple times but it is very erratic. As both models are actually not on the same playing field due to the dependency in the way the architecture functions and it is asymmetrical rather than mirrored.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1"&gt;https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One major issue I need to fix is the fused layer to realign the model on the right to produce usable tokens.&lt;/p&gt; &lt;p&gt;I also need a larger dataset as this will give more of a wider branch of training for the &amp;quot;sharing of info&amp;quot; across models but I find these results majorly agreeable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4"&gt;https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38265</id>
    <title>16Gb GPU around $300 mark, low idle power. Does it exist?</title>
    <updated>2025-03-04T10:45:25+00:00</updated>
    <author>
      <name>/u/Rxunique</name>
      <uri>https://old.reddit.com/user/Rxunique</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got a 4x V100 16gb rig on during the day only, now looking for a low power GPU to keep on 24/7. needing to run 14B model on ollama and immich image recognition.&lt;/p&gt; &lt;p&gt;It for 2U server, so ideally blower cards, but capable of modifying a RTX FAN as well.&lt;/p&gt; &lt;p&gt;P100 idles 40W which is too much for 24/7, P4 only has 8gb. Most RTX with 16gb ram are not that cheap. 12GB card will struggle with 14B model.&lt;/p&gt; &lt;p&gt;Basically something like a half price Tesla T4 or A2 but doesn't have to be as good, or as small. Because for similar budget I might as well add a bit more and wait for brand new RTX50xx or new RX9070 which in theory should idel around 10w ish&lt;/p&gt; &lt;p&gt;I've been searching a while, does such GPU existing or am I searching for unobtanium&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary of replies:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It has to be 2nd hand to fit in such budget, but even though its hard to find one. summarizing so far&lt;/p&gt; &lt;p&gt;$300-$400 range&lt;/p&gt; &lt;ul&gt; &lt;li&gt;some old AMD ones will fit, but no cuda&lt;/li&gt; &lt;li&gt;Modded 2080 Ti 22GB $300-400, but reliability concerning for 24/7 on&lt;/li&gt; &lt;li&gt;Or go even even cheaper 2x 8G cards. but that sacrifices too much inferencing speed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;lt;$500 range&lt;/p&gt; &lt;ul&gt; &lt;li&gt;P40, but this is getting too old, just ok T/s for LLM, no good for image related like SD&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Anything else?&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;$700-800 range&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4060 ti 16gb&lt;/li&gt; &lt;li&gt;T4&lt;/li&gt; &lt;li&gt;A2&lt;/li&gt; &lt;li&gt;Or hold off wait for 50xx price settle or RX9070&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rxunique"&gt; /u/Rxunique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j34snr</id>
    <title>I open sourced my project to analyze your years of Apple Health data with Local Llama</title>
    <updated>2025-03-04T06:40:45+00:00</updated>
    <author>
      <name>/u/Fit_Chair2340</name>
      <uri>https://old.reddit.com/user/Fit_Chair2340</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt; &lt;img alt="I open sourced my project to analyze your years of Apple Health data with Local Llama" src="https://external-preview.redd.it/lLsIvxvl6coJk3dd69rue5IjQS1mwSUfJjRAQiI0jak.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0705f2c7bb303cf84d8b222fd9c148b2417ea6ce" title="I open sourced my project to analyze your years of Apple Health data with Local Llama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around and found out that you can export all your Apple health data. I've been wearing an Apple watch for 8 years and whoop for 3 years. I always check my day to day and week to week stats but I never looked at the data over the years. What if I could send this data to A.I. for analysis? But I also don't want to send my private data to a public LLM. What if I could run the analysis locally?&lt;/p&gt; &lt;p&gt;I exported my data and there was 989MB of data! So I needed to write some code to break this down. The code takes in your export data and gives you options to look at Steps, Distance, Heart rate, Sleep and more. It gave me some cool charts and you can use local llama to run the A.I. analysis!&lt;/p&gt; &lt;p&gt;I was really stressed at work last 2 years.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563"&gt;https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It gave me some CRAZY insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seasonal Anomalies: While there's a general trend of higher activity in spring/summer, some of your most active periods occurred during winter months, particularly in December and January of recent years.&lt;/li&gt; &lt;li&gt;Reversed Weekend Pattern: Unlike most people who are more active on weekends, your data shows consistently lower step counts on weekends, suggesting your physical activity is more tied to workdays than leisure time.&lt;/li&gt; &lt;li&gt;COVID Impact: There's a clear signature of the pandemic in your data, with more erratic step patterns and changed workout routines during 2020-2021, followed by a distinct recovery pattern in late 2021.&lt;/li&gt; &lt;li&gt;Morning Consistency: Your most successful workout periods consistently occur in morning hours, with these sessions showing better heart rate performance compared to other times.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run this on your own computer. No one can access your data. &lt;a href="https://github.com/krumjahn/applehealth"&gt;&lt;strong&gt;Here's the link to the project.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you need more guidance on how to run it (not a programmer), &lt;a href="https://rumjahn.com/how-i-used-a-i-to-analyze-8-years-of-apple-health-fitness-data-to-uncover-actionable-insights/"&gt;check out my detailed instructions here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If people like this, I will make a web app version so you can run it without using code. Give this a like if you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Chair2340"&gt; /u/Fit_Chair2340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T06:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3s9e1</id>
    <title>Frontend-only Ollama web client ?</title>
    <updated>2025-03-05T02:08:41+00:00</updated>
    <author>
      <name>/u/KaKi_87</name>
      <uri>https://old.reddit.com/user/KaKi_87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Is there any Ollama web client that is frontend-only, i.e. stores API URL and chats in &lt;code&gt;localStorage&lt;/code&gt; and communicates directly with Ollama ?&lt;/p&gt; &lt;p&gt;Basically, same as native desktop apps like &lt;a href="https://github.com/Jeffser/Alpaca"&gt;Alpaca&lt;/a&gt; do, but in browser.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaKi_87"&gt; /u/KaKi_87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3s9e1/frontendonly_ollama_web_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3s9e1/frontendonly_ollama_web_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3s9e1/frontendonly_ollama_web_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T02:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3c4ey</id>
    <title>🌡️ LLM Thermometer</title>
    <updated>2025-03-04T14:34:45+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt; &lt;img alt="🌡️ LLM Thermometer" src="https://a.thumbs.redditmedia.com/Q4kNq15yRUaNS3c9JbpuoSTOIwk_3gvnIr2E-bhqCr4.jpg" title="🌡️ LLM Thermometer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm working on a toy/research project called &amp;quot;LLM Thermometer&amp;quot; that tries to answer:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can we infer the temperature setting used by an LLM just by analyzing its outputs?&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;p&gt;The tool:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates multiple responses using the same prompt at different temperatures&lt;/li&gt; &lt;li&gt;Measures semantic similarity between responses using embeddings&lt;/li&gt; &lt;li&gt;Visualizes the relationship between temperature and response similarity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Early results suggest this works :) Higher temperatures consistently produce more diverse responses (lower similarity scores), while lower temperatures generate more consistent outputs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lpjjw2f9nome1.png?width=2876&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f58c5b93ee33483b62ccaa7838acaadf0ccba8"&gt;Similarity between generated texts with the same temperature levels from the prompt: \&amp;quot;What will technology look like in 2050?\&amp;quot; Cooler colors (blue) correspond to lower temperature values, while warmer colors (red) correspond to higher temperature values (range from 0 to 1 with steps of 0.1).&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Technical details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Uses OpenAI Python SDK but can connect to any compatible API endpoint&lt;/li&gt; &lt;li&gt;Works with local models via vLLM's OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Auto generates detailed reports with visualizations&lt;/li&gt; &lt;li&gt;Reports available at: &lt;a href="https://s1m0n38.github.io/llm-thermometer/"&gt;https://s1m0n38.github.io/llm-thermometer/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for prompt suggestions&lt;/h1&gt; &lt;p&gt;I'd appreciate suggestions for prompts that might produce interesting temperature-dependent variations! I've tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;What will technology look like in 2050?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;What's the meaning of life?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;What are the ethical implications of widespread AI adoption?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Write a creative story with six paragraphs.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else should I try? Especially interested in prompts where temperature differences produce qualitatively different responses beyond just variation in wording.&lt;/p&gt; &lt;p&gt;Any feedback or ideas welcome!&lt;/p&gt; &lt;p&gt;The project is on GitHub at &lt;a href="https://github.com/S1M0N38/llm-thermometer/"&gt;https://github.com/S1M0N38/llm-thermometer/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:34:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3thkl</id>
    <title>PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)</title>
    <updated>2025-03-05T03:10:50+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt; &lt;img alt="PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)" src="https://external-preview.redd.it/GpXpL5-MZLw335WIDJMcyqdgdCC-2-IxUj5GgZ4bS2s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=765db037dfefdef59d32d12b9a1ca0123e55f05b" title="PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/x66ccff/PromptRose"&gt;https://github.com/x66ccff/PromptRose&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Prompt Rose&amp;quot; is a sleek productivity tool that, with just a press and hold of the Alt key (customizable), brings up a radial menu similar to those found in the Battlefield game series. Instead of game commands, this menu is filled with quick prompt instructions designed for Large Language Models (LLMs). This eliminates the need for users to search through their notebooks for prompts.&lt;/p&gt; &lt;p&gt;Designed for Windows platforms, &amp;quot;Prompt Rose&amp;quot; also allows for prompt management directly from the right-click system tray.&lt;/p&gt; &lt;p&gt;✨ Features&lt;/p&gt; &lt;p&gt;- **Intuitive Radial Menu**: Press and hold your hotkey to bring up a wheel of prompts&lt;/p&gt; &lt;p&gt;- **Quick Selection**: Simply move your mouse in the direction of the desired prompt and release&lt;/p&gt; &lt;p&gt;- **Customizable Prompts**: Add, edit, or remove prompts through an easy-to-use interface&lt;/p&gt; &lt;p&gt;- **Customizable Hotkey**: Choose your preferred activation key combination&lt;/p&gt; &lt;p&gt;- **Direct Pasting**: Automatically pastes your selected prompt into the active application&lt;/p&gt; &lt;p&gt;- **System Tray Integration**: Minimal footprint with easy access to all functions&lt;/p&gt; &lt;p&gt;- **Persistent Settings**: Your configurations are saved and loaded between sessions &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bi66v08nfsme1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa396622380ccf9948f8a4c0dab7f9643d6ff370"&gt;https://preview.redd.it/bi66v08nfsme1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa396622380ccf9948f8a4c0dab7f9643d6ff370&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T03:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gctr</id>
    <title>Locally hosted homicidal escape room leveraging local inference, agentic workflows, TTS, IOT, beer, and friends</title>
    <updated>2025-03-04T17:33:41+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Hope everyone is having a good Tuesday. I started a project last year and faced some challenges and I’m wondering if some of you want to help with some creative solutions?! If I find the time to work on the project further, I’d love to share the codebase as a thank you or hopefully this inspires some to come up with something similar.&lt;/p&gt; &lt;p&gt;Current stack: 3080ti, Phi-3.5.mini-instruct (FP16), Langchain, Home assistant, tortoise-tts, IOT devices: mainly LED lights.&lt;/p&gt; &lt;p&gt;How it worked was the model with a cloned GLaDOS voice from portal is a sociopathic AI that has control of the room we are in along with the life systems and environment. She was given a brief overview of the objects in the room and generated an escape game on the fly.&lt;/p&gt; &lt;p&gt;This was communicated by LED lights by brightness and color. She would turn the heat up to 200F degrees if we answered a riddle, puzzle, or question wrong. Or just did it anyway because she wanted to.&lt;/p&gt; &lt;p&gt;For the agentic workflow I wrote a simple queue and task system that separate agents worked on. Mostly updating environment variables — like if we were drowning already or not. This could’ve of been a deterministic api call, but I wanted to do it anyway 🤣 The countdowns and other dynamic wild things created on the fly were insane haha!&lt;/p&gt; &lt;p&gt;We communicated by a terminal (laptop) in the center of the room. It was fun for about an hour, but the limitations were apparent.&lt;/p&gt; &lt;p&gt;Challenges: Larger context the model would lose focus and repeat tooling calls. This could be fixed using a newer model and finetuning.&lt;/p&gt; &lt;p&gt;The model’s safeguards still work even when prompted appropriately. Preventing actual fun from a deranged homicidal sociopathic LLM trying to torture and kill us. Maybe abliteration? &lt;a href="https://huggingface.co/blog/mlabonne/abliteration"&gt;https://huggingface.co/blog/mlabonne/abliteration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:33:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3r8ls</id>
    <title>Deepseek V2.5 Becomes No.1 on Copilot Arena</title>
    <updated>2025-03-05T01:17:48+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt; &lt;img alt="Deepseek V2.5 Becomes No.1 on Copilot Arena" src="https://external-preview.redd.it/qz0mj3UiKjc9f5igcmafTaZjnzBMMP1WUULpUFnNC8Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904655aa1b01986e22fef932164110cccbc659a5" title="Deepseek V2.5 Becomes No.1 on Copilot Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the latest Copilot Arena rankings, &lt;strong&gt;Deepseek V2.5 (FIM)&lt;/strong&gt; has reached the &lt;strong&gt;top position&lt;/strong&gt; with an &lt;strong&gt;Arena Score of 1028&lt;/strong&gt;, outperforming strong competitors like &lt;strong&gt;Claude 3.5 Sonnet&lt;/strong&gt; and &lt;strong&gt;Codetral&lt;/strong&gt; to become the highest-ranked AI coding assistant! 🚀&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0q3v0xiurme1.png?width=1740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=333d8e307f646075cbe1f4cbd5da55fe0b31bafe"&gt;Rank&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The leaderboard differs from existing evaluations. In particular, smaller models over perform in static benchmarks compared to real development workflows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db"&gt;https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared to previous benchmarks, Copilot Arena observes more programming languages (PL), natural languages (NL), longer context lengths, multiple task types, and various code structures.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2276cjwuurme1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b61eb97054808650e4dd23bb287a01af99c4a0"&gt;Data Distribution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/iamwaynechi/status/1896996806481109377"&gt;X&lt;/a&gt; &lt;a href="https://lmarena.ai/?leaderboard"&gt;Leaderboard&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.09328"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T01:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3929v</id>
    <title>When will Meta AI get a Llama upgrade already?</title>
    <updated>2025-03-04T11:52:51+00:00</updated>
    <author>
      <name>/u/Cheetah3051</name>
      <uri>https://old.reddit.com/user/Cheetah3051</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been stuck at 3.2 for months&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheetah3051"&gt; /u/Cheetah3051 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T11:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3sk9b</id>
    <title>I think my CPU is bottlenecking my ram for DeepSeek r1 1.58bit</title>
    <updated>2025-03-05T02:23:36+00:00</updated>
    <author>
      <name>/u/GeekyBit</name>
      <uri>https://old.reddit.com/user/GeekyBit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3sk9b/i_think_my_cpu_is_bottlenecking_my_ram_for/"&gt; &lt;img alt="I think my CPU is bottlenecking my ram for DeepSeek r1 1.58bit" src="https://a.thumbs.redditmedia.com/eVLbSg9ua2WPIE1D9zXCunMeOI5uyJ34QE3KAuCx0u4.jpg" title="I think my CPU is bottlenecking my ram for DeepSeek r1 1.58bit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I got a new home lab server and was messing around with Deepseek R1 Unsloth's 1.58 bit since I only have 192GB of ram.... any how look at this usage &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/drk13dns5sme1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9f99fd85562137aafccd4e2e4077ad03a5c153b"&gt;https://preview.redd.it/drk13dns5sme1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9f99fd85562137aafccd4e2e4077ad03a5c153b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yes it is windows I was testing some windows apps before installing Ubuntu ... Also I use Ollama not even some specialty software package you have to compile yourself. &lt;/p&gt; &lt;p&gt;Any how my average tokens seems to be 1.237 T/S &lt;/p&gt; &lt;p&gt;From my paper napkin math I should be able to hit around 5.78 to 7 T/S...&lt;/p&gt; &lt;p&gt;I got that from running 6 channel DDR at 2666 on two sockets giving a 10% decrees in Ram performance for real world and another 10-25% hit in Cross talk between cpus. &lt;/p&gt; &lt;p&gt;Given all of that I noticed my CPU are pegged at 100% even V2 or stage 2 or level 2 boost is staying on to try to keep up... When I first got the system it came with dual 5122 CPU which are 4 core 8 thread cpus I replaced them with 16 core 32 thread with hyper threading disabled for my particular tasks...&lt;/p&gt; &lt;p&gt;But I was wondering does this being slower than the theoretical maximum I could get mean I am CPU bottle necked? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeekyBit"&gt; /u/GeekyBit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3sk9b/i_think_my_cpu_is_bottlenecking_my_ram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3sk9b/i_think_my_cpu_is_bottlenecking_my_ram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3sk9b/i_think_my_cpu_is_bottlenecking_my_ram_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T02:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3lh4x</id>
    <title>Used the deepseek to start my v h s collection. Happy with the results.</title>
    <updated>2025-03-04T21:02:35+00:00</updated>
    <author>
      <name>/u/WingsOfNth</name>
      <uri>https://old.reddit.com/user/WingsOfNth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"&gt; &lt;img alt="Used the deepseek to start my v h s collection. Happy with the results." src="https://b.thumbs.redditmedia.com/b_NGrrSOxyA3TCuGoUhpwPGjc1Mc5kqu5z1MeKH4tGA.jpg" title="Used the deepseek to start my v h s collection. Happy with the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WingsOfNth"&gt; /u/WingsOfNth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j3lh4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T21:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3elmz</id>
    <title>Chain-of-Experts: Unlocking the Communication Power of MoEs</title>
    <updated>2025-03-04T16:23:21+00:00</updated>
    <author>
      <name>/u/finallyifoundvalidUN</name>
      <uri>https://old.reddit.com/user/finallyifoundvalidUN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We propose Chain-of-Experts (CoE), which fundamentally changes sparse Large Language Model (LLM) processing by implementing sequential communication between intra-layer experts within Mixture-of-Experts (MoE) models.&lt;/p&gt; &lt;p&gt;Mixture-of-Experts (MoE) models process information independently in parallel between experts and have high memory requirements. CoE introduces an iterative mechanism enabling experts to &amp;quot;communicate&amp;quot; by processing tokens on top of outputs from other experts.&lt;/p&gt; &lt;p&gt;Experiments show that CoE significantly outperforms previous MoE models in multiple aspects:&lt;/p&gt; &lt;p&gt;Performance: CoE with 2x iterations reduces Math validation loss from 1.20 to 1.12 Scaling: 2x iterations matches performance of 3x expert selections, outperforming layer scaling Efficiency: 17.6% lower memory usage with equivalent performance Flexibility: 823x increase in expert combinations, improving utilization, communication, and specialization&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZihanWang314/coe"&gt;https://github.com/ZihanWang314/coe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finallyifoundvalidUN"&gt; /u/finallyifoundvalidUN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T16:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37yhi</id>
    <title>Just 2B R1 like model achieved "aha" moment in vision task!!!</title>
    <updated>2025-03-04T10:38:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt; &lt;img alt="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" src="https://external-preview.redd.it/FrHOpKWMdqORPFSxLwpGvRx_uoQMNXcCJsvQiFYTNWw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2604aa322cfdf8200f3a6d6f9f972bb244dac34" title="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:38:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bhzb</id>
    <title>Survival Specialist Fine-tune (Llama 3.1- 8B)</title>
    <updated>2025-03-04T14:05:03+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt; &lt;img alt="Survival Specialist Fine-tune (Llama 3.1- 8B)" src="https://external-preview.redd.it/85ZgfWZxijLN2bSUGD1P0g3arLnrKn-sjIsrvAEBZ2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65ea7c4f77726004c20e1a7fce60c95196efde1f" title="Survival Specialist Fine-tune (Llama 3.1- 8B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lolzinventor/Meta-Llama-3.1-8B-SurviveV3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3k8o8</id>
    <title>There is a space on HF where you can convert models to MLX without downloading them</title>
    <updated>2025-03-04T20:10:48+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been downloading models and converting them to MLX with the convert script from the MLX library. This takes lots of time and I'm limited on how big models I can convert by my 16gb ram.&lt;/p&gt; &lt;p&gt;Well I just learned that there is a space on HF where you can convert models, you just enter the model name and the quant and it uploads it straight to your HF profile! No need to download or do anything at all. This also means I'm not limited by my ram, I can convert any model now and so should you :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;https://huggingface.co/spaces/mlx-community/mlx-my-repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:10:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3byj5</id>
    <title>ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models</title>
    <updated>2025-03-04T14:27:00+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt; &lt;img alt="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" src="https://external-preview.redd.it/zs_VjubpLgixBxipH26-eVw1VDZXtfTYy2chbOoXyyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2bad2c795210312fd91bc4ff5801de93d52e3cad" title="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance’s Doubao Large Model Team, in collaboration with the M-A-P open-source community, has announced the release of SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning capabilities of large language models (LLMs) across 285 graduate-level disciplines. This dataset encompasses 26,529 multiple-choice questions, offering a rigorous assessment of LLM performance.&lt;br /&gt; &lt;a href="https://github.com/SuperGPQA/SuperGPQA"&gt;Github&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/m-a-p/SuperGPQA"&gt;HuggingFace&lt;/a&gt; &lt;a href="https://www.arxiv.org/abs/2502.14739"&gt;Paper&lt;/a&gt; &lt;a href="https://supergpqa.github.io"&gt;Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed"&gt;https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fq3ne63lmome1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e93f86a69d8ef6992f76c640ccbafb72504b045"&gt;Performance on SuperGPQA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3dqea5umome1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66a03b81db1827e82a1db111db6581188a2f4f06"&gt;LLM Performance Across Different Categories&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38499</id>
    <title>DiffRhythm - ASLP-lab: generate full songs (4 min) with vocals</title>
    <updated>2025-03-04T10:49:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/ASLP-lab/DiffRhythm"&gt;https://huggingface.co/spaces/ASLP-lab/DiffRhythm&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894"&gt;https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ASLP-lab"&gt;https://github.com/ASLP-lab&lt;/a&gt;&lt;br /&gt; Paper: DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion: &lt;a href="https://arxiv.org/abs/2503.01183"&gt;https://arxiv.org/abs/2503.01183&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32p97</id>
    <title>Qwen 32b coder instruct can now drive a coding agent fairly well</title>
    <updated>2025-03-04T04:29:49+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt; &lt;img alt="Qwen 32b coder instruct can now drive a coding agent fairly well" src="https://external-preview.redd.it/aDJ4N25hdXlvbG1lMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b763684453c2eb0539d13912eebe98f2d438296" title="Qwen 32b coder instruct can now drive a coding agent fairly well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2000d3tolme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3lbck</id>
    <title>SCANN: A Self-Organizing Coherent Attention Neural Network</title>
    <updated>2025-03-04T20:56:00+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt; &lt;img alt="SCANN: A Self-Organizing Coherent Attention Neural Network" src="https://external-preview.redd.it/JLOcLL_amzNZJjQB5eq2ns36cBxnwXJySZseItLLLD8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58af66c266b498548139bfda71fa96ad039e54f4" title="SCANN: A Self-Organizing Coherent Attention Neural Network" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few weeks ago, in my latest deep dive into random thought experiments, I went the furthest I've gone in terms of research/depth/experiments and was able to come out of the other side with an interesting information based field equation which I've now been able to apply to a new ML and neural network for learning. It's in the early stages, but the results so far are incredible with the clear path to scaling into a full LLM architecture.&lt;/p&gt; &lt;p&gt;So then what am I talking about?&lt;/p&gt; &lt;p&gt;Instead of conventional gradient-based optimization used in machine learning models such as logistic regression, random forests, and deep learning, or the attention-based token weighting in LLMs, SCANN employs a fundamentally different approach.&lt;/p&gt; &lt;p&gt;What Makes SCANN Different?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Self-Organization Instead of Training&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Unlike traditional models that explicitly train weights via backpropagation, SCANN allows features to evolve dynamically over time.&lt;/p&gt; &lt;p&gt;The transformation follows a mathematically governed Partial Differential Equation (PDE):&lt;/p&gt; &lt;p&gt;SCANN Equation =&lt;/p&gt; &lt;p&gt;D[ψ[t, x], t] == -γ ψ[t, x] - ∇ ⋅ (D[ψ[t, x]] ∇ ψ[t, x]) +&lt;/p&gt; &lt;p&gt;λnl Sum[ψ[t, xi], {xi, Neighbors}] + β Tanh[ψ[t, x]^2]&lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;p&gt;- Diffusion spreads feature information naturally:&lt;/p&gt; &lt;p&gt;D(ψ) = D0 (1 + α ψ^2)&lt;/p&gt; &lt;p&gt;- Nonlocal interactions allow features to learn from global structures.&lt;/p&gt; &lt;p&gt;- Resonance amplifies meaningful patterns.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN Generalizes Without Dataset-Specific Tuning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;SCANN has been evaluated across multiple datasets (Digits, Wine Classification, Breast Cancer, etc.) and has consistently performed well without dataset-specific retraining.&lt;/p&gt; &lt;p&gt;Increasing the number of time steps improves representation learning, allowing SCANN to refine feature structures dynamically over time.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN vs. LLMs and Traditional Machine Learning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Traditional Machine Learning models (e.g., SVMs, Neural Networks) require explicit parameter training to fit a loss function.&lt;/p&gt; &lt;p&gt;LLMs use layered token attention to interpret complex relationships in text.&lt;/p&gt; &lt;p&gt;SCANN, however, does not rely on pre-set parameters or static learning mechanisms. Instead, it evolves feature representations dynamically, resembling a physical system seeking equilibrium.&lt;/p&gt; &lt;p&gt;Why This is Exciting&lt;/p&gt; &lt;p&gt;SCANN represents a new perspective on representation learning—one that does not depend on large datasets or brute-force optimization. It offers a self-organizing mechanism for feature discovery, potentially revealing patterns in ways that traditional ML approaches cannot.&lt;/p&gt; &lt;p&gt;Further refinements and formalization are ongoing, but these early results highlight SCANN’s potential for a fundamentally different kind of machine learning.&lt;/p&gt; &lt;p&gt;I'll be open sourcing all the code and releasing a paper once I get some more tests done and hopefully a small LLM built from it as well.&lt;/p&gt; &lt;p&gt;In the meantime, if you're interested in the core information-based field equation I built and then integrated into this ML model you can check out all the details and experiments here: &lt;a href="https://github.com/severian42/Informational-Relative-Evolution"&gt;https://github.com/severian42/Informational-Relative-Evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And a more longform paper here: &lt;a href="https://huggingface.co/blog/Severian/informational-relative-evolution"&gt;https://huggingface.co/blog/Severian/informational-relative-evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Test Results:&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88"&gt;https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143"&gt;https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3m8v5</id>
    <title>Cohere Blog: Aya Vision — Expanding the Worlds AI Can See</title>
    <updated>2025-03-04T21:34:39+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt; &lt;img alt="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" src="https://external-preview.redd.it/N2GDJUmAUpd9T37b2tIt2DUV5G6cVnF0cK_wUy2iAqI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1371c07fff0fcedfb4dc146d3550992b34c14bbd" title="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cohere.com/blog/aya-vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T21:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fkax</id>
    <title>LLM Quantization Comparison</title>
    <updated>2025-03-04T17:02:00+00:00</updated>
    <author>
      <name>/u/dat1-co</name>
      <uri>https://old.reddit.com/user/dat1-co</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt; &lt;img alt="LLM Quantization Comparison" src="https://external-preview.redd.it/CGbzq4JDmMgH-DT2hVt-MPGAGrs7Io3E0dHabckY9J8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbd8d1effbe5db86d79260e6b8463c84c31b3a11" title="LLM Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat1-co"&gt; /u/dat1-co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dat1.co/blog/llm-quantization-comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bldn</id>
    <title>C4AI Aya Vision</title>
    <updated>2025-03-04T14:09:38+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt; &lt;img alt="C4AI Aya Vision" src="https://external-preview.redd.it/2FtgBIdrUTjZVqld2wWXrJgxCyutz4lA4knvupaJc-g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cffd187923d62691d6d92d4dd9ba2db2a4f098" title="C4AI Aya Vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/CohereForAI/c4ai-aya-vision-67c4ccd395ca064308ee1484"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3emu0</id>
    <title>Open Source Claude Code (Actual Repo Converted from Binary)</title>
    <updated>2025-03-04T16:24:41+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/dnakov/claude-code/tree/main"&gt;Claude Code Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dnakov/anon-kode"&gt;Fork to enable OpenAI Compatibility&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: This is a decompiled version of Claude's code, not officially open-source. The original title may have been misleading—use at your own discretion.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T16:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3hjxb</id>
    <title>Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark</title>
    <updated>2025-03-04T18:21:37+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt; &lt;img alt="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" src="https://preview.redd.it/6jg8ae9drpme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85bce8c13b47a201032eadf727ee61bfb00869ed" title="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6jg8ae9drpme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T18:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gahy</id>
    <title>NVIDIA’s GeForce RTX 4090 With 96GB VRAM Reportedly Exists; The GPU May Enter Mass Production Soon, Targeting AI Workloads.</title>
    <updated>2025-03-04T17:31:10+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/"&gt;https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly highly interested. If this will be true.&lt;/p&gt; &lt;p&gt;Price around 6k. &lt;/p&gt; &lt;p&gt;Source; &amp;quot;The user did confirm that the one with a 96 GB VRAM won't guarantee stability and that its cost, due to a higher VRAM, will be twice the amount you would pay on the 48 GB edition. As per the user, this is one of the reasons why the factories are considering making only the 48 GB edition but may prepare the 96 GB in about 3-4 months.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:31:10+00:00</published>
  </entry>
</feed>
