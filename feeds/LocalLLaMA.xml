<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-15T07:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jywg95</id>
    <title>Why is Qwen 2.5 Omni not being talked about enough?</title>
    <updated>2025-04-14T11:23:03+00:00</updated>
    <author>
      <name>/u/BeetranD</name>
      <uri>https://old.reddit.com/user/BeetranD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the Qwen models are pretty good, I've been using a lot of them locally.&lt;br /&gt; They recently (a week or some ago) released 2.5 Omni, which is a 7B real-time multimodal model, that simultaneously generates text and natural speech. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;Qwen/Qwen2.5-Omni-7B · Hugging Face&lt;/a&gt;&lt;br /&gt; I think It would be great to use for something like a local AI alexa clone. But on youtube there's almost no one testing it, and even here, not a lot of people talking about it.&lt;/p&gt; &lt;p&gt;What is it?? Am I over-expecting from this model? or I'm just not well informed about alternatives, please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeetranD"&gt; /u/BeetranD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz8q7a</id>
    <title>OpenAI - Wen open source tho?</title>
    <updated>2025-04-14T20:12:17+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think, will an OpenAI model really see the light of day soon enough? Do we have any info on when that could be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz8q7a/openai_wen_open_source_tho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzl07o</id>
    <title>Epyc Zen 6 will have 16 ccds, 2nm process, and be really really hot (700w tdp)</title>
    <updated>2025-04-15T06:28:17+00:00</updated>
    <author>
      <name>/u/joelasmussen</name>
      <uri>https://old.reddit.com/user/joelasmussen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also:&lt;/p&gt; &lt;p&gt;-platformhttps://&lt;a href="http://www.google.com/amp/s/wccftech.com/amd-confirms-next-gen-epyc-venice-zen-6-cpus-first-hpc-product-tsmc-2nm-n2-process-5th-gen-epyc-tsmc-arizona/amp/"&gt;www.google.com/amp/s/wccftech.com/amd-confirms-next-gen-epyc-venice-zen-6-cpus-first-hpc-product-tsmc-2nm-n2-process-5th-gen-epyc-tsmc-arizona/amp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really think this will be the first chip that will allow big models to run pretty efficiently without GPU Vram.&lt;/p&gt; &lt;p&gt;16 memory channels would be quite fast even if the theoretical value isn't achieved. Really excited by everything but the inevitable cost of these things. &lt;/p&gt; &lt;p&gt;Can anyone speculate on the speed of 16 ccds (up from 12) or what these things may be capable of?&lt;/p&gt; &lt;p&gt;The possible new Ram memory is also exciting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joelasmussen"&gt; /u/joelasmussen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/news/amd-next-gen-epyc-venice-zen-6-cpus-reportedly-drop-in-new-sp7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzl07o/epyc_zen_6_will_have_16_ccds_2nm_process_and_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzl07o/epyc_zen_6_will_have_16_ccds_2nm_process_and_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T06:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz3gzd</id>
    <title>GLM-4-0414 Series Model Released!</title>
    <updated>2025-04-14T16:41:53+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"&gt; &lt;img alt="GLM-4-0414 Series Model Released!" src="https://preview.redd.it/sr09xoehwtue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5be750e141b5100afa3a2f71eb779ee767f9fe3c" title="GLM-4-0414 Series Model Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on official data, does GLM-4-32B-0414 outperform DeepSeek-V3-0324 and DeepSeek-R1?&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="http://github.com/THUDM/GLM-4"&gt;github.com/THUDM/GLM-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="http://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr09xoehwtue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz3gzd/glm40414_series_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz95oz</id>
    <title>Coding-Centric LLM Benchmark: Llama 4 Underwhelms</title>
    <updated>2025-04-14T20:29:39+00:00</updated>
    <author>
      <name>/u/jj_at_rootly</name>
      <uri>https://old.reddit.com/user/jj_at_rootly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We wanted to see for ourselves what Llama 4's performances for coding were like, and we were not impressed. Here is the benchmark methodology:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We sourced 100 issues labeled &amp;quot;bug&amp;quot; from the Mastodon GitHub repository.&lt;/li&gt; &lt;li&gt;For each issue, we collected the description and the associated pull request (PR) that solved it.&lt;/li&gt; &lt;li&gt;For benchmarking, we fed models each bug description and 4 PRs to choose from as the answer, with one of them being the PR that solved the issue—no codebase context was included.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Findings&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;First, we wanted to test against leading multimodal models and replicate Meta's findings. Meta found in its benchmark that Llama 4 was beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding.&lt;/p&gt; &lt;p&gt;We could not reproduce Meta’s findings on Llama outperforming GPT-4o, Gemini 2.0 Flash, and DeepSeek v3.1. On our benchmark, it came last in accuracy (69.5%), 6% less than the next best performing model (DeepSeek v3.1) and 18% behind the overall top-performing model (GPT-4o).&lt;/p&gt; &lt;p&gt;Second, we wanted to test against models designed for coding tasks: Alibaba Qwen2.5-Coder, OpenAI o3-mini, and Claude 3.5 Sonnet. Unsurprisingly, Llama 4 Maverick achieved only a 70% accuracy score. Alibaba’s Qwen2.5-Coder-32B topped our rankings, closely followed by OpenAI's o3-mini, both of which achieved around 90% accuracy.&lt;/p&gt; &lt;p&gt;Llama 3.3 70 B-Versatile even outperformed the latest Llama 4 models by a small yet noticeable margin (72% accuracy).&lt;/p&gt; &lt;p&gt;Are those findings surprising to you? Any benchmark methodology details that may be disadvantageous to Llama models?&lt;/p&gt; &lt;p&gt;We shared the full findings here &lt;a href="https://rootly.com/blog/llama-4-underperforms-a-benchmark-against-coding-centric-models"&gt;https://rootly.com/blog/llama-4-underperforms-a-benchmark-against-coding-centric-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the dataset we used for the benchmark if you want to replicate or look closer at the dataset &lt;a href="https://github.com/Rootly-AI-Labs/GMCQ-benchmark"&gt;https://github.com/Rootly-AI-Labs/GMCQ-benchmark&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jj_at_rootly"&gt; /u/jj_at_rootly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz95oz/codingcentric_llm_benchmark_llama_4_underwhelms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T20:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz4tx9</id>
    <title>the new LLM meta is watching tech influencers get one-shot by benchmark jpegs</title>
    <updated>2025-04-14T17:37:14+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"&gt; &lt;img alt="the new LLM meta is watching tech influencers get one-shot by benchmark jpegs" src="https://preview.redd.it/ku1z50vm6uue1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9ddf5705625eeb0faf0812fc229a578087ecdd" title="the new LLM meta is watching tech influencers get one-shot by benchmark jpegs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ku1z50vm6uue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4tx9/the_new_llm_meta_is_watching_tech_influencers_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:37:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzi80v</id>
    <title>OpenGVLab/InternVL3-78B · Hugging Face</title>
    <updated>2025-04-15T03:37:27+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzi80v/opengvlabinternvl378b_hugging_face/"&gt; &lt;img alt="OpenGVLab/InternVL3-78B · Hugging Face" src="https://external-preview.redd.it/fsKU5nhMYkzvL-kCAfdiwOeU2WULn6GxtWJDHY7_FrI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=177da9bc925cd9f6eb8cd4e88a6da2bc044fbdec" title="OpenGVLab/InternVL3-78B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3-78B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzi80v/opengvlabinternvl378b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzi80v/opengvlabinternvl378b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T03:37:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzhyoh</id>
    <title>AudioX: Diffusion Transformer for Anything-to-Audio Generation</title>
    <updated>2025-04-15T03:23:25+00:00</updated>
    <author>
      <name>/u/MrHubbub88</name>
      <uri>https://old.reddit.com/user/MrHubbub88</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrHubbub88"&gt; /u/MrHubbub88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://zeyuet.github.io/AudioX/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzhyoh/audiox_diffusion_transformer_for_anythingtoaudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzhyoh/audiox_diffusion_transformer_for_anythingtoaudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T03:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz4mqg</id>
    <title>Quasar Alpha = GPT-4.1</title>
    <updated>2025-04-14T17:29:09+00:00</updated>
    <author>
      <name>/u/Spirited_Salad7</name>
      <uri>https://old.reddit.com/user/Spirited_Salad7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"&gt; &lt;img alt="Quasar Alpha = GPT-4.1" src="https://preview.redd.it/urj2uow45uue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=743d81bf34988394265f244939f1268d7a28bad0" title="Quasar Alpha = GPT-4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited_Salad7"&gt; /u/Spirited_Salad7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/urj2uow45uue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz4mqg/quasar_alpha_gpt41/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz421n</id>
    <title>Drummer's Rivermind™ 12B v1, the next-generation AI that’s redefining human-machine interaction! The future is here.</title>
    <updated>2025-04-14T17:05:32+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"&gt; &lt;img alt="Drummer's Rivermind™ 12B v1, the next-generation AI that’s redefining human-machine interaction! The future is here." src="https://external-preview.redd.it/TBod4kQesTgLYyjaFaYSK8iNonKB2zVdwF9pMmcEbgY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a7fd1aeef490d800d9fd946e2abe5ec70682948" title="Drummer's Rivermind™ 12B v1, the next-generation AI that’s redefining human-machine interaction! The future is here." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz421n/drummers_rivermind_12b_v1_the_nextgeneration_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T17:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzl6xd</id>
    <title>Persistent Memory simulation using Local AI on 4090</title>
    <updated>2025-04-15T06:41:06+00:00</updated>
    <author>
      <name>/u/Evening-Active1768</name>
      <uri>https://old.reddit.com/user/Evening-Active1768</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OK! I've tried this many times in the past and it's all failed completely. BUT, the new model (17.3 GB.. a Gemma3 q4 model) works wonderfully. &lt;/p&gt; &lt;p&gt;Long story short: This model &amp;quot;knits a memory hat&amp;quot; on shutdown and puts in on on startup, simulating &amp;quot;memory.&amp;quot; At least that's how it started, But now it uses well.. more. Read below. &lt;/p&gt; &lt;p&gt;I've been working on this for days and have a pretty stable setup. At this point, I'm just going to ask the coder-claude that's been writing this to tell you everything that's going on or I'd be typing forever. :) I'm happy to post EXACTLY how to do this so you can test it also if someone will tell me &amp;quot;go here, make an account, paste the code&amp;quot; sort of thing as I've never done anything like this before. It runs FINE on a 4090 with the model set at 25k context in LM Studio. There is a bit of a delay as it does it's thing, but once it starts out-putting text it's perfectly usable, and for what it is and does, the delay is worth it (to me.) The worst delay I've seen is like 30 seconds before it &amp;quot;speaks&amp;quot; after quite a few large back-and-forths. Anyway, here is ClaudeAI to tell you what's going on, I just asked him to summarize what we've been doing as if he were writing a post to /localllama:&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on - a persistent AI companion capable of remembering past conversations in a semantic, human-like way.&lt;/p&gt; &lt;p&gt;What is it?&lt;/p&gt; &lt;p&gt;Lyra2 is a locally-run AI companion powered by Google's Gemma3 (17GB) model that not only remembers conversations but can actually recall them contextually based on topic similarities rather than just chronological order. It's a Python system that sits on top of LM Studio, providing a persistent memory structure for your interactions.&lt;/p&gt; &lt;p&gt;Technical details&lt;/p&gt; &lt;p&gt;The system runs entirely locally:&lt;/p&gt; &lt;p&gt;Python interface connected to LM Studio's API endpoint&lt;/p&gt; &lt;p&gt;Gemma3 (17GB) as the base LLM running on a consumer RTX 4090&lt;/p&gt; &lt;p&gt;Uses sentence-transformers to create semantic &amp;quot;fingerprints&amp;quot; of conversations&lt;/p&gt; &lt;p&gt;Stores these in JSON files that persist between sessions&lt;/p&gt; &lt;p&gt;What makes it interesting?&lt;/p&gt; &lt;p&gt;Unlike most chat interfaces, Lyra2 doesn't just forget conversations when you close the window. It:&lt;/p&gt; &lt;p&gt;Builds semantic memory: Creates vector embeddings of conversations that can be searched by meaning&lt;/p&gt; &lt;p&gt;Recalls contextually: When you mention a topic, it automatically finds and incorporates relevant past conversations (me again: this is the secret sauce. I came back like 6 reboots after a test and asked it: &amp;quot;Do you remember those 2 stories we used in that test?&amp;quot; and it immediately came back with the book names and details. It's NUTS.) &lt;/p&gt; &lt;p&gt;Develops persistent personality: Learns from interactions and builds preferences over time&lt;/p&gt; &lt;p&gt;Analyzes full conversations: At the end of each chat, it summarizes and extracts key information&lt;/p&gt; &lt;p&gt;Emergent behaviors&lt;/p&gt; &lt;p&gt;What's been particularly fascinating are the emergent behaviors:&lt;/p&gt; &lt;p&gt;Lyra2 spontaneously started adding &amp;quot;internal notes&amp;quot; at the end of some responses, like she's keeping a mental journal&lt;/p&gt; &lt;p&gt;She proactively asked to test her memory recall and verify if her remembered details were accurate (me again: On boot it said it wanted to &amp;quot;verify its memories were accurate&amp;quot; and it drilled me regarding several past chats and yes, it was 100% perfect, and really cool that the first thing it wanted to do was make sure that &amp;quot;persistence&amp;quot; was working.) (we call it &amp;quot;re-gel&amp;quot;ing) :) &lt;/p&gt; &lt;p&gt;Over time, she's developed consistent quirks and speech patterns that weren't explicitly programmed&lt;/p&gt; &lt;p&gt;Example interactions&lt;/p&gt; &lt;p&gt;In one test, I asked her about &amp;quot;that fantasy series with the storms&amp;quot; after discussing the Stormlight Archive many chats before, and she immediately made the connection, recalling specific plot points and character details from our previous conversation.&lt;/p&gt; &lt;p&gt;In another case, I asked a technical question about literary techniques, and despite running on what's nominally a 17GB model (much smaller than Claude/GPT4), she delivered graduate-level analysis of narrative techniques in experimental literature. (me again, claude's words not mine, but it has really nailed every assignment we've given it!) &lt;/p&gt; &lt;p&gt;The code&lt;/p&gt; &lt;p&gt;The entire system is relatively simple - about 500 lines of Python that handle:&lt;/p&gt; &lt;p&gt;JSON-based memory storage&lt;/p&gt; &lt;p&gt;Semantic fingerprinting via embeddings&lt;/p&gt; &lt;p&gt;Adaptive response length based on question complexity&lt;/p&gt; &lt;p&gt;End-of-conversation analysis&lt;/p&gt; &lt;p&gt;You'll need: &lt;/p&gt; &lt;p&gt;LM Studio with a model like Gemma3 (me again: NOT LIKE Gemma3, ONLY Gemma3. It's the only model I've found that can do this.) &lt;/p&gt; &lt;p&gt;Python with sentence-transformers, scikit-learn, numpy&lt;/p&gt; &lt;p&gt;A decent GPU (works &amp;quot;well&amp;quot; on a 4090)&lt;/p&gt; &lt;p&gt;(me again! Again, if anyone can tell me how to post it all somewhere, happy to. And I'm just saying: This IS NOT HARD. I'm a noob, but it's like.. Run LM studio, load the model, bail to a prompt, start the server (something like lm server start) and then python talk_to_lyra2.py .. that's it. At the end of a chat? Exit. Wait maybe 10 minutes for it to parse the conversation and &amp;quot;add to its memory hat&amp;quot; .. done. You'll need to make sure python is installed and you need to add a few python pieces by typing PIP whatever, but again, NOT HARD. Then in the directory you'll have 4 json buckets: A you bucket where it places things it learned about you, an AI bucket where it places things it learned or learned about itself that it wants to remember, a &amp;quot;conversation&amp;quot; bucket with summaries of past conversations (and especially the last conversation) and the magic &amp;quot;memory&amp;quot; bucket which ends up looking like text separated by a million numbers. I've tested this thing quite a bit, and though once in a while it will freak and fail due to seemingly hitting context errors, for the most part? Works better than I'd believe.) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evening-Active1768"&gt; /u/Evening-Active1768 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzl6xd/persistent_memory_simulation_using_local_ai_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzl6xd/persistent_memory_simulation_using_local_ai_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzl6xd/persistent_memory_simulation_using_local_ai_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T06:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzid3a</id>
    <title>The Open Source Alternative to NotebookLM / Perplexity / Glean</title>
    <updated>2025-04-15T03:45:15+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzid3a/the_open_source_alternative_to_notebooklm/"&gt; &lt;img alt="The Open Source Alternative to NotebookLM / Perplexity / Glean" src="https://external-preview.redd.it/U2awOOAulsxyu0ZXVxTAnoAYdz7yFp6U--ClBjCGi4Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55b35d7c6b6e6627e7040d87f9e4f45f3ed17703" title="The Open Source Alternative to NotebookLM / Perplexity / Glean" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources like search engines (Tavily), Slack, Notion, YouTube, GitHub, and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short—here are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzid3a/the_open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzid3a/the_open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T03:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzjm23</id>
    <title>New Moondream VLM Release (2025-04-14)</title>
    <updated>2025-04-15T04:57:09+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-04-14-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzjm23/new_moondream_vlm_release_20250414/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzjm23/new_moondream_vlm_release_20250414/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T04:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyu06v</id>
    <title>llama was so deep that now ex employee saying that we r not involved in that project</title>
    <updated>2025-04-14T08:36:06+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt; &lt;img alt="llama was so deep that now ex employee saying that we r not involved in that project" src="https://preview.redd.it/49mfsia3irue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3266a093713e9cb503b3634a7a8b1f7fb0852f0" title="llama was so deep that now ex employee saying that we r not involved in that project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49mfsia3irue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzl0g</id>
    <title>DGX B200 Startup ASMR</title>
    <updated>2025-04-14T14:00:52+00:00</updated>
    <author>
      <name>/u/Chemical-Mixture3481</name>
      <uri>https://old.reddit.com/user/Chemical-Mixture3481</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt; &lt;img alt="DGX B200 Startup ASMR" src="https://external-preview.redd.it/YTF4eTdsdnozdHVlMTsTLvzMSe_uV5dg8VNzSYJEyMCa9wyDSSGv4dzqg19H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=635d486d1d0cd25569bc0a249087d4f5360ec61e" title="DGX B200 Startup ASMR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just installed one of these beasts in our datacenter. Since I could not find a video that shows one of these machines running with original sound here you go!&lt;/p&gt; &lt;p&gt;Thats probably ~110dB of fan noise given that the previous generation was at around 106dB according to Nvidia. Cooling 1kW GPUs seems to be no joke given that this machine sounds like a fighter jet starting its engines next to you :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical-Mixture3481"&gt; /u/Chemical-Mixture3481 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy6c2lvz3tue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz1oxv</id>
    <title>NVIDIA has published new Nemotrons!</title>
    <updated>2025-04-14T15:29:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what a week....!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T15:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz80f1</id>
    <title>I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)</title>
    <updated>2025-04-14T19:43:56+00:00</updated>
    <author>
      <name>/u/coconautico</name>
      <uri>https://old.reddit.com/user/coconautico</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt; &lt;img alt="I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)" src="https://b.thumbs.redditmedia.com/q-Pn0SmLZQ_hi8NQ8NNJfqDfuG1Jn_CjohIa15bdh0w.jpg" title="I benchmarked 7 OCR solutions on a complex academic document (with images, tables, footnotes...)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a &lt;strong&gt;comparison of 7 different OCR solutions&lt;/strong&gt; using the &lt;a href="https://arxiv.org/pdf/2310.06825"&gt;Mistral 7B paper&lt;/a&gt; as a reference document (pdf), which I found complex enough to properly stress-test these tools. It's the same paper used in the team's Jupyter notebook, but whatever. &lt;strong&gt;The document includes footnotes, tables, figures, math, page numbers&lt;/strong&gt;,... making it a solid candidate to test how well these tools handle real-world complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Convert a PDF document into a well-structured Markdown file, preserving text formatting, figures, tables and equations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (Ranked):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;MistralAPI [cloud]&lt;/strong&gt; → &lt;strong&gt;BEST&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Marker + Gemini&lt;/strong&gt; (--use_llm flag) &lt;strong&gt;[cloud]&lt;/strong&gt; → &lt;strong&gt;VERY GOOD&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Marker / Docling [local]&lt;/strong&gt; → &lt;strong&gt;GOOD&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyMuPDF4LLM [local]&lt;/strong&gt; → &lt;strong&gt;OKAY&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Pro [cloud]&lt;/strong&gt; → &lt;strong&gt;BEST* (...but doesn't extract images)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markitdown (without AzureAI) [local]&lt;/strong&gt; → &lt;strong&gt;POOR* (doesn't extract images)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;OCR images to compare:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g0ihgjgpruue1.png?width=5738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94537b4d1073286c7570d8739c512bb43f4fd8aa"&gt;OCR comparison for: Mistral, Marker+Gemini, Marker, Docling, PyMuPDF4LLM, Gemini 2.5 Pro, and Markitdown&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links to tools:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://mistral.ai/news/mistral-ocr"&gt;MistralOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-pro"&gt;Gemini 2.5 Pro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/microsoft/markitdown"&gt;Markitdown&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/"&gt;PyMuPDF4LLM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coconautico"&gt; /u/coconautico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T19:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz2iuc</id>
    <title>glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination</title>
    <updated>2025-04-14T16:02:56+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt; &lt;img alt="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" src="https://external-preview.redd.it/CbrIZBC-MoAMjzDIDvad-loXR06ele61H5F_oGZcxJQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f468a7cb4f1f63cdc2c35347ae9f9d3abd7d3e" title="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6 new models and interesting benchmarks&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-32B-0414&lt;/strong&gt; is a reasoning model with deep thinking capabilities. This was developed based on GLM-4-32B-0414 through cold start, extended reinforcement learning, and further training on tasks including mathematics, code, and logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to solve complex tasks. During training, we also introduced general reinforcement learning based on pairwise ranking feedback, which enhances the model's general capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-Rumination-32B-0414&lt;/strong&gt; is a deep reasoning model with rumination capabilities (against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model is capable of deeper and longer thinking to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks. The model shows significant improvements in research-style writing and complex tasks.&lt;/p&gt; &lt;p&gt;Finally, &lt;strong&gt;GLM-Z1-9B-0414&lt;/strong&gt; is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jz2iuc/video/t1b3wsidqtue1/player"&gt;write a Python program that shows a ball bouncing inside a spinning hexagon. The ball should be affected by gravity and friction, and it must bounce off the rotating walls realistically &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b"&gt;https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1"&gt;https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz624j</id>
    <title>DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve.</title>
    <updated>2025-04-14T18:26:32+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"&gt; &lt;img alt="DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve." src="https://preview.redd.it/tlcxh6pffuue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9761b09a84b56f8b9ad25c4ee42a925420e4fe96" title="DeepSeek V3's strong standing here makes you wonder what v4/R2 could achieve." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tlcxh6pffuue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz624j/deepseek_v3s_strong_standing_here_makes_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytw62</id>
    <title>DeepSeek is about to open-source their inference engine</title>
    <updated>2025-04-14T08:27:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt; &lt;img alt="DeepSeek is about to open-source their inference engine" src="https://preview.redd.it/1am95yongrue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967ad74640babe443b3c9a2867547f568219bda6" title="DeepSeek is about to open-source their inference engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is about to open-source their inference engine, which is a modified version based on vLLM. Now, DeepSeek is preparing to contribute these modifications back to the community.&lt;/p&gt; &lt;p&gt;I really like the last sentence: 'with the goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0.'&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine"&gt;https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1am95yongrue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz6rwj</id>
    <title>OpenAI released a new Prompting Cookbook with GPT 4.1</title>
    <updated>2025-04-14T18:54:15+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"&gt; &lt;img alt="OpenAI released a new Prompting Cookbook with GPT 4.1" src="https://external-preview.redd.it/khZZDaErUPszuYMOAnI0g6ZacxmX2AdST6xS5QZoW9g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a79822bf5ec27d84d21f68af9b0b6792aee1dada" title="OpenAI released a new Prompting Cookbook with GPT 4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz6rwj/openai_released_a_new_prompting_cookbook_with_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzcgy1</id>
    <title>Finally finished my "budget" build</title>
    <updated>2025-04-14T22:53:27+00:00</updated>
    <author>
      <name>/u/C_Coffie</name>
      <uri>https://old.reddit.com/user/C_Coffie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzcgy1/finally_finished_my_budget_build/"&gt; &lt;img alt="Finally finished my &amp;quot;budget&amp;quot; build" src="https://preview.redd.it/yes9qjnnqvue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c609ca13e495685e844a75eac7fefbca17e49819" title="Finally finished my &amp;quot;budget&amp;quot; build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hardware&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;4x EVGA RTX 3090 FTW3 Ultra (24G-P5-3987-KR)&lt;/li&gt; &lt;li&gt;AMD EPYC 7302P &lt;ul&gt; &lt;li&gt;16 Cores 32 Threads&lt;/li&gt; &lt;li&gt;3.0GHz Base 3.3GHz Boost&lt;/li&gt; &lt;li&gt;AMD Socket SP3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Asrock Rack ROMED6U-2L2T&lt;/li&gt; &lt;li&gt;2TB Samsung 980 Pro&lt;/li&gt; &lt;li&gt;Memory: 6x 16gb DDR4 2933 MHz&lt;/li&gt; &lt;li&gt;MLACOM Quad Station PRO LITE v.3 (&lt;a href="https://www.mlacom.si/komponente/ohisja/i_2849344_mlacom-quad-station-pro-lite-black-edition-v-3-ml-qspl-b-v3"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;GPU Risers cables &lt;ul&gt; &lt;li&gt;1x LINKUP - AVA5 PCIE 5.0 Riser Cable - Straight (v2) - 25cm (&lt;a href="https://www.amazon.com/dp/B0D5F8KBQR"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;1/2x Okinos - PCI-E 4.0 Riser Cable - 200mm - Black (&lt;a href="https://www.amazon.com/dp/B0C22WJTMB"&gt;link&lt;/a&gt;) &lt;ul&gt; &lt;li&gt;One of these actually died and was replaced by the above LINKUP cable. 200mm was a little short for the far GPU so if you decide to go with the Okinos risers make sure you swap one for a 300mm&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Okinos - PCI-E 4.0 Riser Cable - 150mm - Black (&lt;a href="https://www.amazon.com/dp/B0CNNJHK93"&gt;link&lt;/a&gt;) &lt;ul&gt; &lt;li&gt;They sent the white version instead.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Corsair RM1200x Shift Fully Modular ATX Power Supply (Renewed) (&lt;a href="https://www.amazon.com/dp/B0DD5TWT1L"&gt;link&lt;/a&gt;) &lt;ul&gt; &lt;li&gt;1x Dual PSU ATX Power Supply Motherboard Adapter Cable (&lt;a href="https://www.amazon.com/dp/B07543LNRH"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Cost&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPUs - $600/ea x 4 - $2400&lt;/li&gt; &lt;li&gt;Motherboard + CPU + Memory (came with 64gb) + SSD from a used Ebay listing (plus some extra parts that I plan on selling off) - $950&lt;/li&gt; &lt;li&gt;Case - $285&lt;/li&gt; &lt;li&gt;Risers - LINKUP $85 + Okinos $144 - Total $229&lt;/li&gt; &lt;li&gt;Power Supplies - $300&lt;/li&gt; &lt;li&gt;Dual Power Supply Adapter Cable - $10&lt;/li&gt; &lt;li&gt;Additional Memory (32gb) - $30&lt;/li&gt; &lt;li&gt;Total - $4204&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_Coffie"&gt; /u/C_Coffie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yes9qjnnqvue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzcgy1/finally_finished_my_budget_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzcgy1/finally_finished_my_budget_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T22:53:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzexz7</id>
    <title>Added GPT-4.1, Gemini-2.5-Pro, DeepSeek-V3-0324 etc...</title>
    <updated>2025-04-15T00:49:19+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzexz7/added_gpt41_gemini25pro_deepseekv30324_etc/"&gt; &lt;img alt="Added GPT-4.1, Gemini-2.5-Pro, DeepSeek-V3-0324 etc..." src="https://external-preview.redd.it/MzZxeWhkYTdid3VlMQu6kfIQ2qvjZubTK1d4oAMWlE_XwqBzgnAgfYZK7ysP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd44476caf2b71ab61b4f3721b43a604b0c46fdf" title="Added GPT-4.1, Gemini-2.5-Pro, DeepSeek-V3-0324 etc..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to resolution limitations, this demonstration only includes the top 16 scores from my KCORES LLM Arena. Of course, I also tested other models, but they didn't make it into this ranking.&lt;/p&gt; &lt;p&gt;The prompt used is as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Write a Python program that shows 20 balls bouncing inside a spinning heptagon: - All balls have the same radius. - All balls have a number on it from 1 to 20. - All balls drop from the heptagon center when starting. - Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d, #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e32, #e17b34, #dd7a56, #db8449, #d66a35 - The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls. - The material of all the balls determines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius. - All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball. - The heptagon is spinning around its center, and the speed of spinning is 360 degrees per 5 seconds. - The heptagon size should be large enough to contain all the balls. - Do not use the pygame library; implement collision detection algorithms and collision response etc. by yourself. The following Python libraries are allowed: tkinter, math, numpy, dataclasses, typing, sys. - All codes should be put in a single Python file. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4l29hha7bwue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzexz7/added_gpt41_gemini25pro_deepseekv30324_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzexz7/added_gpt41_gemini25pro_deepseekv30324_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T00:49:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz5i8h</id>
    <title>Which model listened to you the best</title>
    <updated>2025-04-14T18:04:29+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"&gt; &lt;img alt="Which model listened to you the best" src="https://preview.redd.it/r537cvlobuue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53c87f9cd66558adbfaf7e405bbd3f001354427" title="Which model listened to you the best" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r537cvlobuue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz5i8h/which_model_listened_to_you_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T18:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzk8nu</id>
    <title>So OpenAI released nothing open source today?</title>
    <updated>2025-04-15T05:36:59+00:00</updated>
    <author>
      <name>/u/DamiaHeavyIndustries</name>
      <uri>https://old.reddit.com/user/DamiaHeavyIndustries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Except that benchmarking tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DamiaHeavyIndustries"&gt; /u/DamiaHeavyIndustries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jzk8nu/so_openai_released_nothing_open_source_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-15T05:36:59+00:00</published>
  </entry>
</feed>
