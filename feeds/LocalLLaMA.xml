<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-02T19:05:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mfjn88</id>
    <title>TTS Model Comparisons: My Personal Rankings (So far) of TTS Models</title>
    <updated>2025-08-02T06:32:11+00:00</updated>
    <author>
      <name>/u/iKontact</name>
      <uri>https://old.reddit.com/user/iKontact</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt; &lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt; &lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKontact"&gt; /u/iKontact &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T06:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfk3y2</id>
    <title>MetaStoneTec/XBai-o4</title>
    <updated>2025-08-02T07:00:21+00:00</updated>
    <author>
      <name>/u/ljosif</name>
      <uri>https://old.reddit.com/user/ljosif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried &lt;a href="https://huggingface.co/MetaStoneTec/XBai-o4"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt; &lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance &lt;/p&gt; &lt;p&gt;Have not tried it myself, downloading atm from &lt;a href="https://huggingface.co/mradermacher/XBai-o4-GGUF"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljosif"&gt; /u/ljosif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfldxj</id>
    <title>Small LLM in german</title>
    <updated>2025-08-02T08:22:20+00:00</updated>
    <author>
      <name>/u/Ghulaschsuppe</name>
      <uri>https://old.reddit.com/user/Ghulaschsuppe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ghulaschsuppe"&gt; /u/Ghulaschsuppe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T08:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqr3o</id>
    <title>Ollamacode - Local AI assistant that can create, run and understand your codebase.</title>
    <updated>2025-08-02T13:37:35+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/"&gt; &lt;img alt="Ollamacode - Local AI assistant that can create, run and understand your codebase." src="https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc443c649d71e676f2ac536126f73b6f681be48d" title="Ollamacode - Local AI assistant that can create, run and understand your codebase." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt; &lt;p&gt;Here are a few things I've focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt; &lt;li&gt;I've tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt; &lt;li&gt;It's designed to work with any Ollama model that supports function calling.&lt;/li&gt; &lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tooyipjee/ollamacode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T13:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfuu40</id>
    <title>Gateway/Proxy for Claude-Code to OpenAI API compatible.</title>
    <updated>2025-08-02T16:31:16+00:00</updated>
    <author>
      <name>/u/ziozzang0</name>
      <uri>https://old.reddit.com/user/ziozzang0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing an OpenAI proxy solution for Claude-Code&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ziozzang/claude2openai-proxy"&gt;https://github.com/ziozzang/claude2openai-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Advantages:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.&lt;/li&gt; &lt;li&gt;You can start using Claude Code with any specified model, whether it’s internal or external.&lt;/li&gt; &lt;li&gt;You can also debug if needed.&lt;/li&gt; &lt;li&gt;Anyway, at least from my tests, it works very well. The only issue is the model itself.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;While using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn’t find a solid solution that worked well for multiple users. So, based on &lt;a href="https://github.com/1rgs/claude-code-proxy"&gt;https://github.com/1rgs/claude-code-proxy &lt;/a&gt;, I built a proxy tailored for my use. Since it converts between different protocols, “gateway” might actually be a more fitting term.&lt;/p&gt; &lt;p&gt;Anyway, here are the features:&lt;/p&gt; &lt;p&gt;Full support for Claude Code.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.&lt;/li&gt; &lt;li&gt;Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.&lt;/li&gt; &lt;li&gt;Authentication is done via the ANTHROPIC_API_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below is an example of setting up the server and actually using it from a client:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ANTHROPIC_BASE_URL=http://localhost:8082 \ ANTHROPIC_API_KEY=sk-openapi-auth-token \ ANTHROPIC_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \ ANTHROPIC_SMALL_FAST_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \ claude &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To be honest, I made this to test the openrouter/horizon-beta model. :)&lt;/p&gt; &lt;p&gt;The pipeline works great: Claude Code -(Claude API)-&amp;gt; my modified proxy server -(OpenAI API)-&amp;gt; openrouter/horizon-beta.&lt;/p&gt; &lt;p&gt;By the way, you can find what I built at &lt;a href="https://github.com/ziozzang/claude2openai-proxy"&gt;https://github.com/ziozzang/claude2openai-proxy &lt;/a&gt;. I use it by building it into a container.&lt;/p&gt; &lt;p&gt;To be honest,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;horizon-alpha doesn’t seem to handle Claude Code’s prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)&lt;/li&gt; &lt;li&gt;horizon-beta handles them quite well.&lt;/li&gt; &lt;li&gt;However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don’t feel ideal for automated workflows.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PS.&lt;/p&gt; &lt;p&gt;The whole reason this started was because of Claude Code’s usage limits. LoL...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ziozzang0"&gt; /u/ziozzang0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfv3b0</id>
    <title>Best local LLM that fits with 12GB VRAM?</title>
    <updated>2025-08-02T16:41:55+00:00</updated>
    <author>
      <name>/u/tthane50</name>
      <uri>https://old.reddit.com/user/tthane50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m using Qwen 3 14B right now but haven’t checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tthane50"&gt; /u/tthane50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfrq3v</id>
    <title>What is "tool use", exactly?</title>
    <updated>2025-08-02T14:21:08+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt; &lt;p&gt;Consider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt; &lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8pdo</id>
    <title>China report the finetune deepseek scientific model 40.44% on HLE</title>
    <updated>2025-08-01T21:23:37+00:00</updated>
    <author>
      <name>/u/Afraid_Hall_2971</name>
      <uri>https://old.reddit.com/user/Afraid_Hall_2971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt; &lt;img alt="China report the finetune deepseek scientific model 40.44% on HLE" src="https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19933808cef3cec6dce268be3e9d5d269f435579" title="China report the finetune deepseek scientific model 40.44% on HLE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hg：&lt;a href="https://huggingface.co/ScienceOne-AI/S1-Base-671B"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Afraid_Hall_2971"&gt; /u/Afraid_Hall_2971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rnyzqia76hgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf3tm9</id>
    <title>The “Leaked” 120 B OpenAI Model is not Trained in FP4</title>
    <updated>2025-08-01T18:11:35+00:00</updated>
    <author>
      <name>/u/badbutt21</name>
      <uri>https://old.reddit.com/user/badbutt21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt; &lt;img alt="The “Leaked” 120 B OpenAI Model is not Trained in FP4" src="https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06" title="The “Leaked” 120 B OpenAI Model is not Trained in FP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badbutt21"&gt; /u/badbutt21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g1yk8r6b8ggf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfeazc</id>
    <title>Cerebras Pro Coder Deceptive Limits</title>
    <updated>2025-08-02T01:41:01+00:00</updated>
    <author>
      <name>/u/snipsthekittycat</name>
      <uri>https://old.reddit.com/user/snipsthekittycat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt; &lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snipsthekittycat"&gt; /u/snipsthekittycat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T01:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf92r1</id>
    <title>MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs</title>
    <updated>2025-08-01T21:38:58+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt; &lt;img alt="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" src="https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg" title="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt; &lt;h1&gt;Autonomous Research Mode&lt;/h1&gt; &lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt; &lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt; &lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt; &lt;h1&gt;Document Management&lt;/h1&gt; &lt;p&gt;The application is built around a document management system.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mf92r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfrunn</id>
    <title>It's time to run your own R1, Kimi ... and split the cost of it</title>
    <updated>2025-08-02T14:26:51+00:00</updated>
    <author>
      <name>/u/HammerSpb</name>
      <uri>https://old.reddit.com/user/HammerSpb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HammerSpb"&gt; /u/HammerSpb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfn7pv</id>
    <title>Tool calling is now supported on World's first Intermediate Reasoning model</title>
    <updated>2025-08-02T10:24:52+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt; &lt;p&gt;Updated Model link:- &lt;a href="https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br /&gt; API and Chat page :- &lt;a href="https://helpingai.co"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:24:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfxas1</id>
    <title>Qwen moe in C</title>
    <updated>2025-08-02T18:14:18+00:00</updated>
    <author>
      <name>/u/1Hesham</name>
      <uri>https://old.reddit.com/user/1Hesham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped something I'm really excited about! 🚀 I was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo. That got me thinking... what if I could bring this to pure C? 🤔 Inspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file. The result? Qwen_MOE_C - a complete inference engine that: ✅ Handles sparse MoE computation (only 8 out of 128 experts active) ✅ Supports Grouped Query Attention with proper head ratios ✅ Uses memory mapping for efficiency (~30GB models) ✅ Zero external dependencies (just libc + libm) The beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency. Huge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems. Sometimes the best way to truly understand something is to build it from scratch. 🛠️ Link to the project: &lt;a href="https://github.com/h9-tec/Qwen_MOE_C"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Hesham"&gt; /u/1Hesham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T18:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfll39</id>
    <title>AI models are picking up hidden habits from each other | IBM</title>
    <updated>2025-08-02T08:35:31+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"&gt; &lt;img alt="AI models are picking up hidden habits from each other | IBM" src="https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46e4049e81a052bc62430cfe7e667f62662c693b" title="AI models are picking up hidden habits from each other | IBM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ibm.com/think/news/ai-models-subliminal-learning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T08:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfnq2r</id>
    <title>Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090</title>
    <updated>2025-08-02T10:57:41+00:00</updated>
    <author>
      <name>/u/kargafe</name>
      <uri>https://old.reddit.com/user/kargafe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt; &lt;img alt="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" src="https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f" title="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt; &lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. I used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison. I'm also planning to run some fine-tuning benchmarks later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kargafe"&gt; /u/kargafe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erib4a6t7lgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfitwb</id>
    <title>Skywork MindLink 32B/72B</title>
    <updated>2025-08-02T05:41:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"&gt; &lt;img alt="Skywork MindLink 32B/72B" src="https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7" title="Skywork MindLink 32B/72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/MindLink-32B-0801"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/MindLink-72B-0801"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/im7w319dnjgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T05:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfwckf</id>
    <title>100+ AI Benchmarks list</title>
    <updated>2025-08-02T17:34:52+00:00</updated>
    <author>
      <name>/u/panilyaU</name>
      <uri>https://old.reddit.com/user/panilyaU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt; &lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href="https://github.com/sindresorhus/awesome"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href="https://aibenchmarks.net/"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href="https://github.com/panilya/awesome-ai-benchmarks"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panilyaU"&gt; /u/panilyaU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T17:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfaigh</id>
    <title>We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)</title>
    <updated>2025-08-01T22:40:00+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Organization&lt;/th&gt; &lt;th align="left"&gt;HuggingFace Link&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Modality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;dots.ocr&lt;/td&gt; &lt;td align="left"&gt;REDnote Hilab&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5-Air&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;480B-A35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Base&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intern S1&lt;/td&gt; &lt;td align="left"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;241B-A22B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;49B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 7B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 14B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 32B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;step3&lt;/td&gt; &lt;td align="left"&gt;StepFun&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/stepfun-ai/step3"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;321B-A38B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B-A0.6B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X Instruct-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X PPO-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Small 24B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Mini 3B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B Base&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ling lite-1.5 2506&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-lite-1.5-2506"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ming Lite Omni-1.5&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;20.3B&lt;/td&gt; &lt;td align="left"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 32B 0727&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 4B 0729&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 8B&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;command a vision 07-2025&lt;/td&gt; &lt;td align="left"&gt;Cohere&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-vision-07-2025"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;112B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT V1 40B&lt;/td&gt; &lt;td align="left"&gt;Kwaipilot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;40B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 2B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;671B-A37B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-405B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;405B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;109B-A17B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-70B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 4.0 VL Light&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-4.0-VL-Light"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 3.1&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-3.1"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmOCR 7B 0725&lt;/td&gt; &lt;td align="left"&gt;AllenAI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5v 3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 7B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-7B"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 21B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-21B"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 70B preview SFT&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt; &lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt; &lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;[EDIT] &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyone could publish a new model—with some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:40:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mftipa</id>
    <title>Local or die</title>
    <updated>2025-08-02T15:36:14+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mftipa/local_or_die/"&gt; &lt;img alt="Local or die" src="https://preview.redd.it/weu00abilmgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=873b6f571bbf993d631f859b171dd773b496a41a" title="Local or die" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/weu00abilmgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mftipa/local_or_die/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mftipa/local_or_die/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T15:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfuiri</id>
    <title>Qwen Code + Qwen Coder 30b 3A is insane</title>
    <updated>2025-08-02T16:17:55+00:00</updated>
    <author>
      <name>/u/Flashy_Management962</name>
      <uri>https://old.reddit.com/user/Flashy_Management962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a little remark that if you haven't you definitely should try qwen code &lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br /&gt; I use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt; &lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt; &lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy_Management962"&gt; /u/Flashy_Management962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfvxdo</id>
    <title>What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5</title>
    <updated>2025-08-02T17:17:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15026"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt; &lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt; &lt;p&gt;Disclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt; &lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T17:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfs9qn</id>
    <title>[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations</title>
    <updated>2025-08-02T14:44:41+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use &lt;code&gt;q5_1&lt;/code&gt; for a good balance of memory usage and performance. It works well in PPL tests and in practice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You're bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt; &lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\.(1[6-9]|[2-4][0-9])\.ffn_.*._=CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt; &lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt; &lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt; &lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt; &lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;code&gt;q5_1&lt;/code&gt; or even &lt;code&gt;q8_0&lt;/code&gt; for the draft model KV cache for much better performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt; &lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqejn</id>
    <title>Open-source model that is as intelligent as Claude Sonnet 4</title>
    <updated>2025-08-02T13:21:11+00:00</updated>
    <author>
      <name>/u/vishwa1238</name>
      <uri>https://old.reddit.com/user/vishwa1238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt; &lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt; &lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishwa1238"&gt; /u/vishwa1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T13:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgj0g</id>
    <title>all I need....</title>
    <updated>2025-08-02T03:34:51+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt; &lt;img alt="all I need...." src="https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd" title="all I need...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggc3dzhr0jgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T03:34:51+00:00</published>
  </entry>
</feed>
