<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-17T23:48:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1irva7y</id>
    <title>Gemini 2.0 Thinking responding in Chinese</title>
    <updated>2025-02-17T21:23:06+00:00</updated>
    <author>
      <name>/u/Thedudely1</name>
      <uri>https://old.reddit.com/user/Thedudely1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irva7y/gemini_20_thinking_responding_in_chinese/"&gt; &lt;img alt="Gemini 2.0 Thinking responding in Chinese" src="https://preview.redd.it/aqn2ctr2orje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5869c46246eb1a5b9534f8322c8f9eefcf1ddab5" title="Gemini 2.0 Thinking responding in Chinese" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just discussing GPU architectures with &amp;quot;Gemini 2.0 Flash Thinking with apps&amp;quot; in the Gemini app. I didn't enter any Chinese characters or anything but in the response section it seems to have written the word &amp;quot;acceleration&amp;quot; in Chinese and then clarifies itself in English? There doesn't appear to be any Chinese in the &amp;quot;thinking&amp;quot; section... strange&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thedudely1"&gt; /u/Thedudely1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aqn2ctr2orje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irva7y/gemini_20_thinking_responding_in_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irva7y/gemini_20_thinking_responding_in_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T21:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvswt</id>
    <title>Is there a Local LLM + Viewers that can re-create Claude's HTML Preview?</title>
    <updated>2025-02-17T21:44:14+00:00</updated>
    <author>
      <name>/u/false79</name>
      <uri>https://old.reddit.com/user/false79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvswt/is_there_a_local_llm_viewers_that_can_recreate/"&gt; &lt;img alt="Is there a Local LLM + Viewers that can re-create Claude's HTML Preview?" src="https://b.thumbs.redditmedia.com/FXyVM7R7fza4xi_1JDIl__1oBlJ9HJUUv7dIRJdkvyQ.jpg" title="Is there a Local LLM + Viewers that can re-create Claude's HTML Preview?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The use case I can do today in Claude is &amp;quot;Using Material Design 3, generate a button that says &amp;quot;Hello World&amp;quot; and it will generate the view on a side panel. Can this be done locally today without having to save the HTML locally to a file and then opening it with a browser?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0vz6uy8qrrje1.png?width=1864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d370e28e3c8ddf73bb986131b27d11233390dd70"&gt;https://preview.redd.it/0vz6uy8qrrje1.png?width=1864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d370e28e3c8ddf73bb986131b27d11233390dd70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/false79"&gt; /u/false79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvswt/is_there_a_local_llm_viewers_that_can_recreate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvswt/is_there_a_local_llm_viewers_that_can_recreate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irvswt/is_there_a_local_llm_viewers_that_can_recreate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T21:44:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1irp67y</id>
    <title>How do Browser Automation Agents work?</title>
    <updated>2025-02-17T17:22:23+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing so many of these lately, but I haven't quite understood how they work. Are they using a text or vision based approach? The text-based approach seems intuitive - - get the src of the webpage and feed it to the LLM and query it for the XPath of the form item/element that needs to be clicked/interacted with. Even at this level, I'm curious how this process is made stable and reliable, since the web page source (esp with JS-heavy sites) can have so much irrelevant information that may throw off the LLM and output incorrect XPaths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irp67y/how_do_browser_automation_agents_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irp67y/how_do_browser_automation_agents_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irp67y/how_do_browser_automation_agents_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iro7hw</id>
    <title>How far can we get with models 14b params in size?</title>
    <updated>2025-02-17T16:43:47+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started running LLMs locally on my Mac only a couple of months ago. With my M4 Pro mini and 64 GB of memory, the best model I can run in terms of quality and speed is Qwen 2.5 14b. I can run 32b models too, but they run at less than half the speed. With the 14b model, I get up to 30-35 tokens per second using MLX models with speculative decoding enabled.&lt;/p&gt; &lt;p&gt;It seems that the performance of smaller models is improving rapidly. My understanding is that a current 14b model can outperform older models like GPT 3.5 or some larger models from a few years back.&lt;/p&gt; &lt;p&gt;Is it realistic to expect 14b models in a few years that could perform as well as today's DeepSeek V3, for instance?&lt;/p&gt; &lt;p&gt;I bought the M4 Pro mini in December and plan to keep it for around three years. Do you think I'll see much more capable 14b models within this time that I can run with good speeds on my hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iro7hw/how_far_can_we_get_with_models_14b_params_in_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iro7hw/how_far_can_we_get_with_models_14b_params_in_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iro7hw/how_far_can_we_get_with_models_14b_params_in_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T16:43:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1irv77p</id>
    <title>Did anyone have to deal with system freezes when running multiple GPUs?</title>
    <updated>2025-02-17T21:19:42+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am on an ASUS x99 ws-e motherboard and I just finished plugging 4 3090s into it. The system freezes after a few minutes of light use (browser) no matter what. On Windows 11, PopOS and Ubuntu Mate 24.04. Had the freeze even once in the bios menu.&lt;/p&gt; &lt;p&gt;I tried setting all pcie slots to gen 3 instead of auto, enabled above 4g decoding, tested with one gpu at each time, on all gpus and it froze as well. Tried gaming briefly on each GPU on my other machine and they seem to run just fine, no freeze. Furmark also stable. PSU has more than enough power, cpu remains cool at all times. continuously dumping dmesg into a text file, when the freeze happens, after reboot there are no errors. Same for journalctl -b -1. The RAM was taken from my gaming pc so I know it works fine (has xmp but is disabled since the xeon cpu doesn't support it)&lt;/p&gt; &lt;p&gt;The system just freezes with the card still outputting the latest state but frozen. Can only reboot with hard reset.&lt;/p&gt; &lt;p&gt;It is a really annoying problem that can't be debugged. For now I got another X99 motherboars and another CPU to test with. In the meanwhile did anyone manage to solve such problem in the past?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irv77p/did_anyone_have_to_deal_with_system_freezes_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irv77p/did_anyone_have_to_deal_with_system_freezes_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irv77p/did_anyone_have_to_deal_with_system_freezes_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T21:19:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir3rsl</id>
    <title>Inference speed of a 5090.</title>
    <updated>2025-02-16T21:56:45+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've rented the 5090 on vast and ran my benchmarks (I'll probably have to make a new bech test with more current models but I don't want to rerun all benchs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 5090 is &amp;quot;only&amp;quot; 50% faster in inference than the 4090 (a much better gain than it got in gaming)&lt;/p&gt; &lt;p&gt;I've noticed that the inference gains are almost proportional to the ram speed till the speed is &amp;lt;1000 GB/s then the gain is reduced. Probably at 2TB/s the inference become GPU limited while when speed is &amp;lt;1TB it is vram limited.&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:56:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbtc4</id>
    <title>ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models</title>
    <updated>2025-02-17T04:42:47+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.09696"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1irp09f</id>
    <title>Expose Anemll models locally via API + included frontend</title>
    <updated>2025-02-17T17:15:46+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irp09f/expose_anemll_models_locally_via_api_included/"&gt; &lt;img alt="Expose Anemll models locally via API + included frontend" src="https://external-preview.redd.it/9NtkGCt61cNvHIaehmU99JEmnaYKdsdUydVPA-y0H2o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0c98b6dc151b9410d84f363f83dd7d64d83dff5" title="Expose Anemll models locally via API + included frontend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/alexgusevski/Anemll-Backend-WebUI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irp09f/expose_anemll_models_locally_via_api_included/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irp09f/expose_anemll_models_locally_via_api_included/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvmhv</id>
    <title>What are the odds these are legit?</title>
    <updated>2025-02-17T21:36:53+00:00</updated>
    <author>
      <name>/u/jwil00</name>
      <uri>https://old.reddit.com/user/jwil00</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"&gt; &lt;img alt="What are the odds these are legit?" src="https://b.thumbs.redditmedia.com/nwPSVmSRhkBTlmLfEnbl7r1VJW_spzxnNyBtXwxYWlY.jpg" title="What are the odds these are legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously I’m thinking not great, but the deal was too good to pass up for all that VRAM.&lt;/p&gt; &lt;p&gt;Also, how would I go about confirming their legitimacy once they arrive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwil00"&gt; /u/jwil00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irvmhv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T21:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir9mcw</id>
    <title>Today I am launching OpenArc, a python serving API for faster inference on Intel CPUs, GPUs and NPUs. Low level, minimal dependencies and comes with the first GUI tools for model conversion.</title>
    <updated>2025-02-17T02:40:18+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, a lightweight inference engine built using Optimum-Intel from Transformers to leverage hardware acceleration on Intel devices. &lt;/p&gt; &lt;p&gt;Here are some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strongly typed API with four endpoints&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;/model/load: loads model and accepts ov_config&lt;/li&gt; &lt;li&gt;/model/unload: use gc to purge a loaded model from device memory&lt;/li&gt; &lt;li&gt;/generate/text: synchronous execution, select sampling parameters, token limits : also returns a performance report&lt;/li&gt; &lt;li&gt;/status: see the loaded model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Each endpoint has a pydantic model keeping exposed parameters easy to maintain or extend.&lt;/li&gt; &lt;li&gt;Native chat templates&lt;/li&gt; &lt;li&gt;Conda environment.yaml for portability with a proper .toml coming soon&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Audience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Owners of Intel accelerators&lt;/li&gt; &lt;li&gt;Those with access to high or low end CPU only servers&lt;/li&gt; &lt;li&gt;Edge devices with Intel chips&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OpenArc is my first open source project representing months of work with OpenVINO and Intel devices for AI/ML. Developers and engineers who work with OpenVINO/Transformers/IPEX-LLM will find it's syntax, tooling and documentation complete; new users should find it more approachable than the documentation available from Intel, including the mighty [openvino_notebooks](&lt;a href="https://github.com/openvinotoolkit/openvino%5C_notebooks"&gt;https://github.com/openvinotoolkit/openvino\_notebooks&lt;/a&gt;) which I cannot recommend enough.&lt;/p&gt; &lt;p&gt;My philosophy with OpenArc has been to make the project as low level as possible to promote access to the heart and soul of OpenArc, the conversation object. This is where the chat history lives 'traditionally'; in practice this enables all sorts of different strategies for context management that make more sense for agentic usecases, though OpenArc is low level enough to support many different usecases.&lt;/p&gt; &lt;p&gt;For example, a model you intend to use for a search task might not need a context window larger than 4k tokens; thus, you can store facts from the smaller agents results somewhere else, catalog findings, purge the conversation from conversation and an unbiased small agent tackling a fresh directive from a manager model can be performant with low context. &lt;/p&gt; &lt;p&gt;If we zoom out and think about how the code required for iterative search, database access, reading dataframes, doing NLP or generating synthetic data should be built- at least to me- inference code has no place in such a pipeline. OpenArc promotes API call design patterns for interfacing with LLMs locally that OpenVINO has lacked until now. Other serving platforms/projects have OpenVINO as a plugin or extension but none are dedicated to it's finer details, and fewer have quality documentation regarding the design of solutions that require deep optimization available from OpenVINO.&lt;/p&gt; &lt;p&gt;Coming soon;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Openai proxy&lt;/li&gt; &lt;li&gt;More OV_config documentation. It's quite complex!&lt;/li&gt; &lt;li&gt;docker compose examples&lt;/li&gt; &lt;li&gt;Multi GPU execution- I havent been able to get this working due to driver issues maybe, but as of now OpenArc fully supports it and models at my hf repo linked on git with the &amp;quot;-ns&amp;quot; suffix should work. It's a hard topic and requires more testing before I can document.&lt;/li&gt; &lt;li&gt;Benchmarks and benchmarking scripts&lt;/li&gt; &lt;li&gt;Load multiple models into memory and onto different devices&lt;/li&gt; &lt;li&gt;a Panel dashboard for managing OpenArc&lt;/li&gt; &lt;li&gt;Autogen and smolagents examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking out my project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T02:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1irklio</id>
    <title>Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)</title>
    <updated>2025-02-17T14:06:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt; &lt;img alt="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" src="https://external-preview.redd.it/phQl7qnxBNwj9zS7K_vfIQhmHll-NjLQgZ0PIR30DxA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=623bc922d387ff0d98a441576fd2df6e097cc4a6" title="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/stepfun-ai/step-audio-67b33accf45735bb21131b0b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1irfbxp</id>
    <title>OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android.</title>
    <updated>2025-02-17T08:35:08+00:00</updated>
    <author>
      <name>/u/nojukuramu</name>
      <uri>https://old.reddit.com/user/nojukuramu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"&gt; &lt;img alt="OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android." src="https://external-preview.redd.it/NmVpcXAzbDF2bmplMbq45-8N6tBVs1hyi_SYewJzzGn4zaTb0U8QXs40y_ID.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4d799582407a428caab1781fde079bdc7bcae71" title="OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For how i Did it, I simply run ollama and open-webui in termux and open the web interface in my browser.&lt;/p&gt; &lt;p&gt;For how i run open-webui, First i installed proot-distro so i can install a debian Then login with the debian Then within the debian environment, i installed tmux so i can run multiple consoles at once. Then run ollama serve&lt;/p&gt; &lt;p&gt;This will allow you to run ollama in your device I then installed python3-venv and create an venv Inside it, i run pip install open-webui And then run open-webui serve to start the web interface.&lt;/p&gt; &lt;p&gt;You can run tmux new -s ollama to create a session for multiple panels Then Ctrl+b, Ctrl + &amp;quot; to create new panel. In each panel run ollama serve and run openwebui (openwebui is in venv so activate venv first)&lt;/p&gt; &lt;p&gt;Then open your browser and enter localhost:8080.&lt;/p&gt; &lt;p&gt;Tip: Minimize termux app so it wont get stopped by battery optimization stuff of your phone.&lt;/p&gt; &lt;p&gt;Ps. Sorry for not being specific in instruction but you get the idea right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nojukuramu"&gt; /u/nojukuramu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cqzeqgv1vnje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T08:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwx6q</id>
    <title>DeepSeek-v2.5 dynamic quants anyone?</title>
    <updated>2025-02-17T22:30:13+00:00</updated>
    <author>
      <name>/u/Enturbulated</name>
      <uri>https://old.reddit.com/user/Enturbulated</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The unsloth dynamic quants of DeepSeek-R1 made some waves recently.&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There has been some interest expressed in giving other models the same treatment. Weeks later, not seen much done about it. Maybe I've been looking in the wrong places? Maybe nobody has because DSR1 is particularly amenable to this treatment and there's little real payoff for other models?&lt;/p&gt; &lt;p&gt;Regardless, looking at what other MoE models might benefit, one very easy answer is the DeepSeek v2 model series. Mainly because unsloth's llama.cpp fork for this requires fairly little effort to modify for this use.&lt;/p&gt; &lt;p&gt;So, what the hell.&lt;br /&gt; &lt;a href="https://huggingface.co/Enturbulate/DeepSeek-v2.5-1210-UD-gguf"&gt;https://huggingface.co/Enturbulate/DeepSeek-v2.5-1210-UD-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Five quants posted, iq1_s through iq3_m, ~49GB through ~97GB. imatrix data klepped from bartowski. Thanks!&lt;/p&gt; &lt;p&gt;The quantization strategy is pretty simple-minded, basically just don't let the attention/output layers drop below q4_k. Is this optimal? LOL. Should still perform better than standard llama.cpp low-bit quants.&lt;/p&gt; &lt;p&gt;Anyone want to share thoughts on what other models, if any, might be worth some effort?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enturbulated"&gt; /u/Enturbulated &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T22:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk7vq</id>
    <title>What to expect in 2025 for running big LLMs</title>
    <updated>2025-02-17T13:48:12+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy hardware by the end of this year for running local LLMs. Since Deepseek R1 spoiled me and raised my expectations I was thinking about bigger models (32-70B or maybe hard-quantized R1).&lt;/p&gt; &lt;p&gt;Is there any hardware coming soon or a super efficient model, new architecture etc. In 2025 to enable running these models for &amp;lt;3k Euro at 10+ tokens/s?&lt;/p&gt; &lt;p&gt;What I am watching: - Nvidia Digits - AMD AI Max Pro 395&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwhkz</id>
    <title>Mistral Saba</title>
    <updated>2025-02-17T22:12:21+00:00</updated>
    <author>
      <name>/u/Pleasant-PolarBear</name>
      <uri>https://old.reddit.com/user/Pleasant-PolarBear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"&gt; &lt;img alt="Mistral Saba" src="https://b.thumbs.redditmedia.com/Y-zL84O1MHPzF3H4PHjtXzbGVQMTeskg-HRUs4R7LEk.jpg" title="Mistral Saba" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant-PolarBear"&gt; /u/Pleasant-PolarBear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irwhkz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T22:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1irkkeo</id>
    <title>Mistral Saba | Mistral AI (Not Open Sourced)</title>
    <updated>2025-02-17T14:05:15+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt; &lt;img alt="Mistral Saba | Mistral AI (Not Open Sourced)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Saba | Mistral AI (Not Open Sourced)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/en/news/mistral-saba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlv4q</id>
    <title>The Hugging Face NLP course is back with chapters on fine-tuning LLMs</title>
    <updated>2025-02-17T15:05:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt; &lt;img alt="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" src="https://external-preview.redd.it/tQnPN9F6mNbTPcfQVDpQv5OONbx7hdsHVHxXCjSZIqM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfa4dea237900e2d3b867be223ed501cd9b3f4b7" title="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nlp-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T15:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1irgfkp</id>
    <title>all I said was "hi"</title>
    <updated>2025-02-17T09:55:06+00:00</updated>
    <author>
      <name>/u/CaptTechno</name>
      <uri>https://old.reddit.com/user/CaptTechno</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt; &lt;img alt="all I said was &amp;quot;hi&amp;quot;" src="https://preview.redd.it/16ci5no49oje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de52b259bcf0f525d3c7a4880c7597d54d6e38f4" title="all I said was &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptTechno"&gt; /u/CaptTechno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ci5no49oje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T09:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk3r6</id>
    <title>New (linear complexity ) Transformer architecture achieved improved performance</title>
    <updated>2025-02-17T13:42:34+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://robinwu218.github.io/ToST/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlihr</id>
    <title>LLMs already have ads (sort of)</title>
    <updated>2025-02-17T14:50:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt; &lt;img alt="LLMs already have ads (sort of)" src="https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4b56b82708f12907eed5cb9688415ff2947f8a5" title="LLMs already have ads (sort of)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Your AI assistant might already have a built-in corporate bias&lt;/p&gt; &lt;p&gt;I think most of us here wondered how LLMs will map out to a traditional ad-driven business model. The consensus was that LLMs could be used in a similar way by showing bias towards specific products or brands.&lt;/p&gt; &lt;p&gt;There's a paper in ICLR 2025 that shows that it already happens to an extent: &lt;a href="https://openreview.net/forum?id=odjMSBSWRt"&gt;DarkBench: Benchmarking Dark Patterns in Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark of 660 prompts to test for manipulative behaviors in LLMs&lt;/li&gt; &lt;li&gt;one of the main &amp;quot;dark patterns&amp;quot; they found was &lt;strong&gt;brand bias&lt;/strong&gt; - LLMs actively promoting their parent company's products over competitors &lt;ul&gt; &lt;li&gt;Detected in LLMs from OpenAI, Anthropic, Meta, Google, and Mistral&lt;/li&gt; &lt;li&gt;Mistral 8x7B was was the only model showing high manipulation but NO brand bias (french are le cool again)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458"&gt;https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples of the bias categories as identified by authors:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902"&gt;https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full dataset on HF: &lt;a href="https://huggingface.co/datasets/anonymous152311/darkbench"&gt;https://huggingface.co/datasets/anonymous152311/darkbench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iru2qq</id>
    <title>We added open models support to RA.Aid and need help testing</title>
    <updated>2025-02-17T20:34:58+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"&gt; &lt;img alt="We added open models support to RA.Aid and need help testing" src="https://external-preview.redd.it/aDJudDkyczdmcmplMdTGmhKtGIygTvOfC6af7JgWNHXPN4qj83wOXgAySB8l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=667212a835edd61ecdf20652eb344d105fcc750e" title="We added open models support to RA.Aid and need help testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r5db71s7frje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iry4lu</id>
    <title>How can I optimize my 1.000.000B MoE Reasoning LLM?</title>
    <updated>2025-02-17T23:21:26+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, my mum built this LLM for me called Brain, it has a weird architecture that resembles MoE but its called MoL (Mixture of Lobes), it has around 1 000 000B parameters (synapses) but it's not performing that well on MMLU pro, it gives me a lot of errors with complicated tasks, and I'm struggling to activate the frontal &lt;del&gt;Expert&lt;/del&gt; lobe, it also hallucinates 1/3 of the time, especially at night. It might be some hardware issue since I had no money for an RTX 5090 and I'm instead running it on frozen food and coke. At least it is truly multimodal since it works well with audio and images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T23:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpx0b</id>
    <title>Don’t sleep on The Allen Institute for AI (AI2)</title>
    <updated>2025-02-17T17:51:51+00:00</updated>
    <author>
      <name>/u/dontbanana</name>
      <uri>https://old.reddit.com/user/dontbanana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Allen Institute says its open-source model can beat DeepSeek&lt;/p&gt; &lt;p&gt;“The same tricks: AI2’s models use a novel reinforcement learning technique—training by way of “rewards” and “punishments” for right and wrong outputs—in which the model is taught to solve math or other problems with verifiable answers. DeepSeek used similar reinforcement learning techniques to train its models on reasoning tasks.&lt;/p&gt; &lt;p&gt;“It is pretty much, I would even argue, identical,” Hajishirzi said. “It is very simple… we had it in this paper in late November and DeepSeek came after us. Someone was asking me, ‘Did they actually copy what you did?’ I said, ‘I don’t know. It was so close that each team could come up with this independently.’ So, I don’t know, but it’s open research. A lot of these ideas could be shared.””&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dontbanana"&gt; /u/dontbanana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.emergingtechbrew.com/stories/2025/02/07/allen-institute-open-source-model-deepseek?mbcid=38624075.320719&amp;amp;mblid=76a9d29d5c33&amp;amp;mid=4bf97fa50758e4f9907627b7deaa5807&amp;amp;utm_campaign=etb&amp;amp;utm_medium=newsletter&amp;amp;utm_source=morning_brew"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhttv</id>
    <title>Zonos, the easy to use, 1.6B, open weight, text-to-speech model that creates new speech or clones voices from 10 second clips</title>
    <updated>2025-02-17T11:31:55+00:00</updated>
    <author>
      <name>/u/SoundHole</name>
      <uri>https://old.reddit.com/user/SoundHole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;hr /&gt; &lt;p&gt;I started experimenting with this model that dropped around a week ago &amp;amp; it performs fantastically, but I haven't seen any posts here about it so thought maybe it's my turn to share.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Zonos runs on as little as 8GB vram &amp;amp; converts any text to audio speech. It can also clone voices using clips between 10 &amp;amp; 30 seconds long. In my limited experience toying with the model, the results are convincing, especially if time is taken curating the samples (I recommend &lt;a href="https://www.ocenaudio.com/en/"&gt;Ocenaudio&lt;/a&gt; for a noob friendly audio editor).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;It is amazingly easy to set up &amp;amp; run via Docker (if you are using Linux. Which you should be. I am, by the way).&lt;/p&gt; &lt;p&gt;EDIT: Someone posted a &lt;a href="https://github.com/sdbds/Zonos-for-windows"&gt;Windows friendly fork&lt;/a&gt; that I absolutely cannot vouch for.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;First, install the singular special dependency:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;apt install -y espeak-ng &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Then, instead of running a uv as the authors suggest, I went with the much simpler &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid#docker-installation"&gt;Docker Installation&lt;/a&gt; instructions, which consists of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloning the repo &lt;/li&gt; &lt;li&gt;Running 'docker compose up' inside the cloned directory&lt;/li&gt; &lt;li&gt;Pointing a browser to &lt;a href="http://0.0.0.0:7860/"&gt;http://0.0.0.0:7860/&lt;/a&gt; for the UI&lt;/li&gt; &lt;li&gt;Don't forget to 'docker compose down' when you're finished&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Oh my goodness, it's brilliant!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The model is here: &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;Zonos Transformer&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;There's also a &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;hybrid model&lt;/a&gt;. I'm not sure what the difference is, there's no elaboration, so, I've only used the transformer myself.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you're using Windows... I'm not sure what to tell you. The authors straight up claim Windows is not currently supported but there's always VM's or whatever whatever. Maybe someone can post a solution.&lt;/p&gt; &lt;p&gt;Hope someone finds this useful or fun!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;EDIT: &lt;a href="https://www.sndup.net/crc4m/"&gt;Here's an example&lt;/a&gt; I quickly whipped up on the default settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoundHole"&gt; /u/SoundHole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T11:31:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpozr</id>
    <title>Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!</title>
    <updated>2025-02-17T17:43:11+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt; &lt;img alt="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" src="https://external-preview.redd.it/R6NtBwOFUehmfI110Qr1QSx4QJoALZS5zC4GvG9AcZo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ceb3ec7bb36d8267e1328a5ec597a5541141024" title="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-36B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:43:11+00:00</published>
  </entry>
</feed>
