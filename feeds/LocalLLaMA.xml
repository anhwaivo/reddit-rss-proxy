<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-21T01:11:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m4z64o</id>
    <title>Why are LLMs not able to give an estimate on their own confidence or say that they are not sure about something?</title>
    <updated>2025-07-20T20:27:36+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hallucination is a real problem with LLMs but I wonder is it such a hard problem to assign a confidence value to an inference result?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4zogr</id>
    <title>Offloading layers</title>
    <updated>2025-07-20T20:48:20+00:00</updated>
    <author>
      <name>/u/PawelSalsa</name>
      <uri>https://old.reddit.com/user/PawelSalsa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simple question, how offloading layers work in LLM, so for example if i have 24Gig rtx 3090 and offloading layers, lets say 5 gig each, so the model will offload only 4 of them leaving remaining 4 giga dormant or it will utilize it somehow as well? Asking because many time seeing task menager under performance tab I see unused Vram even though only few layers has been offloaded out of 40 or 60. So it is kind of waste of resources then. Right? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawelSalsa"&gt; /u/PawelSalsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zogr/offloading_layers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zogr/offloading_layers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zogr/offloading_layers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4zx8s</id>
    <title>What do we think of Devstral then?</title>
    <updated>2025-07-20T20:58:25+00:00</updated>
    <author>
      <name>/u/teleadx</name>
      <uri>https://old.reddit.com/user/teleadx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried it and it's quite good (latest) w/ Cline was my set-up. Why is no one talking about it? ðŸ¤”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teleadx"&gt; /u/teleadx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zx8s/what_do_we_think_of_devstral_then/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zx8s/what_do_we_think_of_devstral_then/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zx8s/what_do_we_think_of_devstral_then/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:58:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4lxak</id>
    <title>Semantic chunking using LLMs</title>
    <updated>2025-07-20T10:45:37+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt; &lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt; &lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T10:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vk88</id>
    <title>Anyone interested in adding their fine-tuned / open source models to this benchmark?</title>
    <updated>2025-07-20T18:01:43+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vk88/anyone_interested_in_adding_their_finetuned_open/"&gt; &lt;img alt="Anyone interested in adding their fine-tuned / open source models to this benchmark?" src="https://preview.redd.it/4i6kqeqjj2ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=960183ee3de41ba775a6af7c43eb2a2cd3223994" title="Anyone interested in adding their fine-tuned / open source models to this benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've posted on this sub before, but context is that me and a small team are working on a &lt;a href="https://www.designarena.ai/"&gt;benchmark&lt;/a&gt; to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people. &lt;/p&gt; &lt;p&gt;Right now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you're interested, just send me a DM. &lt;/p&gt; &lt;p&gt;Here are some requirements:&lt;br /&gt; 1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.&lt;br /&gt; 2. Give us a logo and name for the provider/org you want the model to be associated with&lt;br /&gt; 3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around ~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4i6kqeqjj2ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vk88/anyone_interested_in_adding_their_finetuned_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vk88/anyone_interested_in_adding_their_finetuned_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T18:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4zyv1</id>
    <title>Questions about AI for translation</title>
    <updated>2025-07-20T21:00:14+00:00</updated>
    <author>
      <name>/u/neobenedict</name>
      <uri>https://old.reddit.com/user/neobenedict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a solution to translate story text from a game. The translation is very domain specific to the fantasy world of the game.&lt;/p&gt; &lt;p&gt;JP-&amp;gt;EN only.&lt;/p&gt; &lt;p&gt;The text follows a visual novel format, so previous lines provide context to future lines. Generally there's a few hundred sentences per &amp;quot;chapter&amp;quot;. This can be broken down into &amp;quot;scenes&amp;quot; which are generally 50-100 sentences each.&lt;/p&gt; &lt;p&gt;Training data available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Term/Name 1:1 mappings, single word (5000-10000)&lt;/li&gt; &lt;li&gt;Lore information EN:JP mapping (few MB of text)&lt;/li&gt; &lt;li&gt;Unmapped lore information in both languages - basically scrapes of wikis&lt;/li&gt; &lt;li&gt;Per-sentence EN:JP mapping. (100MBs of text)&lt;/li&gt; &lt;li&gt;Per-scene EN:JP mapping. (same text of the above)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assume resources for a local LLM won't be an issue, but nothing into extreme territory (100GB+ VRAM isn't happening for inference, but I can rent servers e.g. 8xH200 140GB for short periods to train).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are there any other fine tuning methods I should look into for this domain?&lt;/li&gt; &lt;li&gt;What would be a good starting point? (this is an academic exercise for now, so any licence is fine)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neobenedict"&gt; /u/neobenedict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zyv1/questions_about_ai_for_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zyv1/questions_about_ai_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4zyv1/questions_about_ai_for_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T21:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4qdo6</id>
    <title>which is the best tiny vlm to recognize nsfw pics?</title>
    <updated>2025-07-20T14:32:25+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Mimo-7B. It has a decent quality at this size. but for nsfw, it can only work with anime pics. for realistic, it refused.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m52h8x</id>
    <title>Best RAG pipeline for math-heavy documents?</title>
    <updated>2025-07-20T22:47:12+00:00</updated>
    <author>
      <name>/u/PO-ll-UX</name>
      <uri>https://old.reddit.com/user/PO-ll-UX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m looking for a solid RAG pipeline that works well with SGLang + AnythingLLM. Something that can handle technical docs, math textbooks with lots of formulas, research papers, and diagrams. The RAG in AnythingLLM is, well, not great. What setups actually work for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PO-ll-UX"&gt; /u/PO-ll-UX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T22:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5586n</id>
    <title>Best novel writing workflow?</title>
    <updated>2025-07-21T00:54:02+00:00</updated>
    <author>
      <name>/u/AccidentalFolklore</name>
      <uri>https://old.reddit.com/user/AccidentalFolklore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m writing a novel thatâ€™s near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. Iâ€™m subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. &lt;/p&gt; &lt;p&gt;Whatâ€™s the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I donâ€™t understand if Iâ€™m supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. &lt;/p&gt; &lt;p&gt;Want something thatâ€™s really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. &lt;/p&gt; &lt;p&gt;My hardware: - CPU: Ryzen 7950x - GPU: 3090 - RAM: 96GB 6400mhz &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccidentalFolklore"&gt; /u/AccidentalFolklore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T00:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ag6u</id>
    <title>Hackers are never sleeping</title>
    <updated>2025-07-19T23:40:20+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt; &lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt; &lt;p&gt;Don't let your guard down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4sdsg</id>
    <title>I built a desktop tool to auto-organize files using local LLMs (open source, cross-platform)</title>
    <updated>2025-07-20T15:55:49+00:00</updated>
    <author>
      <name>/u/ph0tone</name>
      <uri>https://old.reddit.com/user/ph0tone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just wanted to share a use case where local LLMs are genuinely helpful for daily workflows: file organization.&lt;/p&gt; &lt;p&gt;I've been working on a C++ desktop app called &lt;em&gt;AI File Sorter&lt;/em&gt; â€“ it uses local LLMs via &lt;code&gt;llama.cpp&lt;/code&gt; to help organize messy folders like &lt;code&gt;Downloads&lt;/code&gt; or &lt;code&gt;Desktop&lt;/code&gt;. Not sort files into folders solely based on extension or filename patterns, but based on what each file actually is supposed to do or does. Basically: what would normally take me a great deal of time for dragging and sorting can now be done in a few.&lt;/p&gt; &lt;p&gt;It's cross-platform (Windows/macOS/Linux), and fully open-source.&lt;/p&gt; &lt;p&gt;ðŸ”— &lt;a href="https://github.com/hyperfield/ai-file-sorter"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/HlEer13.png"&gt;Screenshot 1&lt;/a&gt; - LLM selection and download&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/KCxk6Io.png"&gt;Screenshot 2&lt;/a&gt; - Select a folder to scan&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/QTUG5KB.png"&gt;Screenshot 3&lt;/a&gt; - Review, edit and confirm or continue later&lt;/p&gt; &lt;p&gt;You can download the installer for Windows in &lt;a href="https://github.com/hyperfield/ai-file-sorter/releases"&gt;Releases&lt;/a&gt; or the Standalone ZIP from the &lt;a href="https://filesorter.app/download/"&gt;app's website&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Installers for Linux and macOS are coming up. You can, however, easily &lt;a href="https://github.com/hyperfield/ai-file-sorter/blob/main/README.md"&gt;build the app from source&lt;/a&gt; for Linux or macOS.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;ðŸ§  How it works&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;You choose which model you want the app to interface with. The app will download the model for you. You can switch models later on.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You point the app at a folder, and it feeds a prompt to the model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It then suggests folder categories like &lt;code&gt;Operating Systems / Linux distributions&lt;/code&gt;, &lt;code&gt;Programming / Scripts&lt;/code&gt;, &lt;code&gt;Images / Logos&lt;/code&gt;, etc.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can review and approve before anything is moved, and you can continue the same sorting session later from where you left off.&lt;/p&gt; &lt;p&gt;Models tested: - LLaMa 3 (3B) - Mistral (7B) - With CUDA / OpenCL / OpenBLAS support - Other GPU back-ends can also be enabled on &lt;code&gt;llama.cpp&lt;/code&gt; compile&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Try it out&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Windows: &lt;a href="https://sourceforge.net/projects/ai-file-sorter/"&gt;SourceForge&lt;/a&gt; or &lt;a href="https://github.com/hyperfield/ai-file-sorter/releases"&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Linux/macOS: build from source (instructions in the &lt;a href="https://github.com/hyperfield/ai-file-sorter/blob/main/README.md"&gt;README&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Iâ€™d love feedback from others using local models, especially around: - Speed and accuracy in categorizing files - Model suggestions that might be more efficient - Any totally different way to approach this problem? - Is this local LLM use case actually useful to you or people like you, or should the app shift its focus?&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ph0tone"&gt; /u/ph0tone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4n7fh</id>
    <title>AI Model Juggler automatically and transparently switches between LLM and image generation backends and models</title>
    <updated>2025-07-20T12:01:26+00:00</updated>
    <author>
      <name>/u/Casual-Godzilla</name>
      <uri>https://old.reddit.com/user/Casual-Godzilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt; &lt;img alt="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" src="https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20ef388781d4fdb76232ce88c5fd1aa46da9841" title="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI Model Juggler is a simple utility for serving multiple LLM and image generation backends or models as if simultaneously while only requiring enough VRAM for one at a time. It is written in Python, but has no external dependencies, making installation as simple as downloading the code.&lt;/p&gt; &lt;p&gt;That might sound a lot like &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt;, but this one is considerably less sophisticated. If you're already using llama-swap and are happy with it, AI Model Juggler (I'm already starting to get tired of typing the name) will probably not be of much interest to you. I created this as a cursory reading of llama-swap's readme gave the impression that it only supports backends that support the OpenAI API, which excludes image generation through &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;AI Model Juggler has a couple of tricks for keeping things fast. First, it allows unloading the image generation backend's model while keeping the backend running. This saves considerable time on image generation startup. It also supports saving and restoring llama.cpp's KV-cache to reduce prompt re-processing.&lt;/p&gt; &lt;p&gt;The project is in its very early stages, and the list of its limitations is longer than that of supported features. Most importantly, it currently only supports &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; for LLM inference and &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;Stable Diffusion web UI&lt;/a&gt; / &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt; for image generation. Other backends could be easily added, but it makes limited sense to add ones that don't either start fast or else allow fast model unloading and reloading. The current pair does very well on this front, to the point that switching between them is almost imperceptible in many contexts, provided that the storage utilized is sufficiently fast.&lt;/p&gt; &lt;p&gt;The way request routing currently works (redirection, not proxying) makes AI Model Juggler less than an ideal choice for using the backends' built-in web UIs, and is only intended for exposing the APIs. It works well with applications such as &lt;a href="https://github.com/SillyTavern/SillyTavern"&gt;SillyTavern&lt;/a&gt;, though.&lt;/p&gt; &lt;p&gt;The project more or less meets my needs in its current state, but I'd be happy to improve it to make it more useful for others, so feedback, suggestions and feature requests are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Casual-Godzilla"&gt; /u/Casual-Godzilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/makedin/AI-Model-Juggler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9nn</id>
    <title>Chess Llama - Training a tiny Llama model to play chess</title>
    <updated>2025-07-20T15:51:13+00:00</updated>
    <author>
      <name>/u/LazyGuy-_-</name>
      <uri>https://old.reddit.com/user/LazyGuy-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt; &lt;img alt="Chess Llama - Training a tiny Llama model to play chess" src="https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295d14462b9edc402d72b6436f8270d15502e48a" title="Chess Llama - Training a tiny Llama model to play chess" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LazyGuy-_-"&gt; /u/LazyGuy-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lazy-guy.github.io/blog/chessllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4fs2t</id>
    <title>Context Rot: How Increasing Input Tokens Impacts LLM Performance</title>
    <updated>2025-07-20T04:17:04+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt; &lt;img alt="Context Rot: How Increasing Input Tokens Impacts LLM Performance" src="https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693" title="Context Rot: How Increasing Input Tokens Impacts LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt; &lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt; &lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt; &lt;p&gt;[Report]: &lt;a href="https://research.trychroma.com/context-rot"&gt;https://research.trychroma.com/context-rot&lt;/a&gt; &lt;/p&gt; &lt;p&gt;[Youtube]: &lt;a href="https://www.youtube.com/watch?v=TUjQuC4ugak"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Open-source Codebase]: &lt;a href="https://github.com/chroma-core/context-rot"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x8dkgvkifydf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T04:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o37k</id>
    <title>MediPhi-Instruct</title>
    <updated>2025-07-20T12:47:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt; &lt;img alt="MediPhi-Instruct" src="https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9841ead70ffdc17a5775d37d5326e57acfc45ef" title="MediPhi-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MediPhi-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4yo0g</id>
    <title>DiffRhythm 1.2 music generation model produces "Avicii vs Nicky Romero - I Could Be the One" nearly verbatim</title>
    <updated>2025-07-20T20:06:46+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt; &lt;img alt="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" src="https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc68db074fff49faf68ce5480e7b3265555e05bf" title="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.&lt;/p&gt; &lt;p&gt;Skip to 1:00 in the video to skip the generation process&lt;/p&gt; &lt;p&gt;Seed: 50518556518147&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ulng63nd53ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4u7j6</id>
    <title>What's the most crackhead garbage local LLM setup you can think of?</title>
    <updated>2025-07-20T17:08:13+00:00</updated>
    <author>
      <name>/u/caraccidentGAMING</name>
      <uri>https://old.reddit.com/user/caraccidentGAMING</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I've been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp;amp; two mi50 cards &amp;amp; calling it a day. Would that work? probably i guess, hell you'd even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let's hear what you guys can think of &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caraccidentGAMING"&gt; /u/caraccidentGAMING &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T17:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4mfs8</id>
    <title>Next big thing after LLMs - World Model [explained on the example of V-JEPA2]</title>
    <updated>2025-07-20T11:17:11+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt; &lt;img alt="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" src="https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d257e66a7ebc3ed1e883ca7dc0ba9dadc223c155" title="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I'm starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt; &lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt; &lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt; &lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt; &lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part, the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision. And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt; &lt;p&gt;Let's see how Meta approaches this issue &lt;a href="https://arxiv.org/pdf/2506.09985"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt; &lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel. So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that donâ€™t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic armâ€™s interaction.&lt;/p&gt; &lt;p&gt;This time, they encode part of a video and also give information about robotâ€™s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt; &lt;p&gt;After training, the robot, powered by this model, in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt; &lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0ivgtibj0ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4of82</id>
    <title>What's the smartest tiny LLM you've actually used?</title>
    <updated>2025-07-20T13:04:37+00:00</updated>
    <author>
      <name>/u/Luston03</name>
      <uri>https://old.reddit.com/user/Luston03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for something small but still usable. What's your go-to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luston03"&gt; /u/Luston03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r4j1</id>
    <title>Open source is humanityâ€™s last hope!</title>
    <updated>2025-07-20T15:04:05+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just making this post as I want opinions on the idea that if open source doesnâ€™t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to Godâ€™s from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. Whatâ€™s you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4y3cj</id>
    <title>Fine-tuned her the perfect local model. Still got APIâ€™d ðŸ’”</title>
    <updated>2025-07-20T19:43:10+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt; &lt;img alt="Fine-tuned her the perfect local model. Still got APIâ€™d ðŸ’”" src="https://preview.redd.it/xitr9w9f13ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f099b23bb6d2f68855a9689333e79824231cdf0" title="Fine-tuned her the perfect local model. Still got APIâ€™d ðŸ’”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xitr9w9f13ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T19:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m52h10</id>
    <title>I posted 3 weeks ago about training my own model. Progress report.</title>
    <updated>2025-07-20T22:46:55+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt; &lt;img alt="I posted 3 weeks ago about training my own model. Progress report." src="https://b.thumbs.redditmedia.com/tGNWILy7NUwjWBRL__Qs6HOJImwQ5Z22Np_wBFcIDdM.jpg" title="I posted 3 weeks ago about training my own model. Progress report." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I posted that I wanted to train an LLM for under $1000 here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal). Cost projection is about $500 for this run. It has flash attention 2, a 3:1 GQA, a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset). The corpus is english only, which I'm hoping will give it an edge.&lt;/p&gt; &lt;p&gt;I have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.&lt;/p&gt; &lt;p&gt;Now at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.&lt;/p&gt; &lt;p&gt;Happy to answer any questions! Pic is the beautiful loss curve.&lt;/p&gt; &lt;p&gt;Edit: It's called Libremodel I, codename Gigi, and I made a website with more info here: &lt;a href="https://libremodel.xyz"&gt;https://libremodel.xyz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1"&gt;https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T22:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vw29</id>
    <title>Ikllamacpp repository gone, or it is only me?</title>
    <updated>2025-07-20T18:14:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T18:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9mt</id>
    <title>I'm sorry Zuck please don't leave us we were just having fun</title>
    <updated>2025-07-20T15:51:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt; &lt;img alt="I'm sorry Zuck please don't leave us we were just having fun" src="https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2" title="I'm sorry Zuck please don't leave us we were just having fun" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9mxxen7w1ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:11+00:00</published>
  </entry>
</feed>
