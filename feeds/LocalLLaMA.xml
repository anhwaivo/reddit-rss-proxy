<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-31T03:35:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1idwkly</id>
    <title>The DeepSeek-R1-Distill-Qwen-14B is amazing, it solved these relatively new math questions.</title>
    <updated>2025-01-30T20:25:38+00:00</updated>
    <author>
      <name>/u/junior600</name>
      <uri>https://old.reddit.com/user/junior600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwkly/the_deepseekr1distillqwen14b_is_amazing_it_solved/"&gt; &lt;img alt="The DeepSeek-R1-Distill-Qwen-14B is amazing, it solved these relatively new math questions." src="https://b.thumbs.redditmedia.com/-oarXrBD6Vzljt17aJxYcyBplWAAmlcMkFwT65jrNcg.jpg" title="The DeepSeek-R1-Distill-Qwen-14B is amazing, it solved these relatively new math questions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I write this post because I want to say that these distilled models made from deepseek are pretty dope, and I don't understand why many people bash them. I'm using the DeepSeek-R1-Distill-Qwen-14B Q4, and I have to say it is amazing for me. It has solved a lot of these math questions that require reasoning. These questions are relatively new (from last month).&lt;/p&gt; &lt;p&gt;&lt;a href="https://kskedlaya.org/putnam-archive/2024.pdf"&gt;https://kskedlaya.org/putnam-archive/2024.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the solutions are here&lt;/p&gt; &lt;p&gt;&lt;a href="https://kskedlaya.org/putnam-archive/2024s.pdf"&gt;https://kskedlaya.org/putnam-archive/2024s.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I post the A4 and the A6 questions solved by the model. I copy the reasoning part on pastebin because it is too long lol&lt;/p&gt; &lt;p&gt;A4&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/AtJHs2kf"&gt;https://pastebin.com/AtJHs2kf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A6&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/76NHjqRU"&gt;https://pastebin.com/76NHjqRU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here is the final answers. For example, chatgpt 4o mini get them wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k77n7kchx6ge1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1a35da0889a9736f7520ae8e72317d5edef0d88"&gt;https://preview.redd.it/k77n7kchx6ge1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1a35da0889a9736f7520ae8e72317d5edef0d88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/czg6aruhx6ge1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1770394c11152cbe6b8a8c69a89020cfefa1b1f"&gt;https://preview.redd.it/czg6aruhx6ge1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1770394c11152cbe6b8a8c69a89020cfefa1b1f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/junior600"&gt; /u/junior600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwkly/the_deepseekr1distillqwen14b_is_amazing_it_solved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwkly/the_deepseekr1distillqwen14b_is_amazing_it_solved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idwkly/the_deepseekr1distillqwen14b_is_amazing_it_solved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T20:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ideaxu</id>
    <title>Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs</title>
    <updated>2025-01-30T04:22:34+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt; &lt;img alt="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" src="https://b.thumbs.redditmedia.com/SlGpr_siDY7Rr_nl1h9FbbkgpwtHXQX47AlZAVKy8LM.jpg" title="Nvidia cuts FP8 training performance in half on RTX 40 and 50 series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to their new RTX Blackwell GPU architecture whitepaper, Nvidia appears to have cut FP8 training performance in half on RTX 40 and 50 series GPUs after DeepSeek successfully trained their SOTA V3 and R1 models using FP8. &lt;/p&gt; &lt;p&gt;In their original Ada Lovelace whitepaper, table 2 in Appendix A shows the 4090 having &lt;strong&gt;660.6 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate without sparsity, which is the same as FP8 with FP16 accumulate. The new Blackwell paper shows half the performance for the 4090 at just &lt;strong&gt;330.3 TFlops&lt;/strong&gt; of FP8 with FP32 accumulate, and the 5090 has just &lt;strong&gt;419 TFlops&lt;/strong&gt; vs &lt;strong&gt;838 TFlops&lt;/strong&gt; for FP8 with FP16 accumulate. &lt;/p&gt; &lt;p&gt;FP32 accumulate is a must when it comes to training because FP16 doesn't have the necessary precision and dynamic range required. &lt;/p&gt; &lt;p&gt;If this isn't a mistake, then it means Nvidia lobotomized their Geforce lineup to further dissuade us from using them for AI/ML training, and it could potentially be reversible for the RTX 40 series at least, as this was likely done through a driver update.&lt;/p&gt; &lt;p&gt;This is quite unfortunate but not unexpected as Nvidia has a known history of artificially limiting Geforce GPUs for AI training since the Turing architecture, while their Quadro and datacenter GPUs continue to have the full performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955"&gt;https://preview.redd.it/x3qfea1352ge1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c20a53057eb2bf15bbf65db4900af638fef9955&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134"&gt;https://preview.redd.it/lk3ch91352ge1.jpg?width=1934&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d267c0312fe0be00175e616512101dce69113134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;RTX Blackwell GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RTX Ada Lovelace GPU Architecture Whitepaper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"&gt;https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ideaxu/nvidia_cuts_fp8_training_performance_in_half_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T04:22:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp6n6</id>
    <title>Deepseek is hosted on Huawei cloud</title>
    <updated>2025-01-30T15:15:46+00:00</updated>
    <author>
      <name>/u/Reasonable-Climate66</name>
      <uri>https://old.reddit.com/user/Reasonable-Climate66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the IP resolved in China. The chat endpoints is from Huawei DC&lt;/p&gt; &lt;p&gt;DS could be using Singapore Huawei region for WW and Shanghai region for CN users.&lt;/p&gt; &lt;p&gt;So demand for Nvidia card for training and Huawei GPU for inference is real. &lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png"&gt;https://i.postimg.cc/0QyjxTkh/Screenshot-20250130-230756.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png"&gt;https://i.postimg.cc/FHknCz0B/Screenshot-20250130-230812.png&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Climate66"&gt; /u/Reasonable-Climate66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp6n6/deepseek_is_hosted_on_huawei_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtnnh</id>
    <title>Mistral Small 3 24b Q6 initial test results</title>
    <updated>2025-01-30T18:24:26+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its... kind of rough but kind of amazing?&lt;/p&gt; &lt;p&gt;It's good. It's VERY smart, but really rough around the edges if I look closely. Let me explain teo things I noticed.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It doesn't follow instructions well, basically useless for JSON formatting or anything where it has to adhere to a response style. Kind of odd as Mistral Small 2 22b was superb here.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It writes good code with random errors. If you're even a mediocre dev you'll find this fine, but it includes several random imports that don't get used and seems to randomly declare/cache things and never refer to them again&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Smart, but rough. Probably the new king of general purpose models that fit into 24gb. I still suspect that Qwen-Coder 32b will win in real world coding, and perhaps even the older Codestral 22b will be better suited in coding for now, but I haven't yet tested it on all of my repos/use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtnnh/mistral_small_3_24b_q6_initial_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie3mv7</id>
    <title>What is your favorite 12/13B model for NSFW RP?</title>
    <updated>2025-01-31T01:43:21+00:00</updated>
    <author>
      <name>/u/NullHypothesisCicada</name>
      <uri>https://old.reddit.com/user/NullHypothesisCicada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I guess it’s that time of the year. Last year, I’ve tested a lot of M-N models such as violet-lotus, mag-mell, etc. Though there are still some minor problems for each models, such as incoherent after 10k context, only suitable for 3rd person roleplay and so on.&lt;/p&gt; &lt;p&gt;Since they’re all released probably about half a year ago, I want to ask you what’s your favorite for some sweet sweaty RP?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullHypothesisCicada"&gt; /u/NullHypothesisCicada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T01:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido2up</id>
    <title>Mistral Small</title>
    <updated>2025-01-30T14:24:15+00:00</updated>
    <author>
      <name>/u/MLTyrunt</name>
      <uri>https://old.reddit.com/user/MLTyrunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral Small&lt;/p&gt; &lt;p&gt;Apache 2.0, 81% MMLU, 150 tokens/s&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/news/mistral-small-3/"&gt;https://mistral.ai/news/mistral-small-3/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLTyrunt"&gt; /u/MLTyrunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido2up/mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1idro19</id>
    <title>DeepSeek R1 scores between o1 and o1-mini on NYT Connections</title>
    <updated>2025-01-30T17:02:39+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt; &lt;img alt="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" src="https://preview.redd.it/e8ov1yb3x5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=663523084a040c4c952820d45759a6a4e7a87469" title="DeepSeek R1 scores between o1 and o1-mini on NYT Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e8ov1yb3x5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idro19/deepseek_r1_scores_between_o1_and_o1mini_on_nyt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1idwgay</id>
    <title>Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface</title>
    <updated>2025-01-30T20:20:28+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"&gt; &lt;img alt="Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface" src="https://external-preview.redd.it/KMwppOY-W87gB9d3tmURowTBAI22RUNa2m2fmKkqML0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92bbafd261aeee71b2a7db5b902101dab7c7ea22" title="Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/open-r1?utm_source=tldrai#what-is-deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T20:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqql6</id>
    <title>Mistral Small 3 24b's Context Window is Remarkably Efficient</title>
    <updated>2025-01-30T16:23:25+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt; &lt;img alt="Mistral Small 3 24b's Context Window is Remarkably Efficient" src="https://b.thumbs.redditmedia.com/tUYsJoEn9u94ym2whVhsPOc7Lcfh9qD4M48XkP1073Y.jpg" title="Mistral Small 3 24b's Context Window is Remarkably Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using the Mistral Small 3 24b-q6k model with a full 32K context (Q8 KV cache), and I still have 1.6GB of VRAM left.&lt;br /&gt; In comparison, Qwen2.5 32b Q4 KL is roughly the same size, but I could only manage to get 24K context before getting dangerously close to running out of VRAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730"&gt;https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T16:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvuch</id>
    <title>Re-Distilling DeepSeek R1</title>
    <updated>2025-01-30T19:55:23+00:00</updated>
    <author>
      <name>/u/sightio</name>
      <uri>https://old.reddit.com/user/sightio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve improved DeepSeek R1 distilled models using logits distillation—delivering +4-14% gains on GSM8K while only spending $3-18 per training run.&lt;/p&gt; &lt;p&gt;Details at &lt;a href="https://mobiusml.github.io/r1_redistill_blogpost/"&gt;https://mobiusml.github.io/r1_redistill_blogpost/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models are available on Hugging Face - run them efficiently with HQQ! &lt;a href="https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d"&gt;https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sightio"&gt; /u/sightio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idokcx</id>
    <title>Mistral new open models</title>
    <updated>2025-01-30T14:47:21+00:00</updated>
    <author>
      <name>/u/konilse</name>
      <uri>https://old.reddit.com/user/konilse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt; &lt;img alt="Mistral new open models" src="https://preview.redd.it/5nnsoy4295ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d39024b2c7d0acbb55e2f3d01eee2b120c949e0" title="Mistral new open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral base and instruct 24B &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/konilse"&gt; /u/konilse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nnsoy4295ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1idzimg</id>
    <title>Mistral Small 3 knows the truth</title>
    <updated>2025-01-30T22:30:25+00:00</updated>
    <author>
      <name>/u/magicduck</name>
      <uri>https://old.reddit.com/user/magicduck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"&gt; &lt;img alt="Mistral Small 3 knows the truth" src="https://preview.redd.it/8rp05jjjj7ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af0e6a9f5574e3c1cae3becd10fc86657b8b07d3" title="Mistral Small 3 knows the truth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magicduck"&gt; /u/magicduck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rp05jjjj7ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idzxix</id>
    <title>Mistral-Small-24B-2501 vs Mistral-Small-2409</title>
    <updated>2025-01-30T22:48:32+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"&gt; &lt;img alt="Mistral-Small-24B-2501 vs Mistral-Small-2409" src="https://preview.redd.it/705ahg8qm7ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b60f35e508f91598f803cbba687f8180633bbe1c" title="Mistral-Small-24B-2501 vs Mistral-Small-2409" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/705ahg8qm7ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1idnyhh</id>
    <title>mistralai/Mistral-Small-24B-Base-2501 · Hugging Face</title>
    <updated>2025-01-30T14:18:23+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Base-2501 · Hugging Face" src="https://external-preview.redd.it/lDGKmq6pSZNpISh4piV15abwPTUoM5lDEjjJ9qZ_vd4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56053b8ce77cd587b1abeda9737783c65c0ebab8" title="mistralai/Mistral-Small-24B-Base-2501 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idt9xz</id>
    <title>Watch this SmolAgent save me over 100 hours of work.</title>
    <updated>2025-01-30T18:08:42+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt; &lt;img alt="Watch this SmolAgent save me over 100 hours of work." src="https://external-preview.redd.it/eXpvaDN2aXY4NmdlMaIWY-pKRTEFed4oaflr_50jeaU7y6AfPZ2q49QYyqUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ea3707bc7016b8ca7da0ee5c72fef8602edfdba" title="Watch this SmolAgent save me over 100 hours of work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2gcviv86ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:08:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp1z5</id>
    <title>No synthetic data?</title>
    <updated>2025-01-30T15:09:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt; &lt;img alt="No synthetic data?" src="https://preview.redd.it/98dq1wg2d5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448fe61c33c8db28d89becf7c1d0ccbcf95ea88a" title="No synthetic data?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's reallllllly rare in 2025, did I understand this correctly? They didn't use any synthetic data to train this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/98dq1wg2d5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iduk3b</id>
    <title>Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)</title>
    <updated>2025-01-30T19:02:02+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt; &lt;img alt="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" src="https://preview.redd.it/gazbvr6gi6ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6de32bcaa9f8ae8ff3f2ab317c2401bd2f5b73" title="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gazbvr6gi6ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido3fn</id>
    <title>Are there ½ million people capable of running locally 685B params models?</title>
    <updated>2025-01-30T14:25:02+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"&gt; &lt;img alt="Are there ½ million people capable of running locally 685B params models?" src="https://b.thumbs.redditmedia.com/nUAmR_7owY5oJQcrzV0vL3H93-ccvgV-SDlaKg3CSyw.jpg" title="Are there ½ million people capable of running locally 685B params models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ido3fn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1idva1j</id>
    <title>Welcome back, Le Mistral!</title>
    <updated>2025-01-30T19:31:40+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt; &lt;img alt="Welcome back, Le Mistral!" src="https://preview.redd.it/4td7dsrjn6ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcee379bd06ff66ce0c2532f18c365ea37c8d6d1" title="Welcome back, Le Mistral!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4td7dsrjn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny3w</id>
    <title>Mistral Small 3</title>
    <updated>2025-01-30T14:17:56+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt; &lt;img alt="Mistral Small 3" src="https://preview.redd.it/kj3s0jvr35ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317aadc49155a8df1074618844c589ea3d2753d" title="Mistral Small 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kj3s0jvr35ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie0a8u</id>
    <title>QWEN just launched their chatbot website</title>
    <updated>2025-01-30T23:03:37+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt; &lt;img alt="QWEN just launched their chatbot website" src="https://preview.redd.it/vzgzfrhlp7ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa67cbeae08d4c800e7b5dc088c0330556268f" title="QWEN just launched their chatbot website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link: &lt;a href="https://chat.qwenlm.ai/"&gt;https://chat.qwenlm.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vzgzfrhlp7ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T23:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv7yb</id>
    <title>Marc Andreessen on Anthropic CEO's Call for Export Controls on China</title>
    <updated>2025-01-30T19:29:13+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt; &lt;img alt="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" src="https://preview.redd.it/wlsi25dcn6ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d695bb3258d357570ad11762d15df689f13fe2a8" title="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wlsi25dcn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtkll</id>
    <title>Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more.</title>
    <updated>2025-01-30T18:20:59+00:00</updated>
    <author>
      <name>/u/deoxykev</name>
      <uri>https://old.reddit.com/user/deoxykev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt; &lt;img alt="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." src="https://external-preview.redd.it/VCPkBGJsVaggWY7c9V20KQQGCJhrF411vyVYUsHeuns.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=495bbbb03e5ebeff92050c2a71f7e340cb4bbebc" title="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deoxykev"&gt; /u/deoxykev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1idz487</id>
    <title>'we're in this bizarre world where the best way to learn about llms... is to read papers by chinese companies. i do not think this is a good state of the world' - us labs keeping their architectures and algorithms secret is ultimately hurting ai development in the us.' - Dr Chris Manning</title>
    <updated>2025-01-30T22:13:22+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700560500416881"&gt;https://x.com/atroyn/status/1884700560500416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:13:22+00:00</published>
  </entry>
</feed>
