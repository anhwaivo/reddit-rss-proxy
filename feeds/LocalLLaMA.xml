<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-16T05:25:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m10jln</id>
    <title>Obsidian note summarizer using local LLMs</title>
    <updated>2025-07-16T02:04:17+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m10jln/obsidian_note_summarizer_using_local_llms/"&gt; &lt;img alt="Obsidian note summarizer using local LLMs" src="https://external-preview.redd.it/yuy9PgfOKjGlILvMppfg7R35e-ilKg7LUqrLRKWCHk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd05d281daa15cb64e1f4bbc7f9338c798cce286" title="Obsidian note summarizer using local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rosmur/obsidian-summairize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m10jln/obsidian_note_summarizer_using_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m10jln/obsidian_note_summarizer_using_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T02:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m10o3o</id>
    <title>Finally, an LLM Router That Thinks Like an Engineer - And Its Local</title>
    <updated>2025-07-16T02:10:26+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸ”— Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; ðŸ“„ Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;br /&gt; Integrated and available via Arch: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@dracattusdev/finally-an-llm-router-that-thinks-like-an-engineer-96ccd8b6a24e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m10o3o/finally_an_llm_router_that_thinks_like_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m10o3o/finally_an_llm_router_that_thinks_like_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T02:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0d0vz</id>
    <title>Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)</title>
    <updated>2025-07-15T09:15:07+00:00</updated>
    <author>
      <name>/u/yingyn</name>
      <uri>https://old.reddit.com/user/yingyn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt; &lt;img alt="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" src="https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg" title="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here're the findings!&lt;/p&gt; &lt;p&gt;If there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yingyn"&gt; /u/yingyn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m0d0vz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T09:15:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m118is</id>
    <title>Use claudecode with local models</title>
    <updated>2025-07-16T02:38:02+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month. So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code. Well, many folks are already doing that with local. So if you don't know, now you know. This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.&lt;/p&gt; &lt;p&gt;Start your local LLM API &lt;/p&gt; &lt;p&gt;Install claude code&lt;/p&gt; &lt;p&gt;install a proxy - &lt;a href="https://github.com/1rgs/claude-code-proxy"&gt;https://github.com/1rgs/claude-code-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit the &lt;a href="http://server.py"&gt;server.py&lt;/a&gt; proxy and point it to your OpenAI endpoint, could be llama.cpp, ollama, vllm, whatever you are running. &lt;/p&gt; &lt;p&gt;Add the line above load_dotenv&lt;br /&gt; +litellm.api_base = &amp;quot;http://yokujin:8083/v1&amp;quot; # use your localhost name/IP/ports&lt;/p&gt; &lt;p&gt;Start the proxy according to the docs which will run it in localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_BASE_URL=http://localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_AUTH_TOKEN=&amp;quot;sk-localkey&amp;quot;&lt;/p&gt; &lt;p&gt;run claude code&lt;/p&gt; &lt;p&gt;I just created my first code then decided to post this. I'm running the latest mistral-small-24b on that host. I'm going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T02:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0pxot</id>
    <title>Just tried out the Exaone 4.0 1.2b bf16 and i'm extremely suprised at how good a 1.2b can be!</title>
    <updated>2025-07-15T18:39:33+00:00</updated>
    <author>
      <name>/u/cloudxaas</name>
      <uri>https://old.reddit.com/user/cloudxaas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone found any issues with Exaone 4.0 1.2b yet? the bf16 version i've tried does 11tok/s on my amd 5600G using cpu only inference and it doesnt seemed to repeat itself (the kind that goes on and on and on). It does repeat itself but it will end and that's occasional. I'm very impressed with it.&lt;/p&gt; &lt;p&gt;What are your thoughts about this? It's kind of usable to me for filtering spam or vulgar words etc.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cloudxaas"&gt; /u/cloudxaas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T18:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0r95k</id>
    <title>2 M3 Ultraâ€™s 512GB running Kimi K2 quant 4 with mlx-lm and mlx.distributed</title>
    <updated>2025-07-15T19:28:42+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems to run at a descent speed :&lt;br /&gt; &lt;a href="https://x.com/awnihannun/status/1943723599971443134"&gt;https://x.com/awnihannun/status/1943723599971443134&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0r95k/2_m3_ultras_512gb_running_kimi_k2_quant_4_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0r95k/2_m3_ultras_512gb_running_kimi_k2_quant_4_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0r95k/2_m3_ultras_512gb_running_kimi_k2_quant_4_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T19:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0yqq2</id>
    <title>Kimi K2 vs. Claude vs. OpenAI | Cursor Real-World Research Task</title>
    <updated>2025-07-16T00:37:29+00:00</updated>
    <author>
      <name>/u/LeveredRecap</name>
      <uri>https://old.reddit.com/user/LeveredRecap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Comparison of the output from Kimi K2, Claude 4.0 and OpenAI (o3-pro; 4.1):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://macro.com/app/md/3f71ab3b-1b25-48b2-83cf-ea771c033f64/md/44f05c78-a96b-46ac-b0c0-c1917216334d"&gt;Kimi K2 vs. Claude vs. OpenAI | Cursor Real-World Research Task&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I personally think Claude 4.0 Sonnet remains the top LLM for performing research tasks and agentic reasoning, followed by o3-pro&lt;/p&gt; &lt;p&gt;However, Kimi K2 is quite impressive, and a step in the right direction for open-source models reaching parity with closed-source models in real-life, not benchmarks&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sonnet followed instructions accurately with no excess verbiage, and was straight to the pointâ€”responded with well-researched points (and counterpoints)&lt;/li&gt; &lt;li&gt;K2 was very comprehensive and generated some practical insights, similar to o3-pro, but there was a substantial amount of &amp;quot;fluff&amp;quot;â€”the model is, evidently, one of the top reasoning models without question; however, seems to &amp;quot;overthink&amp;quot; and hedge each insight too much&lt;/li&gt; &lt;li&gt;o3-pro was comprehensive but sort of trailed from the promptâ€”seemed instructional, rather than research-oriented&lt;/li&gt; &lt;li&gt;4.1 was too vague and the output touched on the right concepts, yet did not &amp;quot;peel the onion&amp;quot; enoughâ€”comparable to Gemini 2.5 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Couple Points:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Same Prompt Word-for-Word&lt;/li&gt; &lt;li&gt;Reasoning Mode&lt;/li&gt; &lt;li&gt;One-Shot Output&lt;/li&gt; &lt;li&gt;API Usage (Including Kimi-Researcher)&lt;/li&gt; &lt;li&gt;Memory Wiped&lt;/li&gt; &lt;li&gt;No Personalization&lt;/li&gt; &lt;li&gt;No Custom Instructions (Default)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My rankings: (1) Claude Sonnet 4.0, (2) Kimi K2, (3) o3 pro, and (4) GPT 4.1&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeveredRecap"&gt; /u/LeveredRecap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0yqq2/kimi_k2_vs_claude_vs_openai_cursor_realworld/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0yqq2/kimi_k2_vs_claude_vs_openai_cursor_realworld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0yqq2/kimi_k2_vs_claude_vs_openai_cursor_realworld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0j7w4</id>
    <title>Swiss Open LLM</title>
    <updated>2025-07-15T14:27:35+00:00</updated>
    <author>
      <name>/u/bleeckerj</name>
      <uri>https://old.reddit.com/user/bleeckerj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In late summer 2025, a publicly developed large language model (LLM) will be released â€” co-created by researchers at EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS).&lt;/p&gt; &lt;p&gt;This LLM will be fully open: This openness is designed to support broad adoption and foster innovation across science, society, and industry. &lt;/p&gt; &lt;p&gt;A defining feature of the model is its multilingual fluency in over 1,000 languages.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bleeckerj"&gt; /u/bleeckerj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0j7w4/swiss_open_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T14:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mnjk</id>
    <title>Kimi K2 at ~200 tps on Groq</title>
    <updated>2025-07-15T16:37:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"&gt; &lt;img alt="Kimi K2 at ~200 tps on Groq" src="https://external-preview.redd.it/Stw5ew6ARua3PKojcWVyE-uMkKHKq3GsO7UrzRoqJ50.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5aa7094d7d761b72304995d549ba6ca5c343e2" title="Kimi K2 at ~200 tps on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It also works on Groq's free plan&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mnjk/kimi_k2_at_200_tps_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0uoqo</id>
    <title>IQ2_KL 345.687 GiB (2.892 BPW) Kimi-K2-Instruct GGUF ik exclusive!</title>
    <updated>2025-07-15T21:40:41+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/"&gt; &lt;img alt="IQ2_KL 345.687 GiB (2.892 BPW) Kimi-K2-Instruct GGUF ik exclusive!" src="https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe89633f63f298fcdd1962f72916307a0c4801ec" title="IQ2_KL 345.687 GiB (2.892 BPW) Kimi-K2-Instruct GGUF ik exclusive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For you big rig runners who are fan's of ik_llama.cpp I just released a unique recipe of Kimi-K2-Instruct suitable for running on &amp;quot;only&amp;quot; ~368GB RAM - or less if you got any of that $weet $weet VRAM!&lt;/p&gt; &lt;p&gt;The perplexity clocks in at &lt;code&gt;3.2741 +/- 0.01689&lt;/code&gt; which is not much higher (worse) than the full massive 1TB &lt;code&gt;Q8_0&lt;/code&gt; baseline score of &lt;code&gt;2.9507 +/- 0.01468&lt;/code&gt; despite being 34% of the full size!&lt;/p&gt; &lt;p&gt;The new &lt;code&gt;IQ2_KL&lt;/code&gt; quant type just came out this week and I couldn't wait to give it a go. It is runs fast on both CUDA and CPU backend and packs in a ton of quality at only 2.69 bpw!&lt;/p&gt; &lt;p&gt;Wendell over at level1techs just hooked me up with a new remote rig with enough RAM and kioxia flash drives to actually maneuver this barge of a model, so big thanks as usual!&lt;/p&gt; &lt;p&gt;I'll be releasing some more sizes soon so feel free to open a discussion on hf if there is a target break point size you'd like to see.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Remember&lt;/em&gt; this quant only runs on ik_llama.cpp, instructions are on the github to download build and run any quants you already have as well as my quants.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T21:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0zy1a</id>
    <title>New documentation / explainer for GGUF quantization</title>
    <updated>2025-07-16T01:35:32+00:00</updated>
    <author>
      <name>/u/mojojojo_24</name>
      <uri>https://old.reddit.com/user/mojojojo_24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's surprisingly little documentation on how GGUF quantization works, including legacy / I-quants / K-quants and the importance matrix.&lt;/p&gt; &lt;p&gt;The maintainers made it &lt;a href="https://github.com/ggml-org/llama.cpp/pull/1684#issuecomment-2474462323"&gt;pretty clear&lt;/a&gt; it's not their priority to write a paper either. Currently, people are just piecing information together from Reddit threads and Medium articles (which are often wrong). So I spent some time combing through the llama.cpp quantization code and put together a public GitHub repo that hopefully brings some clarity and can function as an unofficial explainer / documentation.&lt;/p&gt; &lt;p&gt;Contributions are welcome, as long as they are backed by reliable sources! &lt;a href="https://github.com/iuliaturc/gguf-docs"&gt;https://github.com/iuliaturc/gguf-docs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mojojojo_24"&gt; /u/mojojojo_24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T01:35:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0lyjn</id>
    <title>Kimi has impressive coding performance! Even deep into context usage.</title>
    <updated>2025-07-15T16:11:58+00:00</updated>
    <author>
      <name>/u/mattescala</name>
      <uri>https://old.reddit.com/user/mattescala</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.&lt;/p&gt; &lt;p&gt;Ever since Unsloth released their quantized version of Kimi K2 yesterday, Iâ€™ve been giving it a real workout. Iâ€™ve mostly been pairing it with Roo Code, and honestlyâ€¦ Iâ€™m blown away.&lt;/p&gt; &lt;p&gt;Back in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save moneyâ€”letâ€™s be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.&lt;/p&gt; &lt;p&gt;Up until now, Iâ€™ve tried DeepSeek V3, R1, R1 0528â€”you name it. Nothing comes close to what Iâ€™m seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didnâ€™t need much context. For anything big or complex, Iâ€™d have to use Claude.&lt;/p&gt; &lt;p&gt;But now thatâ€™s changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, itâ€™s making changes to a C++ firmware projectâ€”&lt;em&gt;deep&lt;/em&gt; into a 90,000-token contextâ€”and itâ€™s nailing the search and replace stuff in Roo Code without getting lost or mixing things up.&lt;/p&gt; &lt;p&gt;Just wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik_llama. Seriously, none of this would be possible without you all. Youâ€™re the real MVPs.&lt;/p&gt; &lt;p&gt;If youâ€™re curious about my setup: Iâ€™m running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattescala"&gt; /u/mattescala &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0rk8t</id>
    <title>Notes on Kimi K2: A Deepseek derivative but the true Sonnet 3.6 Succesor</title>
    <updated>2025-07-15T19:40:06+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just like that, out of nowhere, we have an open-source Claude 4 Sonnet, or better yet, and this is no joke. I have been using the Kimi model for some time, and it truly feels the rightful successor to Claude 3.6 Sonnet. What Deepseek is to OpenAI, Kimi is to Anthropic.&lt;/p&gt; &lt;p&gt;K2 isn't truly a different model; it uses Deepseek v3 architecture. You can find that in the model config, but there are some subtle yet key improvements that resulted in such drastic improvements.&lt;/p&gt; &lt;h1&gt;Kimi K2 vs. DsV3 architecture&lt;/h1&gt; &lt;p&gt;This is from Liu Shaowei's Zhihu post.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Number of experts = 384 vs. 256&lt;/strong&gt;: 1.5x more experts for improving overall model ability, and helps lower the train/val loss, yielding better quality at the same &lt;em&gt;activated-parameter&lt;/em&gt; cost and inference FLOPs. But also a 50% spike in memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Number of attention heads = 64 vs 128&lt;/strong&gt;: They halve the attention-head count, shrinking the QKV projection weights from 10 GB to 5 GB per EP rank, which more than offsets the 50 % memory spike by yielding a net 2.5 GB saving while simultaneously halving pre-fill latency and leaving the KV-cache size unchanged.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;first_k_dense = 1 vs 3:&lt;/strong&gt; Kimi replaced the first layer with a dense layer after observing that the router in layer-1 consistently produced severe load imbalance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n_group = 1 vs. 8&lt;/strong&gt;: Dropping expert grouping frees every GPU to route to any of the 384 experts, letting EPLB handle load balancing while shrinking memory and widening the modelâ€™s effective capacity.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;MuonCLIP&lt;/h1&gt; &lt;p&gt;One of the key contributor of Kimi's success. Kimi went with Muon, more token efficient than AdamW. But it wasn't before tested for such a large model. To overcome they added a drop-in extension qk-clip. This helped to transplant Muonâ€™s 2Ã— token-efficiency into a 1-trillion-parameter regime without its historical Achillesâ€™ heel: qk-clip rescales the query and key projections after every Muon update.&lt;/p&gt; &lt;h1&gt;How good in comparison to Claude 4 Sonnet?&lt;/h1&gt; &lt;p&gt;Kimi k2's positioning directly challenged Claude 4 Sonnet, the current SOTA agentic model. The k2 was specifically RL'd for extensive tool-use scenarios. However, it's not just good at tool use, it is surprisingly creative at writing and coding.&lt;/p&gt; &lt;p&gt;Some observations&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The K2 feels most natural to talk to than any available models. Zero sycophancy, no assumption, it just sticks to the point. Though I still find Sonnet 4 to be more attentive to instructions.&lt;/li&gt; &lt;li&gt;It has the simillar vibes of Claude 3.6 Sonnet, understands user intention better and more grounded response.&lt;/li&gt; &lt;li&gt;K2 has a better taste.&lt;/li&gt; &lt;li&gt;The coding is surprisingly good, though Sonnet will still be better at raw coding as for some task I found myself going back to it.&lt;/li&gt; &lt;li&gt;The best part it is roughly 1/12th of Sonnet's cost. Crazy times indeed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find the complete note here: &lt;a href="https://composio.dev/blog/notes-on-kimi-k2"&gt;Notes on Kimi K2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know your experience with the new Kimi K2 and how do you think it compares to Claude for agentic coding and other agentic tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T19:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0t5m9</id>
    <title>NousResearch/Hermes-3-Dataset Release</title>
    <updated>2025-07-15T20:40:29+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0t5m9/nousresearchhermes3dataset_release/"&gt; &lt;img alt="NousResearch/Hermes-3-Dataset Release" src="https://external-preview.redd.it/g5l1tkQ2HSn589PTaWbB3QKqerkODydxmoPjrvEScfw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ab9040ca58534a8200b40ff6f5423544d7a18d" title="NousResearch/Hermes-3-Dataset Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently, Hermes 4 671B is going to be released sometime this month as well per their Discord. No idea if it is based on the base model or either V3/R1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0t5m9/nousresearchhermes3dataset_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0t5m9/nousresearchhermes3dataset_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T20:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m13eb2</id>
    <title>AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO "Blackwell" With 24 GB VRAM</title>
    <updated>2025-07-16T04:28:39+00:00</updated>
    <author>
      <name>/u/Rich_Repeat_22</name>
      <uri>https://old.reddit.com/user/Rich_Repeat_22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" src="https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82787d21e7b0821fdce5a034706e0598040c7cc4" title="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Said it when this was presented that will have MSRP around RTX5080 since AMD decided to bench it against that card and not some workstation grade RTX.... ðŸ¥³&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Repeat_22"&gt; /u/Rich_Repeat_22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-radeon-ai-pro-r9700-32-gb-gpu-listed-pricing-around-1250-half-price-nvidia-rtx-pro-blackwell-24-gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T04:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0onbu</id>
    <title>Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding â€” and it costs less</title>
    <updated>2025-07-15T17:51:19+00:00</updated>
    <author>
      <name>/u/Aralknight</name>
      <uri>https://old.reddit.com/user/Aralknight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"&gt; &lt;img alt="Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding â€” and it costs less" src="https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf6c0f2e02a4bd86aa09677603889292d8fa2d0e" title="Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding â€” and it costs less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aralknight"&gt; /u/Aralknight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/07/14/alibaba-backed-moonshot-releases-kimi-k2-ai-rivaling-chatgpt-claude.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0g2mk</id>
    <title>Well, if anyone was waiting for Llama 4 Behemoth, it's gone</title>
    <updated>2025-07-15T12:09:02+00:00</updated>
    <author>
      <name>/u/Ok-Elevator5091</name>
      <uri>https://old.reddit.com/user/Ok-Elevator5091</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"&gt; &lt;img alt="Well, if anyone was waiting for Llama 4 Behemoth, it's gone" src="https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7417749598717bd5400069706a3c0d563e32ab4" title="Well, if anyone was waiting for Llama 4 Behemoth, it's gone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're likely getting a closed source model instead &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Elevator5091"&gt; /u/Ok-Elevator5091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T12:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0k22v</id>
    <title>mistralai/Voxtral-Mini-3B-2507 Â· Hugging Face</title>
    <updated>2025-07-15T15:00:20+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"&gt; &lt;img alt="mistralai/Voxtral-Mini-3B-2507 Â· Hugging Face" src="https://external-preview.redd.it/Fqp3ABstOuPD3LEzj5sGjjSlveTWbvbitVuSNOGFlaY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=709cf21da4c4a0ff86a826909c9d3b8548549207" title="mistralai/Voxtral-Mini-3B-2507 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0k22v/mistralaivoxtralmini3b2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T15:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mg5b</id>
    <title>Least sycophantic AI yet? Kimi K2</title>
    <updated>2025-07-15T16:30:01+00:00</updated>
    <author>
      <name>/u/PrimaryBalance315</name>
      <uri>https://old.reddit.com/user/PrimaryBalance315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy crap this thing has sass. First time I've ever engaged with an AI that replied &amp;quot;No.&amp;quot;&lt;br /&gt; That's it. It was fantastic.&lt;/p&gt; &lt;p&gt;Actually let me grab some lines from the conversation -&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Thermodynamics kills the romance&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Everything else is commentary&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;If your 'faith' can be destroyed by a single fMRI paper or a bad meditation session, it's not faith, it's a hypothesis&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Bridges that don't creak aren't being walked on&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And my favorite zinger - &lt;strong&gt;&amp;quot;Beautiful scaffolding with no cargo yet&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Fucking Killing it Moonshot. Like this thing never once said &amp;quot;that's interesting&amp;quot; or &amp;quot;great question&amp;quot; - it just went straight for the my intelligence every single time. It's like talking to someone that genuinely doesn't give a shit if you can handle the truth or not. Just pure &amp;quot;Show me or shut up&amp;quot;. It makes me think instead of feeling good about thinking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryBalance315"&gt; /u/PrimaryBalance315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0twqa</id>
    <title>Alternative to llama.cpp for Apple Silicon</title>
    <updated>2025-07-15T21:09:43+00:00</updated>
    <author>
      <name>/u/darkolorin</name>
      <uri>https://old.reddit.com/user/darkolorin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"&gt; &lt;img alt="Alternative to llama.cpp for Apple Silicon" src="https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3332c993dd26519a4ef1b63d265d7a6c44d33516" title="Alternative to llama.cpp for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community,&lt;/p&gt; &lt;p&gt;We wrote our own inference engine based on Rust for Apple Silicon. It's open sourced under MIT license.&lt;/p&gt; &lt;p&gt;Why we do this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;should be easy to integrate&lt;/li&gt; &lt;li&gt;believe that app UX will completely change in a recent years&lt;/li&gt; &lt;li&gt;it faster than llama.cpp in most of the cases&lt;/li&gt; &lt;li&gt;sometimes it is even faster than MLX from Apple&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speculative decoding right now tightened with platform (trymirai). Feel free to try it out.&lt;/p&gt; &lt;p&gt;Would really appreciate your feedback. Some benchmarks are in readme of the repo. More and more things we will publish later (more benchmarks, support of VLM &amp;amp; TTS/STT is coming soon).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkolorin"&gt; /u/darkolorin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trymirai/uzu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T21:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0y3a6</id>
    <title>Fine-tuning Leaderboard!</title>
    <updated>2025-07-16T00:07:13+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"&gt; &lt;img alt="Fine-tuning Leaderboard!" src="https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477a0620594fc1ec25f6cb09693ff29925108ee4" title="Fine-tuning Leaderboard!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does &lt;em&gt;not&lt;/em&gt; correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There's something to leaving the model less-trained like a blank canvas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://predibase.com/fine-tuning-index"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0slrh</id>
    <title>support for Kimi-K2 has been merged into llama.cpp</title>
    <updated>2025-07-15T20:19:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support for Kimi-K2 has been merged into llama.cpp" src="https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f61a192157fcd7aff2e2fa3bf1c60d78f2fa97" title="support for Kimi-K2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14654"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T20:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0nutb</id>
    <title>Totally lightweight local inference...</title>
    <updated>2025-07-15T17:22:09+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt; &lt;img alt="Totally lightweight local inference..." src="https://preview.redd.it/r05r0wfvn2df1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c622b493252fd700bcdd538ffef56559bdbbcd5" title="Totally lightweight local inference..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r05r0wfvn2df1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0v9m1</id>
    <title>Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!</title>
    <updated>2025-07-15T22:04:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt; &lt;img alt="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ETH Zurich &amp;amp; EPFL Public LLM â€“ Technical Specs â€¢ Release: Late summer 2025 â€¢ Developers: EPFL, ETH Zurich, Swiss National Supercomputing Centre (CSCS), Swiss universities â€¢ Model sizes: 8B and 70B parameters (fully open weights and code, Apache 2.0 license) â€¢ Multilinguality: Fluency in 1,000+ languages (trained on &amp;gt;1,500 languages; ~60% English, ~40% non-English; code and math included) â€¢ Training data: &amp;gt;15 trillion tokens, high-quality, transparent, reproducible, with web-crawling opt-outs respected â€¢ Training hardware: Alps supercomputer (CSCS, Lugano), &amp;gt;10,000 NVIDIA Grace Hopper Superchips, 100% carbon-neutral electricity â€¢ Compliance: Swiss data protection and copyright laws, EU AI Act transparency â€¢ Intended use: Science, society, industry; fully public download, detailed documentation on model architecture and training â€¢ Initiative: Swiss AI Initiative, 800+ researchers, 20M+ GPU hours/year, funded by ETH Board (2025â€“2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T22:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0z1zx</id>
    <title>Your unpopular takes on LLMs</title>
    <updated>2025-07-16T00:52:41+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mine are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;All the popular public benchmarks are nearly worthless when it comes to a model's general ability. Literaly the only good thing we get out of them is a rating for &amp;quot;can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up&amp;quot;, which does have some value. I think the people who maintain the benchmarks know this too, but we're all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. That's because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don't even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON'T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:52:41+00:00</published>
  </entry>
</feed>
