<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-25T11:06:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m80gsn</id>
    <title>GLM-4.5 Is About to Be Released</title>
    <updated>2025-07-24T10:10:17+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt; &lt;img alt="GLM-4.5 Is About to Be Released" src="https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e" title="GLM-4.5 Is About to Be Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vLLM commit: &lt;a href="https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;modelscope/ms-swift commit: &lt;a href="https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're going to get a 106B-A12B (Air) model and a 355B-A32B model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T10:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ewlx</id>
    <title>Level1tech runs deepseek on am5 and it's not that bad!</title>
    <updated>2025-07-24T20:10:36+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"&gt; &lt;img alt="Level1tech runs deepseek on am5 and it's not that bad!" src="https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b8b07ebd60262e21a7df05d698d427003581ec8" title="Level1tech runs deepseek on am5 and it's not that bad!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am5 9000x3d 128gb ram (2*64) and a 3090&lt;/p&gt; &lt;p&gt;I promised i watch it but I couldn't get what exact quant nor speed.&lt;br /&gt; He said this was &amp;quot;compressed to 20% of the og model&amp;quot; so something like a q2.&lt;br /&gt; Regarding speed it seems very very descent&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/T17bpGItqXw?feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T20:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8bps2</id>
    <title>We just open sourced NeuralAgent: The AI Agent That Lives On Your Desktop and Uses It Like You Do!</title>
    <updated>2025-07-24T18:07:52+00:00</updated>
    <author>
      <name>/u/Nearby_Tart_9970</name>
      <uri>https://old.reddit.com/user/Nearby_Tart_9970</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It's now open source. &lt;/p&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/withneural/neuralagent"&gt;https://github.com/withneural/neuralagent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our website: &lt;a href="https://www.getneuralagent.com"&gt;https://www.getneuralagent.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Give us a star if you like the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_Tart_9970"&gt; /u/Nearby_Tart_9970 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T18:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8aeh3</id>
    <title>Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness</title>
    <updated>2025-07-24T17:18:51+00:00</updated>
    <author>
      <name>/u/pheonis2</name>
      <uri>https://old.reddit.com/user/pheonis2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"&gt; &lt;img alt="Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness" src="https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b6aa3eabfc6cb4a722e695f90d9e84bf9d54a1f" title="Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Boson AI has recently open-sourced the Higgs Audio V2 model.&lt;br /&gt; &lt;a href="https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . &lt;/p&gt; &lt;p&gt;Notably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pheonis2"&gt; /u/pheonis2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rcsam20avuef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T17:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vmoi</id>
    <title>I wrote an AI Agent that works better than I expected. Here are 10 learnings.</title>
    <updated>2025-07-25T10:30:30+00:00</updated>
    <author>
      <name>/u/Js8544</name>
      <uri>https://old.reddit.com/user/Js8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Tools first.&lt;/strong&gt; Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start with general, low-level tools.&lt;/strong&gt; For example, bash is a powerful tool that can cover most needs. You don't need to start with a full suite of 100 tools.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start with a single agent.&lt;/strong&gt; Once you have all the basic tools, test them with a single react agent. It's extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start with the best models.&lt;/strong&gt; There will be a lot of problems with your system, so you don't want the model's ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trace and log your agent.&lt;/strong&gt; Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Identify the bottlenecks.&lt;/strong&gt; There's a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn't know how to do something, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterate based on the bottleneck.&lt;/strong&gt; There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You can combine workflows with agents and it may work better.&lt;/strong&gt; If your objective is specialized and there's a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trick: Utilize the filesystem as a hack.&lt;/strong&gt; Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Another Trick: Ask Claude Code how to write agents.&lt;/strong&gt; Claude Code is the best agent we have out there. Even though it's not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Js8544"&gt; /u/Js8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m88s09</id>
    <title>Qwen's third bomb: Qwen3-MT</title>
    <updated>2025-07-24T16:17:55+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt; &lt;img alt="Qwen's third bomb: Qwen3-MT" src="https://b.thumbs.redditmedia.com/LksViWDcxO1eQ0ZQpLUVRXks4wbjVGa9UjqigE3hofA.jpg" title="Qwen's third bomb: Qwen3-MT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a translation model.&lt;/p&gt; &lt;p&gt;Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support for 92 Languages&lt;/strong&gt;: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Customizability&lt;/strong&gt;: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low Latency &amp;amp; Cost Efficiency&lt;/strong&gt;: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf"&gt;benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen-mt/"&gt;https://qwenlm.github.io/blog/qwen-mt/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m85vhw</id>
    <title>new mistralai/Magistral-Small-2507 !?</title>
    <updated>2025-07-24T14:27:29+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt; &lt;img alt="new mistralai/Magistral-Small-2507 !?" src="https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e" title="new mistralai/Magistral-Small-2507 !?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vu80</id>
    <title>N + N size GPU != 2N sized GPU, go big if you can</title>
    <updated>2025-07-25T10:43:20+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to. Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc. Multiple GPUs can be annoying.&lt;/p&gt; &lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb. If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt; &lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt; &lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context. More context with same sized GPU, and it would be faster too!&lt;/p&gt; &lt;p&gt;Go as big as you can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8oc9j</id>
    <title>Stagnation in Knowledge Density</title>
    <updated>2025-07-25T03:10:19+00:00</updated>
    <author>
      <name>/u/Federal-Effective879</name>
      <uri>https://old.reddit.com/user/Federal-Effective879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.&lt;/p&gt; &lt;p&gt;I've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt; &lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt; &lt;li&gt;Mistral Large (2411)&lt;/li&gt; &lt;li&gt;Kimi K2&lt;/li&gt; &lt;li&gt;Cohere Command-A (2503)&lt;/li&gt; &lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt; &lt;li&gt;Llama 4 Maverick&lt;/li&gt; &lt;li&gt;Llama 3.3 70B&lt;/li&gt; &lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt; &lt;li&gt;Dots.LLM1&lt;/li&gt; &lt;li&gt;Gemma 3 27B&lt;/li&gt; &lt;li&gt;Mistral Small 3.2&lt;/li&gt; &lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt; &lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt; &lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.&lt;/p&gt; &lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt; &lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal-Effective879"&gt; /u/Federal-Effective879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8tmhd</id>
    <title>ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025</title>
    <updated>2025-07-25T08:20:10+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T08:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8tt3m</id>
    <title>[AutoBE] We made AI-friendly Compilers for Vibe Coding, achieving 100% build success (open-source, AWS Kiro like)</title>
    <updated>2025-07-25T08:32:38+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tt3m/autobe_we_made_aifriendly_compilers_for_vibe/"&gt; &lt;img alt="[AutoBE] We made AI-friendly Compilers for Vibe Coding, achieving 100% build success (open-source, AWS Kiro like)" src="https://external-preview.redd.it/ODZwOGNwcXpjemVmMbEyKLUkRt18zSeWPIOzcFJ36V17QmYBupRI--Edwqnz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35bed088cf87386d7333f7fe88e825499e28b093" title="[AutoBE] We made AI-friendly Compilers for Vibe Coding, achieving 100% build success (open-source, AWS Kiro like)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The video is sped up; it actually takes about 20-30 minutes.&lt;/p&gt; &lt;p&gt;Also, &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; is still the alpha version development, so there may be some bugs, or &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; generated backend application can be something different from what you expected.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Github Repository: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Generation Result: &lt;a href="https://github.com/wrtnlabs/autobe-example-bbs"&gt;https://github.com/wrtnlabs/autobe-example-bbs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Detailed Article: &lt;a href="https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html"&gt;https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We are honored to introduce &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; to you. &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; is an open-source project developed by Wrtn Technologies (Korean AI startup company), a vibe coding agent that automatically generates backend applications.&lt;/p&gt; &lt;p&gt;One of &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt;'s key features is that it always generates code with 100% compilation success. The secret lies in our proprietary compiler system. Through our self-developed compilers, we support AI in generating type-safe code, and when AI generates incorrect code, the compiler detects it and provides detailed feedback, guiding the AI to generate correct code.&lt;/p&gt; &lt;p&gt;Through this approach, &lt;a href="https://github.com/wrtnlabs/autobe"&gt;&lt;code&gt;AutoBE&lt;/code&gt;&lt;/a&gt; always generates backend applications with 100% compilation success. When AI constructs AST (Abstract Syntax Tree) data through function calling, our proprietary compiler validates it, provides feedback, and ultimately generates complete source code.&lt;/p&gt; &lt;p&gt;About the detailed content, please refer to the following blog article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html"&gt;https://wrtnlabs.io/autobe/articles/autobe-ai-friendly-compilers.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Waterfall Model&lt;/th&gt; &lt;th&gt;AutoBE Agent&lt;/th&gt; &lt;th&gt;Compiler AST Structure&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Requirements&lt;/td&gt; &lt;td&gt;Analyze&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Analysis&lt;/td&gt; &lt;td&gt;Analyze&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Design&lt;/td&gt; &lt;td&gt;Database&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;&lt;code&gt;AutoBePrisma.IFile&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Design&lt;/td&gt; &lt;td&gt;API Interface&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;&lt;code&gt;AutoBeOpenApi.IDocument&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Testing&lt;/td&gt; &lt;td&gt;E2E Test&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;&lt;code&gt;AutoBeTest.IFunction&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Development&lt;/td&gt; &lt;td&gt;Realize&lt;/td&gt; &lt;td&gt;Not yet&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ymo71qqzczef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tt3m/autobe_we_made_aifriendly_compilers_for_vibe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tt3m/autobe_we_made_aifriendly_compilers_for_vibe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T08:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ozb0</id>
    <title>China's Bytedance releases Seed LiveInterpret simultaneous interpretation model</title>
    <updated>2025-07-25T03:43:35+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seed.bytedance.com/en/seed_liveinterpret"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m83644</id>
    <title>China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks</title>
    <updated>2025-07-24T12:33:36+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt; &lt;img alt="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" src="https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa69848c81b950052de8eb2024c390e13024272" title="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dln1</id>
    <title>Qwen 3 Thinking is coming very soon</title>
    <updated>2025-07-24T19:19:39+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt; &lt;img alt="Qwen 3 Thinking is coming very soon" src="https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b99a58da488472ec93d5842e37998def1cbe76" title="Qwen 3 Thinking is coming very soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61i8pt44hvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8qj9w</id>
    <title>Why I Forked Qwen Code</title>
    <updated>2025-07-25T05:07:40+00:00</updated>
    <author>
      <name>/u/ryanwang4thepeople</name>
      <uri>https://old.reddit.com/user/ryanwang4thepeople</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.&lt;/p&gt; &lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt; &lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt; &lt;li&gt;My priorities as a solo developer probably don't align with respective model companies.&lt;/li&gt; &lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt; &lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt; &lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt; &lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt; &lt;li&gt;Documentation&lt;/li&gt; &lt;li&gt;Multi-model support????&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Maybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryanwang4thepeople"&gt; /u/ryanwang4thepeople &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T05:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vjna</id>
    <title>Qwen/Qwen3-235B-A22B-Thinking-2507</title>
    <updated>2025-07-25T10:25:07+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Thinking-2507" src="https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdec699720d09b0abd832855f564b348eefd2304" title="Qwen/Qwen3-235B-A22B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:25:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8uozu</id>
    <title>Announcing the open-source release of Wan2.2. Stay tuned.</title>
    <updated>2025-07-25T09:31:22+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1948654675575668959?t=HLbGkqoAgFio6XLkqS8ueg&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8uozu/announcing_the_opensource_release_of_wan22_stay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8uozu/announcing_the_opensource_release_of_wan22_stay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T09:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dgfu</id>
    <title>Qwen3-235B-A22B-Thinking-2507 is about to be released</title>
    <updated>2025-07-24T19:14:14+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 is about to be released" src="https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f" title="Qwen3-235B-A22B-Thinking-2507 is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6l84nwc3gvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ven3</id>
    <title>Qwen/Qwen3-235B-A22B-Thinking-2507</title>
    <updated>2025-07-25T10:16:41+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Thinking-2507" src="https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdec699720d09b0abd832855f564b348eefd2304" title="Qwen/Qwen3-235B-A22B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m88jdh</id>
    <title>Ok next big open source model also from China only ! Which is about to release</title>
    <updated>2025-07-24T16:08:57+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt; &lt;img alt="Ok next big open source model also from China only ! Which is about to release" src="https://preview.redd.it/j6rwug34juef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c" title="Ok next big open source model also from China only ! Which is about to release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6rwug34juef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T16:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ud84</id>
    <title>A contamination-free coding benchmark shows AI may not be as excellent as claimed</title>
    <updated>2025-07-25T09:09:46+00:00</updated>
    <author>
      <name>/u/Creepy-Document4034</name>
      <uri>https://old.reddit.com/user/Creepy-Document4034</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creepy-Document4034"&gt; /u/Creepy-Document4034 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T09:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8l648</id>
    <title>Executive Order: "Preventing Woke AI in the Federal Government"</title>
    <updated>2025-07-25T00:36:06+00:00</updated>
    <author>
      <name>/u/NunyaBuzor</name>
      <uri>https://old.reddit.com/user/NunyaBuzor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt; &lt;img alt="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunyaBuzor"&gt; /u/NunyaBuzor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T00:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vhp3</id>
    <title>Amazing qwen 3 updated thinking model just released !! Open source !</title>
    <updated>2025-07-25T10:21:49+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"&gt; &lt;img alt="Amazing qwen 3 updated thinking model just released !! Open source !" src="https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d728468419b7ffc3426c85447250b3cc034f70a" title="Amazing qwen 3 updated thinking model just released !! Open source !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nx5d8w74yzef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vegq</id>
    <title>Qwen3-235B-A22B-Thinking-2507 released!</title>
    <updated>2025-07-25T10:16:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 released!" src="https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f579818ebd6748b55b90f802c28f4d37095432e" title="Qwen3-235B-A22B-Thinking-2507 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt; &lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving: ✅ Improved performance in logical reasoning, math, science &amp;amp; coding ✅ Better general skills: instruction following, tool use, alignment ✅ 256K native context for deep, long-form understanding&lt;/p&gt; &lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvx1dbl5xzef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8myxl</id>
    <title>Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.</title>
    <updated>2025-07-25T02:02:13+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt; &lt;img alt="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." src="https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d" title="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl9jgkkzgxef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T02:02:13+00:00</published>
  </entry>
</feed>
