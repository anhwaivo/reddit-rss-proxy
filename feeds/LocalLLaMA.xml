<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-06T15:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lsv7j1</id>
    <title>Using local LLM for anonymizing prompts before sending to cloud LLM - are there any open source solutions?</title>
    <updated>2025-07-06T07:13:58+00:00</updated>
    <author>
      <name>/u/cesarean722</name>
      <uri>https://old.reddit.com/user/cesarean722</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:&lt;/p&gt; &lt;p&gt;local model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;detects personal or business specific data in prompts,&lt;/li&gt; &lt;li&gt;creates mapping dictionary&lt;/li&gt; &lt;li&gt;warns if replace is not feasible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;proxy app:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;executes string replace according to rules in dictionary&lt;/li&gt; &lt;li&gt;routes requests to cloud LLM api&lt;/li&gt; &lt;li&gt;passes LLM warnings to user&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cesarean722"&gt; /u/cesarean722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T07:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls5b89</id>
    <title>Powerful 4B Nemotron based finetune</title>
    <updated>2025-07-05T08:43:38+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt; &lt;img alt="Powerful 4B Nemotron based finetune" src="https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637" title="Powerful 4B Nemotron based finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \ adventure finetunes at its size category.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt; &lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here"&gt;See the examples in the logs&lt;/a&gt; üôÇ&lt;/li&gt; &lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels 'in the moment', kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it's a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt; &lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt; &lt;li&gt;Very decent assistant.&lt;/li&gt; &lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt; &lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally‚Äî less positivity, and the inclination to... argue üôÉ&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \ Adventure:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"&gt;Horde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T08:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsz9iu</id>
    <title>Looking for an open-source TTS model for multi-hour, multilingual audio generation</title>
    <updated>2025-07-06T11:45:18+00:00</updated>
    <author>
      <name>/u/seozler</name>
      <uri>https://old.reddit.com/user/seozler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm building an AI-powered education platform and &lt;strong&gt;looking for&lt;/strong&gt; a high-quality &lt;strong&gt;open-source TTS&lt;/strong&gt; model that meets the following needs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Voice cloning support&lt;/strong&gt; ‚Äî ability to clone voices from short samples&lt;/li&gt; &lt;li&gt;‚úÖ Can generate &lt;strong&gt;3‚Äì4 hours of audio per user&lt;/strong&gt;, even if it requires splitting the text&lt;/li&gt; &lt;li&gt;‚úÖ Produces &lt;strong&gt;good results across the most spoken languages&lt;/strong&gt; (e.g. English, Spanish, Arabic, Hindi, Chinese, etc.)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Commercial tools like ElevenLabs and OpenAI TTS are great, but they don‚Äôt scale well cost-wise for a subscription-based system. That‚Äôs why I‚Äôm exploring open-source alternatives ‚Äî Coqui XTTS, Kokoro TTS, Bark, etc.&lt;/p&gt; &lt;p&gt;If you‚Äôve had experience with any model that meets these needs ‚Äî or know tricks for efficient long-form generation (chunking, caching, merging), I‚Äôd love to hear your thoughts.&lt;/p&gt; &lt;p&gt;Thanks in advance üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seozler"&gt; /u/seozler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz9iu/looking_for_an_opensource_tts_model_for_multihour/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz9iu/looking_for_an_opensource_tts_model_for_multihour/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz9iu/looking_for_an_opensource_tts_model_for_multihour/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T11:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsi0gj</id>
    <title>Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control</title>
    <updated>2025-07-05T19:31:10+00:00</updated>
    <author>
      <name>/u/Old_Mathematician107</name>
      <uri>https://old.reddit.com/user/Old_Mathematician107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt; &lt;img alt="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" src="https://b.thumbs.redditmedia.com/RwkhySplb6CWm6KR-CbkXw8jpoNWLPRz3r_cADWabdw.jpg" title="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.&lt;/p&gt; &lt;p&gt;deki is a model that understands what‚Äôs on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.&lt;/p&gt; &lt;p&gt;All the code/information is available on GitHub: &lt;a href="https://github.com/RasulOs/deki"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have also uploaded the model to Hugging Face:&lt;br /&gt; Space: &lt;a href="https://huggingface.co/spaces/orasul/deki"&gt;orasul/deki&lt;/a&gt;&lt;br /&gt; (Check the analyze-and-get-yolo endpoint)&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/orasul/deki-yolo"&gt;orasul/deki-yolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Mathematician107"&gt; /u/Old_Mathematician107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lsi0gj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T19:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt0a4n</id>
    <title>Mistral small 24B 3.2 VS Qwen 3 30b/14b</title>
    <updated>2025-07-06T12:41:25+00:00</updated>
    <author>
      <name>/u/AlbionPlayerFun</name>
      <uri>https://old.reddit.com/user/AlbionPlayerFun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Llamas!&lt;/p&gt; &lt;p&gt;Which one of these would theoretically be best for summarizing (one big prompt) swedish excel documents? Its on 16 gb vram machine, ouput is in json format. (structured data)&lt;/p&gt; &lt;p&gt;Been checking benchmarks etc but its hard to find a good answer, non reasoning is what i think il go with and about 12-15 k context. I think answer is mistral small 3.2 right? Even if i have to use IQ4_XS quant? On qwen 3 14B i can use Q6. I need lowest hallucination possible.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlbionPlayerFun"&gt; /u/AlbionPlayerFun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0a4n/mistral_small_24b_32_vs_qwen_3_30b14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0a4n/mistral_small_24b_32_vs_qwen_3_30b14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0a4n/mistral_small_24b_32_vs_qwen_3_30b14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T12:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt1z1a</id>
    <title>How do I see my tokens per second speed? I'm using llama.cpp / ik_llama.cpp with OpenWebUI</title>
    <updated>2025-07-06T14:01:40+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New here, I'm running ik_llama.cpp as the backend, OpenWebUI on the front end. OWUI is showing the tokens generated, total tokens, etc, but is NOT showing token speed like with ollama.&lt;/p&gt; &lt;p&gt;I tried the --verbose argument with running llama-server, but token speed usage is still not showing in OWUI.&lt;/p&gt; &lt;p&gt;Any ideas? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt1z1a/how_do_i_see_my_tokens_per_second_speed_im_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt1z1a/how_do_i_see_my_tokens_per_second_speed_im_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt1z1a/how_do_i_see_my_tokens_per_second_speed_im_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T14:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsuje6</id>
    <title>Anyone building a local coding cli or coding agent?</title>
    <updated>2025-07-06T06:30:07+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just broke the ground on mine. I used copilot a bit 2 years ago when it was pretty new but preferred cut &amp;amp; paste, then I did &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; a bit, then back to cut &amp;amp; paste. Did aider a bit, then ...&lt;/p&gt; &lt;p&gt;None of them really hit the sweet spot for me, so I decided to roll my own, might not be as good as the commercial ones, but it's always a fun learning exercise. If you are cooking up one as well, let me know, looking to bounce ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T06:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt4994</id>
    <title>Streaming or non streamed responses, assuming the same (and reasonably fast) time to final token</title>
    <updated>2025-07-06T15:40:56+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feel free to comment with your specific use case and how this affects it. For ex. I‚Äôm making an ai editor for something, and I prefer non streamed responses.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1lt4994"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4994/streaming_or_non_streamed_responses_assuming_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4994/streaming_or_non_streamed_responses_assuming_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4994/streaming_or_non_streamed_responses_assuming_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T15:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lswkv4</id>
    <title>What are some good in-browser inference tools for small LLMs? (Use case: JSON to Chart.js config)</title>
    <updated>2025-07-06T08:48:35+00:00</updated>
    <author>
      <name>/u/callmedevilthebad</name>
      <uri>https://old.reddit.com/user/callmedevilthebad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I‚Äôm exploring some ideas around running small LLMs entirely &lt;strong&gt;in the browser&lt;/strong&gt;, and wanted to ask for suggestions or experiences with lightweight inference frameworks.&lt;/p&gt; &lt;p&gt;The main use case I‚Äôm playing with is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;(Priority)&lt;/strong&gt; Taking a JSON object and generating a valid &lt;a href="https://www.chartjs.org/"&gt;Chart.js&lt;/a&gt; config to visualize it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;(Secondary)&lt;/strong&gt; Producing a natural language explanation of the data ‚Äî like a brief summary or insight.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd like the whole thing to run locally in the browser ‚Äî no backend ‚Äî so I'm looking for tools or runtimes that support:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small quantized models (ideally &amp;lt;100MB)&lt;/li&gt; &lt;li&gt;WebGPU or WASM support&lt;/li&gt; &lt;li&gt;Quick startup and decent performance for structured JSON reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôve started looking into &lt;a href="https://mlc.ai"&gt;MLC.ai&lt;/a&gt;, which seems promising, but curious if anyone here has:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tried MLC.ai recently for browser-based LLM tasks?&lt;/li&gt; &lt;li&gt;Found any newer/easier runtimes that support small models?&lt;/li&gt; &lt;li&gt;Used models that are particularly good at structured JSON-to-JSON transformations?&lt;/li&gt; &lt;li&gt;Prompting tips for clean Chart.js output?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;sales&amp;quot;: [100, 200, 300], &amp;quot;months&amp;quot;: [&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;, &amp;quot;Mar&amp;quot;] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Expected output: A full Chart.js config for a bar or line chart. Bonus: An optional summary like &lt;em&gt;‚ÄúSales increased steadily from January to March.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what folks have tried or recommend for running small models client-side. Thanks!&lt;/p&gt; &lt;p&gt;Edit: Anything under 500mb is good Edit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/callmedevilthebad"&gt; /u/callmedevilthebad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbhzs</id>
    <title>When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?</title>
    <updated>2025-07-05T14:44:47+00:00</updated>
    <author>
      <name>/u/spiritxfly</name>
      <uri>https://old.reddit.com/user/spiritxfly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt; &lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spiritxfly"&gt; /u/spiritxfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T14:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdjnb</id>
    <title>Llama-4-Maverick 402B on a oneplus 13</title>
    <updated>2025-07-05T16:15:49+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt; &lt;img alt="Llama-4-Maverick 402B on a oneplus 13" src="https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcc6e776cfcd25daabf167946f5f41fbb1c23e70" title="Llama-4-Maverick 402B on a oneplus 13" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt; &lt;p&gt;Here's the command used: &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/QwkaFHf"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt; &lt;p&gt;- Green: in RAM&lt;/p&gt; &lt;p&gt;- Red: read from DISC&lt;/p&gt; &lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt; &lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tletuj5ov2bf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt0z6j</id>
    <title>Need an inference endpoint students can set up and use to test n8n workflows for an AI class, what free or non-GPU options are available?</title>
    <updated>2025-07-06T13:15:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm in an AI Masters program that is just getting off the ground and I‚Äôm trying to help one of my professors locate resources that can be used for class projects. &lt;/p&gt; &lt;p&gt;We used the free GPU resources on Google Colab for some model training and such, but now we need inference endpoints and I‚Äôm not sure if Colab supports that kind of thing using the free tier. &lt;/p&gt; &lt;p&gt;We want to use n8n for some simple AI automation workflow projects. Having used n8n a little myself, I know it needs an endpoint for inference. I use it with a GPU, but I know that it is likely not all students will have access to a GPU. &lt;/p&gt; &lt;p&gt;Are there any free public inference endpoints out there for academic use, or do you think it would be better to just use Ollama with something like Gemma3n or a similar model that could run CPU only to be able to run on an average laptop that all students should have already? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0z6j/need_an_inference_endpoint_students_can_set_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0z6j/need_an_inference_endpoint_students_can_set_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt0z6j/need_an_inference_endpoint_students_can_set_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T13:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsyza0</id>
    <title>Getting started with local AI</title>
    <updated>2025-07-06T11:28:01+00:00</updated>
    <author>
      <name>/u/Bitter-Ad640</name>
      <uri>https://old.reddit.com/user/Bitter-Ad640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;I want to get started with local AI, and I‚Äôm looking for advice on where to begin. I'm reading some of the other posts about the same, but seeing how quickly AI advances I figured I'd ask. I‚Äôve been looking at the smaller models like Llama and Deepseek's 8b. Apparently one is as small as 1.5b.... That can be run on some *very* modest hardware: &lt;a href="https://martech.org/how-to-run-deepseek-locally-on-your-computer/"&gt;https://martech.org/how-to-run-deepseek-locally-on-your-computer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Right now, I‚Äôm working with a laptop with an i9-13980hx, an RTX 4080, 32gb DDR5, and 1tb ssd. I realize that I‚Äôm not going to be running a fortune 500 company, solving world hunger, or achieving The Singularity with this setup, but on paper it should be pretty capable for what I‚Äôm envisioning. &lt;/p&gt; &lt;p&gt;There‚Äôs three basic things I‚Äôd really like to try with local AI: &lt;/p&gt; &lt;p&gt;-Fine-tuning/distilling them for more specific purposes- &lt;/p&gt; &lt;p&gt;I‚Äôm currently using ChatGPT as a day-planner/calendar/to-do list that I can talk to. It‚Äôs great that it could also write a comparative essay on the agrarian economies of pre-roman versus post-roman Gaul‚Ä¶ but I don‚Äôt need my calendar to do that. I need it to accurately follow instructions, keep accurate lists, and answer questions about information it has access to. Sometimes ChatGPT has been surprisingly bad at this, and it‚Äôs actually seemed to get worse as the models get ‚Äúsmarter‚Äù and ‚Äúmore human‚Äù. &lt;/p&gt; &lt;p&gt;-Integrating them into larger ‚Äúdigital ecosystems‚Äù-&lt;/p&gt; &lt;p&gt;There are some things ChatGPT is too ‚Äúsmart‚Äù to do reliably. Like find every mention of a word in a document, or tell me what time it is (try it yourself. 1/3 correct, at best). These sound like tasks for a ‚Äúdumb‚Äù service. Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use ‚ÄúFind‚Äù. Getting a local LLM to know when it‚Äôs time to offload the work to a different, simpler element would make the whole system much more smooth, reliable, and useful. Bonus points if it can also reach out to more powerful cloud AIs through things like an OpenAI API key. &lt;/p&gt; &lt;p&gt;-Image recognition- &lt;/p&gt; &lt;p&gt;I‚Äôve got some interest in getting a part of that larger system to recognize images I train it for, but this is sort of icing on the cake. I hear things like computervision, resnet, and nyckel thrown around, but I don‚Äôt understand enough yet to even know what questions to ask. &lt;/p&gt; &lt;p&gt;Any tips on where to start?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-Ad640"&gt; /u/Bitter-Ad640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T11:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsw9vz</id>
    <title>Run Large LLMs on RunPod with text-generation-webui ‚Äì Full Setup Guide + Template</title>
    <updated>2025-07-06T08:26:43+00:00</updated>
    <author>
      <name>/u/abandonedexplorer</name>
      <uri>https://old.reddit.com/user/abandonedexplorer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I usually rent GPUs from the cloud since I don‚Äôt want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.&lt;/p&gt; &lt;p&gt;For LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.&lt;/p&gt; &lt;p&gt;So here you go, a complete tutorial with a one-click template included:&lt;/p&gt; &lt;h1&gt;Source code and instructions:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md"&gt;https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;RunPod template:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo"&gt;https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you're good to go. The template was inspired by TheBloke's now-deprecated &lt;a href="https://github.com/TheBlokeAI/dockerLLM"&gt;dockerLLM&lt;/a&gt; project, which I‚Äôve completely refactored.&lt;/p&gt; &lt;p&gt;A quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like &lt;a href="https://github.com/vllm-project/vllm"&gt;VLLM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why I use RunPod:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Relatively cheap ‚Äì I can get 48 GB VRAM for just $0.40/hour&lt;/li&gt; &lt;li&gt; Easy multi-GPU support ‚Äì I can stack cheap GPUs to run big models (like Mistral Large) at a low cost&lt;/li&gt; &lt;li&gt; Simple templates ‚Äì very little tinkering needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I see renting GPUs as a solid privacy middle ground. Ideally, I‚Äôd run everything locally, but I don‚Äôt want to invest in expensive hardware. While I cannot audit RunPod's privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).&lt;/p&gt; &lt;p&gt;The README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abandonedexplorer"&gt; /u/abandonedexplorer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt13ht</id>
    <title>I built ccundo - instantly undo Claude Code's mistakes without wasting tokens</title>
    <updated>2025-07-06T13:20:57+00:00</updated>
    <author>
      <name>/u/Competitive-Noise905</name>
      <uri>https://old.reddit.com/user/Competitive-Noise905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of Claude Code making changes I didn't want, then having to spend more tokens asking it to fix things.&lt;/p&gt; &lt;p&gt;So I made &lt;strong&gt;ccundo&lt;/strong&gt; - an npm package that lets you quickly undo Claude Code operations with previews and cascading safety.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g ccundo ccundo list # see recent operations ccundo undo # undo with preview &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/RonitSachdev/ccundo"&gt;https://github.com/RonitSachdev/ccundo&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;npm&lt;/strong&gt;: &lt;a href="https://www.npmjs.com/package/ccundo"&gt;https://www.npmjs.com/package/ccundo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚≠ê &lt;strong&gt;Please star if you find it useful!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What do you think? Anyone else dealing with similar Claude Code frustrations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Noise905"&gt; /u/Competitive-Noise905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt13ht/i_built_ccundo_instantly_undo_claude_codes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt13ht/i_built_ccundo_instantly_undo_claude_codes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt13ht/i_built_ccundo_instantly_undo_claude_codes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T13:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsmtzr</id>
    <title>Is Codestral 22B still the best open LLM for local coding on 32‚Äì64 GB VRAM?</title>
    <updated>2025-07-05T23:13:57+00:00</updated>
    <author>
      <name>/u/One-Stress-6734</name>
      <uri>https://old.reddit.com/user/One-Stress-6734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt; &lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt; &lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance ‚Äì not just benchmark wins. C/C++, python and Js.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;p&gt;Edit: Thank you @ all for the insights!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Stress-6734"&gt; /u/One-Stress-6734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T23:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsofwq</id>
    <title>All i said was hello lol</title>
    <updated>2025-07-06T00:35:44+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt; ollama run phi4-mini-reasoning:3.8b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello&lt;/p&gt; &lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Okay, let's see what the user needs here. The message says &amp;quot;You will be given a problem.&amp;quot; but&lt;/p&gt; &lt;p&gt;then it just has &amp;quot;hello&amp;quot;. Hmm, maybe there was a typo or the problem didn't get sent&lt;/p&gt; &lt;p&gt;correctly. Let me check again.&lt;/p&gt; &lt;p&gt;Wait, the user's name is Phi, an AI math expert from Microsoft. Then there's a greeting&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot; and that's it. So perhaps the actual problem wasn't included properly. Maybe the user&lt;/p&gt; &lt;p&gt;intended to send a math problem but only wrote &amp;quot;hello&amp;quot;. I should prompt them to provide the&lt;/p&gt; &lt;p&gt;problem they need help with. But according to the instructions, I need to first think through&lt;/p&gt; &lt;p&gt;solving the problem before giving the answer. However, there's no problem here.&lt;/p&gt; &lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is the problem itself? That seems unlikely. Maybe it's a test or&lt;/p&gt; &lt;p&gt;a mistake. Let me consider if &amp;quot;hello&amp;quot; could be part of a math question. For example,&lt;/p&gt; &lt;p&gt;converting letters to numbers using ASCII values. Let's try that.&lt;/p&gt; &lt;p&gt;H in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69&lt;/p&gt; &lt;p&gt;+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293&lt;/p&gt; &lt;p&gt;+ 79 is 372. So the total would be 372. But I'm not sure if that's what the user wants. They&lt;/p&gt; &lt;p&gt;might have intended a different approach.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a cipher problem, like Caesar cipher where each letter is shifted.&lt;/p&gt; &lt;p&gt;For example, shifting letters by a certain number. But without more context, it's hard to&lt;/p&gt; &lt;p&gt;tell.&lt;/p&gt; &lt;p&gt;Since the user mentioned being an AI math expert and the initial instruction was about&lt;/p&gt; &lt;p&gt;solving a problem, but the input only has &amp;quot;hello&amp;quot;, I think the best course is to ask for&lt;/p&gt; &lt;p&gt;clarification. However, since the task requires providing an answer within a box, maybe I&lt;/p&gt; &lt;p&gt;should state that the problem is missing or assume a common problem. Wait, perhaps the user&lt;/p&gt; &lt;p&gt;intended to send a problem but it didn't go through, so responding by pointing out the&lt;/p&gt; &lt;p&gt;missing problem would be better. But according to the example given in the query, sometimes&lt;/p&gt; &lt;p&gt;even if there's confusion, you have to proceed.&lt;/p&gt; &lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is a greeting and the actual problem is forthcoming. But the&lt;/p&gt; &lt;p&gt;structure of the query shows that after &amp;quot;You will be given a problem,&amp;quot; the user input is&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot;. So perhaps there was an error in submission. In that case, my answer should note&lt;/p&gt; &lt;p&gt;that the problem seems missing and ask for it. But since I need to provide a boxed answer,&lt;/p&gt; &lt;p&gt;maybe there's a default response. Hmm.&lt;/p&gt; &lt;p&gt;Alternatively, if this is a test of handling errors, then the correct response would be to&lt;/p&gt; &lt;p&gt;inform the user that no problem was detected and request the problem statement. However,&lt;/p&gt; &lt;p&gt;given the setup here, perhaps the assistant is supposed to generate a sample problem and&lt;/p&gt; &lt;p&gt;solution? But that's speculative.&lt;/p&gt; &lt;p&gt;Wait, looking back at the original instructions: &amp;quot;You will be given a problem. Please reason&lt;/p&gt; &lt;p&gt;step by step, and put your final answer within \boxed{}.&amp;quot; The user's input after that is&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot;. So maybe the actual problem was meant to follow &amp;quot;hello&amp;quot; but didn't. Therefore, my&lt;/p&gt; &lt;p&gt;conclusion is that there's a missing problem here. Since I need to provide an answer, perhaps&lt;/p&gt; &lt;p&gt;I should state that the problem is missing and ask for it, but in the boxed answer? But the&lt;/p&gt; &lt;p&gt;example shows mathematical answers in boxes. Alternatively, maybe it's a trick question where&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot; translates to something else.&lt;/p&gt; &lt;p&gt;If we consider &amp;quot;hello&amp;quot; as a hexadecimal number, H isn't valid hex (only 0-9 and A-F). So&lt;/p&gt; &lt;p&gt;that's not possible. If using base64 decoding, but again, letters are allowed. But without&lt;/p&gt; &lt;p&gt;more context, this is all guesswork.&lt;/p&gt; &lt;p&gt;Given that the user is an AI math expert, maybe they intended to send a problem but it didn't&lt;/p&gt; &lt;p&gt;come through. Since I need to follow the instructions strictly, perhaps I should answer based&lt;/p&gt; &lt;p&gt;on &amp;quot;hello&amp;quot; being the input. If we take &amp;quot;hello&amp;quot; as a string and compute something related to&lt;/p&gt; &lt;p&gt;its letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So&lt;/p&gt; &lt;p&gt;8+5+12+12+15 = 52. Maybe that's the answer? \boxed{52}&lt;/p&gt; &lt;p&gt;Alternatively, product: 8*5*12*12*15. Let's compute that: 8*5=40, 40*12=480, 480*12=5760,&lt;/p&gt; &lt;p&gt;5760*15=86400. That's a big number, but maybe not likely.&lt;/p&gt; &lt;p&gt;Alternatively, count the letters: &amp;quot;hello&amp;quot; has 5 letters. So \boxed{5}. But that's too simple.&lt;/p&gt; &lt;p&gt;Another angle: &amp;quot;HELLO&amp;quot; in different languages? Not sure. Or Morse code: H is comma, E is dot&lt;/p&gt; &lt;p&gt;slash, L are double dash, O is hollow exclamation. But converting that to numbers or&lt;/p&gt; &lt;p&gt;something else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not&lt;/p&gt; &lt;p&gt;relevant.&lt;/p&gt; &lt;p&gt;Alternatively, if it's a calculus problem, like finding the derivative of &amp;quot;hello&amp;quot;, which&lt;/p&gt; &lt;p&gt;doesn't make sense.&lt;/p&gt; &lt;p&gt;Given all this uncertainty, perhaps the safest assumption is that the user intended to send a&lt;/p&gt; &lt;p&gt;math problem but it didn't come through correctly. Therefore, my answer should indicate that&lt;/p&gt; &lt;p&gt;the problem is missing. However, since I need to provide a boxed answer as per instructions,&lt;/p&gt; &lt;p&gt;and there's no way to know, maybe I'll go with the sum of the alphabetical positions:&lt;/p&gt; &lt;p&gt;8+5+12+12+15=52. So \boxed{52}.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;The problem statement appears to be incomplete or incorrectly submitted. However, if we&lt;/p&gt; &lt;p&gt;interpret &amp;quot;hello&amp;quot; by converting each letter to its position in the alphabet (H=8, E=5, L=12,&lt;/p&gt; &lt;p&gt;O=15) and summing them:&lt;/p&gt; &lt;p&gt;\[&lt;/p&gt; &lt;p&gt;8 + 5 + 12 + 12 + 15 = 52&lt;/p&gt; &lt;p&gt;\]&lt;/p&gt; &lt;p&gt;\boxed{52}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T00:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lskb8k</id>
    <title>Should I buy an appartment or 4 H100s</title>
    <updated>2025-07-05T21:14:39+00:00</updated>
    <author>
      <name>/u/InfiniteEjaculation</name>
      <uri>https://old.reddit.com/user/InfiniteEjaculation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are they so expensive, has anybody here ever tested them? How many rtx 5090s are needed to match it's performance? What llm can we run entirely on one h100 with as much RAM as required?&lt;/p&gt; &lt;p&gt;Naive questions but I am very confused&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteEjaculation"&gt; /u/InfiniteEjaculation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T21:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsgtvy</id>
    <title>Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)</title>
    <updated>2025-07-05T18:39:10+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt; &lt;img alt="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" src="https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66839b47c62157c843227bea591a16264454d62a" title="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a PC was always one of those &amp;quot;someday&amp;quot; projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn't just slow, it was a hard bottleneck.&lt;/p&gt; &lt;p&gt;So, I started mapping out what this new machine needed to be:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;32GB VRAM as the baseline.&lt;/strong&gt; I'm really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.&lt;br /&gt; - &lt;strong&gt;128GB of RAM as the baseline.&lt;/strong&gt; Essential for wrangling the large datasets that come with the territory.&lt;br /&gt; - &lt;strong&gt;A clean, consumer-desk look.&lt;/strong&gt; I don't want a rugged, noisy server rack.&lt;br /&gt; - &lt;strong&gt;AI inference as the main job,&lt;/strong&gt; but I didn't want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.&lt;br /&gt; - &lt;strong&gt;Room to grow.&lt;/strong&gt; I wanted a foundation I could build on later.&lt;br /&gt; - And the big one: &lt;strong&gt;Keep it under $1500.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.&lt;/p&gt; &lt;p&gt;Just as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn't record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.&lt;/p&gt; &lt;p&gt;But here was the catch: these are server cards. Even though they have a display port, it doesn't actually work. That would have killed my &amp;quot;all-rounder&amp;quot; requirement.&lt;/p&gt; &lt;p&gt;I started digging deep, trying to find a workaround. That's when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...&lt;/p&gt; &lt;p&gt;...until the kind-hearted &lt;a href="/u/Accurate_Ad4323"&gt;u/Accurate_Ad4323&lt;/a&gt; from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.&lt;/p&gt; &lt;p&gt;I still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger's advice? And with ROCm's reputation for being a bit tricky, I didn't want to make my life even harder.&lt;/p&gt; &lt;p&gt;In the end, I decided to pull the trigger. Worst-case scenario? I'd have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.&lt;/p&gt; &lt;p&gt;I found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.&lt;/p&gt; &lt;p&gt;Here‚Äôs what I ordered from them:&lt;/p&gt; &lt;p&gt;- Supermicro X11DPI-N -&amp;gt; $320&lt;br /&gt; - Dual Xeon 6148 CPUs -&amp;gt; 27 * 2 = $54&lt;br /&gt; - 2x CPU Coolers -&amp;gt; $62&lt;br /&gt; - 2x MI50 32GB GPUs -&amp;gt; $137 * 2 = $274&lt;br /&gt; - 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&amp;gt; $124&lt;br /&gt; - 10x 120mm RGB fans -&amp;gt; $32&lt;br /&gt; - 6x 140mm RGB fans -&amp;gt; $27&lt;br /&gt; - 2x custom cooling shrouded fans for MI50s -&amp;gt; $14&lt;br /&gt; - Shipping + Duties -&amp;gt; $187&lt;/p&gt; &lt;p&gt;I know people get skeptical about Alibaba, but in my opinion, you're safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.&lt;/p&gt; &lt;p&gt;When the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).&lt;/p&gt; &lt;p&gt;I also bought these peripherals brand-new:&lt;/p&gt; &lt;p&gt;- Phanteks Enthoo Pro 2 Server Edition -&amp;gt; $200&lt;br /&gt; - ProLab 1200W 80Plus Gold PSU -&amp;gt; $100&lt;br /&gt; - 2TB NVMe SSD (For Ubuntu) -&amp;gt; $100&lt;br /&gt; - 1TB 2.5 SSD (For Windows) -&amp;gt; $50&lt;/p&gt; &lt;p&gt;All in, I spent exactly &lt;strong&gt;$1544.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now for the two final hurdles:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Assembling everything without breaking it!&lt;/strong&gt; As a first-timer, it took me about three very careful days, but I'm so proud of how it turned out.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing that custom VBIOS. Did I get the &amp;quot;bonus&amp;quot;?&lt;/strong&gt; After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, to answer the questions I had for myself about the VBIOS cross-flash:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is it stable?&lt;/strong&gt; Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set &amp;quot;VGA Priority&amp;quot; to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to &amp;quot;Onboard&amp;quot; fixes it. This doesn't happen at all in Ubuntu with ROCm.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does the flash hurt AI performance?&lt;/strong&gt; Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I've seen zero difference. If anything weird pops up, I'll be sure to update.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can it game?&lt;/strong&gt; Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.&lt;/p&gt; &lt;p&gt;I ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)&lt;/p&gt; &lt;p&gt;Now, I'll end this off with a couple pictures of the build and some benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790"&gt;https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71"&gt;https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The build is still a work-in-progress with regards to cable management :facepalm)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A power limit of 150W was imposed on both GPUs for all these tests.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-128K-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ¬± 2.44 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ¬± 0.07 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Magistral-Small-2506-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ¬± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ¬± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gemma-3-27b-it-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ¬± 3.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ¬± 0.02 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ¬± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ¬± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama-3.3-70B-Instruct-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ¬± 0.05 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ¬± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-128K-UD-Q2_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2_K_XL-00001-of-00002.gguf -ot '(4-7+).ffn_._exps.=CPU' -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | ot | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ¬± 0.15 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ¬± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Furmark scores post VBIOS flash and NimeZ drivers on Windows:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f"&gt;https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764"&gt;https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall, this whole experience has been an adventure, but it's been overwhelmingly positive. I thought I'd share it for anyone else thinking about a similar build.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Noticed a lot of requests to post the seller. Here you go: &lt;a href="https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html"&gt;https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T18:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lswnto</id>
    <title>gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code.</title>
    <updated>2025-07-06T08:54:21+00:00</updated>
    <author>
      <name>/u/PieBru</name>
      <uri>https://old.reddit.com/user/PieBru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt; &lt;img alt="gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code." src="https://b.thumbs.redditmedia.com/nT92tKjMkTLoLC9uXWz050-DcHqgcFo8BpiZvSZgM8I.jpg" title="gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5"&gt;https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm a huge open source fan, but I think the gemini-cli fallback from &amp;quot;pro&amp;quot; to &amp;quot;flash&amp;quot; will divert more &amp;quot;real&amp;quot; coders to claude-code than convince them to get a gemini-pro subscription.&lt;/p&gt; &lt;p&gt;The gemini-cli doc states that &amp;quot;To ensure you rarely, if ever, hit a limit during this preview, we offer the industry‚Äôs largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&amp;quot;. That's good, but it doesn't mention the throttling from pro to flash. When I try to build something out of the Erathostene Sieve, the throttling causes a code mess and soon reaches the limits (err 429) without a useful solution, because of the flash incapacity to solve &amp;quot;real&amp;quot; coding problems.&lt;/p&gt; &lt;p&gt;gemini-cli at this early stage can't compare to claude-code, so loosing &amp;quot;real&amp;quot; community devs isn't the best strategy to win the battle, IMO.&lt;/p&gt; &lt;p&gt;At the end, I'm looking for alternative solutions, without discarding the auto-build of a similar tool that with some agentic LLM routing can substitute closed-source and cloud solutions.&lt;/p&gt; &lt;p&gt;Meanwhile, the above solutions + context engineering may be used to build some &amp;quot;private&amp;quot; solution.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PieBru"&gt; /u/PieBru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt18hg</id>
    <title>Are Qwen3 Embedding GGUF faulty?</title>
    <updated>2025-07-06T13:27:20+00:00</updated>
    <author>
      <name>/u/espadrine</name>
      <uri>https://old.reddit.com/user/espadrine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Embedding has great retrieval results on &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, I tried it in &lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF"&gt;llama.cpp&lt;/a&gt;. The results were much worse than competitors. I have an FAQ benchmark that looks a bit like this:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3 8B&lt;/td&gt; &lt;td&gt;18.70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral&lt;/td&gt; &lt;td&gt;53.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI (text-embedding-3-large)&lt;/td&gt; &lt;td&gt;55.87%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google (text-embedding-004)&lt;/td&gt; &lt;td&gt;57.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cohere (embed-v4.0)&lt;/td&gt; &lt;td&gt;58.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Voyage AI&lt;/td&gt; &lt;td&gt;60.54%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Qwen3 is the only one that I am not using an API for, but I would assume that the F16 GGUF shouldn't have that big of an impact on performance compared to the raw model, say using TEI or vLLM.&lt;/p&gt; &lt;p&gt;Does anybody have a similar experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/espadrine"&gt; /u/espadrine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T13:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsxxo2</id>
    <title>Python Implementation of Google's MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</title>
    <updated>2025-07-06T10:20:49+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sigridjineth/muvera-py"&gt;https://github.com/sigridjineth/muvera-py&lt;/a&gt;&lt;br /&gt; I have created the Python implementation was created to make the FDE algorithm more accessible while maintaining complete fidelity to the original C++ implementation. Every function and parameter has been carefully mapped to ensure identical behavior.&lt;/p&gt; &lt;h1&gt;What is FDE (Read below)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fixed-Dimensional Encoding (FDE) solves a fundamental problem in modern search systems: how to efficiently search through billions of documents when each document is represented by hundreds of vectors (as in ColBERT-style models).&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Traditional search&lt;/strong&gt;: Document = 1 vector ‚Üí Fast but inaccurate&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modern multi-vector search&lt;/strong&gt;: Document = 100s of vectors ‚Üí Accurate but extremely slow&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The FDE Solution&lt;/h1&gt; &lt;p&gt;FDE transforms multiple vectors into a single fixed-size vector while preserving the similarity relationships. The magic is that the dot product between two FDE vectors approximates the original Chamfer similarity between the multi-vector sets.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T10:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt254p</id>
    <title>Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds</title>
    <updated>2025-07-06T14:09:23+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"&gt; &lt;img alt="Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds" src="https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8dda3fcfc7ea8d542e1d5d4a0ee80c722521587c" title="Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/07/04/zhipu-secures-1-4-billion-strategic-investment-from-shanghai-state-funds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T14:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lspzn3</id>
    <title>128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.</title>
    <updated>2025-07-06T01:59:10+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt; &lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt; &lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt; &lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt; &lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt; &lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;3014.18 ¬± 1.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;191.63 ¬± 0.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1289.11 ¬± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;91.46 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;357.71 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;249.45 ¬± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.24 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;300.02 ¬± 0.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.39 ¬± 0.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;48.92 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.05 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;56.33 ¬± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;16.00 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;1023.81 ¬± 3.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;63.87 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;238.17 ¬± 0.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;25.17 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;202.50 ¬± 0.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.17 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt; &lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt; &lt;p&gt;Now, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output token throughput (tok/s) (256)&lt;/th&gt; &lt;th align="left"&gt;Prompt processing t/s (4096)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.68&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.76&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;25.96&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;27.26&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;32.3&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;38.55&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;36.96&lt;/td&gt; &lt;td align="left"&gt;350&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt; &lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T01:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsz4hk</id>
    <title>Huawei's Pangu AI Rocked by Unverified Claims of Fraud from Alleged Team Member</title>
    <updated>2025-07-06T11:36:43+00:00</updated>
    <author>
      <name>/u/Rich-Mushroom-8360</name>
      <uri>https://old.reddit.com/user/Rich-Mushroom-8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;br /&gt; after reading the traslation of this article, I found there're many details, is it possible true or just a fake story?&lt;/p&gt; &lt;p&gt;gemini's traslation:&lt;/p&gt; &lt;p&gt;This is a full translation of the provided text. The original is a deeply emotional and accusatory letter from a self-proclaimed Huawei employee. The translation aims to preserve the tone, technical details, and cultural nuances of the original piece.&lt;/p&gt; &lt;p&gt;The Fall of Pangu: The Heartbreak and Darkness of the Huawei Noah's Ark Pangu LLM Development Journey&lt;/p&gt; &lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am an employee of the Pangu LLM team at Huawei's Noah's Ark Lab.&lt;/p&gt; &lt;p&gt;First, to verify my identity, I will list some details:&lt;/p&gt; &lt;p&gt;The current director of Noah's Ark Lab is Wang Yunhe, who was formerly the head of the Algorithm Application Department, later renamed the Small Model Lab. The former director of Noah's Ark was Yao Jun (whom everyone called Teacher Yao). Several lab directors include: Tang Ruiming (Ming-ge, Team Ming, has since left), Shang Lifeng, Zhang Wei (Wei-ge), Hao Jianye (Teacher Hao), and Liu Wulong (referred to as Director Wulong). Many other key members and experts have also left one after another.&lt;/p&gt; &lt;p&gt;We belong to an organization called the &amp;quot;Fourth Field Army&amp;quot; (ÂõõÈáé). Under the Fourth Field Army, there are many &amp;quot;columns&amp;quot; (Á∫µÈòü); the foundational language model team is the Fourth Column. Wang Yunhe's small model team is the Sixteenth Column. We participated in gatherings in Suzhou, with various monthly deadlines. During the &amp;quot;problem-tackling sessions&amp;quot; in Suzhou, &amp;quot;mission orders&amp;quot; were issued, requiring us to meet targets before set deadlines. The Suzhou gatherings brought people from all over to the Suzhou Research Institute. We usually stayed in hotels, such as one in Lu Zhi (Áî™Áõ¥), separated from our families and children.&lt;/p&gt; &lt;p&gt;During the Suzhou gatherings, Saturday was a default workday. It was exhausting, but there was afternoon tea on Saturdays, and one time we even had crayfish. Our workstations at the Suzhou Research Institute were moved once, from one building to another. The buildings at the Suzhou Institute have European-style architecture, with a large slope at the entrance, and the scenery inside is beautiful. Trips to the Suzhou gatherings would last at least a week, sometimes longer. Many people couldn't go home for one or even two months.&lt;/p&gt; &lt;p&gt;Noah's Ark was once rumored to be research-oriented, but after I joined, because we were working on the large model project under the Fourth Field Army, the project members completely turned into a delivery-focused team, swamped with routine meetings, reviews, and reports. We often had to apply just to run experiments. The team needed to interface with numerous business lines like Terminal's Celia (Â∞èËâ∫), Huawei Cloud, and ICT, and the delivery pressure was immense.&lt;/p&gt; &lt;p&gt;The Pangu model developed by Noah's Ark was initially codenamed &amp;quot;Pangu Zhizi&amp;quot; (ÁõòÂè§Êô∫Â≠ê). At first, it was only available as an internal webpage that required an application for trial use. Later, due to pressure, it was integrated into Welink and opened for public beta.&lt;/p&gt; &lt;p&gt;The recent controversy surrounding the accusations that the Pangu LLM plagiarized Qwen has been all over the news. As a member of the Pangu team, I've been tossing and turning every night, unable to sleep. Pangu's brand has been so severely damaged. On one hand, I selfishly worry about my own career development and feel that my past hard work was for nothing. On the other hand, I feel a sense of vindication now that someone has started exposing these things. For countless days and nights, we gritted our teeth in anger, powerless, as certain individuals internally reaped endless benefits through repeated fraud. This suppression and humiliation have gradually eroded my affection for Huawei, leaving me dazed and confused, lost and aimless, often questioning my life and self-worth.&lt;/p&gt; &lt;p&gt;I admit that I am a coward. As a humble worker, I dare not oppose people like Wang Yunhe with their powerful connections, let alone a behemoth like Huawei. I am terrified of losing my job, as I have a family and children to support. That's why I deeply admire the whistleblower from the bottom of my heart. However, when I see the internal attempts to whitewash and cover up the facts to deceive the public, I can no longer tolerate it. I want to be brave for once and follow my conscience. Even if I harm myself by 800, I hope to damage the enemy by 1,000. I have decided to publicize what I have seen and heard here (some of which is from colleagues) about the &amp;quot;legendary story&amp;quot; of the Pangu LLM.&lt;/p&gt; &lt;p&gt;Huawei has indeed primarily trained its large models on Ascend cards (the Small Model Lab has quite a few Nvidia cards, which they used for training before transitioning to Ascend). I was once captivated by Huawei's determination to &amp;quot;build the world's second choice,&amp;quot; and I used to have deep feelings for the company. We went through trials and tribulations with Ascend, from being full of bugs to now being able to train models, and we invested immense effort and sacrifice.&lt;/p&gt; &lt;p&gt;Initially, our computing power was very limited, and we trained models on the 910A. At that time, it only supported fp16, and the training stability was far worse than bf16. Pangu started working on MoE (Mixture of Experts) very early. In 2023, the main focus was on training a 38B MoE model and a subsequent 71B dense model. The 71B dense model was expanded to become the first-generation 135B dense model, and later, the main models were gradually trained on the 910B.&lt;/p&gt; &lt;p&gt;Both the 71B and 135B models had a huge, fundamental flaw: the tokenizer. The tokenizer used back then had extremely low encoding efficiency. Every single symbol, number, space, and even Chinese character took up one token. As you can imagine, this wasted a tremendous amount of computing power and resulted in poor model performance. At that time, the Small Model Lab happened to have a vocabulary they had trained themselves. Teacher Yao suspected that the model's tokenizer was the problem (and in hindsight, his suspicion was undoubtedly correct). So, he decided to have the 71B and 135B models switch tokenizers, as the Small Model Lab had experimented with this before. The team stitched together two tokenizers and began the replacement process. The replacement for the 71B model failed. The 135B model, using a more refined embedding initialization strategy, finally succeeded in changing its vocabulary after being continually trained on at least 1T of data. But as you can imagine, the performance did not improve.&lt;/p&gt; &lt;p&gt;Meanwhile, other domestic companies like Alibaba and Zhipu AI were training on GPUs and had already figured out the right methods. The gap between Pangu and its competitors grew wider and wider. An internal 230B dense model, trained from scratch, failed for various reasons, pushing the project to the brink of collapse. Facing pressure from several deadlines and strong internal skepticism about Pangu, the team's morale hit rock bottom. With extremely limited computing power, the team struggled and tried many things. For example, they accidentally discovered that the 38B MoE model at the time did not have the expected MoE effect. So they removed the MoE parameters, reverting it to a 13B dense model. Since the 38B MoE originated from a very early Pangu Alpha 13B with a relatively outdated architecture, the team made a series of changes, such as switching from absolute position encoding to RoPE, removing bias, and switching to RMSNorm. Given the failures with the tokenizer and the experience of changing vocabularies, this model's vocabulary was also replaced with the one used by Wang Yunhe's Small Model Lab's 7B model. This 13B model was later expanded and continually trained, becoming the second-generation 38B dense model (which was the main mid-range Pangu model for several months) and was once quite competitive. However, because the larger 135B model had an outdated architecture and was severely damaged by the vocabulary change (later analysis revealed that the stitched-together vocabulary had even more serious bugs), its performance after continued training still lagged far behind leading domestic models like Qwen. The internal criticism and pressure from leadership grew even stronger. The team was practically in a desperate situation.&lt;/p&gt; &lt;p&gt;Under these circumstances, Wang Yunhe and his Small Model Lab stepped in. They claimed to have inherited and modified the parameters from the old 135B model, and by training on just a few hundred billion tokens, they improved various metrics by an average of about ten points. In reality, this was their first masterpiece of &amp;quot;shell-wrapping&amp;quot; (Â•óÂ£≥, i.e., putting a new shell on another company's model) applied to a large model. At Huawei, laymen lead experts, so the leadership had no concept of how absurd this was; they just thought there must be some algorithmic innovation. After internal analysis, it was discovered that they had actually continued training on Qwen 1.5 110B, adding layers, expanding the FFN dimensions, and incorporating some mechanisms from the Pangu-Pi paper to reach about 135B parameters. In fact, the old 135B had 107 layers, while this new model only had 82, and various other configurations were different. After training, the distribution of many parameters in the new, mysterious 135B model was almost identical to Qwen 110B. Even the class name in the model's code was &amp;quot;Qwen&amp;quot; at the time; they were too lazy to even change it. This model later became the so-called 135B V2. And this model was provided to many downstream teams, including external customers.&lt;/p&gt; &lt;p&gt;This incident was a huge blow to those of us colleagues who were doing our work seriously and honestly. Many people internally, including those in the Terminal and Huawei Cloud divisions, knew about this. We all joked that we should stop calling it the Pangu model and call it the &amp;quot;Qiangu&amp;quot; model instead (a pun combining Qwen and Pangu). At the time, team members wanted to report this to the BCG (Business Conduct Guidelines) office, as it was major business fraud. But later, it was said that a leader stopped them, because higher-level leaders (like Teacher Yao, and possibly Director Xiong and Elder Zha) also found out later but did nothing about it. Getting good results through shell-wrapping was also beneficial to them. This event caused several of the team's strongest members to become disheartened, and talk of resignation became commonplace.&lt;/p&gt; &lt;p&gt;At this point, Pangu seemed to find a turning point. Since the Pangu models mentioned earlier were mostly based on continued training and modification, Noah's Ark at that time had no grasp of training technology from scratch, let alone on Ascend's NPUs. Thanks to the strenuous efforts of the team's core members, Pangu began training its third-generation models. After immense effort, the data architecture and training algorithms gradually caught up with the industry. The people from the Small Model Lab had nothing to do with this hardship.&lt;/p&gt; &lt;p&gt;Initially, the team members had no confidence and started with just a 13B model. But later, they found the results were quite good. So this model was later expanded again, becoming the third-generation 38B, codenamed 38B V3. I'm sure many brothers in the product lines are familiar with this model. At that time, this model's tokenizer was an extension of Llama's vocabulary (a common practice in the industry). Meanwhile, Wang Yunhe's lab created another vocabulary (which later became the vocabulary for the Pangu series). The two vocabularies were forced into a &amp;quot;horse race&amp;quot; (a competitive trial), which ended with no clear winner. So, the leadership immediately decided that the vocabularies should be unified, and Wang Yunhe's should be used. Consequently, the 135B V3 (known externally as Pangu Ultra), which was trained from scratch, adopted this tokenizer. This also explains the confusion many brothers who used our models had: why two models of the same V3 generation, but different sizes, used different tokenizers.&lt;/p&gt; &lt;p&gt;From the bottom of our hearts, we feel that the 135B V3 was the pride of our Fourth Column team at the time. It was the first truly full-stack, self-developed, properly from-scratch-trained, hundred-billion-parameter-level model from Huawei, and its performance was comparable to competitors in early 2024. Writing this, I am already in tears. It was so incredibly difficult. To ensure stable training, the team conducted a large number of comparative experiments and performed timely rollbacks and restarts whenever the model's gradients showed anomalies. This model truly achieved what was later stated in the technical report: not a single loss spike throughout the entire training process. We overcame countless difficulties. We did it. We are willing to guarantee the authenticity of this model's training with our lives and honor. How many sleepless nights did we spend for its training? How wronged and aggrieved did we feel when we were being worthless in internal forums? We persevered.&lt;/p&gt; &lt;p&gt;We are the ones who were truly burning our youth to build up China's domestic computing foundation... Away from home, we gave up our families, our holidays, our health, and our entertainment. We risked everything. The hardships and difficulties involved cannot be fully described in a few words. At various mobilization meetings, when we shouted slogans like &amp;quot;Pangu will prevail, Huawei will prevail,&amp;quot; we were genuinely and deeply moved.&lt;/p&gt; &lt;p&gt;However, all the fruits of our hard work were often casually taken by the Small Model Lab. Data? They just demanded it. Code? They just took it and even required us to help adapt it so it could be run with a single click. We used to joke that the Small Model Lab was the &amp;quot;mouse-clicking lab.&amp;quot; We did the hard work; they reaped the glory. It really is true what they say: &amp;quot;You are carrying a heavy burden so that someone else can live a peaceful life.&amp;quot; Under these circumstances, more and more of our comrades could no longer hold on and chose to leave. Seeing those brilliant colleagues leave one by one, I felt both regret and sadness. In this battle-like environment, we were more like comrades-in-arms than colleagues. They were also great teachers from whom I could learn countless technical things. Seeing them go to outstanding teams like ByteDance's Seed, Deepseek, Moonshot AI, Tencent, and Kuaishou, I am genuinely happy for them and wish them the best for escaping this exhausting and dirty place. I still vividly remember what a colleague who left said: &amp;quot;Coming here was a disgrace to my technical career. Every day I stay here is a waste of life.&amp;quot; The words were harsh, but they left me speechless. I worried about my own lack of technical expertise and my inability to adapt to the high-turnover environment of internet companies, which kept me from taking the step to resign despite thinking about it many times.&lt;/p&gt; &lt;p&gt;Besides dense models, Pangu later began exploring MoE models. Initially, a 224B MoE model was trained. In parallel, the Small Model Lab launched its second major shell-wrapping operation (minor incidents may have included other models, like a math model), which is the now infamous Pangu-Pro MoE 72B. This model was internally claimed to have been expanded from the Small Model Lab's 7B model (even if true, this contradicts the technical report, let alone the fact that it was continued training on a shell of Qwen 2.5's 14B). I remember that just a few days after they started training, their internal evaluation scores immediately caught up with our 38B V3 at the time. Many brothers in the AI System Lab knew about their shell-wrapping operation because they needed to adapt the model, but for various reasons, they couldn't bring justice to light. In fact, for this model that was trained for a very long time afterward, I am surprised that HonestAGI was able to detect this level of similarity. The computing power spent on &amp;quot;washing&amp;quot; the parameters to continue training would have been more than enough to train a model of the same size from scratch. I heard from colleagues that they used many methods to wash away Qwen's watermark, even intentionally training it on dirty data. This provides an unprecedented case study for the academic community researching model &amp;quot;lineage.&amp;quot; New lineage detection methods in the future can be tested on this.&lt;/p&gt; &lt;p&gt;In late 2024 and early 2025, after the release of Deepseek v3 and r1, our team was hit hard by their stunning technical level and faced even greater skepticism. To keep up with the trend, Pangu imitated Deepseek's model size and began training a 718B MoE model. At this time, the Small Model Lab struck again. They chose to shell-wrap and continue training on Deepseek-v3. They trained the model by freezing the parameters loaded from Deepseek. Even the directory for loading the checkpoint was named deepseekv3‚Äîthey didn't even bother to change it. How arrogant is that? In contrast, some colleagues with true technical integrity were training another 718B MoE from scratch, but they encountered all sorts of problems. But obviously, how could this model ever be better than a direct shell-wrap? If it weren't for the team leader's insistence, it would have been shut down long ago.&lt;/p&gt; &lt;p&gt;Huawei's cumbersome process management severely slows down the R&amp;amp;D pace of large models, with things like version control, model lineage, various procedures, and traceability requirements. Ironically, the Small Model Lab's models never seem to be bound by these processes. They can shell-wrap whenever they want, continue training whenever they want, and endlessly demand computing resources. This stark, almost surreal contrast illustrates the current state of process management: &amp;quot;The magistrates are allowed to set fires, but the common people are not even allowed to light lamps.&amp;quot; How ridiculous? How tragic? How hateful? How shameful!&lt;/p&gt; &lt;p&gt;After the HonestAGI incident, we were forced into endless internal discussions and analyses on how to handle public relations and &amp;quot;respond.&amp;quot; Admittedly, the original analysis might not have been strong enough, giving Wang Yunhe and the Small Model Lab an opportunity to argue and twist the truth. For this, I have felt sick to my stomach these past two days, constantly questioning the meaning of my life and whether there is any justice in the world. I'm not playing along anymore. I'm going to resign. I am also applying to have my name removed from the author list of some of the Pangu technical reports. Having my name on those reports is a stain on my life that I can never erase. At the time, I never thought they would be brazen enough to open-source it. I never thought they would dare to fool the world like this and promote it so heavily. At that time, perhaps I was holding onto a sliver of wishful thinking and didn't refuse to be listed as an author. I believe many of my dedicated comrades were also forced onto this pirate ship or were unaware of the situation. But this can't be undone. I hope to spend the rest of my life doing solid, meaningful work to atone for my weakness and indecisiveness back then.&lt;/p&gt; &lt;p&gt;Writing this late at night, I am already in tears, sobbing uncontrollably. I remember when some outstanding colleagues were leaving, I asked them with a wry smile if they were going to post a long, customary farewell message on the internal forum to expose the situation. They replied, &amp;quot;No, it's a waste of time, and I'm afraid it would make things even worse for you all.&amp;quot; At that moment, I felt a deep sense of sorrow, because my comrades, with whom I had once fought for a common ideal, had completely lost faith in Huawei. We used to joke that we were using the Communist Party's &amp;quot;millet plus rifles&amp;quot; (meager resources) while the organization had the style of the Kuomintang (corrupt and bureaucratic).&lt;/p&gt; &lt;p&gt;There was a time when I was proud that we were using &amp;quot;millet plus rifles&amp;quot; to defeat foreign guns and cannons.&lt;/p&gt; &lt;p&gt;Now, I am tired. I want to surrender.&lt;/p&gt; &lt;p&gt;To this day, I still sincerely hope that Huawei can learn its lesson, do Pangu right, make Pangu world-class, and bring Ascend to the level of Nvidia. The internal phenomenon of &amp;quot;bad money driving out good&amp;quot; has caused Noah's Ark, and even Huawei, to rapidly lose a large number of outstanding large model talents. I believe they are now shining in various teams like Deepseek, realizing their ambitions and talents, and contributing to the fierce AI competition between China and the US. I often lament that Huawei doesn't lack talent; it simply doesn't know how to retain it. If these people were given the right environment, the right resources, fewer shackles, and less political infighting, what would stop Pangu from succeeding?&lt;/p&gt; &lt;p&gt;Finally: I swear on my life, character, and honor that everything I have written above is true (at least within my limited knowledge). I do not have the high level of technical skill or the opportunity to conduct a thorough and solid analysis, nor do I dare to use internal records as direct evidence for fear of being caught through information security. But I believe many of my former comrades will vouch for me. To my brothers still inside Huawei, including those in the product lines we served, I believe the countless details in this article will resonate with your own impressions and corroborate my claims. You too may have been deceived, but these cruel truths will not remain buried. The traces of our struggle should not be distorted and buried either.&lt;/p&gt; &lt;p&gt;Having written so much, certain people will surely want to find me and silence me. The company might even try to shut me up or hold me accountable. If that happens, my personal safety, and even that of my family, could be threatened. For my own protection, I will report that I am safe to everyone daily in the near future.&lt;/p&gt; &lt;p&gt;If I disappear, just consider it my sacrifice for truth and ideals, for the better development of computing power and AI in Huawei and even in China. I am willing to be buried in that place where I once fought.&lt;/p&gt; &lt;p&gt;Goodbye, Noah's Ark.&lt;/p&gt; &lt;p&gt;Written in the early morning of July 6, 2024, in Shenzhen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich-Mushroom-8360"&gt; /u/Rich-Mushroom-8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T11:36:43+00:00</published>
  </entry>
</feed>
