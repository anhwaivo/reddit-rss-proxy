<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-05T04:24:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ii15ha</id>
    <title>Do Gigabyte AI TOP motherboards support Linux well?</title>
    <updated>2025-02-05T03:45:16+00:00</updated>
    <author>
      <name>/u/cinedog959</name>
      <uri>https://old.reddit.com/user/cinedog959</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sort of a tangential topic, but I figured I would ask here since this is the target audience of these motherboards. &lt;strong&gt;If you own one of these AI TOP motherboards can you confirm if they have good compatibility with Linux?&lt;/strong&gt; From what I see, I think the answer is a yes. But I'd love some confirmation from actual owners.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm planning a new Intel 285k build and was originally going to choose the ASUS ROG MAXIMUS Z890 EXTREME, but I saw a post indicating that the &lt;a href="https://rog-forum.asus.com/t5/gaming-motherboards/asus-z890-extreme-thunderbolt-5-issues-in-linux/td-p/1052427"&gt;Thunderbolt 5 ports on the board were not working under Linux&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;My next choice was the newly launched &lt;a href="https://www.gigabyte.com/Motherboard/Z890-AORUS-XTREME-AI-TOP"&gt;Z890 AORUS XTREME AI TOP&lt;/a&gt; motherboard.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.gigabyte.com/consumer/ai-top/"&gt;Gigabyte's advertising for AI TOP products&lt;/a&gt; always mentions their LLM tuning software, which runs on both Linux and Windows. I assume they wouldn't advertise Linux if they didn't at least test it on their hardware, right? My hope is that this is a roundabout way of saying their AI TOP products are designed to support Linux.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cinedog959"&gt; /u/cinedog959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii15ha/do_gigabyte_ai_top_motherboards_support_linux_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii15ha/do_gigabyte_ai_top_motherboards_support_linux_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii15ha/do_gigabyte_ai_top_motherboards_support_linux_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T03:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9h11</id>
    <title>Ok, you LLaMA-fobics, Claude does have a moat, and impressive one</title>
    <updated>2025-02-04T04:24:57+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you know me, you might know I eat local LLMs for breakfast, ever since the first Llama with its &amp;quot;I have a borked tokenizer, but I love you&amp;quot; vibes came about. So this isn't some uneducated guess.&lt;/p&gt; &lt;p&gt;A few days ago, I was doing some C++ coding and tried Claude, which was working shockingly well, until it wanted MoooOOOoooney. So I gave in, mid-code, just to see how far this would go.&lt;/p&gt; &lt;p&gt;Darn. Triple darn. Quadruple darn.&lt;/p&gt; &lt;p&gt;Here’s the skinny: No other model understands code with the shocking capabilities of Sonet 3.5. You can fight me on this, and I'll fight back.&lt;/p&gt; &lt;p&gt;This thing is insane. And I’m not just making some simple &amp;quot;snake game&amp;quot; stuff. I have 25 years of C++ under my belt, so when I need something, I need something I &lt;em&gt;actually&lt;/em&gt; struggle with.&lt;/p&gt; &lt;p&gt;There were so many instances where I felt this was Coding AI (and I’m &lt;em&gt;very&lt;/em&gt; cautious about calling token predictors AI), but it’s just &lt;em&gt;insane.&lt;/em&gt; In three days, I made a couple of classes that would have taken me months, and this thing chews through 10K-line classes like bubble gum.&lt;/p&gt; &lt;p&gt;Of course, I made it cry a few times when things didn’t work… and didn’t work… and &lt;em&gt;didn’t work.&lt;/em&gt; Then Claude wrote an entirely new set of code just to test the old code, and at the end we sorted it out.&lt;/p&gt; &lt;p&gt;A lot of my code was for visual components, so I’d describe what I saw on the screen. It was like programming over the phone, yet it still got things right!&lt;/p&gt; &lt;p&gt;Told it, &amp;quot;Add multithreading&amp;quot; boom. Done. Unique mutexes. Clean as a whistle.&lt;/p&gt; &lt;p&gt;Told it: &amp;quot;Add multiple undo and redo to this class: The simplest 5 minutes in my programming carrier - and I've been adding and struggling with undo/redo in my stuff many times. &lt;/p&gt; &lt;p&gt;The code it writes is &lt;em&gt;incredibly&lt;/em&gt; well-structured. I feel like a messy duck playing in the mud by comparison.&lt;/p&gt; &lt;p&gt;I realized a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It gives me the best solution when I &lt;em&gt;don’t&lt;/em&gt; over-explain (codexplain) how I &lt;em&gt;think&lt;/em&gt; the structure or flow should be. Instead, if I just let it do its thing and pretend I’m stupid, it works better.&lt;/li&gt; &lt;li&gt;Many times, it automatically adds things I &lt;em&gt;didn’t&lt;/em&gt; ask for, but would have ultimately needed, so it’s not just predicting tokens, it’s predicting my &lt;em&gt;next&lt;/em&gt; request.&lt;/li&gt; &lt;li&gt;More than once, it chose a future-proof, open-ended solution &lt;em&gt;as if&lt;/em&gt; it expected we’d be building on it further and I was pretty surprised later when I wanted to add something how ready the code was&lt;/li&gt; &lt;li&gt;It comprehends alien code like nothing else I’ve seen. Just throw in my mess.&lt;/li&gt; &lt;li&gt;When I was wrong and it was right, it didn't took my wrong stance, but explained to me where I might got my idea wrong, even pointing on a part of the code I probably overlooked - which was the EXACT reason why I was wrong. When model can keep it's cool without trying to please me all the time, it is something!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My previous best model for coding was Google Gemini 2, but in comparison, it feels confused for serious code, creating complex confused structure that didn't work anyway. .&lt;/p&gt; &lt;p&gt;I got my money’s worth in the first &lt;em&gt;ten minutes.&lt;/em&gt; The next 30.98 days? Just a bonus.&lt;/p&gt; &lt;p&gt;I’m saying this because while I &lt;em&gt;love&lt;/em&gt; Llama and I’m deep into the local LLM phase, this actually feels like magic. &lt;em&gt;So someone does thing s right, IMHO.&lt;/em&gt;&lt;br /&gt; Also, it is still next token predictor, that's even more impressive than if it actually reads the code.....&lt;/p&gt; &lt;p&gt;My biggest nightmare now: What if they take it away.... or &amp;quot;improve&amp;quot; it....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T04:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihv5fw</id>
    <title>Benchmarking Llama on Mobile using React Native and ExecuTorch</title>
    <updated>2025-02-04T22:56:52+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv5fw/benchmarking_llama_on_mobile_using_react_native/"&gt; &lt;img alt="Benchmarking Llama on Mobile using React Native and ExecuTorch" src="https://preview.redd.it/6ddov9nxc7he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fced593de4ed3f9d2aff6b43a9cb4b73c5aae69" title="Benchmarking Llama on Mobile using React Native and ExecuTorch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ddov9nxc7he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv5fw/benchmarking_llama_on_mobile_using_react_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv5fw/benchmarking_llama_on_mobile_using_react_native/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T22:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii0llc</id>
    <title>L3.3-Damascus-R1</title>
    <updated>2025-02-05T03:16:10+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all! This is an updated and rehualed version of Nevoria-R1 and OG Nevoria using community feedback on several different experimental models (Experiment-Model-Ver-A, L3.3-Exp-Nevoria-R1-70b-v0.1 and L3.3-Exp-Nevoria-70b-v0.1) with it i was able to dial in merge settings of a new merge method called SCE and the new model configuration.&lt;/p&gt; &lt;p&gt;This model utilized a completely custom base model this time around.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-Damascus-R1"&gt;https://huggingface.co/Steelskull/L3.3-Damascus-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-Steel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0llc/l33damascusr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0llc/l33damascusr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0llc/l33damascusr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T03:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpmg4</id>
    <title>Comparing different LLMs against different programming languages to see which are best for AI-driven coding</title>
    <updated>2025-02-04T19:09:11+00:00</updated>
    <author>
      <name>/u/terhechte</name>
      <uri>https://old.reddit.com/user/terhechte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"&gt; &lt;img alt="Comparing different LLMs against different programming languages to see which are best for AI-driven coding" src="https://external-preview.redd.it/f19UVEQKn2NUgogunr84spwvcNElFvuYeuIGFDIxA0k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e92484264a14897b17895cda1987bbd61718444e" title="Comparing different LLMs against different programming languages to see which are best for AI-driven coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terhechte"&gt; /u/terhechte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpvdm</id>
    <title>How does the use of 10 000 GPUs to train a model work?</title>
    <updated>2025-02-04T19:19:19+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the next state of the model depends on the previous state of the model by how gradient descent works, wouldn't you only need about 100 GPUs or so to train the largest models available? Like, you could only process the dataset serially, so how does hyperscaling of models work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihlx8q</id>
    <title>New "Kiwi" model on lmsys arena</title>
    <updated>2025-02-04T16:39:56+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like Grok-3 and Grok-3-mini to me...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihxypo</id>
    <title>Cheap, Effectual, Robot Arms Open Weight model, meet Pi0</title>
    <updated>2025-02-05T01:06:13+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.physicalintelligence.company/download/pi0.pdf"&gt;https://www.physicalintelligence.company/download/pi0.pdf&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/pi0"&gt;https://huggingface.co/blog/pi0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;open weights robotics VLM (SigLIP 400M + Gemma 2.6B at 50hz) &lt;/p&gt; &lt;p&gt;it can fold laundry, pair of robot arm can be made with BOM of less than $300&lt;/p&gt; &lt;p&gt;Build your robot arms:&lt;br /&gt; &lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;https://github.com/TheRobotStudio/SO-ARM100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T01:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih3nc6</id>
    <title>US Bill proposed to jail people who download Deepseek</title>
    <updated>2025-02-03T23:37:51+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt; &lt;img alt="US Bill proposed to jail people who download Deepseek" src="https://external-preview.redd.it/aUM4Zo5M60iArpLso3HHGaMmvhrgIEshjneVeh2Hvq4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4794828de090d8686e777cd2679cfb73fcb8c4f" title="US Bill proposed to jail people who download Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/senator-hawley-proposes-jail-time-for-people-who-download-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T23:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii0i9b</id>
    <title>Llama-3.3 and Qwen2.5 speed comparisons on a 4-GPU / 120GB VRAM system</title>
    <updated>2025-02-05T03:11:22+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a couple of speed tests with Llama-3.3 70B Instruct, Qwen-2.5 72B Instruct, and Qwen2.5 Coder 32B Instruct where I asked each of them to &amp;quot;write a flappy bird game in Python that will run on a MacBook&amp;quot;. For this test I didn't care about code quality or results, I just wanted the model to output approximately 1k tokens of code, for which the task of writing flappy bird is perfect. &lt;/p&gt; &lt;p&gt;The only data I was really interested in comparing was raw prompt processing speed and inference/generation speed. I figured some of the folks round here might be curious about the numbers, so here they are.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro M12SWA-TF motherboard&lt;/li&gt; &lt;li&gt;AMD Ryzen Threadripper Pro 3945WX&lt;/li&gt; &lt;li&gt;128GB DDR4 RAM&lt;/li&gt; &lt;li&gt;NVMe SSDs&lt;/li&gt; &lt;li&gt;1x EVGA RTX 3090 Ti 24GB&lt;/li&gt; &lt;li&gt;1x Pny RTX A6000 48GB&lt;/li&gt; &lt;li&gt;2x EVGA RTX 3090 FTW3 24GB&lt;/li&gt; &lt;li&gt;EVGA 2000W PSU running on dedicated 240V/20A 60Hz (USA)&lt;/li&gt; &lt;li&gt;All GPUs throttled at 250W&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Software setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu Linux&lt;/li&gt; &lt;li&gt;tabbyAPI / exllamav2 (8bpw exl2 quants unless otherwise noted)&lt;/li&gt; &lt;li&gt;tensor parallel enabled&lt;/li&gt; &lt;li&gt;speculative decoding (draft mode) enabled&lt;/li&gt; &lt;li&gt;context lengths for Llama and Qwen are empirically the ceiling of what I can fit in available VRAM (120GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLama-3.3 70B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Llama-3.2-3B-Instruct-exl2_8.obpw&lt;/li&gt; &lt;li&gt;Main Model: Llama-3.3-70B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 108,544 bytes&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 44.12 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 30.89 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 72B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-3B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-72B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 128,512&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 97.83 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 37.93 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 1.5B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-1.5B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 246.16 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 65.24 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-3B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 201.84 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 57.08 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I find it interesting that Qwen 72B was faster than Llama 70B by a whopping 7 tokens/sec despite each model using a 3B draft model, Qwen being 2B parameters larger, and Qwen having an extra 20k bytes of context. My guess is that the output of the smaller Qwen model is more closely matched to its larger counterpart than the Llama models, which therefore boosts the speed of speculative decoding... but I'm just pulling guesses out of my butt. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T03:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihuoym</id>
    <title>I made a program to let two LLM agents talk to each other</title>
    <updated>2025-02-04T22:37:11+00:00</updated>
    <author>
      <name>/u/-famiu-</name>
      <uri>https://old.reddit.com/user/-famiu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got really into local LLMs. Naturally, I wondered what would happen if I made two LLMs with different system prompts talk to each other. This eventually lead me to writing this project. I'm genuinely quite proud of where it is now. It's a CLI application, but I've worked quite hard on the UI.&lt;/p&gt; &lt;p&gt;Link to project: &lt;a href="https://github.com/famiu/llm_conversation"&gt;https://github.com/famiu/llm_conversation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to check it out. Give it a star if you like it. You can get some genuinely hilarious interactions with the right system prompts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-famiu-"&gt; /u/-famiu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihuoym/i_made_a_program_to_let_two_llm_agents_talk_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihuoym/i_made_a_program_to_let_two_llm_agents_talk_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihuoym/i_made_a_program_to_let_two_llm_agents_talk_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T22:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihvrb8</id>
    <title>New (Evil) Thinking Model: Skynet-3B</title>
    <updated>2025-02-04T23:23:42+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Today, we are releasing a new experimental model: &lt;strong&gt;Art-Skynet-3B&lt;/strong&gt;, fine-tuned on &lt;strong&gt;LLaMa 3.2 3B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This experiment explores developing models capable of reasoning like &lt;strong&gt;DeepSeek-r1&lt;/strong&gt; and &lt;strong&gt;OpenAI-o3&lt;/strong&gt;, with a long-term goal of &lt;em&gt;world domination&lt;/em&gt; (as a test, of course 😉).&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Model card:&lt;/strong&gt; &lt;a href="https://huggingface.co/AGI-0/Art-Skynet-3B"&gt;https://huggingface.co/AGI-0/Art-Skynet-3B&lt;/a&gt; &lt;em&gt;(Leave a like on the repo if you enjoy this model!)&lt;/em&gt;&lt;br /&gt; 🔹 &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;Note: If the model doesn’t seem to work, try regenerating the answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T23:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihm8pl</id>
    <title>Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!</title>
    <updated>2025-02-04T16:52:46+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt; &lt;img alt="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" src="https://external-preview.redd.it/Isnr897tQLSa3f7knpDj7eFO6fjkOWORPvRfD442vlo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24f28b0bdfd72abaef01b7d591bc2878895b848b" title="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Anubis-Pro-105B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihrmf3</id>
    <title>Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction</title>
    <updated>2025-02-04T20:30:27+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"&gt; &lt;img alt="Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction" src="https://external-preview.redd.it/rYnB4HiKhgUDfmglDGVR2RBw6zihcpf49RKEaD8Ygz0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df76ccdc3d22de19b2ce8c76b6e1ee50cc8b2d3e" title="Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lolzinventor/Llama-3.1-8B-BeyondReality"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf0gb</id>
    <title>DeepSeek-R1's correct answers are generally shorter</title>
    <updated>2025-02-04T10:49:48+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt; &lt;img alt="DeepSeek-R1's correct answers are generally shorter" src="https://preview.redd.it/duiwqfpzq3he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29a0abef7ab5410cdc5f56f319bd47c9d15c366b" title="DeepSeek-R1's correct answers are generally shorter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/duiwqfpzq3he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T10:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpzn2</id>
    <title>Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers</title>
    <updated>2025-02-04T19:24:03+00:00</updated>
    <author>
      <name>/u/thedudear</name>
      <uri>https://old.reddit.com/user/thedudear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt; &lt;img alt="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" src="https://b.thumbs.redditmedia.com/MEP1WTwX54qNwJ5gIxf_f4NzTdNmrqOfmjo_k4cVUrg.jpg" title="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I decided that three RTX 3090s janked together with brackets and risers just wasn’t enough; I wanted a cleaner setup and a fourth 3090. To make that happen, I needed a new platform.&lt;/p&gt; &lt;p&gt;My requirements were: at least four double-spaced PCIe x16 slots, ample high-speed storage interfaces, and ideally, high memory bandwidth to enable some level of CPU offloading without tanking inference speed. Intel’s new Xeon lineup didn’t appeal to me, the P/E core setup seems more geared towards datacenters, and the pricing was brutal. Initially, I considered Epyc Genoa, but with the launch of Turin and its Zen 5 cores plus higher DDR5 speeds, I decided to go straight for it.&lt;/p&gt; &lt;p&gt;Due to the size of the SP5 socket and its 12 memory channels, boards with full 12-channel support sacrifice PCIe slots. The only board that meets my PCIe requirements, the ASRock GENOAD8X-2T/TCM, has just 8 DIMM slots, meaning we have to say goodbye to four whole memory channels.&lt;/p&gt; &lt;p&gt;Getting it up and running was an adventure. At the time, ASRock hadn’t released any Turin-compatible BIOS ROMs, despite claiming that an update to 10.03 was required (which wasn’t even available for download). The beta ROM they supplied refused to flash, failing with no discernible reason. Eventually, I had to resort to a ROM programmer (CH341a) and got it running on version 10.05.&lt;/p&gt; &lt;p&gt;If anyone has questions about the board, BIOS, or setup, feel free to ask, I’ve gotten way more familiar with this board than I ever intended to.&lt;/p&gt; &lt;p&gt;CPU: Epyc Turin 9355P - 32 Cores (8 CCD), 256 MB cache, 3.55 GHz Boosting 4.4 GHz - $3000 USD from cafe.electronics on Ebay (now ~$3300 USD).&lt;/p&gt; &lt;p&gt;RAM: 256 GB Corsair WS (CMA256GX5M8B5600C40) @ 5600 MHz - $1499 CAD (now ~$2400 - WTF!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.asrockrack.com/general/productdetail.asp?Model=GENOAD8X-2T/BCM#Specifications"&gt;Asrock GENOAD8X-2T/TCM Motherboard&lt;/a&gt; - ~$1500 CAD but going up in price&lt;/p&gt; &lt;p&gt;First off, a couple of benchmarks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fag5favty5he1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a6b92917f908dedbe73201fc6fc48e820aa3a5"&gt;Passmark Memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p8e60vy946he1.png?width=879&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b08b8cc914a890e567b0e7aeb5f9e42251e855b9"&gt;Passmark CPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/slq3s3ub46he1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2f6711ae24b230edef6eeea872c229a293518be"&gt;CPU-Z Info Page - The chip seems to always be boosting to 4.4 GHz, which I don't mind. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekz7wf2d46he1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5112a56f91feb7ae1ea8bc946b5603e52a3ecb59"&gt;CPU-Z Bench - My i9 9820x would score ~7k @ 4.6 GHz. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;And finally some LMStudio (0 layers offloaded) tests:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/on0n624n66he1.png?width=340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d96479be841451a073caff569adb52d2e9387a00"&gt;Prompt: \&amp;quot;Write a 1000 word story about france's capital\&amp;quot; Llama-3.3-70B-Q8, 24 Threads. Model used 72 GB in RAM. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/je5ljie976he1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=809d046e8b19f1cdd903e09135bba50b734fae0f"&gt;Deepseek-R1-Distill-Llama-8B (Q8), 24 threads, 8.55 GB in memory. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm happy to run additional tests and benchmarks—just wanted to put this out there so people have the info and can weigh in on what they'd like to see. CPU inference is very usable for smaller models (&amp;lt;20B), while larger ones are still best left to GPUs/cloud (not that we didn’t already know this).&lt;/p&gt; &lt;p&gt;That said, we’re on a promising trajectory. With a 12-DIMM board (e.g., Supermicro H13-SSL) or a dual-socket setup (pending improvements in multi-socket inference), we could, within a year or two, see CPU inference becoming cost-competitive with GPUs on a per-GB-of-memory basis. Genoa chips have dropped significantly in price over the past six months—9654 (96-core) now sells for $2,500–$3,000—making this even more feasible.&lt;/p&gt; &lt;p&gt;I'm optimistic about continued development in CPU inference frameworks, as they could help alleviate the current bottleneck: VRAM and Nvidia’s AI hardware monopoly. My main issue is that for pure inference, GPU compute power is vastly underutilized—memory capacity and bandwidth are the real constraints. Yet consumers are forced to pay thousands for increasingly powerful GPUs when, for inference alone, that power is often unnecessary. Here’s hoping CPU inference keeps progressing!&lt;/p&gt; &lt;p&gt;Anyways, let me know your thoughts, and i'll do what I can to provide additional info.&lt;/p&gt; &lt;p&gt;Added:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zwvjz8nps6he1.png?width=946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1fb93ebb3d182906b528370fd2c17de20796b41"&gt;Likwid-Bench: 334 GB/s (likwid-bench -t load -i 128 -w M0:8GB)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek-R1-GGUF-IQ1_S:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;With Hyper V / SVM Disabled: }, &amp;quot;stats&amp;quot;: { &amp;quot;stopReason&amp;quot;: &amp;quot;eosFound&amp;quot;, &amp;quot;tokensPerSecond&amp;quot;: 6.620692403810844, &amp;quot;numGpuLayers&amp;quot;: -1, &amp;quot;timeToFirstTokenSec&amp;quot;: 1.084, &amp;quot;promptTokensCount&amp;quot;: 12, &amp;quot;predictedTokensCount&amp;quot;: 303, &amp;quot;totalTokensCount&amp;quot;: 315 } { &amp;quot;indexedModelIdentifier&amp;quot;: &amp;quot;unsloth/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf&amp;quot;, &amp;quot;identifier&amp;quot;: &amp;quot;deepseek-r1&amp;quot;, &amp;quot;loadModelConfig&amp;quot;: { &amp;quot;fields&amp;quot;: [ { &amp;quot;key&amp;quot;: &amp;quot;llm.load.llama.cpuThreadPoolSize&amp;quot;, &amp;quot;value&amp;quot;: 60 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.contextLength&amp;quot;, &amp;quot;value&amp;quot;: 4096 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.numExperts&amp;quot;, &amp;quot;value&amp;quot;: 24 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.llama.acceleration.offloadRatio&amp;quot;, &amp;quot;value&amp;quot;: 0 } ] .... }, &amp;quot;useTools&amp;quot;: false } }, &amp;quot;stopStrings&amp;quot;: [] } }, { &amp;quot;key&amp;quot;: &amp;quot;llm.prediction.llama.cpuThreads&amp;quot;, &amp;quot;value&amp;quot;: 30 } ] }, &amp;quot;stats&amp;quot;: { &amp;quot;stopReason&amp;quot;: &amp;quot;eosFound&amp;quot;, &amp;quot;tokensPerSecond&amp;quot;: 5.173145579251154, &amp;quot;numGpuLayers&amp;quot;: -1, &amp;quot;timeToFirstTokenSec&amp;quot;: 1.149, &amp;quot;promptTokensCount&amp;quot;: 12, &amp;quot;predictedTokensCount&amp;quot;: 326, &amp;quot;totalTokensCount&amp;quot;: 338 } } --- Disabled Hyper V, got much better numbers, see above --- &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedudear"&gt; /u/thedudear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihv2lz</id>
    <title>AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch</title>
    <updated>2025-02-04T22:53:21+00:00</updated>
    <author>
      <name>/u/Darkstar4125</name>
      <uri>https://old.reddit.com/user/Darkstar4125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt; &lt;img alt="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" src="https://external-preview.redd.it/3Nw1ySi8I9t9qNGxMrnWh6X9nJQqmR4w8i1-IlVJwSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa55aebd954efb86ff5114a964670c3f21850a30" title="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkstar4125"&gt; /u/Darkstar4125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/02/02/ai-systems-with-unacceptable-risk-are-now-banned-in-the-eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T22:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihyutf</id>
    <title>Open Euro LLM launches</title>
    <updated>2025-02-05T01:49:49+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openeurollm.eu/launch-press-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T01:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihd0rr</id>
    <title>Deepseek researcher says it only took 2-3 weeks to train R1&amp;R1-Zero</title>
    <updated>2025-02-04T08:18:16+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt; &lt;img alt="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" src="https://b.thumbs.redditmedia.com/btJal_QKjmZuASeULU0hjpBHdbEWu1y_eBgzmjTEd1U.jpg" title="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ihd0rr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T08:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpdjh</id>
    <title>What to expect from Mistral's upcoming reasoning models?</title>
    <updated>2025-02-04T18:59:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt; &lt;img alt="What to expect from Mistral's upcoming reasoning models?" src="https://preview.redd.it/sa87uqtg66he1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4e05d3454d29e6d5755deb360afc707a7f84aa" title="What to expect from Mistral's upcoming reasoning models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sa87uqtg66he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T18:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihh15n</id>
    <title>Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination</title>
    <updated>2025-02-04T12:57:18+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt; &lt;img alt="Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination" src="https://b.thumbs.redditmedia.com/EnNfi2ijJYZ4t9ufiyUC2FCEhoi78PEnlcIcOHPzAtE.jpg" title="Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0"&gt;https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/"&gt;https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T12:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihxbpe</id>
    <title>DeepSeek banned from Australian Government Devices</title>
    <updated>2025-02-05T00:36:23+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt; &lt;img alt="DeepSeek banned from Australian Government Devices" src="https://external-preview.redd.it/UT58snSyDCf1en1PY0Usy8X0-HIt9R0OcuYrukJSr_0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f710938759a6b3631d87cc438df355ecc2726248" title="DeepSeek banned from Australian Government Devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.abc.net.au/news/2025-02-04/deepseek-banned-from-federal-government-devices/104896770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T00:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihtpa2</id>
    <title>I just want to thank all organisations that did not stop open sourcing their results</title>
    <updated>2025-02-04T21:55:28+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a moment, I feared that entities like ClosedAI and Anthropic might alter the open-source paradigm in the realm of Machine Learning. Fortunately, it appears they have not succeeded, and the open-source community has emerged victorious. While the battle is far from over, and we may need to fight even harder, this initial triumph belongs to open source, to all of us.&lt;/p&gt; &lt;p&gt;Let's extend our gratitude to every organization, large and small, that has shared their models, papers, and code with the community. This collaborative spirit is essential for democratizing AI and achieving Artificial General Intelligence (AGI) collectively. By ensuring that the benefits of AI are accessible to all, rather than being monopolized by a few egomaniacs, we foster a more equitable future.&lt;/p&gt; &lt;p&gt;Let us continue to promote open-source initiatives and leave behind those who resist the democratization of AI. By embracing transparency and collaboration, we can build a future where AI serves the interests of all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T21:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqwnd</id>
    <title>OpenAI deep research but it's open source</title>
    <updated>2025-02-04T20:01:31+00:00</updated>
    <author>
      <name>/u/Thomjazz</name>
      <uri>https://old.reddit.com/user/Thomjazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt; &lt;img alt="OpenAI deep research but it's open source" src="https://external-preview.redd.it/VhiwZJj7J5TUIfA6ujpiUeYD8CI4AKINeo7sLJZlD5Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e05cfb506bcac565f349480a4b0d9ba18f4768b1" title="OpenAI deep research but it's open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46"&gt;https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thomjazz"&gt; /u/Thomjazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihph9f</id>
    <title>In case you thought your feedback was not being heard</title>
    <updated>2025-02-04T19:03:04+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt; &lt;img alt="In case you thought your feedback was not being heard" src="https://preview.redd.it/nvf2f1j876he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8871a5964dc8f2db918ba181e1157fc626b71" title="In case you thought your feedback was not being heard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nvf2f1j876he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:03:04+00:00</published>
  </entry>
</feed>
