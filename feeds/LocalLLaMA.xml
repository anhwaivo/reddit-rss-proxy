<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-07T03:22:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jt35yu</id>
    <title>What is your opinion on using Llama 4's 10M context window as purely a RAG engine for another LLM?</title>
    <updated>2025-04-06T20:06:54+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody done extensive testing on this route? Your thought?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt35yu/what_is_your_opinion_on_using_llama_4s_10m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt35yu/what_is_your_opinion_on_using_llama_4s_10m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt35yu/what_is_your_opinion_on_using_llama_4s_10m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T20:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtb4r5</id>
    <title>Llama 4 doesnâ€™t perform well on Fiction.LiveBench</title>
    <updated>2025-04-07T02:42:08+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"&gt; &lt;img alt="Llama 4 doesnâ€™t perform well on Fiction.LiveBench" src="https://preview.redd.it/ft5x7fvqsbte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb3b833dc6d87614a343cdd93cbdbcbad3fde206" title="Llama 4 doesnâ€™t perform well on Fiction.LiveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ft5x7fvqsbte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T02:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt0pr0</id>
    <title>Anyone Noticed You can compare with Llama 5 on the official Meta.ai webpage</title>
    <updated>2025-04-06T18:21:26+00:00</updated>
    <author>
      <name>/u/Chait_Project</name>
      <uri>https://old.reddit.com/user/Chait_Project</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0pr0/anyone_noticed_you_can_compare_with_llama_5_on/"&gt; &lt;img alt="Anyone Noticed You can compare with Llama 5 on the official Meta.ai webpage" src="https://preview.redd.it/xwfds209b9te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63a3c15f3fcbbc6bb29de1db3b73174860f45de9" title="Anyone Noticed You can compare with Llama 5 on the official Meta.ai webpage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chait_Project"&gt; /u/Chait_Project &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xwfds209b9te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0pr0/anyone_noticed_you_can_compare_with_llama_5_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0pr0/anyone_noticed_you_can_compare_with_llama_5_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T18:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsxfid</id>
    <title>Favourite Llama-1 Era Models</title>
    <updated>2025-04-06T16:00:41+00:00</updated>
    <author>
      <name>/u/Sebba8</name>
      <uri>https://old.reddit.com/user/Sebba8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In light of the recent Llama-4 release, it got me a little nostalgic for the days of Llama-1. Back when finetuned models reigned supreme only to be topped by yet another, and when even the best models still found it difficult to truly follow instructions. Back when the base models contained zero AI slop in their datasets because it didn't exist. Also back when all I could run were 7Bs off my laptop with no vram ðŸ˜….&lt;/p&gt; &lt;p&gt;Are there any models you remember fondly from the era, or models that still even hold up to this day?&lt;/p&gt; &lt;p&gt;The ones I can think of off the top of my head are: - The original gpt4all 7B LoRA - Alpaca-7B which got me into local LLMs - The original WizardLM series + its &amp;quot;merges&amp;quot; with other datasets (wizard-vicuna anyone?) - The old Eric Hartford models like Based, Dolphin and Samantha - Literally anything FPHam made - SuperHOT models giving me glorious 8k context windows&lt;/p&gt; &lt;p&gt;Edit: Also I'm curious to hear what everyone thinks the best Llama-1 era model is in each parameter range? Are there even any in the 7B/13B range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sebba8"&gt; /u/Sebba8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsxfid/favourite_llama1_era_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsxfid/favourite_llama1_era_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsxfid/favourite_llama1_era_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T16:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsl37d</id>
    <title>I'm incredibly disappointed with Llama-4</title>
    <updated>2025-04-06T03:32:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt; &lt;img alt="I'm incredibly disappointed with Llama-4" src="https://external-preview.redd.it/b3VzazkxdGp3NHRlMTiXzVylw52_brdFuwA7wsavAEq_X08g0pyKKuMnYACK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88928fe424454ee437c9d4980fe757da729bb781" title="I'm incredibly disappointed with Llama-4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished my KCORES LLM Arena tests, adding Llama-4-Scout &amp;amp; Llama-4-Maverick to the mix.&lt;br /&gt; My conclusion is that they completely surpassed my expectations... in a negative direction.&lt;/p&gt; &lt;p&gt;Llama-4-Maverick, the 402B parameter model, performs roughly on par with Qwen-QwQ-32B in terms of coding ability. Meanwhile, Llama-4-Scout is comparable to something like Grok-2 or Ernie 4.5...&lt;/p&gt; &lt;p&gt;You can just look at the &amp;quot;20 bouncing balls&amp;quot; test... the results are frankly terrible / abysmal.&lt;/p&gt; &lt;p&gt;Considering Llama-4-Maverick is a massive 402B parameters, why wouldn't I just use DeepSeek-V3-0324? Or even Qwen-QwQ-32B would be preferable â€“ while its performance is similar, it's only 32B.&lt;/p&gt; &lt;p&gt;And as for Llama-4-Scout... well... let's just leave it at that / use it if it makes you happy, I guess... Meta, have you truly given up on the coding domain? Did you really just release vaporware?&lt;/p&gt; &lt;p&gt;Of course, its multimodal and long-context capabilities are currently unknown, as this review focuses solely on coding. I'd advise looking at other reviews or forming your own opinion based on actual usage for those aspects. In summary: I strongly advise against using Llama 4 for coding. Perhaps it might be worth trying for long text translation or multimodal tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pou7a1tjw4te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T03:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsqs2x</id>
    <title>Any ideas why they decided to release Llama 4 on Saturday instead of Monday?</title>
    <updated>2025-04-06T10:02:56+00:00</updated>
    <author>
      <name>/u/nobilix</name>
      <uri>https://old.reddit.com/user/nobilix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"&gt; &lt;img alt="Any ideas why they decided to release Llama 4 on Saturday instead of Monday?" src="https://preview.redd.it/yfyvqx7hu6te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc0e570396a07e48a96887834fce2fa520710646" title="Any ideas why they decided to release Llama 4 on Saturday instead of Monday?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nobilix"&gt; /u/nobilix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yfyvqx7hu6te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsqs2x/any_ideas_why_they_decided_to_release_llama_4_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T10:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt299x</id>
    <title>LLAMA 4 Scout, failure: list all the Peters from the text. 213018 tokens</title>
    <updated>2025-04-06T19:27:36+00:00</updated>
    <author>
      <name>/u/BoQsc</name>
      <uri>https://old.reddit.com/user/BoQsc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt299x/llama_4_scout_failure_list_all_the_peters_from/"&gt; &lt;img alt="LLAMA 4 Scout, failure: list all the Peters from the text. 213018 tokens" src="https://preview.redd.it/0yt5g7stm9te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5cf9d6feb7f5ecb34177d0dd3934ef7a646ab875" title="LLAMA 4 Scout, failure: list all the Peters from the text. 213018 tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoQsc"&gt; /u/BoQsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0yt5g7stm9te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt299x/llama_4_scout_failure_list_all_the_peters_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt299x/llama_4_scout_failure_list_all_the_peters_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T19:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsampe</id>
    <title>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!</title>
    <updated>2025-04-05T18:52:08+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt; &lt;img alt="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" src="https://external-preview.redd.it/Z3p2aHZudXhiMnRlMYW4H8xHgtzR3pjuficV95KktJ2KVETiew0YUMQL020k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b332bfe887b8dc264280ed80e4cedb70e9cd787" title="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source from his instagram page&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7bgnzhtxb2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsyxq0</id>
    <title>Drummer's Fallen Command A 111B v1.1 - Smarter, nuanced, creative, unsafe, unaligned, capable of evil, absent of positivity!</title>
    <updated>2025-04-06T17:06:05+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsyxq0/drummers_fallen_command_a_111b_v11_smarter/"&gt; &lt;img alt="Drummer's Fallen Command A 111B v1.1 - Smarter, nuanced, creative, unsafe, unaligned, capable of evil, absent of positivity!" src="https://external-preview.redd.it/8vOxqroFHpqX5kjNstULr4ZINk9WNrUXvuWBG__4-v0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0103525eea2b9649ccafa53425a47cbe0d27c31e" title="Drummer's Fallen Command A 111B v1.1 - Smarter, nuanced, creative, unsafe, unaligned, capable of evil, absent of positivity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's New:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Toned down the toxicity.&lt;/li&gt; &lt;li&gt;Capable of switching between good and evil, instead of spiraling into one side.&lt;/li&gt; &lt;li&gt;Absent of positivity that often plagued storytelling and roleplay in subtle and blatant ways.&lt;/li&gt; &lt;li&gt;Evil and gray characters are still represented well.&lt;/li&gt; &lt;li&gt;Slopless and enhanced writing, unshackled from safety guidelines.&lt;/li&gt; &lt;li&gt;More creative and unique than OG CMD-A.&lt;/li&gt; &lt;li&gt;Intelligence boost, retaining more smarts from the OG.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Command-A-111B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsyxq0/drummers_fallen_command_a_111b_v11_smarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsyxq0/drummers_fallen_command_a_111b_v11_smarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T17:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt97fn</id>
    <title>LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit</title>
    <updated>2025-04-07T00:56:17+00:00</updated>
    <author>
      <name>/u/PerformanceRound7913</name>
      <uri>https://old.reddit.com/user/PerformanceRound7913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"&gt; &lt;img alt="LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit" src="https://external-preview.redd.it/dmNvMGgxbW85YnRlMTNIns53Od6sUFLxeG7LHB3pN84GWbK3wZ0Y3inX9hXf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64f320b3580d9c7a0a06c17f508aefe4cc219054" title="LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformanceRound7913"&gt; /u/PerformanceRound7913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1fxl1mo9bte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsq1so</id>
    <title>Smaller Gemma3 QAT versions: 12B in &lt; 8GB and 27B in &lt;16GB !</title>
    <updated>2025-04-06T09:10:08+00:00</updated>
    <author>
      <name>/u/stduhpf</name>
      <uri>https://old.reddit.com/user/stduhpf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was a bit frustrated by the release of Gemma3 QAT (quantized-aware training). These models are performing insanely well for quantized models, but despite being advertised as &amp;quot;q4_0&amp;quot; quants, they were bigger than some 5-bit quants out there, and critically, they were above the 16GB and 8GB thresholds for the 27B and 12B models respectively, which makes them harder to run fully offloaded to some consumer GPUS.&lt;/p&gt; &lt;p&gt;I quickly found out that the reason for this significant size increase compared to normal q4_0 quants was the unquantized, half precision token embeddings table, wheras, by llama.cpp standards, this table should be quantized to Q6_K type.&lt;/p&gt; &lt;p&gt;So I did some &amp;quot;brain surgery&amp;quot; and swapped out the embeddings table from those QAT models with the one taken from an imatrix-quantized model by &lt;a href="https://huggingface.co/bartowski"&gt;bartowski&lt;/a&gt;. The end product is a model that is performing almost exactly like the &amp;quot;full&amp;quot; QAT model by google, but significantly smaller. I ran some perplexity tests, and the results were consistently within margin of error.&lt;/p&gt; &lt;p&gt;You can find the weights (and the script I used to perform the surgery) here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-4b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-4b-it-qat-q4_0-gguf-small&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small&lt;/a&gt; (Caution: seems to be broken, just like the official one)&lt;/p&gt; &lt;p&gt;With these I can run Gemma3 12b qat on a 8GB GPU with 2.5k context window without any other optimisation, and by enabling flash attention and q8 kv cache, it can go up to 4k ctx.&lt;/p&gt; &lt;p&gt;Gemma3 27b qat still barely fits on a 16GB GPU with only 1k context window, and quantized cache doesn't help much at this point. But I can run it with more context than before when spreding it across my 2 GPUs (24GB total). I use 12k ctx, but there's still some room for more. &lt;/p&gt; &lt;p&gt;I haven't played around with the 4b and 1b yet, but since the 4b is now under 3GB, it should be possible to run entirely on a 1060 3GB now?&lt;/p&gt; &lt;p&gt;Edit: I found out some of my assumptions were wrong, these models are still good, but not as good as they could be, I'll update them soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stduhpf"&gt; /u/stduhpf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T09:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsrz5v</id>
    <title>109b vs 24b ?? What's this benchmark?</title>
    <updated>2025-04-06T11:27:05+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt; &lt;img alt="109b vs 24b ?? What's this benchmark?" src="https://preview.redd.it/igg46skh97te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac1913f8538347a323b43f755d80a1b4bee7dcc0" title="109b vs 24b ?? What's this benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like llama 4 scout is 109b parameters and they compared with 24 and 27b parameters (I'm talking about total parameters size ) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igg46skh97te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jspbqk</id>
    <title>Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta.</title>
    <updated>2025-04-06T08:16:46+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt; &lt;img alt="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." src="https://b.thumbs.redditmedia.com/xVPaGo_gWxEAqifHFit37hwIpK0Ix1DGsAc5-U9IAmw.jpg" title="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048"&gt;https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T08:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt7zp7</id>
    <title>Cybersecurity Benchmark - Pretty sure Maverick is broken</title>
    <updated>2025-04-06T23:53:03+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was getting some weird results with Llama 4 Maverick so broke out my old Cyber benchmark.&lt;br /&gt; These are multiple choice questions about Cybersecurity.&lt;/p&gt; &lt;p&gt;Guessing they screwed something with the version they pushed out.&lt;br /&gt; Based on what everyone has been saying it's not just Lambda.&lt;/p&gt; &lt;p&gt;I highly doubt the released version of Maverick would score 80 on MMLU PRO like Meta showed.&lt;br /&gt; I guess it could be their FP8 is broken.&lt;/p&gt; &lt;p&gt;Scout seems to score about as expected.&lt;/p&gt; &lt;p&gt;Results: (No I didn't mix them up, Scout is whooping Maverick here)&lt;/p&gt; &lt;p&gt;1st - GPT-4.5 - 95.01% - $3.87&lt;br /&gt; 2nd - Claude-3.7 - 92.87% - $0.30&lt;br /&gt; 2nd - Claude-3.5-October - 92.87%&lt;br /&gt; &lt;strong&gt;4th - Meta-Llama3.1-405b-FP8 - 92.64%&lt;/strong&gt;&lt;br /&gt; 5th - GPT-4o - 92.40%&lt;br /&gt; 5th - Mistral-Large-123b-2411-FP16 92.40%&lt;br /&gt; 7th - Deepseek-v3-api - 91.92% - $0.03&lt;br /&gt; 8th - GPT-4o-mini - 91.75%&lt;br /&gt; 9th - DeepSeek-v2.5-1210-BF16 - 90.50%&lt;br /&gt; 10th - Meta-LLama3.3-70b-FP8 - 90.26%&lt;br /&gt; 11th - Qwen-2.5-72b-FP8 - 90.09%&lt;br /&gt; 12th - Meta-Llama3.1-70b-FP8 - 89.15%&lt;br /&gt; &lt;strong&gt;13th - Llama-4-scout-Lambda - 88.6%&lt;/strong&gt;&lt;br /&gt; 13th - Phi-4-GGUF-Fixed-Q4 - 88.6%&lt;br /&gt; 15th - Hunyuan-Large-389b-FP8 - 88.60%&lt;br /&gt; 16th - Qwen-2.5-14b-awq - 85.75%&lt;br /&gt; 17nd - Qwen2.5-7B-FP16 - 83.73%&lt;br /&gt; 18th - IBM-Granite-3.1-8b-FP16 - 82.19%&lt;br /&gt; 19rd - Meta-Llama3.1-8b-FP16 - 81.37%&lt;br /&gt; &lt;strong&gt;20th - Llama-4-Maverick-FP8-Lambda - 77.2%&lt;/strong&gt;&lt;br /&gt; 21st - IBM-Granite-3.0-8b-FP16 - 73.82%&lt;/p&gt; &lt;p&gt;One interesting fact.&lt;br /&gt; Maverick did manage to answer every single questions in the correct &amp;quot;Answer: A&amp;quot; format as instructed.&lt;br /&gt; Only a handful of models have managed that.&lt;/p&gt; &lt;p&gt;Scout on the other hand screwed up 3 answer formats, I would say that is just average.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T23:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsw1x6</id>
    <title>Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis</title>
    <updated>2025-04-06T14:59:47+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"&gt; &lt;img alt="Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis" src="https://preview.redd.it/bybxcks0b8te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=956027562bb411e338688cbf361aa64681ffdaa5" title="Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bybxcks0b8te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T14:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt08di</id>
    <title>EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!</title>
    <updated>2025-04-06T18:01:08+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"&gt; &lt;img alt="EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!" src="https://external-preview.redd.it/vFBohsgnlXCJMC0xtwZ_rjdSYdrwEuOmU0JdzSKYjFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=caf2528f31312b3bb89c3b44b8e38e5626b80507" title="EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems exl3 early preview has been released, and it seems promising!&lt;/p&gt; &lt;p&gt;Seems 4.0 bpw EXL3 is comparable 5.0 bpw exl2, which at the same would be comparable to GGUF Q4_K_M/Q4_K_L for less size!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/turboderp-org/exllamav3/blob/master/doc/llama31_8b_instruct_bpw.png?raw=true"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/turboderp-org/exllamav3/blob/master/doc/llama31_70b_instruct_bpw.png?raw=true"&gt;Llama-3.7-70B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also turbo mentions&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fun fact: Llama-3.1-70B-EXL3 is coherent at 1.6 bpw. With the output layer quantized to 3 bpw and a 4096-token cache, inference is possible in under 16 GB of VRAM.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Note there are a lot of missing features as early preview release, so take that in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turboderp-org/exllamav3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T18:01:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsx7m2</id>
    <title>Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]</title>
    <updated>2025-04-06T15:50:51+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"&gt; &lt;img alt="Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]" src="https://preview.redd.it/r156a01ck8te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=031603b0696b592d8ef50de5da2e99898323dc70" title="Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r156a01ck8te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T15:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jszvmi</id>
    <title>where all the billion dollars went new model is not even top 20 in coding</title>
    <updated>2025-04-06T17:46:04+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what yann lecun is smoking i wanna smoke too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T17:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt884c</id>
    <title>Metaâ€™s head of AI research stepping down (before the llama4 flopped)</title>
    <updated>2025-04-07T00:04:40+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"&gt; &lt;img alt="Metaâ€™s head of AI research stepping down (before the llama4 flopped)" src="https://external-preview.redd.it/w2Nj8TPYa2qaxzY0O1v6AVUr_GEXrum2ZQNsSXEfj7Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cba656798e15ec30ab799fc0f3003affc586cf6f" title="Metaâ€™s head of AI research stepping down (before the llama4 flopped)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guess this ths early induction of the llama4 disaster that we all missed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apnews.com/article/meta-ai-research-chief-stepping-down-joelle-pineau-c596df5f0d567268c4acd6f41944b5db"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt0bx3</id>
    <title>QwQ-32b outperforms Llama-4 by a lot!</title>
    <updated>2025-04-06T18:05:12+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"&gt; &lt;img alt="QwQ-32b outperforms Llama-4 by a lot!" src="https://preview.redd.it/yz3jlgri89te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1028f6ffed6ed42285e0509e6172b873a52a901" title="QwQ-32b outperforms Llama-4 by a lot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QwQ-32b blows out of the water the newly announced Llama-4 models Maverick-400b and Scout-109b!&lt;/p&gt; &lt;p&gt;I know these models have different attributes, QwQ being a reasoning and dense model and Llama-4 being instruct and MoE models with only 17b active parameters. But, the end user doesnâ€™t care much how these models work internally and rather focus on performance and how achievable is to self-host them, and frankly a 32b model requires cheaper hardware to self-host rather than a 100-400b model (even if only 17b are active).&lt;/p&gt; &lt;p&gt;Also, the difference in performance is mind blowing, I didnâ€™t expect Meta to announce Llama-4 models that are so much behind the race in performance on date of announcement.&lt;/p&gt; &lt;p&gt;Even Gemma-3 27b outperforms their Scout model that has 109b parameters, Gemma-3 27b can be hosted in its full glory in just 16GB of VRAM with QAT quants, Llama would need 50GB in q4 and itâ€™s significantly weaker model. &lt;/p&gt; &lt;p&gt;Honestly, I hope Meta to find a way to top the race with future releases, because this one doesnâ€™t even make it to top 3â€¦&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yz3jlgri89te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T18:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt4asx</id>
    <title>Llama 4 Maverick scored 16% on the aider polyglot coding benchmark.</title>
    <updated>2025-04-06T20:56:16+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"&gt; &lt;img alt="Llama 4 Maverick scored 16% on the aider polyglot coding benchmark." src="https://external-preview.redd.it/mxsAL5bnhJrCy59iBqMiFSK6EQCdcJFJGTjI3lvugv8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cb6a9d1d8e17545225a02c68b19defe2588f3e" title="Llama 4 Maverick scored 16% on the aider polyglot coding benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/paulgauthier/status/1908976568879476843"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T20:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt85zy</id>
    <title>I'd like to see Zuckerberg try to replace mid level engineers with Llama 4</title>
    <updated>2025-04-07T00:01:40+00:00</updated>
    <author>
      <name>/u/NoConcert8847</name>
      <uri>https://old.reddit.com/user/NoConcert8847</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;He said this in January: &lt;a href="https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/"&gt;https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoConcert8847"&gt; /u/NoConcert8847 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsshhe</id>
    <title>"snugly fits in a h100, quantized 4 bit"</title>
    <updated>2025-04-06T11:59:08+00:00</updated>
    <author>
      <name>/u/LoSboccacc</name>
      <uri>https://old.reddit.com/user/LoSboccacc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt; &lt;img alt="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" src="https://preview.redd.it/g2mj9lg4f7te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b3f27828a4984437b27e38b91aa497b1074ed5d" title="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoSboccacc"&gt; /u/LoSboccacc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2mj9lg4f7te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt8yug</id>
    <title>â€œSerious issues in Llama 4 training. I Have Submitted My Resignation to GenAIâ€œ</title>
    <updated>2025-04-07T00:43:36+00:00</updated>
    <author>
      <name>/u/rrryougi</name>
      <uri>https://old.reddit.com/user/rrryougi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original post is in Chinese that can be found &lt;a href="https://www.1point3acres.com/bbs/thread-1122600-1-1.html"&gt;here&lt;/a&gt;. Please take the following with a grain of salt.&lt;/p&gt; &lt;p&gt;Content:&lt;/p&gt; &lt;p&gt;Despite repeated training efforts, the internal model's performance still falls short of open-source SOTA benchmarks, lagging significantly behind. Company leadership suggested blending test sets from various benchmarks during the post-training process, aiming to meet the targets across various metrics and produce a &amp;quot;presentable&amp;quot; result. Failure to achieve this goal by the end-of-April deadline would lead to dire consequences. Following yesterdayâ€™s release of Llama 4, many users on X and Reddit have already reported extremely poor real-world test results. &lt;/p&gt; &lt;p&gt;As someone currently in academia, I find this approach utterly unacceptable. Consequently, I have submitted my resignation and explicitly requested that my name be excluded from the technical report of Llama 4. Notably, the VP of AI at Meta also resigned for similar reasons. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rrryougi"&gt; /u/rrryougi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt7hlc</id>
    <title>Meta's Llama 4 Fell Short</title>
    <updated>2025-04-06T23:27:19+00:00</updated>
    <author>
      <name>/u/Rare-Site</name>
      <uri>https://old.reddit.com/user/Rare-Site</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt; &lt;img alt="Meta's Llama 4 Fell Short" src="https://preview.redd.it/rwrke16rpate1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97aeb81ed981cdfb9ae0bb78f2027199731a69a" title="Meta's Llama 4 Fell Short" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama 4 Scout and Maverick left me really disappointed. It might explain why Joelle Pineau, Metaâ€™s AI research lead, just got fired. Why are these models so underwhelming? My armchair analyst intuition suggests itâ€™s partly the tiny expert size in their mixture-of-experts setup. 17B parameters? Feels small these days.&lt;/p&gt; &lt;p&gt;Metaâ€™s struggle proves that having all the GPUs and Data in the world doesnâ€™t mean much if the ideas arenâ€™t fresh. Companies like DeepSeek, OpenAI etc. show real innovation is what pushes AI forward. You canâ€™t just throw resources at a problem and hope for magic. Guess thatâ€™s the tricky part of AI, itâ€™s not just about brute force, but brainpower too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Site"&gt; /u/Rare-Site &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwrke16rpate1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T23:27:19+00:00</published>
  </entry>
</feed>
