<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-31T23:48:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l09u5c</id>
    <title>What local LLM and IDE have documentation indexing like Cursor's @Docs?</title>
    <updated>2025-05-31T22:48:40+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cursor will read and index code documentation but it doesn't work with local LLMs, not even via the ngrok method recently it seems (ie spoofing a local LLM with an OpenAI compatible API and using ngrok to tunnel localhost to a remote URL). VSCode doesn't have it, nor Windsurf, it seems. I see only Continue.dev has the same @Docs functionality, are there more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09u5c/what_local_llm_and_ide_have_documentation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09u5c/what_local_llm_and_ide_have_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l09u5c/what_local_llm_and_ide_have_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T22:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kztjgp</id>
    <title>How are Intel gpus for local models</title>
    <updated>2025-05-31T10:02:49+00:00</updated>
    <author>
      <name>/u/Unusual_Pride_6480</name>
      <uri>https://old.reddit.com/user/Unusual_Pride_6480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Say the b580 plus ryzen cpu and lots of ram&lt;/p&gt; &lt;p&gt;Does anyone have experience with this and what are your thoughts especially on Linux say fedora &lt;/p&gt; &lt;p&gt;I hope this makes sense I'm a bit out of my depth &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unusual_Pride_6480"&gt; /u/Unusual_Pride_6480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l03iep</id>
    <title>"Fill in the middle" video generation?</title>
    <updated>2025-05-31T18:06:33+00:00</updated>
    <author>
      <name>/u/randomqhacker</name>
      <uri>https://old.reddit.com/user/randomqhacker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My dad has been taking photos when he goes hiking. He always frames them the same, and has taken photos for every season over the course of a few years. Can you guys recommend a video generator that can &amp;quot;fill in the middle&amp;quot; such that I can produce a video in between each of the photos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomqhacker"&gt; /u/randomqhacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l03iep/fill_in_the_middle_video_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l03iep/fill_in_the_middle_video_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l03iep/fill_in_the_middle_video_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T18:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzrujd</id>
    <title>GPU-enabled Llama 3 inference in Java from scratch</title>
    <updated>2025-05-31T08:05:09+00:00</updated>
    <author>
      <name>/u/mikebmx1</name>
      <uri>https://old.reddit.com/user/mikebmx1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"&gt; &lt;img alt="GPU-enabled Llama 3 inference in Java from scratch" src="https://external-preview.redd.it/zGi8MlTX6KdNBAxbzYKKNHv02BqKK2KcEgsGUzULDDk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=114333efebb949ba294cb4a365eba5f89e344050" title="GPU-enabled Llama 3 inference in Java from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikebmx1"&gt; /u/mikebmx1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/beehive-lab/GPULlama3.java"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l03f2h</id>
    <title>LLM Extension for Command Palette: A way to chat with LLM without opening new windows</title>
    <updated>2025-05-31T18:02:51+00:00</updated>
    <author>
      <name>/u/GGLio</name>
      <uri>https://old.reddit.com/user/GGLio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l03f2h/llm_extension_for_command_palette_a_way_to_chat/"&gt; &lt;img alt="LLM Extension for Command Palette: A way to chat with LLM without opening new windows" src="https://external-preview.redd.it/ZWduanl1Y2ZvNTRmMYoemgkJP9kpJlL4F7uhfpuBmeMH1-UkrRZT_-5NJ7bo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=005e0ff00ebb5d6a941f65feb6f10a2fe59b720d" title="LLM Extension for Command Palette: A way to chat with LLM without opening new windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kfxl36/proof_of_concept_ollama_chat_in_powertoys_command"&gt;last post&lt;/a&gt; got some nice feedbacks on what was just a small project, it motivated me to put this &lt;a href="https://apps.microsoft.com/detail/9NPK6KSDLC81"&gt;on Microsoft store&lt;/a&gt; and also on winget, which means now the extension can be directly installed from the &lt;a href="https://learn.microsoft.com/en-us/windows/powertoys/command-palette/overview"&gt;PowerToys Command Palette&lt;/a&gt; install extension command! To be honest, I first made this project just so that I don't have to open and manage a new window when talking to chatbots, but it seems others also like to have something like this, so here it is and I'm glad to be able to make it available for more people.&lt;/p&gt; &lt;p&gt;On top of that, apart from chatting with LLMs through Ollama in the initial prototype, it is now also able to use OpenAI, Google, and Mistral services, and to my surprise more people I've talked to prefers Google Gemini than other services (or is it just because of the recent 2.5 Pro/Flash release?). And here is the open-sourced code: &lt;a href="https://github.com/LioQing/llm-extension-for-cmd-pal"&gt;LioQing/llm-extension-for-cmd-pal: An LLM extension for PowerToys Command Palette&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGLio"&gt; /u/GGLio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/54dvyzcfo54f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l03f2h/llm_extension_for_command_palette_a_way_to_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l03f2h/llm_extension_for_command_palette_a_way_to_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T18:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzz7t4</id>
    <title>Use MCP to run computer use in a VM.</title>
    <updated>2025-05-31T15:04:34+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"&gt; &lt;img alt="Use MCP to run computer use in a VM." src="https://external-preview.redd.it/MHJhMDJ6dWV1NDRmMSBTlOtFiw3CN60nCKAl7ym9Md7o0mszJARyFHwBNilc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f04b4f66e0cfeee4830916f3a623e4070f0018c" title="Use MCP to run computer use in a VM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p51trp5fu44f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l05ypt</id>
    <title>Speaker separation and transcription</title>
    <updated>2025-05-31T19:53:49+00:00</updated>
    <author>
      <name>/u/Khipu28</name>
      <uri>https://old.reddit.com/user/Khipu28</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any software, llm or example code to do speaker separation and transcription from a mono recording source?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Khipu28"&gt; /u/Khipu28 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05ypt/speaker_separation_and_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05ypt/speaker_separation_and_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l05ypt/speaker_separation_and_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T19:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzlb8g</id>
    <title>Unlimited Speech to Speech using Moonshine and Kokoro, 100% local, 100% open source</title>
    <updated>2025-05-31T01:34:33+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rhulha.github.io/Speech2Speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T01:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l09i8f</id>
    <title>What are the top creative writing models ?</title>
    <updated>2025-05-31T22:33:14+00:00</updated>
    <author>
      <name>/u/TheArchivist314</name>
      <uri>https://old.reddit.com/user/TheArchivist314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone I wanted to know what are the top models that are good at creative writing. I'm looking for ones I can run on my card. I've got a 4070. It has 12GB of Vram. I've got 64GB of normal ram. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheArchivist314"&gt; /u/TheArchivist314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T22:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0843f</id>
    <title>The Quest for 100k - LLAMA.CPP Setting for a Noobie</title>
    <updated>2025-05-31T21:29:39+00:00</updated>
    <author>
      <name>/u/LostHisDog</name>
      <uri>https://old.reddit.com/user/LostHisDog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SO there was a post about eeking 100k context out of gemma3 27b on a 3090 and I really wanted to try it... but never setup llama.cpp before and being a glutton for punishment decided I wanted a GUI too in the form of open-webui. I think I got most of it working with an assortment of help from various AI's but the post suggested about 35t/s and I'm only managing about 10t/s. This is my startup file for llama.cpp, mostly settings copied from the other post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&amp;quot;@echo off&amp;quot;&lt;br /&gt; set SERVER_PATH=X:\llama-cpp\llama-server.exe&lt;br /&gt; set MODEL_PATH=X:\llama-cpp\models\gemma-3-27b-it-q4_0.gguf&lt;br /&gt; set MMPROJ_PATH=X:\llama-cpp\models\mmproj-model-f16-27B.gguf&lt;/p&gt; &lt;p&gt;&amp;quot;%SERVER_PATH%&amp;quot; ^&lt;br /&gt; --host &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; --port 8080 ^&lt;br /&gt; --model &amp;quot;%MODEL_PATH%&amp;quot; ^&lt;br /&gt; --ctx-size 102400 ^&lt;br /&gt; --cache-type-k q8_0 --cache-type-v q8_0 ^&lt;br /&gt; --flash-attn ^&lt;br /&gt; -ngl 999 -ngld 999 ^&lt;br /&gt; --no-mmap ^&lt;br /&gt; --mmproj &amp;quot;%MMPROJ_PATH%&amp;quot; ^&lt;br /&gt; --temp 1.0 ^&lt;br /&gt; --repeat-penalty 1.0 ^&lt;br /&gt; --min-p 0.01 ^&lt;br /&gt; --top-k 64 ^&lt;br /&gt; --top-p 0.95&lt;/p&gt; &lt;p&gt;Anything obvious jump out to you wise folks that already have this working well or any ideas for what I could try? 100k at 35t/s sounds magical so would love to get there is I could. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostHisDog"&gt; /u/LostHisDog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0843f/the_quest_for_100k_llamacpp_setting_for_a_noobie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0843f/the_quest_for_100k_llamacpp_setting_for_a_noobie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0843f/the_quest_for_100k_llamacpp_setting_for_a_noobie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T21:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzq4fp</id>
    <title>M3 Ultra Binned (256GB, 60-Core) vs Unbinned (512GB, 80-Core) MLX Performance Comparison</title>
    <updated>2025-05-31T06:09:42+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I recently decided to invest in an M3 Ultra model for running LLMs, and after a &lt;em&gt;lot&lt;/em&gt; of deliberation, I wanted to share some results that might help others in the same boat.&lt;/p&gt; &lt;p&gt;One of my biggest questions was the actual performance difference between the binned and unbinned M3 Ultra models. It's pretty much impossible for a single person to own and test both machines side-by-side, so there aren't really any direct, apples-to-apples comparisons available online.&lt;/p&gt; &lt;p&gt;While there are some results out there (like on the llama.cpp GitHub, where someone compared the 8B model), they didn't really cover my use case—I'm using MLX as my backend and working with much larger models (235B and above). So the available benchmarks weren’t all that relevant for me.&lt;/p&gt; &lt;p&gt;To be clear, my main reason for getting the M3 Ultra wasn't to run Deepseek models—those are just way too large to use with long context windows, even on the Ultra. My primary goal was to run the Qwen3 235B model.&lt;/p&gt; &lt;p&gt;So I’m sharing my own benchmark results comparing 4-bit and 6-bit quantization for the Qwen3 235B model on a decently long context window (~10k tokens). Hopefully, this will help anyone else who's been stuck with the same questions I had!&lt;/p&gt; &lt;p&gt;Let me know if you have questions, or if there’s anything else you want to see tested.&lt;br /&gt; Just keep in mind that the model sizes are massive, so I might not be able to cover every possible benchmark.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Side note: In the end, I decided to return the 256GB model and stick with the 512GB one. Honestly, 256GB of memory seemed sufficient for most use cases, but since I plan to keep this machine for a while (and also want to experiment with Deepseek models), I went with 512GB. I also think it’s worth using the 80-core GPU. The pp speed difference was bigger than I expected, and for me, that’s one of the biggest weaknesses of Apple silicon. Still, thanks to the MoE architecture, the 235B models run at a pretty usable speed!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Binned (256GB, 60-Core)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 40.09&lt;br /&gt; prompt_eval_duration: 35.41&lt;br /&gt; generation_duration: 4.68&lt;br /&gt; prompt_tokens_per_second: 260.58&lt;br /&gt; generation_tokens_per_second: 22.6&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 43.23&lt;br /&gt; prompt_eval_duration: 38.9&lt;br /&gt; generation _duration: 4.33&lt;br /&gt; prompt_tokens_per_second: 237.2&lt;br /&gt; generation_tokens_per_second: 18.93&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Unbinned (512GB, 80-Core)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 31.33&lt;br /&gt; prompt_eval_duration: 26.76&lt;br /&gt; generation_duration: 4.57&lt;br /&gt; prompt_tokens_per_second: 344.84&lt;br /&gt; generation_tokens_per_second: 23.22&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 32.56&lt;br /&gt; prompt_eval_duration: 28.31&lt;br /&gt; generation _duration: 4.25&lt;br /&gt; prompt_tokens_per_second: 325.96&lt;br /&gt; generation_tokens_per_second: 19.31&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T06:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l02hmq</id>
    <title>deepseek/deepseek-r1-0528-qwen3-8b stuck on infinite tool loop. Any ideas?</title>
    <updated>2025-05-31T17:23:44+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've downloaded the official Deepseek distillation from their official sources and it does seem a touch smarter. However, when using tools, it often gets stuck forever trying to use them. Do you know why this is going on, and if we have any workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l06g4l</id>
    <title>The SRE’s Guide to High Availability Open WebUI Deployment Architecture</title>
    <updated>2025-05-31T20:15:14+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"&gt; &lt;img alt="The SRE’s Guide to High Availability Open WebUI Deployment Architecture" src="https://external-preview.redd.it/XpbGGkJKPGpF-WdM9CwPHoy0zCWwEbDV6ozBsv9F_h8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b02d3e90330be8f56dd819511015b3b554630c8" title="The SRE’s Guide to High Availability Open WebUI Deployment Architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on my real world experiences running Open WebUI for thousands of concurrent users, this guide covers the best practices for deploying stateless Open WebUI containers (Kubernetes Pods, Swarm services, ECS etc), Redis and external embeddings, vector databases and put all that behind a load balancer that understands long-lived WebSocket upgrades.&lt;/p&gt; &lt;p&gt;When you’re ready to graduate from single container deployment to a distributed HA architecture for Open WebUI, this is where you should start! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://taylorwilsdon.medium.com/the-sres-guide-to-high-availability-open-webui-deployment-architecture-2ee42654eced"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T20:15:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzrfop</id>
    <title>Getting sick of companies cherry picking their benchmarks when they release a new model</title>
    <updated>2025-05-31T07:36:35+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get why they do it. They need to hype up their thing etc. But cmon a bit of academic integrity would go a long way. Every new model comes with the claim that it outcompetes older models that are 10x their size etc. Like, no. Maybe I'm an old man shaking my fist at clouds here I don't know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T07:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kze1r6</id>
    <title>Ollama run bob</title>
    <updated>2025-05-30T20:06:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt; &lt;img alt="Ollama run bob" src="https://preview.redd.it/v4krpd9g7z3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2201e590a1b08cca19a9ca4d26c56ddf0e869e85" title="Ollama run bob" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4krpd9g7z3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzw65c</id>
    <title>AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market</title>
    <updated>2025-05-31T12:41:19+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt; &lt;img alt="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" src="https://external-preview.redd.it/bnCoi_QMP0ucYNmMDpD8YzNjydtxrrZkZROQJhXvr2s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=981e9ab36322decfefbeb6831d8e913c9f0d6692" title="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-pro-385-spotted-on-geekbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T12:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l06f7r</id>
    <title>Most powerful &lt; 7b parameters model at the moment?</title>
    <updated>2025-05-31T20:14:03+00:00</updated>
    <author>
      <name>/u/ventilador_liliana</name>
      <uri>https://old.reddit.com/user/ventilador_liliana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know which is the best model less than 7b currently available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ventilador_liliana"&gt; /u/ventilador_liliana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T20:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l049hr</id>
    <title>Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)</title>
    <updated>2025-05-31T18:38:56+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"&gt; &lt;img alt="Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)" src="https://external-preview.redd.it/a2RzcmN3MGp3NTRmMQcy6PVwRQbV7yy14JYjj4jOMAMqB9rDPOOSK6pFaFzH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c24fbad79aeb47f5f19e401647c9bc11e9bcde0c" title="Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;AutoBE: Backend Vibe Coding Agent Achieving 100% Compilation Success&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Github Repository: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Playground Website: &lt;a href="https://stackblitz.com/github/wrtnlabs/autobe-playground-stackblitz"&gt;https://stackblitz.com/github/wrtnlabs/autobe-playground-stackblitz&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo Result (Generated backend applications by AutoBE) &lt;ul&gt; &lt;li&gt;&lt;a href="https://stackblitz.com/edit/autobe-demo-bbs"&gt;Bullet-in Board System&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://stackblitz.com/edit/autobe-demo-shopping"&gt;E-Commerce&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I previously posted about this same project on Reddit, but back then the Prisma (ORM) agent side only had around 70% success rate.&lt;/p&gt; &lt;p&gt;The reason was that the error messages from the Prisma compiler for AI-generated incorrect code were so unintuitive and hard to understand that even I, as a human, struggled to make sense of them. Consequently, the AI agent couldn't perform proper corrections based on these cryptic error messages.&lt;/p&gt; &lt;p&gt;However, today I'm back with AutoBE that truly achieves 100% compilation success. I solved the problem of Prisma compiler's unhelpful and unintuitive error messages by directly building the Prisma AST (Abstract Syntax Tree), implementing validation myself, and creating a custom code generator.&lt;/p&gt; &lt;p&gt;This approach bypasses the original Prisma compiler's confusing error messaging altogether, enabling the AI agent to generate consistently compilable backend code.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Introducing AutoBE: The Future of Backend Development&lt;/p&gt; &lt;p&gt;We are immensely proud to introduce AutoBE, our revolutionary open-source vibe coding agent for backend applications, developed by Wrtn Technologies.&lt;/p&gt; &lt;p&gt;The most distinguished feature of AutoBE is its exceptional 100% success rate in code generation. AutoBE incorporates built-in TypeScript and Prisma compilers alongside OpenAPI validators, enabling automatic technical corrections whenever the AI encounters coding errors. Furthermore, our integrated review agents and testing frameworks provide an additional layer of validation, ensuring the integrity of all AI-generated code.&lt;/p&gt; &lt;p&gt;What makes this even more remarkable is that backend applications created with AutoBE can seamlessly integrate with our other open-source projects—Agentica and AutoView—to automate AI agent development and frontend application creation as well. In theory, this enables complete full-stack application development through vibe coding alone.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Alpha Release: 2025-06-01&lt;/li&gt; &lt;li&gt;Beta Release: 2025-07-01&lt;/li&gt; &lt;li&gt;Official Release: 2025-08-01&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;AutoBE currently supports comprehensive requirements analysis and derivation, database design, and OpenAPI document generation (API interface specification). All core features will be completed by the beta release, while the integration with Agentica and AutoView for full-stack vibe coding will be finalized by the official release.&lt;/p&gt; &lt;p&gt;We eagerly anticipate your interest and support as we embark on this exciting journey.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f2df0y0jw54f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T18:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1l01bfe</id>
    <title>Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!</title>
    <updated>2025-05-31T16:34:06+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt; &lt;img alt="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" src="https://external-preview.redd.it/ZG81Yjhkenk2NTRmMdgqNWupVXy_ZPAevb2tTQhA9R_THDnUrLckbufzOiAz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6b4e7c264f8e0ba31013dc39fa0ffa3f2a2d820" title="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I have spent a couple weekends giving the tiny Qwen3 0.6B model the ability to show off its underutilized tool calling abilities by using remote MCP servers. I am pleasantly surprised at how well it can chain tools. Additionally, I gave it the option to limit how much it can think to avoid the &amp;quot;overthinking&amp;quot; issue reasoning models (especially Qwen) can have. &lt;a href="https://muellerzr.github.io/til/end_thinking.html"&gt;This implementation was largely inspired by a great article from Zach Mueller outlining just that.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Also, this project is an adaptation of &lt;a href="https://github.com/huggingface/transformers.js-examples/tree/main/qwen3-webgpu"&gt;Xenova's Qwen3 0.6 WebGPU code in transformers.js-examples&lt;/a&gt;, it was a solid starting point to work with Qwen3 0.6B. &lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;HF Space Link: &lt;a href="https://huggingface.co/spaces/callbacked/Qwen3-MCP"&gt;https://huggingface.co/spaces/callbacked/Qwen3-MCP&lt;/a&gt;&lt;br /&gt; Repo: &lt;a href="https://github.com/callbacked/qwen3-mcp"&gt;https://github.com/callbacked/qwen3-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Footnote: With Qwen3 8B having a distillation from R1-0528, I really hope we can see that trickle down to other models including Qwen3 0.6B. Seeing how much more intelligent the other models can get off of R1-0528 would be a cool thing see in action!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r495cezy654f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T16:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l033vh</id>
    <title>Best models to try on 96gb gpu?</title>
    <updated>2025-05-31T17:49:48+00:00</updated>
    <author>
      <name>/u/sc166</name>
      <uri>https://old.reddit.com/user/sc166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX pro 6000 Blackwell arriving next week. What are the top local coding and image/video generation models I can try? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sc166"&gt; /u/sc166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l05wpz</id>
    <title>Has anyone managed to get a non Google AI to run</title>
    <updated>2025-05-31T19:51:20+00:00</updated>
    <author>
      <name>/u/Gabrielmorrow</name>
      <uri>https://old.reddit.com/user/Gabrielmorrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"&gt; &lt;img alt="Has anyone managed to get a non Google AI to run" src="https://preview.redd.it/8yt7shdl964f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=739e2d5a72bad15d28de3d6a9992852dc9157d9f" title="Has anyone managed to get a non Google AI to run" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the new Google edge gallery app? I'm wondering if deepseek or a version of it can be ran locally with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gabrielmorrow"&gt; /u/Gabrielmorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8yt7shdl964f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T19:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l05hpu</id>
    <title>llama-server, gemma3, 32K context *and* speculative decoding on a 24GB GPU</title>
    <updated>2025-05-31T19:32:46+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp keeps cooking! Draft model support with SWA landed this morning and early tests show up to 30% improvements in performance. Fitting it all on a single 24GB GPU was tight. The 4b as a draft model had a high enough acceptance rate to make a performance difference. Generating code had the best speed ups and creative writing got slower. &lt;/p&gt; &lt;p&gt;Tested on dual 3090s: &lt;/p&gt; &lt;h3&gt;4b draft model&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt&lt;/th&gt; &lt;th&gt;n&lt;/th&gt; &lt;th&gt;tok/sec&lt;/th&gt; &lt;th&gt;draft_n&lt;/th&gt; &lt;th&gt;draft_accepted&lt;/th&gt; &lt;th&gt;ratio&lt;/th&gt; &lt;th&gt;Δ %&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;create a one page html snake game in javascript&lt;/td&gt; &lt;td&gt;1542&lt;/td&gt; &lt;td&gt;49.07&lt;/td&gt; &lt;td&gt;1422&lt;/td&gt; &lt;td&gt;956&lt;/td&gt; &lt;td&gt;0.67&lt;/td&gt; &lt;td&gt;26.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;write a snake game in python&lt;/td&gt; &lt;td&gt;1904&lt;/td&gt; &lt;td&gt;50.67&lt;/td&gt; &lt;td&gt;1709&lt;/td&gt; &lt;td&gt;1236&lt;/td&gt; &lt;td&gt;0.72&lt;/td&gt; &lt;td&gt;31.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;write a story about a dog&lt;/td&gt; &lt;td&gt;982&lt;/td&gt; &lt;td&gt;33.97&lt;/td&gt; &lt;td&gt;1068&lt;/td&gt; &lt;td&gt;282&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;-14.4%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Scripts and configurations can be found on &lt;a href="https://github.com/mostlygeek/llama-swap/wiki/gemma3-27b-100k-context"&gt;llama-swap's wiki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama-swap config: &lt;/p&gt; &lt;p&gt;```yaml macros: &amp;quot;server-latest&amp;quot;: /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap&lt;/p&gt; &lt;p&gt;# quantize KV cache to Q8, increases context but # has a small effect on perplexity # &lt;a href="https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt; &amp;quot;q8-kv&amp;quot;: &amp;quot;--cache-type-k q8_0 --cache-type-v q8_0&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;gemma3-args&amp;quot;: | --model /path/to/models/gemma-3-27b-it-q4_0.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95&lt;/p&gt; &lt;p&gt;models: # fits on a single 24GB GPU w/ 100K context # requires Q8 KV quantization &amp;quot;gemma&amp;quot;: env: # 3090 - 35 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 11.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1&amp;quot; cmd: | ${server-latest} ${q8-kv} ${gemma3-args} --ctx-size 102400 --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# single GPU w/ draft model (lower context) &amp;quot;gemma-fit&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot; cmd: | ${server-latest} ${q8-kv} ${gemma3-args} --ctx-size 32000 --ctx-size-draft 32000 --model-draft /path/to/models/gemma-3-4b-it-q4_0.gguf --draft-max 8 --draft-min 4&lt;/p&gt; &lt;p&gt;# Requires 30GB VRAM for 100K context and non-quantized cache # - Dual 3090s, 38.6 tok/sec # - Dual P40s, 15.8 tok/sec &amp;quot;gemma-full&amp;quot;: env: # 3090 - 38 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 15.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-ea4&amp;quot; cmd: | ${server-latest} ${gemma3-args} --ctx-size 102400 --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf #-sm row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# Requires: 35GB VRAM for 100K context w/ 4b model # with 4b as a draft model # note: --mmproj not compatible with draft models&lt;/p&gt; &lt;p&gt;&amp;quot;gemma-draft&amp;quot;: env: # 3090 - 38 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; cmd: | ${server-latest} ${gemma3-args} --ctx-size 102400 --model-draft /path/to/models/gemma-3-4b-it-q4_0.gguf --ctx-size-draft 102400 --draft-max 8 --draft-min 4 ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T19:32:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzv322</id>
    <title>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</title>
    <updated>2025-05-31T11:41:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T11:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzzshu</id>
    <title>Google lets you run AI models locally</title>
    <updated>2025-05-31T15:29:15+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/"&gt;https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzsa70</id>
    <title>China is leading open source</title>
    <updated>2025-05-31T08:35:25+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt; &lt;img alt="China is leading open source" src="https://preview.redd.it/6stw9ivzw24f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87af4f2951867765dd0c43808b34253b587103b5" title="China is leading open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6stw9ivzw24f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:35:25+00:00</published>
  </entry>
</feed>
