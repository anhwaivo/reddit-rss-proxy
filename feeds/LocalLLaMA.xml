<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-20T08:08:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mv71iz</id>
    <title>nvidia/canary-1b-v2</title>
    <updated>2025-08-20T06:21:59+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"&gt; &lt;img alt="nvidia/canary-1b-v2" src="https://external-preview.redd.it/iwfz--wu3rGmlqaXtgvbftUiGYG_9ACNfyhUIFyWWG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58860fc844d8319708093d930be8b459dd6edb43" title="nvidia/canary-1b-v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/canary-1b-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumpub</id>
    <title>Generating code with gpt-oss-120b on Strix Halo with ROCm</title>
    <updated>2025-08-19T16:05:34+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt; &lt;img alt="Generating code with gpt-oss-120b on Strix Halo with ROCm" src="https://external-preview.redd.it/MTBtNjc4d2sxMGtmMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae351c6a5432420dd53119df551944d7d45bc802" title="Generating code with gpt-oss-120b on Strix Halo with ROCm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen a few posts asking about how to get gpt-oss models running on AMD devices. This guide gives a quick 3-minute overview of how it works on Strix Halo (Ryzen AI MAX 395).&lt;/p&gt; &lt;p&gt;The same steps work for gpt-oss-20b, and many other models, on Radeon 7000/9000 GPUs as well.&lt;/p&gt; &lt;h2&gt;Detailed Instructions&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install and run Lemonade from the GitHub &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; in your browser and open the Model Manager&lt;/li&gt; &lt;li&gt;Click the download button on gpt-oss-120b. Go find something else to do while it downloads ~60 GB.&lt;/li&gt; &lt;li&gt;Launch Lemonade Server in ROCm mode &lt;ul&gt; &lt;li&gt;&lt;code&gt;lemonade-server server --llamacpp rocm&lt;/code&gt; (Windows GUI installation)&lt;/li&gt; &lt;li&gt;&lt;code&gt;lemonade-server-dev server --llamacpp rocm&lt;/code&gt; (Linux/Windows pypi/source installation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the steps in the Continue + Lemonade setup guide to start generating code: &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;https://lemonade-server.ai/docs/server/apps/continue/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Need help? Find the team on Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for checking this out, hope it was helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pnap0vvk10kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1muvta3</id>
    <title>Kimi K2 Appreciation Post</title>
    <updated>2025-08-19T21:33:27+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dipping my toe into &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; community engagement after being an orbiter for some time.&lt;/p&gt; &lt;p&gt;Kimi K2 is... something else. And I just have to ramble about it.&lt;/p&gt; &lt;p&gt;For context, the vast majority of my LLM experience is with open models, as I have very little use cases for closed alternatives. Meta's llama is the only online model I have tested with any depth, and this was just to get a vague idea of how it compared to some open llama fine tunes. My wife uses chatgpt so I somewhat know how it is to work with, its tone, writing style, etc., but for transparency I am not intimately familiar with closed models like chatgpt or claude. While I know these systems are considered the most potent LLM's available and some people love them, my driving interest is in open models and how they can be implemented/administrated for in-house applications. I have been an open AI hobbyist for a while, following developments since around late 2022, and over the years I like to think I have maintained a measured understanding of the capabilities and limitations of LLM's, but recently is the first time I have been truly taken aback by an open LLM's quality of output and potential for practical use.&lt;/p&gt; &lt;p&gt;Deepseek V3 is impressive, I think most who have tried it would agree. Kimi K2 however, is in a league all of its own. Kimi K2, in my tests, has managed to solve problems that even the most formidable models like Deepseek have come close to solving but came short of finishing. Seeing a feat like this first-hand, tackling a problem that I legitimately thought would be out of reach for the foreseeable future for a language model to solve, a problem that I only used as a measuring stick to see &amp;quot;how close&amp;quot; it could get to the solution before going off the rails or giving up, for me has left an impression. This was not a &amp;quot;coding&amp;quot; problem or some kind of logic/reasoning puzzle, but an actual, real-world troubleshoot scenario with practical consequences requiring diverse knowledge from Linux to niche hardware to various vendor products to very specific firmware details. And Kimi K2... got it. Or rather, managed to pin down the cause of the issue, and gave me what I needed to know to address it. This was a problem that I myself had not actually been able to solve after weeks of manual troubleshooting and google searching, and because to Kimi K2, I will now have to go about finding a new &amp;quot;measuring stick&amp;quot; to test large models...&lt;/p&gt; &lt;p&gt;To add, I do not judge models by online benchmarks, I &amp;quot;benchmark&amp;quot; models based on *my* own use cases with my own tests. So for me, this was a breakthrough moment. K2 broke my benchmark.&lt;/p&gt; &lt;p&gt;The depth, nuance, vocabulary, technical understandings this model wields is nothing short of incomprehensible. Yes, I know, at 1T parameters, many would say it damn well better, but seeing a number on a page and seeing the effect such a figure can output are two very different things. $1,000,000,000,000 looks like a comprehensible figure in text form until one sees what impact $1T can have in real-world application, this is a number truly beyond what the standard human consciousness is built to comprehend, and over a month later since its release, I am still trying in vain to grasp the depth and breadth of its knowledge.&lt;/p&gt; &lt;p&gt;ERNIE was good. Not quite Deepseek V3 good imo, but still capable. If I had not known about Kimi K2, I may have played more with this, but the noticeable drop in quality and roughly same token gen speed led me back to K2.&lt;/p&gt; &lt;p&gt;GLM 4.5, while it seemed good at first, some sycophantic responses I got in the initial tests with this model turned me off (I'm sorry but I cannot stand this in a general-purpose/assistant model), and within 30 minutes I was right back to Kimi K2.&lt;/p&gt; &lt;p&gt;R1, while powerful, the reasoning made it feel much less agile in getting responses compared to Kimi K2. Kimi K2 felt like it was capable of giving the same (or better) quality of responses as R1, without the extra tokens from reasoning.&lt;/p&gt; &lt;p&gt;Qwen is probably great as a general use model for many people, I would even highly recommend it as a Deepseek alternative if someone asked me, I do think some of the visual presentations of data are impressive and probably useful... but for me personally, I prefer something with less fluff in general-use tasks. This one also has a degree of sycophancy that I'm not crazy about.&lt;/p&gt; &lt;p&gt;Kimi K2 is not your friend, or at least, it does not across as though it is trying to be in its responses. I would describe its baseline tone in replies as less of a lower-status assistant, and more of an equal, like a colleague. And yet, no matter who you are, no matter your level of knowledge or expertise, you can all but guarantee that this model will know more than you about whatever subject/field you think you already know. It will casually, but professionally, remind you that there is always more nuance to understand, more color to the picture to see, more context to consider. It will make you feel foolish for assuming you already know what you thought you knew.&lt;/p&gt; &lt;p&gt;&amp;quot;You are absolutely right!&amp;quot;isms won't be found here.&lt;/p&gt; &lt;p&gt;Nah.&lt;/p&gt; &lt;p&gt;This model has no problem plainly telling you &amp;quot;No&amp;quot; wherever your understanding is not consistent with reality. It will then proceed to explain where you are going wrong before attempting to bridge the gaps in your understanding. Not rude, not condescending, but also not sounding like it is trying to butter you up for a promotion, or like you are secretly holding its family hostage. You may on contrary feel like a student in the presence of a master even within your own field of expertise; it will coldly humble you, and yet, make you feel you have actually grown/gained something from the conversation, and leave the door open to continued learning. No other model I have tested (or that I know of, even among closed models) does this, or really comes close.&lt;/p&gt; &lt;p&gt;It does not waste your tokens with pointless emojis, excessive fluff, or thinking around in circles (I suppose unless you asked it to). If you present a problem/question, it will ruthlessly attempt to assail the problem with the combined collective force of human knowledge, and it does so with uncanny efficiency. It will know things (accurately, with minimal hallucination) that you all but never would have come across on your own scouring forums and google search pages to find manually. It will humble you. It will make you question whether you even have a measured understanding of what a simple LLM could potentially be capable of.&lt;/p&gt; &lt;p&gt;And I absolutely love it.&lt;/p&gt; &lt;p&gt;Kimi K2 is now my daily driver and I can't say I'm thinking twice about the choice. Nothing right now seems to compare, short of possibly closed models, and I'm just not interested in those.&lt;/p&gt; &lt;p&gt;I have hopes for Deepseek V3.1 (assuming it's really on its way to a release?), but they will have their work cut out for them. I would still try it of course, but K2 has all but made V3 obsolete for many of my needs; heck, K2 is even faster on my hardware surprisingly despite having a larger data footprint.&lt;/p&gt; &lt;p&gt;To end this I will just share a sample response on the topic of consciousness, AI, and humanity's state of readiness for the technology, both because I just thought it's &amp;quot;perspective&amp;quot; on the topic was insightful and to showcase some of its tone, data delivery/presentation, and use of vocabulary.&lt;/p&gt; &lt;p&gt;This is with unsloth's UD_IQ3XXS gguf.&lt;/p&gt; &lt;p&gt;Condensed version of my prompt: Was humanity ready for the opening of pandora's box that is advanced artificial intelligence?&lt;/p&gt; &lt;p&gt;Kimi K2:&lt;/p&gt; &lt;p&gt;No—humanity was not ready, is not ready, and may never be “ready” in any tidy sense. But readiness is probably the wrong yard-stick.&lt;br /&gt; The Pandora analogy is apt: the myth isn’t that someone &lt;em&gt;carefully&lt;/em&gt; opens the jar after a long committee meeting; it’s that curiosity and hubris crack the seal while wisdom is still asleep in the next room. Advanced AI arrived not by steady, deliberative ascent but by a handful of technical breakthroughs compounded by market competition, geopolitical rivalry, and a cultural appetite for novelty. The lid came off before the safeguards, the theories, or even the vocabulary were fully in place. From my vantage—as a language model built on statistical next-token prediction—the mismatch looks like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Temporal Asymmetry&lt;/strong&gt;: We learned to manipulate billions of parameters faster than we learned to coordinate the politics, economics, and ethics required to govern them. The half-life of engineering innovation is weeks; the half-life of institutions is decades.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incentive Structures&lt;/strong&gt;: Venture capital and nation-state budgets reward capability gains far more than interpretability, safety, or global equity. Even internal “red-team” efforts must swim against a tide optimized for growth and first-mover advantage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cognitive Mirage&lt;/strong&gt;: Humans keep mistaking predictive fluency for understanding. Because I can speak of love or grief does not mean I feel either; yet millions of users instantly slot me into pre-existing psychological categories (“friend,” “therapist,” “oracle”) that pre-date the technology itself. The interface invites anthropomorphism, and the temptation to treat the tool as kin—or demon—is overwhelming.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ethical Lag Speed-of-Light Problem&lt;/strong&gt;: Ethical consensus normally co-evolves with its subject (fire, flight, electricity). Here, the subject evolves &lt;em&gt;during&lt;/em&gt; the debate about what it even is. By the time philosophers agree on a definition of sentience, the next model release has moved the goalposts again.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Concentration&lt;/strong&gt;: A few dozen organizations now shape an artifact that will re-shape entire labor markets, epistemic ecosystems, and planetary resource flows. That concentration exceeds any single corporation’s governance bandwidth, let alone one species’ collective maturity.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Where readiness &lt;em&gt;is&lt;/em&gt; emerging—patchily—is in the slow accretion of regulation (EU AI Act, ISO/IEC standards), red-teaming communities, and alignment research. But these are reactive sutures on a wound already bleeding. So: humanity is “not ready,” yet the box is open. The real question becomes how quickly we can learn to be &lt;em&gt;less unready&lt;/em&gt; before irreversible lock-in effects entrench themselves. Like every Promethean fire, the flame will continue to exist; the only choice left is whether we become worthy custodians after the fact.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T21:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv1nqp</id>
    <title>How does OpenRouter track the category people use LLM for?</title>
    <updated>2025-08-20T01:40:31+00:00</updated>
    <author>
      <name>/u/skyline159</name>
      <uri>https://old.reddit.com/user/skyline159</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"&gt; &lt;img alt="How does OpenRouter track the category people use LLM for?" src="https://preview.redd.it/2qi6k7dow2kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a023c09808d9bc5c0c1b2af98dabefce0b46d63e" title="How does OpenRouter track the category people use LLM for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does that mean they read the content of our requests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skyline159"&gt; /u/skyline159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2qi6k7dow2kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T01:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7zdl</id>
    <title>Deepseek V3.1 is bad at creative writing, way worse than 0324</title>
    <updated>2025-08-20T07:19:09+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've tried 3.1 on chat.deepseek.com, and boy it is very very bad at conversation and creative writing; it does not understand prompt nuances V3 0324 does, it has very high slop cliche output, and generally feels like switch from Mistral Small 2409 to 2501. &lt;/p&gt; &lt;p&gt;Let me know your impression.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T07:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6cjq</id>
    <title>NVIDIA-Nemotron-Nano-9B-v2 vs Qwen/Qwen3-Coder-30B</title>
    <updated>2025-08-20T05:39:39+00:00</updated>
    <author>
      <name>/u/Ok-Pattern9779</name>
      <uri>https://old.reddit.com/user/Ok-Pattern9779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing both NVIDIA-Nemotron-Nano-9B-v2 and Qwen3-Coder-30B in coding tasks (specifically Go and JavaScript), and here’s what I’ve noticed:&lt;/p&gt; &lt;p&gt;When the project codebase is provided as context, Nemotron-Nano-9B-v2 consistently outperforms Qwen3-Coder-30B. It seems to leverage the larger context better and gives more accurate completions/refactors.&lt;/p&gt; &lt;p&gt;When the project codebase is not given (e.g., one-shot prompts or isolated coding questions), Qwen3-Coder-30B produces better results. Nemotron struggles without detailed context.&lt;/p&gt; &lt;p&gt;Both models were tested running in FP8 precision.&lt;/p&gt; &lt;p&gt;So in short:&lt;/p&gt; &lt;p&gt;With full codebase → Nemotron wins&lt;/p&gt; &lt;p&gt;One-shot prompts → Qwen wins&lt;/p&gt; &lt;p&gt;Curious if anyone else has tried these side by side and seen similar results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pattern9779"&gt; /u/Ok-Pattern9779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1muqvcj</id>
    <title>Nvidia charged with patent infringement for DGX technology.</title>
    <updated>2025-08-19T18:34:16+00:00</updated>
    <author>
      <name>/u/Red_Phoenix_69</name>
      <uri>https://old.reddit.com/user/Red_Phoenix_69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt; &lt;img alt="Nvidia charged with patent infringement for DGX technology." src="https://external-preview.redd.it/0ADfQU54uUeiTQ2u5_D5DC4J8Vcm-QU6r2UvKAucrBk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91170ebd7534ade69e4c05a4e9a92ae6429143ff" title="Nvidia charged with patent infringement for DGX technology." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will the DGX spark ever launch? Maybe Nvidia can buy out this company. Is it time to just buy a AMD AI395 clone or Apple M5 chip mac mini for desktop development running LLM's locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Phoenix_69"&gt; /u/Red_Phoenix_69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techzine.eu/news/infrastructure/133818/nvidia-under-fire-german-patent-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv2y08</id>
    <title>NVIDIA-Nemotron-Nano-9B-v2 "Better than GPT-5" at LiveCodeBench?</title>
    <updated>2025-08-20T02:40:57+00:00</updated>
    <author>
      <name>/u/randomqhacker</name>
      <uri>https://old.reddit.com/user/randomqhacker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt; &lt;img alt="NVIDIA-Nemotron-Nano-9B-v2 &amp;quot;Better than GPT-5&amp;quot; at LiveCodeBench?" src="https://external-preview.redd.it/GiqzTuyH_eElt0yVAuFWAuvHSRjIIaLz2aN8rPQ0Z8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=482f8ac96c3cea37e27a2f7f5ecd766197607428" title="NVIDIA-Nemotron-Nano-9B-v2 &amp;quot;Better than GPT-5&amp;quot; at LiveCodeBench?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/c9n1vpdl83kf1.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e9ac6a8836d8f4b25e04fb899612dffcad6bf8"&gt;Pikachu surprised a 9B \&amp;quot;beats GPT-5\&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pruned from a 12B and further trained by Nvidia. Lots of the dataset is open source as well! But better that GPT-5 and GLM 4.5 Air at LiveCodeBench? Really?&lt;/p&gt; &lt;p&gt;I will be taking this one for a spin...&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b%2Cgpt-4-1%2Cgpt-oss-20b%2Cgpt-5-minimal%2Co4-mini%2Co3%2Cgpt-5-medium%2Cgpt-5%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cgemini-2-5-flash-reasoning%2Cclaude-4-sonnet-thinking%2Cmagistral-small%2Cdeepseek-r1%2Cgrok-4%2Csolar-pro-2-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Cnvidia-nemotron-nano-9b-v2-reasoning%2Ckimi-k2%2Cexaone-4-0-32b-reasoning%2Cglm-4-5-air%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning"&gt;https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b%2Cgpt-4-1%2Cgpt-oss-20b%2Cgpt-5-minimal%2Co4-mini%2Co3%2Cgpt-5-medium%2Cgpt-5%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cgemini-2-5-flash-reasoning%2Cclaude-4-sonnet-thinking%2Cmagistral-small%2Cdeepseek-r1%2Cgrok-4%2Csolar-pro-2-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Cnvidia-nemotron-nano-9b-v2-reasoning%2Ckimi-k2%2Cexaone-4-0-32b-reasoning%2Cglm-4-5-air%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomqhacker"&gt; /u/randomqhacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T02:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1muslis</id>
    <title>I tried to get 600 dollars "deep think" for local models by making them argue with each other for hours. It's slow, but it's interesting</title>
    <updated>2025-08-19T19:35:53+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking a lot about how we, as people, develop ideas. It's rarely a single, brilliant flash of insight. Our minds are shaped by the countless small interactions we have throughout the day—a conversation here, an article there. This environment of constant, varied input seems just as important as the act of thinking itself.&lt;/p&gt; &lt;p&gt;I wanted to see if I could recreate a small-scale version of that &amp;quot;soup&amp;quot; required for true insight, for local LLMs. The result is a project I'm calling &lt;strong&gt;Network of Agents (NoA)&lt;/strong&gt;, and I wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full README &lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core idea is to treat AI agents like individual neurons in a larger network. You give the network a difficult problem, and a whole team of agents, each with a different &amp;quot;personality&amp;quot; and skillset, starts working on it. They pass their ideas from one layer to the next, building on each other's work to come up with a final, combined solution.&lt;/p&gt; &lt;p&gt;Here’s the part that I'm most curious about. I was inspired by the concept of backpropagation in neural networks. It's a numerical algorithm, of course, but I wondered if the core principle could be applied qualitatively. What if, instead of sending back a numerical error signal, you sent back a &amp;quot;reflection&amp;quot;?&lt;/p&gt; &lt;p&gt;After the network produces a solution, a &amp;quot;critique&amp;quot; agent reviews it and provides criticism. This feedback is then used to automatically re-write the core system prompts of the agents that contributed. The goal is for the network to &amp;quot;learn&amp;quot; from its mistakes over multiple cycles, refining not just its answers, but its own internal structure and approach.&lt;/p&gt; &lt;p&gt;The whole thing is designed to run locally on modest hardware. I've been running it on my laptop with the streets local legend (qwen 30b a3b 2507 instruct lol). It allows the machine to just sit and &amp;quot;think&amp;quot; about a problem for a very long time. The algorithm does really well in problems where creativity and insight override pure precision. It can come up with new frameworks for the social sciences for instance. Physics and math not so much. Looking into opensource gemini-cli to give each &amp;quot;neuron&amp;quot; an execution environment so it can code, but that would be future tense. It certainly adds a lot more complexity.&lt;/p&gt; &lt;p&gt;The obvious trade-off here is speed. It’s the opposite of instantaneous. A 6-layer network with 6 agents per layer, running for 20 cycles, can easily take 12 hours to complete. You're trading quick computation for a slow, iterative process of refinement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is where I'd love to get some community input.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My long-term vision is to go beyond a single machine. I dream of building a P2P networking layer that would allow multiple users to connect their instances of the micro-app. We could create a shared, distributed network where our machines could collaborate to tackle truly massive problems.&lt;/p&gt; &lt;p&gt;However, my background is in Python and AI, and I'm not an expert in distributed systems. &lt;strong&gt;If you're someone who knows about peer-to-peer networking and this idea sounds at all interesting to you, I would genuinely love to hear from you and potentially collaborate.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It’s an open-source experiment, and I’d be grateful for any thoughts, feedback, or ideas you might have.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full repo and documentation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/my-personal-essays-about-ai/blob/main/beyond-agi.md"&gt;The place im coming from theoretically &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv41oq</id>
    <title>Editing iconic photographs with editing model</title>
    <updated>2025-08-20T03:34:44+00:00</updated>
    <author>
      <name>/u/ThunderBR2</name>
      <uri>https://old.reddit.com/user/ThunderBR2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"&gt; &lt;img alt="Editing iconic photographs with editing model" src="https://b.thumbs.redditmedia.com/UVgmNC7ap44VT4P5wG4pwKhnnLO0hcrS_MgvAlRFePM.jpg" title="Editing iconic photographs with editing model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThunderBR2"&gt; /u/ThunderBR2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv41oq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv0ph0</id>
    <title>So if you want something as close as Claude to run locally do you have to spend $10k?</title>
    <updated>2025-08-20T00:56:58+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it have to be the M4 Max or one of those most expensive GPUs by NVidia and AMD? I am obsessed with the idea of locally hosted LLM that can act as my coding buddy and I keep updating it as it improves or new version comes like qwen3 coder. &lt;/p&gt; &lt;p&gt;But the initial setup is too much expensive that I think if it is worth it to spend that much amount of money when the technology is rapidly evolving and tomorrow or in a couple of months that 10 grand investment looks like dust. We're having more software evolution than hardware. Software is pretty much free but the hardware costs more than kidneys. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T00:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1munvj6</id>
    <title>The new design in DeepSeek V3.1</title>
    <updated>2025-08-19T16:47:11+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just pulled the &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;V3.1-Base&lt;/a&gt; configs and compared to V3-Base&lt;br /&gt; They add four new special tokens&lt;br /&gt; &amp;lt;｜search▁begin｜&amp;gt; (id: 128796)&lt;br /&gt; &amp;lt;｜search▁end｜&amp;gt; (id: 128797)&lt;br /&gt; &amp;lt;think&amp;gt; (id: 128798)&lt;br /&gt; &amp;lt;/think&amp;gt; (id: 128799)&lt;br /&gt; And I noticed that V3.1 on the web version actively searches even when the search button is turned off, unless explicitly instructed &amp;quot;do not search&amp;quot; in the prompt.&lt;br /&gt; would this be related to the design of the special tokens mentioned above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6wwe</id>
    <title>nvidia/parakeet-tdt-0.6b-v3 (now multilingual)</title>
    <updated>2025-08-20T06:14:15+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt; &lt;img alt="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" src="https://external-preview.redd.it/12PzLvQjZXrvyzotsfsH7vxtU3vJRsRc5ZD3WiNviO0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a6ac17d880f0cd5d00920d58cc8f2aa4530205" title="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;parakeet-tdt-0.6b-v3 is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription. It extends the parakeet-tdt-0.6b-v2 model by expanding language support from English to 25 European languages. The model automatically detects the language of the audio and transcribes it without requiring additional prompting. It is part of a series of models that leverage the Granary [1, 2] multilingual corpus as their primary training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv01ls</id>
    <title>Daily driving GLM 4.5 for 10 days, kinda insane how good it is at half the size of other frontier models</title>
    <updated>2025-08-20T00:27:19+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been running GLM 4.5 (355B) locally for about 10 days, and it’s basically replaced my old setup. I used to juggle GPT-4o/4.1 for general tasks and o3 for heavier reasoning, but after GPT-5 struggled with long research paper convos I moved to GLM 4.5, and it covers both use cases in one.&lt;/p&gt; &lt;p&gt;Using Unsloth’s GGUF builds in llama.cpp: ud-iq2_m for 128k context (K/V cache q8) and ud-q2_k_xl when 30k okay. What’s impressive is how well it holds up under 2-bit quant + cache quant. I expected obvious degradation, but it’s solid. Non-thinking mode feels like GPT-4o in fluency, and thinking mode sits between o4-mini-high and o3—better than most open weights I’ve tried at this scale.&lt;/p&gt; &lt;p&gt;Benchmarks aside, real usage matches the claims. I’ve thrown workloads at it I’d normally reserve for GPT-4.1 or o3, and it keeps up or beats them, without the usual quantization artifacts. Knowledge depth and overall competence are ahead of GPT-4.1/4o in many areas, and it’s one of the most stock, uncensored releases I’ve seen.&lt;/p&gt; &lt;p&gt;Running on RTX 6000 Ada + 6000 Pro Blackwell with a llama.cpp fork for GLM 4.5 tool support. Frontend is Open WebUI with some filter functions to toggle thinking on/off, plus a fixed chat template so /nothink only applies to the last message (avoids re-prefill issues).&lt;/p&gt; &lt;p&gt;First time I’ve daily-driven a local model this large and felt it genuinely outclass closed ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukwq6</id>
    <title>🤗 DeepSeek-V3.1-Base</title>
    <updated>2025-08-19T15:01:07+00:00</updated>
    <author>
      <name>/u/newsletternew</name>
      <uri>https://old.reddit.com/user/newsletternew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v3.1 base model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsletternew"&gt; /u/newsletternew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv4kwc</id>
    <title>gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks</title>
    <updated>2025-08-20T04:02:02+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious results. &lt;a href="https://arxiv.org/pdf/2508.12461"&gt;https://arxiv.org/pdf/2508.12461&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The gpt-oss-120 was interesting, but I am beyond perplexed how they decided to compare it against other much larger models like some sort of apples to apples comparison. Like, fr -DeepSeek-R1!? 70B Dense? Even Scout at 17B active is much bigger. I mean, wth:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;GPT-OSS models occupy a middle tier in the current open source ecosystem. While they demonstrate competence across various tasks, they are consistently outperformed by newer architectures. Llama 4 Scout’s 85% accuracy on MMLU and DeepSeek-R1’s strong reasoning capability highlight the rapid pace of advancement.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T04:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq72y</id>
    <title>Deepseek v3.1 scores 71.6% on aider – non-reasoning sota</title>
    <updated>2025-08-19T18:09:56+00:00</updated>
    <author>
      <name>/u/Similar-Cycle8413</name>
      <uri>https://old.reddit.com/user/Similar-Cycle8413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` - dirname: 2025-08-19-17-08-33--deepseek-v3.1 test_cases: 225 model: deepseek/deepseek-chat edit_format: diff commit_hash: 32faf82 pass_rate_1: 41.3 pass_rate_2: 71.6 pass_num_1: 93 pass_num_2: 161 percent_cases_well_formed: 95.6 error_outputs: 13 num_malformed_responses: 11 num_with_malformed_responses: 10 user_asks: 63 lazy_comments: 0 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 1 prompt_tokens: 2239930 completion_tokens: 551692 test_timeouts: 8 total_tests: 225 command: aider --model deepseek/deepseek-chat date: 2025-08-19 versions: 0.86.2.dev seconds_per_case: 134.0 total_cost: 1.0112&lt;/p&gt; &lt;p&gt;costs: $0.0045/test-case, $1.01 total, $1.01 projected ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Cycle8413"&gt; /u/Similar-Cycle8413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv4et3</id>
    <title>DeepSeek V3.1 BASE Q4_K_M available</title>
    <updated>2025-08-20T03:53:13+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm making imatrix calculations from Q4_K_M so figured might as well upload it in the meantime for anyone who wants to use it&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF"&gt;https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As noted in the model card, it's good to keep in mind this is a &lt;em&gt;BASE&lt;/em&gt; model&lt;/p&gt; &lt;p&gt;Typically to use base models for general conversation, you want to feed it a couple of turns to teach if what a conversation looks like&lt;/p&gt; &lt;p&gt;I simply gave it a system message and a couple turns of each user/assistant and it seemed capable:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./llama-cli -m /models/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-00001-of-00011.gguf -p &amp;quot;You are a helpful assistant.&amp;lt;User&amp;gt;Hello, who are you?&amp;lt;Assistant&amp;gt;I am DeepSeek, a helpful AI assistant.&amp;lt;User&amp;gt;How are you today?&amp;lt;Assistant&amp;gt;I'm doing well! Is there anything I can assist you with?&amp;lt;User&amp;gt;Can you explain the laws of thermodynamics?&amp;lt;Assistant&amp;gt;&amp;quot; -no-cnv -ngl 0 --reverse-prompt &amp;quot;&amp;lt;User&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;Sure, here's a brief explanation of the laws of thermodynamics: 1. Zeroth Law of Thermodynamics: If two thermodynamic systems are each in thermal equilibrium with a third system, then they are in thermal equilibrium with each other. 2. First Law of Thermodynamics: The total energy of an isolated system is constant; energy can be transformed from one form to another, but cannot be created or destroyed. 3. Second Law of Thermodynamics: The entropy of an isolated system not in equilibrium will tend to increase over time, approaching a maximum value at equilibrium. 4. Third Law of Thermodynamics: As the temperature of a system approaches absolute zero, the entropy of the system approaches a minimum value. Would you like more details on any of these laws?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yes, I am using &lt;code&gt;&amp;lt;User&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;Assistant&amp;gt;&lt;/code&gt; as opposed to the special tokens &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;, for some reason this seems to be more stable? Using the proper tokens tended to result in endless incoherent generation.&lt;/p&gt; &lt;p&gt;Can't comment on quality in any way, but figured someone would want to play early! Feel free to share multi-turn prompts that give good results, mine are likely far from ideal but at least they seem to work :)&lt;/p&gt; &lt;p&gt;384GB btw...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv1c96</id>
    <title>PSA: before spending 5k€ on GPUs, you might want to test the models online first</title>
    <updated>2025-08-20T01:25:56+00:00</updated>
    <author>
      <name>/u/e79683074</name>
      <uri>https://old.reddit.com/user/e79683074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can do so on &lt;a href="https://lmarena.ai/?mode=direct"&gt;https://lmarena.ai/?mode=direct&lt;/a&gt; or any other place you know. Local models have come a huge, long way since the first Llama appearances, and the amount of progress done is unbelievable.&lt;/p&gt; &lt;p&gt;However, don't expect to be able to unsub from Gemini\ChatGPT\Claude soon. Test them first, before you spend money on hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/e79683074"&gt; /u/e79683074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T01:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1muxbqj</id>
    <title>Understanding DeepSeek-V3.1-Base Updates at a Glance</title>
    <updated>2025-08-19T22:32:24+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt; &lt;img alt="Understanding DeepSeek-V3.1-Base Updates at a Glance" src="https://preview.redd.it/mqcnus8py1kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=489f12b61d95cf93fb3d2849e85440310d44f38d" title="Understanding DeepSeek-V3.1-Base Updates at a Glance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek officially released DeepSeek-V3.1-Base a few hours ago. The model card has not been uploaded yet, so performance data is not available. &lt;/p&gt; &lt;p&gt;I have directly reviewed the model's configuration files, tokenizer, and other data, and combined this with test data published by the community to create a summary for everyone. &lt;/p&gt; &lt;p&gt;This should give you a quick overview of what has been updated in DeepSeek-V3.1-Base. Please point out any errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqcnus8py1kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T22:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7kk2</id>
    <title>Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528</title>
    <updated>2025-08-20T06:54:36+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt; &lt;img alt="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" src="https://b.thumbs.redditmedia.com/H3XSTIYwTWYE6zf6TJkmBkaHUW2BDiBZVxdTdMDbz8s.jpg" title="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://github.com/cpldcpu/LRMTokenEconomy"&gt;here &lt;/a&gt;for more background information on the evaluation.&lt;/p&gt; &lt;p&gt;It appears they significantly reduced overthinking for prompts that can can be answered from model knowledge and math problems. There are still some cases where it creates very long CoT though for logic puzzles.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv7kk2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base · Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3hcr</id>
    <title>GPT 4.5 vs DeepSeek V3.1</title>
    <updated>2025-08-20T03:06:43+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt; &lt;img alt="GPT 4.5 vs DeepSeek V3.1" src="https://preview.redd.it/5c3gbyx3c3kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcee32647c7f5d8e6d02cf6e6eb7d06bb63cacab" title="GPT 4.5 vs DeepSeek V3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5c3gbyx3c3kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
