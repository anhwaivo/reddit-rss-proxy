<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-24T05:36:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ktt3i8</id>
    <title>Google Veo 3 Computation Usage</title>
    <updated>2025-05-23T20:02:24+00:00</updated>
    <author>
      <name>/u/Spiritual-Neat889</name>
      <uri>https://old.reddit.com/user/Spiritual-Neat889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any asumptions what google veo 3 may cost in computation? &lt;/p&gt; &lt;p&gt;I just want to see if there is a chance of model becoming local available. Or how their price may develop over time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual-Neat889"&gt; /u/Spiritual-Neat889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktvs5j</id>
    <title>AM5 or TRX4 for local LLMs?</title>
    <updated>2025-05-23T21:58:28+00:00</updated>
    <author>
      <name>/u/Ponce_DeLeon</name>
      <uri>https://old.reddit.com/user/Ponce_DeLeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, I am just now dipping my toes in local LLMs and wanting to run LLaMa 70B locally, had some questions regarding the hardware side of things before I start spending more money.&lt;/p&gt; &lt;p&gt;My main concern is whether to go with the AM5 platform or TRX4 for local inferencing and minor fine-tuning on smaller models here and there.&lt;/p&gt; &lt;p&gt;Here are some reasons for why I am considering AM5 vs TRX4;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AM5&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PCIe 5.0 &lt;/li&gt; &lt;li&gt;DDR5&lt;/li&gt; &lt;li&gt;Zen 5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TRX4 (I cant afford newer gens)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64+ PCIe lanes&lt;/li&gt; &lt;li&gt;Supports more memory&lt;/li&gt; &lt;li&gt;Way better motherboard selection for workstations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Since I wanted to run something like LLaMa3 70B at Q4_K_M with decent tokens/sec, I will most likely end up getting a second 3090. AM5 supports PCIe 5.0 x16 and it can be bifurcated to x8, which is comparable in speed to 4.0 x16(?) So in terms of an AM5 system I would be looking at a 9950x for the cpu, and dual 3090s at pcie 5.0 x8/x8 with however much ram/dimms I can use that would be stable. It would be DDR5 clocked at a much higher frequency than the DDR4 on the TRX4 (but on TRX4 I can use way more memory).&lt;/p&gt; &lt;p&gt;And for the TRX4 system my budget would allow for a 3960x for the cpu, along with the same dual 3090s but at pcie 4.0 x16/x16 instead of 5.0 x8/x8, and probably around 256gb of ddr4 ram. I am leaning more towards the AM5 option because I dont ever plan on scaling up to more than 2 GPUs (trying to fit everything inside a 4U rackmount) so pcie 5.0 x8/x8 would do fine for me I think, also the 9950x is on much newer architecture and seems to beat the 3960x in almost every metric. Also, although there are stability issues, it looks like I can get away with 128 of ram on the 9950x as well. &lt;/p&gt; &lt;p&gt;Would this be a decent option for a workstation build? or should I just go with the TRX4 system? Im so torn on which to decide and thought some extra opinions could help. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ponce_DeLeon"&gt; /u/Ponce_DeLeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktvs5j/am5_or_trx4_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktvs5j/am5_or_trx4_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktvs5j/am5_or_trx4_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T21:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktzxni</id>
    <title>I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno</title>
    <updated>2025-05-24T01:24:19+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzxni/im_building_an_ai_interview_prep_tool_to_get_real/"&gt; &lt;img alt="I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno" src="https://external-preview.redd.it/cmdxNms1ajl0bTJmMZnC2Xmlj1tbjjNvTSMwNtpmFzPX21OzgpZxL5ufaAPv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27ee1d6b41f38668e002eb15e832c2c52da37422" title="I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an AI-powered interview preparation tool because I know how tough it can be to get good, specific feedback when practising for technical interviews.&lt;/p&gt; &lt;p&gt;The idea is to use local Large Language Models (via Ollama) to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Analyse your resume and extract key skills.&lt;/li&gt; &lt;li&gt;Generate dynamic interview questions based on those skills and chosen difficulty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;And most importantly: Evaluate your answers!&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After you go through a mock interview session (answering questions in the app), you'll go to an Evaluation Page. Here, an AI &amp;quot;coach&amp;quot; will analyze all your answers and give you feedback like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An overall score.&lt;/li&gt; &lt;li&gt;What you did well.&lt;/li&gt; &lt;li&gt;Where you can improve.&lt;/li&gt; &lt;li&gt;How you scored on things like accuracy, completeness, and clarity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I'd love your input:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As someone practicing for interviews, would you prefer feedback immediately after each question, or all at the end?&lt;/li&gt; &lt;li&gt;What kind of feedback is most helpful to you? Just a score? Specific examples of what to say differently?&lt;/li&gt; &lt;li&gt;Are there any particular pain points in interview prep that you wish an AI tool could solve?&lt;/li&gt; &lt;li&gt;What would make an AI interview coach truly valuable for you?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a passion project (using Python/FastAPI on the backend, React/TypeScript on the frontend), and I'm keen to build something genuinely useful. Any thoughts or feature requests would be amazing!&lt;/p&gt; &lt;p&gt;ðŸš€ &lt;strong&gt;P.&lt;/strong&gt;S. This project was a ton of fun, and I'm itching for my next AI challenge! If you or your team are doing innovative work in &lt;strong&gt;Computer Vision or LL&lt;/strong&gt;Ms and are looking for a passionate dev, I'd love to chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1y00f0j9tm2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzxni/im_building_an_ai_interview_prep_tool_to_get_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzxni/im_building_an_ai_interview_prep_tool_to_get_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T01:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktsrqv</id>
    <title>Cosyvoice 2 vs Dia 1.6b - which one is better overall?</title>
    <updated>2025-05-23T19:48:21+00:00</updated>
    <author>
      <name>/u/Xodnil</name>
      <uri>https://old.reddit.com/user/Xodnil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone get to test both tts models? If yes, which sounds more realistic from your POV?&lt;/p&gt; &lt;p&gt;Both models are very close, but I find CosyVoice slightly ahead due to its zero-shot capabilities; however, one downside is that you may need to use specific models for different tasks (e.g., zero-shot, cross-lingual).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nari-labs/dia"&gt;https://github.com/nari-labs/dia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;https://github.com/FunAudioLLM/CosyVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xodnil"&gt; /u/Xodnil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsrqv/cosyvoice_2_vs_dia_16b_which_one_is_better_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsrqv/cosyvoice_2_vs_dia_16b_which_one_is_better_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsrqv/cosyvoice_2_vs_dia_16b_which_one_is_better_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T19:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiere</id>
    <title>A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG</title>
    <updated>2025-05-23T12:33:08+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt; &lt;img alt="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" src="https://preview.redd.it/bn39fvozzi2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9702ce1baab0703350e9800e0619c24d489b70eb" title="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This project demonstrates how to implement Cache-Augmented Generation (CAG) in an LLM and shows its performance gains compared to RAG. &lt;/p&gt; &lt;p&gt;Project Link: &lt;a href="https://github.com/ronantakizawa/cacheaugmentedgeneration"&gt;https://github.com/ronantakizawa/cacheaugmentedgeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CAG preloads document content into an LLMâ€™s context as a precomputed key-value (KV) cache. &lt;/p&gt; &lt;p&gt;This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. &lt;/p&gt; &lt;p&gt;CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bn39fvozzi2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktz4ss</id>
    <title>Ollama Qwen2.5-VL 7B &amp; OCR</title>
    <updated>2025-05-24T00:41:58+00:00</updated>
    <author>
      <name>/u/PleasantCandidate785</name>
      <uri>https://old.reddit.com/user/PleasantCandidate785</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Started working with data extraction from scanned documents today using Open WebUI, Ollama and Qwen2.5-VL 7B. I had some shockingly good initial results, but when I tried to get the model to extract more data it started loosing detail that it had previously reported correctly.&lt;/p&gt; &lt;p&gt;One issue was that the images I am dealing with a are scanned as individual page TIFF files with CCITT Group4 Fax compression. I had to convert them to individual JPG files to get WebUI to properly upload them. It has trouble maintaining the order of the files, though. I don't know if it's processing them through pytesseract in random order, or if they are returned out of order, but if I just select say a 5-page document and grab to WebUI, they upload in random order. Instead, I have to drag the files one at a time, in order into WebUI to get anything near to correct.&lt;/p&gt; &lt;p&gt;Is there a better way to do this?&lt;/p&gt; &lt;p&gt;Also, how could my prompt be improved?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;These images constitute a scanned legal document. Please give me the following information from the text: 1. Document type (Examples include but are not limited to Warranty Deed, Warranty Deed with Vendors Lien, Deed of Trust, Quit Claim Deed, Probate Document) 2. Instrument Number 3. Recording date 4. Execution Date Defined as the date the instrument was signed or acknowledged. 5. Grantor (If this includes any special designations including but not limited to &amp;quot;and spouse&amp;quot;, &amp;quot;a single person&amp;quot;, &amp;quot;as executor for&amp;quot;, please include that designation.) 6. Grantee (If this includes any special designations including but not limited to &amp;quot;and spouse&amp;quot;, &amp;quot;a single person&amp;quot;, &amp;quot;as executor for&amp;quot;, please include that designation.) 7. Legal description of the property, 8. Any References to the same property, 9. Any other documents referred to by this document. Legal description is defined as the lot numbers (if any), Block numbers (if any), Subdivision name (if any), Number of acres of property (if any), Name of the Survey of Abstract and Number of the Survey or abstract where the property is situated. A reference to the same property is defined as any instance where a phrase similar to &amp;quot;being the same property described&amp;quot; followed by a list of tracts, lots, parcels, or acreages and a document description. Other documents referred to by this document includes but is not limited to any deeds, mineral deeds, liens, affidavits, exceptions, reservations, restrictions that might be mentioned in the text of this document. Please provide the items in list format with the item designation formatted as bold text. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The system seems to get lost with this prompt whereas as more simple prompt like &lt;/p&gt; &lt;pre&gt;&lt;code&gt;These images constitute a legal document. Please give me the following information from the text: 1. Grantor, 2. Grantee, 3. Legal description of the property, 4. any other documents referred to by this document. Legal description is defined as the lot numbers (if any), Block numbers (if any), Subdivision name (if any), Number of acres of property (if any), Name of the Survey of Abstract and Number of the Survey or abstract where the property is situated. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;gives a better response with the same document, but is missing some details.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantCandidate785"&gt; /u/PleasantCandidate785 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktz4ss/ollama_qwen25vl_7b_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktz4ss/ollama_qwen25vl_7b_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktz4ss/ollama_qwen25vl_7b_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T00:41:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktqgk0</id>
    <title>Tested Qwen3 all models on CPU (i5-10210U), RTX 3060 12GB, and RTX 3090 24GB</title>
    <updated>2025-05-23T18:12:05+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Model Testing Results (CPU + GPU)&lt;/p&gt; &lt;p&gt;Model | Hardware | Load | Answer | Speed (t/s)&lt;/p&gt; &lt;p&gt;------------------|--------------------------------------------|--------------------|---------------------|------------&lt;/p&gt; &lt;p&gt;Qwen3-0.6B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 31.65&lt;/p&gt; &lt;p&gt;Qwen3-1.7B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 14.87&lt;/p&gt; &lt;p&gt;Qwen3-4B | Laptop (i5-10210U, 16GB RAM) | CPU only | Correct (misleading)| 7.03&lt;/p&gt; &lt;p&gt;Qwen3-8B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 4.06&lt;/p&gt; &lt;p&gt;Qwen3-8B | Desktop (5800X, 32GB RAM, RTX 3060) | 100% GPU | Incorrect | 46.80&lt;/p&gt; &lt;p&gt;Qwen3-14B | Desktop (5800X, 32GB RAM, RTX 3060) | 94% GPU / 6% CPU | Correct | 19.35&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Laptop (i5-10210U, 16GB RAM) | CPU only | Correct | 3.27&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Desktop (5800X, 32GB RAM, RTX 3060) | 49% GPU / 51% CPU | Correct | 15.32&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Desktop (5800X, 64GB RAM, RTX 3090) | 100% GPU | Correct | 105.57&lt;/p&gt; &lt;p&gt;Qwen3-32B | Desktop (5800X, 64GB RAM, RTX 3090) | 100% GPU | Correct | 30.54&lt;/p&gt; &lt;p&gt;Qwen3-235B-A22B | Desktop (5800X, 128GB RAM, RTX 3090) | 15% GPU / 85% CPU | Correct | 2.43&lt;/p&gt; &lt;p&gt;Here is the full video of all tests: &lt;a href="https://youtu.be/kWjJ4F09-cU"&gt;https://youtu.be/kWjJ4F09-cU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T18:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgxxa</id>
    <title>AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning</title>
    <updated>2025-05-23T11:15:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt; &lt;img alt="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" src="https://external-preview.redd.it/_aQtUZTp2VBwp5MK35YBXI25HOZhHuEgT9O1MgXLN7I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb4068255f83d79055a6f21dccc859d949b32f54" title="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksyicp</id>
    <title>Introducing the world's most powerful model</title>
    <updated>2025-05-22T18:45:16+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt; &lt;img alt="Introducing the world's most powerful model" src="https://preview.redd.it/hqx8fzosod2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96d0c448070aead295d21d9be7e8fd395520a72b" title="Introducing the world's most powerful model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqx8fzosod2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T18:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktqsog</id>
    <title>"Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small" can't they just say they have fine tuned mistral small or it's kind of wrapper?</title>
    <updated>2025-05-23T18:25:47+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"&gt; &lt;img alt="&amp;quot;Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small&amp;quot; can't they just say they have fine tuned mistral small or it's kind of wrapper?" src="https://external-preview.redd.it/WzIS2JjZo2kST66vrrt4qEsayLue07AZ01pMdBT5Wtc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd18d82c4dcfc9c4f7f92bcffba8e25084b30453" title="&amp;quot;Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small&amp;quot; can't they just say they have fine tuned mistral small or it's kind of wrapper?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sarvam.ai/blogs/sarvam-m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T18:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgvoe</id>
    <title>server audio input has been merged into llama.cpp</title>
    <updated>2025-05-23T11:12:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt; &lt;img alt="server audio input has been merged into llama.cpp" src="https://external-preview.redd.it/w-TAeYFPuT8QOBKphdcEnCVLkPeOPrOjKse263sRyos.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30ef27dc1e8b5fa00168ba96a589759da20990b" title="server audio input has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13714"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktojxe</id>
    <title>So what are some cool projects you guys are running on you local llms?</title>
    <updated>2025-05-23T16:55:33+00:00</updated>
    <author>
      <name>/u/itzikhan</name>
      <uri>https://old.reddit.com/user/itzikhan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find good ideas to implement on my setup, or maybe get some inspiration to do something on my own &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itzikhan"&gt; /u/itzikhan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T16:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktm0hd</id>
    <title>AI becoming too sycophantic? Noticed Gemini 2.5 praising me instead of solving the issue</title>
    <updated>2025-05-23T15:11:54+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I get the feeling that the trend of making AI more inclined towards flattery and overly focused on a user's feelings is somehow degrading its ability to actually solve problems. Is it just me? For instance, I've recently noticed that Gemini 2.5, instead of giving a direct solution, will spend time praising me, saying I'm using the right programming paradigms, blah blah blah, and that my code should generally work. In the end, it was no help at all. Qwen2 32B, on the other hand, just straightforwardly pointed out my error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktkhp8</id>
    <title>Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results</title>
    <updated>2025-05-23T14:08:12+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt; &lt;img alt="Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results" src="https://b.thumbs.redditmedia.com/DNvc1pfuOj2yfO0pW08qBgw58D4CJdgut3-YAYKWjzQ.jpg" title="Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finished benchmarking Claude 4 (Sonnet) across a range of document understanding tasks, and the results areâ€¦ not that good. It's currently &lt;strong&gt;ranked 7th overall&lt;/strong&gt; on the leaderboard.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weak performance in OCR â€“ Claude 4 lags behind even smaller models like GPT-4.1-nano and InternVL3-38B-Instruct.&lt;/li&gt; &lt;li&gt;Rotation sensitivity â€“ We tested OCR robustness with slightly rotated images ([-5Â°, +5Â°]). Most large models had a 2â€“3% drop in accuracy. Claude 4 dropped 9%.&lt;/li&gt; &lt;li&gt;Poor on handwritten documents â€“ Scored only 51.64%, while Gemini 2.0 Flash got 71.24%. It also struggled with handwritten datasets in other tasks like key information extraction.&lt;/li&gt; &lt;li&gt;Chart VQA and visual tasks â€“ Performed decently but still behind Gemini, Claude 3.7, and GPT-4.5/o4-mini.&lt;/li&gt; &lt;li&gt;Long document understanding â€“ Claude 3.7 Sonnet (reasoning:low) ranked 1st. Claude 4 Sonnet ranked 13th.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One bright spot: table extraction&lt;/strong&gt; â€“ Claude 4 Sonnet is currently ranked 1st, narrowly ahead of Claude 3.7 Sonnet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/72zkmcyogj2f1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8fb9e86ca0bcfe129e25dab934d06818f7d638"&gt;https://preview.redd.it/72zkmcyogj2f1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8fb9e86ca0bcfe129e25dab934d06818f7d638&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://idp-leaderboard.org/"&gt;https://idp-leaderboard.org/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Codebase: &lt;a href="https://github.com/NanoNets/docext"&gt;https://github.com/NanoNets/docext&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How has everyoneâ€™s experience with the models been so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktudaj</id>
    <title>Best local coding model right now?</title>
    <updated>2025-05-23T20:57:09+00:00</updated>
    <author>
      <name>/u/Combinatorilliance</name>
      <uri>https://old.reddit.com/user/Combinatorilliance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I was very active here about a year ago, but I've been using Claude a lot the past few months.&lt;/p&gt; &lt;p&gt;I do like claude a lot, but it's not magic and smaller models are actually quite a lot nicer in the sense that I have far, far more control over &lt;/p&gt; &lt;p&gt;I have a 7900xtx, and I was eyeing gemma 27b for local coding support?&lt;/p&gt; &lt;p&gt;Are there any other models I should be looking at? Qwen 3 maybe?&lt;/p&gt; &lt;p&gt;Perhaps a model specifically for coding? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Combinatorilliance"&gt; /u/Combinatorilliance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktudaj/best_local_coding_model_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T20:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku1444</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on Your Phone</title>
    <updated>2025-05-24T02:28:43+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt; &lt;img alt="A Privacy-Focused Perplexity That Runs Locally on Your Phone" src="https://external-preview.redd.it/H2OOCv1bv050E4CBNwcheCR0p5galvx3UpT4d2t0NLs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dc808e4d075ada1aefee047e33becc1859e20d5" title="A Privacy-Focused Perplexity That Runs Locally on Your Phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player"&gt;https://reddit.com/link/1ku1444/video/e80rh7mb5n2f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! ðŸ‘‹&lt;/p&gt; &lt;p&gt;I wanted to share &lt;strong&gt;MyDeviceAI&lt;/strong&gt; - a completely private alternative to Perplexity that runs entirely on your device. If you're tired of your search queries being sent to external servers and want the power of AI search without the privacy trade-offs, this might be exactly what you're looking for.&lt;/p&gt; &lt;h1&gt;What Makes This Different&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Complete Privacy&lt;/strong&gt;: Unlike Perplexity or other AI search tools, MyDeviceAI keeps everything local. Your search queries, the results, and all processing happen on your device. No data leaves your phone, period.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SearXNG Integration&lt;/strong&gt;: The app now comes with built-in SearXNG search - no configuration needed. You get comprehensive search results with image previews, all while maintaining complete privacy. SearXNG aggregates results from multiple search engines without tracking you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Local AI Processing&lt;/strong&gt;: Powered by Qwen 3, the AI model runs entirely on your device. Modern iPhones get lightning-fast responses, and even older models are fully supported (just a bit slower).&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Free &amp;amp; Open Source&lt;/strong&gt;: Check out the code at &lt;a href="http://github.com/navedmerchant/MyDeviceAI"&gt;MyDeviceAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search + AI&lt;/strong&gt;: Get the best of both worlds - current information from the web processed by local AI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: 30+ days of conversation history, all stored locally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thinking Mode&lt;/strong&gt;: Complex reasoning capabilities for challenging problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Wait Time&lt;/strong&gt;: Model loads asynchronously in the background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personalization&lt;/strong&gt;: Beta feature for custom user contexts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Recent Updates&lt;/h1&gt; &lt;p&gt;The latest release includes a prettier UI, out-of-the-box SearXNG integration, image previews with search results, and tons of bug fixes.&lt;/p&gt; &lt;p&gt;This app has completely replaced ChatGPT for me, I am a very curious person and keep using it for looking up things that come to my mind, and its always spot on. I also compared it with Perplexity and while Perplexity has a slight edge in some cases, MyDeviceAI generally gives me the correct information and completely to the point. Download at: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281"&gt;MyDeviceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback. Please leave a review on the AppStore if this worked for you and solved a problem, and if you like to support further development of this App!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T02:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktpz29</id>
    <title>Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?</title>
    <updated>2025-05-23T17:52:15+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt; &lt;img alt="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" src="https://b.thumbs.redditmedia.com/n5NQk6PSosGh3XCgL-CRJ01dhOkMxqKYXWlOm95gUJw.jpg" title="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sits on my office desk for running very large context prompts (50K words) with QwQ 32B. Gotta be offline because they have a lot of P.I.I.&lt;/p&gt; &lt;p&gt;Had it in a Mechanic Master c34plus (25L) but CPU fans (Scythe Grand Tornado 3,000rpm) kept ramping up because two 5090s were blasting the radiator in a confined space, and could only fit a 1300W PSU in that tiny case which meant heavy power limiting for the CPU and GPUs.&lt;/p&gt; &lt;p&gt;Paid $3,200 each for the 5090 FE's and would have paid more. Couldn't be happier and this rig turns what used to take me 8 hours into 5 minutes of prompt processing and inference + 15 minutes of editing to output complicated 15 page reports.&lt;/p&gt; &lt;p&gt;Anytime I show a coworker what it can do, they immediately throw money at me and tell me to build them a rig, so I tell them I'll get them 80% of the performance for about $2,200 and I've built two dual 3090 local Al rigs for such coworkers so far.&lt;/p&gt; &lt;p&gt;Frame is a 3D printed one from Etsy by ArcadeAdamsParts. There were some minor issues with it, but Adam was eager to address them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktpz29"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T17:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktklo5</id>
    <title>Unmute by Kyutai: Make LLMs listen and speak</title>
    <updated>2025-05-23T14:12:46+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems nicely polished and apparently works with any LLM. Open-source in the coming weeks.&lt;/p&gt; &lt;p&gt;Demo uses Gemma 3 12B as base LLM (demo link in the blog post, reddit seems to auto-delete my post if I include it here).&lt;/p&gt; &lt;p&gt;If any Kyutai dev happens to lurk here, would love to hear about the memory requirements of the TTS &amp;amp; STT models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kyutai.org/2025/05/22/unmute.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T14:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktsqit</id>
    <title>Best Vibe Code tools (like Cursor) but are free and use your own local LLM?</title>
    <updated>2025-05-23T19:46:58+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company.&lt;/p&gt; &lt;p&gt;Does anybody know of any good Vibe Coding tools, as good or better than Cursor, that run on your own local LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;EDIT: Especially tools that integrate with ollama's API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T19:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktoh78</id>
    <title>LLMI system I (not my money) got for our group</title>
    <updated>2025-05-23T16:52:23+00:00</updated>
    <author>
      <name>/u/SandboChang</name>
      <uri>https://old.reddit.com/user/SandboChang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt; &lt;img alt="LLMI system I (not my money) got for our group" src="https://preview.redd.it/lgjexuw8ak2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3260ccc53dd2f7cca5692637366920fd7a9928ec" title="LLMI system I (not my money) got for our group" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandboChang"&gt; /u/SandboChang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lgjexuw8ak2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T16:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiq99</id>
    <title>I accidentally too many P100</title>
    <updated>2025-05-23T12:48:51+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt; &lt;img alt="I accidentally too many P100" src="https://b.thumbs.redditmedia.com/IdF4SU4XHKp-_JI6o-Y6kol8-cLrv94jdBxKlq9CTYI.jpg" title="I accidentally too many P100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I had quite positive results with a P100 last summer, so when R1 came out, I decided to try if I could put 16 of them in a single pc... and I could.&lt;/p&gt; &lt;p&gt;Not the fastest think in the universe, and I am not getting awesome PCIE speed (2@4x). But it works, is still cheaper than a 5090, and I hope I can run stuff with large contexts.&lt;/p&gt; &lt;p&gt;I hoped to run llama4 with large context sizes, and scout runs almost ok, but llama4 as a model is abysmal. I tried to run Qwen3-235B-A22B, but the performance with llama.cpp is pretty terrible, and I haven't been able to get it working with the vllm-pascal (ghcr.io/sasha0552/vllm:latest).&lt;/p&gt; &lt;p&gt;If you have any pointers on getting Qwen3-235B to run with any sort of parallelism, or want me to benchmark any model, just say so!&lt;/p&gt; &lt;p&gt;The MB is a 2014 intel S2600CW with dual 8-core xeons, so CPU performance is rather low. I also tried to use MB with an EPYC, but it doesn't manage to allocate the resources to all PCIe devices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktiq99"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kty4mh</id>
    <title>Anyone else prefering non thinking models ?</title>
    <updated>2025-05-23T23:50:26+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far Ive experienced non CoT models to have more curiosity and asking follow up questions. Like gemma3 or qwen2.5 72b. Tell them about something and they ask follow up questions, i think CoT models ask them selves all the questions and end up very confident. I also understand the strength of CoT models for problem solving, and perhaps thats where their strength is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kty4mh/anyone_else_prefering_non_thinking_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktlz3w</id>
    <title>96GB VRAM! What should run first?</title>
    <updated>2025-05-23T15:10:20+00:00</updated>
    <author>
      <name>/u/Mother_Occasion_8076</name>
      <uri>https://old.reddit.com/user/Mother_Occasion_8076</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt; &lt;img alt="96GB VRAM! What should run first?" src="https://preview.redd.it/co0zhh06sj2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64b43f0124c5d5b397b2efd848e6e83c1dcfcfdc" title="96GB VRAM! What should run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to make a fake company domain name to order this from a supplier. They wouldnâ€™t even give me a quote with my Gmail address. I got the card though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother_Occasion_8076"&gt; /u/Mother_Occasion_8076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co0zhh06sj2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktzwgq</id>
    <title>Ollama finally acknowledged llama.cpp officially</title>
    <updated>2025-05-24T01:22:35+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the 0.7.1 release, they introduce the capabilities of their multimodal engine. At the end in the &lt;a href="https://imgur.com/a/zKMizcr"&gt;acknowledgments section&lt;/a&gt; they thanked the GGML project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/multimodal-models"&gt;https://ollama.com/blog/multimodal-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktzwgq/ollama_finally_acknowledged_llamacpp_officially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-24T01:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktx15j</id>
    <title>Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜</title>
    <updated>2025-05-23T22:56:42+00:00</updated>
    <author>
      <name>/u/RoyalCities</name>
      <uri>https://old.reddit.com/user/RoyalCities</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt; &lt;img alt="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜" src="https://external-preview.redd.it/b3A3aWt5dmIzbTJmMSKAZduYkWK-j-eA22aXbm6vzflALmDerWrgdNPvGQZJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=179ccde64a277eb295ce8f54c6f88facef7ddb65" title="Guys! I managed to build a 100% fully local voice AI with Ollama that can have full conversations, control all my smart devices AND now has both short term + long term memory. ðŸ¤˜" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found out recently that Amazon/Alexa is going to use ALL users vocal data with ZERO opt outs for their new Alexa+ service so I decided to build my own that is 1000x better and runs fully local. &lt;/p&gt; &lt;p&gt;The stack uses Home Assistant directly tied into Ollama. The long and short term memory is a custom automation design that I'll be documenting soon and providing for others. &lt;/p&gt; &lt;p&gt;This entire set up runs 100% local and you could probably get away with the whole thing working within / under 16 gigs of VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalCities"&gt; /u/RoyalCities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iigum5tb3m2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T22:56:42+00:00</published>
  </entry>
</feed>
