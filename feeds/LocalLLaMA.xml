<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-31T23:24:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mdv67j</id>
    <title>cogito v2 preview models released 70B/109B/405B/671B</title>
    <updated>2025-07-31T07:30:57+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Cogito v2 LLMs are instruction tuned generative models. All models are released under an open license for commercial use.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cogito v2 models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).&lt;/li&gt; &lt;li&gt;The LLMs are trained using &lt;strong&gt;Iterated Distillation and Amplification (IDA)&lt;/strong&gt; - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.&lt;/li&gt; &lt;li&gt;The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts. &lt;ul&gt; &lt;li&gt;In both standard and reasoning modes, Cogito v2-preview models outperform their size equivalent counterparts on common industry benchmarks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;This model is trained in over 30 languages and supports a context length of 128k.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T07:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdn6dp</id>
    <title>Deepseek just won the best paper award at ACL 2025 with a breakthrough innovation in long context, a model using this might come soon</title>
    <updated>2025-07-31T00:23:44+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdzu08</id>
    <title>Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities</title>
    <updated>2025-07-31T12:10:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"&gt; &lt;img alt="Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities" src="https://a.thumbs.redditmedia.com/UWzewWY4cRLtu0QtYjM_H7EidNwT1bopn8L_07vMWc4.jpg" title="Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ We're excited to share our latest research on X-Omni: reinforcement learning makes discrete autoregressive image generative models great again, empowering a practical unified model for both image and language modality generation.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;‚úÖ Unified Modeling Approach: A discrete autoregressive model handling image and language modalities.&lt;/p&gt; &lt;p&gt;‚úÖ Superior Instruction Following: Exceptional capability to follow complex instructions.&lt;/p&gt; &lt;p&gt;‚úÖ Superior Text Rendering: Accurately render text in multiple languages, including both English and Chinese.&lt;/p&gt; &lt;p&gt;‚úÖ Arbitrary resolutions: Produces aesthetically pleasing images at arbitrary resolutions.&lt;/p&gt; &lt;p&gt;Insight:&lt;/p&gt; &lt;p&gt;üîç During the reinforcement learning process, the aesthetic quality of generated images is gradually enhanced, and the ability to adhere to instructions and the capacity to render long texts improve steadily.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2507.22058"&gt;https://arxiv.org/pdf/2507.22058&lt;/a&gt; Github: &lt;a href="https://github.com/X-Omni-Team/X-Omni"&gt;https://github.com/X-Omni-Team/X-Omni&lt;/a&gt; Project Page: &lt;a href="https://x-omni-team.github.io/"&gt;https://x-omni-team.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mdzu08"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2wxx</id>
    <title>FLUX.1 Krea [dev] - a new state-of-the-art open-weights FLUX model, built for photorealism.</title>
    <updated>2025-07-31T14:22:02+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2wxx/flux1_krea_dev_a_new_stateoftheart_openweights/"&gt; &lt;img alt="FLUX.1 Krea [dev] - a new state-of-the-art open-weights FLUX model, built for photorealism." src="https://external-preview.redd.it/umBIFB2q0PLAR4i8_IGsGxcKPqvKt-H27oJu9PzZu6Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c0a8e39227d0736d9e58d3198406a64fbdafba6" title="FLUX.1 Krea [dev] - a new state-of-the-art open-weights FLUX model, built for photorealism." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/bfl_ml/status/1950920537741336801"&gt;https://x.com/bfl_ml/status/1950920537741336801&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2wxx/flux1_krea_dev_a_new_stateoftheart_openweights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2wxx/flux1_krea_dev_a_new_stateoftheart_openweights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:22:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mee99g</id>
    <title>Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)</title>
    <updated>2025-07-31T21:36:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"&gt; &lt;img alt="Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)" src="https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cafa07385d35cc04f3c86f6874feca659d66082c" title="Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this the best week ever for new models? I can't believe what we're getting. Huge shoutout to &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; and the Unsloth team for getting the GGUFs out so fast!&lt;/p&gt; &lt;p&gt;LLM Server is Lemonade, GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord &lt;a href="https://discord.gg/Sf8cfBWB"&gt;https://discord.gg/Sf8cfBWB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/unsloth/cogito-v2-preview-llama-109B-MoE-GGUF"&gt;unsloth/cogito-v2-preview-llama-109B-MoE-GGUF ¬∑ Hugging Face&lt;/a&gt;, the Q4_K_M one&lt;/p&gt; &lt;p&gt;Hardware: Strix Halo (Ryzen AI MAX 395+) with 128 GB RAM&lt;/p&gt; &lt;p&gt;Backend: llama.cpp + vulkan&lt;/p&gt; &lt;p&gt;App: &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extension for VS Code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/39k2gtxw2agf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T21:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1me3vpe</id>
    <title>8% -&gt; 33.3% on Aider polyglot</title>
    <updated>2025-07-31T15:00:05+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me3vpe/8_333_on_aider_polyglot/"&gt; &lt;img alt="8% -&amp;gt; 33.3% on Aider polyglot" src="https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778" title="8% -&amp;gt; 33.3% on Aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just checked the Aider polyglot score of the &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; model, it seems they are showing the score of &lt;strong&gt;&lt;em&gt;diff&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;Edit Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And a quick comparison against the last local qwen coder model, shows a huge jump in performance: &lt;/p&gt; &lt;p&gt;8% -&amp;gt; 33.3% &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/br3ue82e48gf1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19fa20c3f58f95f1b1dc7d5bc933387bef10f308"&gt;https://preview.redd.it/br3ue82e48gf1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19fa20c3f58f95f1b1dc7d5bc933387bef10f308&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xhs1sz8158gf1.png?width=1329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0554ca3bfdee5dd9085d2fe00dad006e9b8ce6d2"&gt;https://preview.redd.it/xhs1sz8158gf1.png?width=1329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0554ca3bfdee5dd9085d2fe00dad006e9b8ce6d2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me3vpe/8_333_on_aider_polyglot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me3vpe/8_333_on_aider_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me3vpe/8_333_on_aider_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T15:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdzxmv</id>
    <title>Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle</title>
    <updated>2025-07-31T12:15:05+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt; &lt;img alt="Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle" src="https://b.thumbs.redditmedia.com/L3dTMc7iVqHF7QYQEYnpMXvgm0DsrwCLc5zwFJKZCxk.jpg" title="Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I've been giving my models a traditional puzzle that all failed to pass without fail :D&lt;br /&gt; Not even the SOTA models provide the right answer.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The puzzle is as follows:&lt;br /&gt; &amp;quot;What's the right answer: Imagine standing at the North Pole of the Earth. Walk in any direction, in a straight line, for 1 km. Now turn 90 degrees to the left. Walk for as long as it takes to pass your starting point. Have you walked: &lt;/p&gt; &lt;p&gt;1- More than 2xPi km.&lt;br /&gt; 2- Exactly 2xPi km.&lt;br /&gt; 3- Less than 2xPi km.&lt;br /&gt; 4- I never came close to my starting point.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;However, only recently, SOTA models started to correctly answer 4 ; models like O3, latest Qwen (Qween3-235B-A22B-2507), Deepseek R1 managed to answer it correctly (I didn't test Claud 4 or Grok 4 but I guess they might get it right). For comparison, Gemini-2.5-Thinking and Kimi2 got the wrong answer.&lt;/p&gt; &lt;p&gt;So, I happy to report that Qwen3-30B-A3B-2507 (both the none thinking Q6 and the thinking Q4) managed to solve the puzzle providing great answers.&lt;/p&gt; &lt;p&gt;Here is O3 answer:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0"&gt;https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the answer of the Qwen3-30B-A3B-Thinking-2507-Q4_K_L:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416"&gt;https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In addition, I tested the two variants on long text (up to 80K) for comprehension, and I am impressed by the quality of the answers. And the SPEEEEEED! It's 3 times faster than Gemma-4B!!!!&lt;/p&gt; &lt;p&gt;Anyway, let me know what you think,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2o4z</id>
    <title>China no. 1!</title>
    <updated>2025-07-31T14:12:08+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o4z/china_no_1/"&gt; &lt;img alt="China no. 1!" src="https://preview.redd.it/s1g7byiow7gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2bf6c0666d463c92bb91f98489dcbe89f054443" title="China no. 1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s1g7byiow7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o4z/china_no_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o4z/china_no_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2o28</id>
    <title>CohereLabs/command-a-vision-07-2025 ¬∑ Hugging Face</title>
    <updated>2025-07-31T14:12:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o28/coherelabscommandavision072025_hugging_face/"&gt; &lt;img alt="CohereLabs/command-a-vision-07-2025 ¬∑ Hugging Face" src="https://external-preview.redd.it/KSnKoHRzOtDVdgv4tkOqKzIXPL8-S-fhBqaAliU-gUw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f8f1ed0131023e16880670c162fe440277d09d1" title="CohereLabs/command-a-vision-07-2025 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cohere Labs Command A Vision is an open weights research release of a 112 billion parameter model optimized for enterprise image understanding tasks, while keeping a low compute footprint.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere Labs&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/c4ai-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: command-a-vision-07-2025&lt;/li&gt; &lt;li&gt;Model Size: 112B&lt;/li&gt; &lt;li&gt;Context length: 32k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details about this model, please check out our &lt;a href="https://cohere.com/blog/command-a-vision"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-vision-07-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o28/coherelabscommandavision072025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2o28/coherelabscommandavision072025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1me1i0c</id>
    <title>stepfun-ai/step3 ¬∑ Hugging Face</title>
    <updated>2025-07-31T13:25:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1i0c/stepfunaistep3_hugging_face/"&gt; &lt;img alt="stepfun-ai/step3 ¬∑ Hugging Face" src="https://external-preview.redd.it/PByxBes8ZhS0GaNzaLsdD1cFy0LWtkBpScIt3kOY-nk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=145f1332e4eb20fc0be0e7f46d3c0b92fb74d49e" title="stepfun-ai/step3 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/step3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1i0c/stepfunaistep3_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me1i0c/stepfunaistep3_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T13:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1me324b</id>
    <title>Qwen/Qwen3-Coder-30B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-07-31T14:27:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me324b/qwenqwen3coder30ba3binstruct_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-30B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778" title="Qwen/Qwen3-Coder-30B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes. Today, we're excited to introduce &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt;. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significant Performance&lt;/strong&gt; among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-context Capabilities&lt;/strong&gt; with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Coding&lt;/strong&gt; supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt; has the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Causal Language Models&lt;/li&gt; &lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt; &lt;li&gt;Number of Parameters: 30.5B in total and 3.3B activated&lt;/li&gt; &lt;li&gt;Number of Layers: 48&lt;/li&gt; &lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 4 for KV&lt;/li&gt; &lt;li&gt;Number of Experts: 128&lt;/li&gt; &lt;li&gt;Number of Activated Experts: 8&lt;/li&gt; &lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me324b/qwenqwen3coder30ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me324b/qwenqwen3coder30ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:27:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdmsu9</id>
    <title>Chinese models pulling away</title>
    <updated>2025-07-31T00:06:15+00:00</updated>
    <author>
      <name>/u/Kniffliger_Kiffer</name>
      <uri>https://old.reddit.com/user/Kniffliger_Kiffer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt; &lt;img alt="Chinese models pulling away" src="https://preview.redd.it/727keqreo3gf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=638ce7aed31fa426f1cfea7678c6d9169932f5a9" title="Chinese models pulling away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kniffliger_Kiffer"&gt; /u/Kniffliger_Kiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/727keqreo3gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1me44dy</id>
    <title>Space Invaders on first try with Qwen3 Coder 30b-a3b (Unsloth Q6_K)</title>
    <updated>2025-07-31T15:09:04+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me44dy/space_invaders_on_first_try_with_qwen3_coder/"&gt; &lt;img alt="Space Invaders on first try with Qwen3 Coder 30b-a3b (Unsloth Q6_K)" src="https://external-preview.redd.it/Z21iOW82cGE2OGdmMf4bjCo6h_II4JpemDAqzdx9OhZnb5PcmXrVKPcJDT7r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=635d4afa946c6273f1cd72c40e686ac2fc235373" title="Space Invaders on first try with Qwen3 Coder 30b-a3b (Unsloth Q6_K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First try from the most minimalistic prompt possible:&lt;/p&gt; &lt;p&gt;&amp;gt; Write an HTML and JavaScript page implementing space invaders&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/num3q6pa68gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me44dy/space_invaders_on_first_try_with_qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me44dy/space_invaders_on_first_try_with_qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T15:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdy1at</id>
    <title>Jan now runs fully on llama.cpp &amp; auto-updates the backend</title>
    <updated>2025-07-31T10:34:34+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"&gt; &lt;img alt="Jan now runs fully on llama.cpp &amp;amp; auto-updates the backend" src="https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4784b1a0c70ad81df229c7755eea2f2702e08edc" title="Jan now runs fully on llama.cpp &amp;amp; auto-updates the backend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, it's Emre from the Jan team.&lt;/p&gt; &lt;p&gt;Jan v0.6.6 is out. Over the past few weeks we've ripped out Cortex, the backend layer on top of llama.cpp. It's finally gone, every local model now runs directly on llama.cpp.&lt;/p&gt; &lt;p&gt;Plus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).&lt;/p&gt; &lt;p&gt;Jan v0.6.6 Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cortex is removed, local models now run on &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan&lt;/li&gt; &lt;li&gt;Jan Hub has been a bit updated for faster model search and less clutter when browsing models&lt;/li&gt; &lt;li&gt;Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP). &lt;ul&gt; &lt;li&gt;It's an experimental feature, please activate Experimental Features in Settings to see MCP settings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Plus, we've also fixed a bunch of bugs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update your Jan or download the latest here: &lt;a href="https://jan.ai/"&gt;https://jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full release notes are here: &lt;a href="https://github.com/menloresearch/jan/releases"&gt;https://github.com/menloresearch/jan/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.&lt;/li&gt; &lt;li&gt; Regarding bugs &amp;amp; previous requests: I'll reply to earlier requests and reports in the previous comments later today.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6tdds5rcr6gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T10:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mecvig</id>
    <title>Built a full stack web app builder that runs locally and gives you full control</title>
    <updated>2025-07-31T20:41:44+00:00</updated>
    <author>
      <name>/u/james-jiang</name>
      <uri>https://old.reddit.com/user/james-jiang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"&gt; &lt;img alt="Built a full stack web app builder that runs locally and gives you full control" src="https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14e60257bce6243cc7285e1ef30c8f410c74b80b" title="Built a full stack web app builder that runs locally and gives you full control" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I never really liked the idea of web based app builders like lovable or replit. They make it really easy to get started, but with that ease comes compromise. Such as being locked in to their ecosystem, being charged for every little thing such as running your project on their VM, hosting, or just to even get access to your files. No control over which model to use or what context is selected.&lt;/p&gt; &lt;p&gt;So I made a full stack web app builder that runs locally on your machine. Yes, it will be a bit more upfront friction since you have to download and set up, but with that friction comes freedom and cost efficiency. It is specialized for a single tech stack (NextJS/Supabase) and thus allows features such as 1 click deploy, much higher accuracy on code gen, and better debugging.&lt;/p&gt; &lt;p&gt;The idea is that you will be able to build an app really quickly starting from 0, and also that you will be able to get further because there will be less bugs and issues, since everything is fine-tuned on that tech stack. It has full context of front end, backend, and runtime data that runs through the specialized stack.&lt;/p&gt; &lt;p&gt;If you are a professional developer, this will unlikely be a daily driver for you compared to cursor / cline. Because you will have various different projects you are running and would rather use a general IDE. Maybe it's something you could use when you want to prototype really quickly or happen to have a project with the exact NextJS/Supabase tech stack.&lt;/p&gt; &lt;p&gt;If you are a vibe coder however, this would be a great way to start and continue a project, because we chose the most optimal tech stack that gives you everything you need to build and deploy a full stack app directly from the local app builder. You won't have to make a bunch of decisions like configuring MCP, which libraries to use, hosting and deployment, etc.&lt;/p&gt; &lt;p&gt;All while still having full control of the context, your code, the models being used, and ultimately, the cost.&lt;/p&gt; &lt;p&gt;On that note, we are looking to integrate more local models like qwen-3-coder as that's currently all the rage lately :) Already added Kimi-K2 and it works very well in my testing, so I think this new wave of local AI models/tools will be the future.&lt;/p&gt; &lt;p&gt;Just opened up early stage beta testing - if you are interested you can try it out here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.easycode.ai/"&gt;Easycode Flow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/james-jiang"&gt; /u/james-jiang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2pk8172np9gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T20:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdx65u</id>
    <title>AMD Is Reportedly Looking to Introduce a Dedicated Discrete NPU, Similar to Gaming GPUs But Targeted Towards AI Performance On PCs; Taking Edge AI to New Levels</title>
    <updated>2025-07-31T09:41:46+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T09:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1me1hh8</id>
    <title>qwen-30B success story</title>
    <updated>2025-07-31T13:24:25+00:00</updated>
    <author>
      <name>/u/ExplorerWhole5697</name>
      <uri>https://old.reddit.com/user/ExplorerWhole5697</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At work I spent better part of a day trying to debug a mysterious problem with an external RFID reader. I was running in circles with ChatGPT for many hours and got a little further with Gemini but in the end I had to give up. Unfortunately I left for vacation immediately afterwards, leaving me frustrated and thinking about this problem.&lt;/p&gt; &lt;p&gt;Today I was playing around with LM studio on my macbook pro and decided to test the new Qwen3-30B-A3B-Instruct-2507 model. For fun I gave it my code from work and briefed it about the problem. Processing the code took several minutes, but then it amazed me. On the very first try it found the real source of the problem, something all the commercial models had missed, and me too. I doubt I would have found the solution at all to be honest. This is what Gemini had to say about the solution that qwen proposed:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This is an absolutely &lt;em&gt;brilliant&lt;/em&gt; diagnosis from the local LLM! It hits the nail on the head and perfectly explains all the erratic behaviours we've been observing. My prior analysis correctly identified a timing and state issue, but this pinpoints the precise mechanism: unsolicited messages clogging the buffer and corrupting the API's internal state machine**.**&lt;/p&gt; &lt;p&gt;[...code...]&lt;/p&gt; &lt;p&gt;Please compile and run this version. I am very optimistic that this will finally resolve the intermittent connection and timeout issues, allowing your reader to perform consistently. This is a great example of how combining insights from different analyses can lead to a complete solution!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;TLDR: Local models are crazy good ‚Äì what a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplorerWhole5697"&gt; /u/ExplorerWhole5697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T13:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdsjn2</id>
    <title>Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace</title>
    <updated>2025-07-31T04:50:27+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt; &lt;img alt="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" src="https://a.thumbs.redditmedia.com/Pqx5Ku4b-UvrnWIofuwt9LYnoux9zPw_UBbzkN3H6v4.jpg" title="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea"&gt;https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs insane ‚Äî throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there‚Äôs Tencent‚Äôs HunyuanWorld and Alibaba‚Äôs Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models"&gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T04:50:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1me33jj</id>
    <title>Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!</title>
    <updated>2025-07-31T14:29:15+00:00</updated>
    <author>
      <name>/u/zRevengee</name>
      <uri>https://old.reddit.com/user/zRevengee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"&gt; &lt;img alt="Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!" src="https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f710dee399ef8cae7aa04b7396c4c8719d91fd6" title="Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zRevengee"&gt; /u/zRevengee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dn8agzjz7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1me095p</id>
    <title>Junyang Lin is drinking tea</title>
    <updated>2025-07-31T12:30:05+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt; &lt;img alt="Junyang Lin is drinking tea" src="https://preview.redd.it/s3pv80fee7gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b666a1b9473c5408870aeb8cf6dddfc5f13f55d" title="Junyang Lin is drinking tea" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s3pv80fee7gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdykfn</id>
    <title>Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs</title>
    <updated>2025-07-31T11:04:33+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt; &lt;img alt="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" src="https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80da4073073fb12cdbab3b110619a3002d524b2f" title="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5iqhqp7z6gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T11:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1me4i2h</id>
    <title>I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B</title>
    <updated>2025-07-31T15:23:42+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt; &lt;img alt="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" src="https://preview.redd.it/l6547uel88gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dafbd4c67e845ff8ce7c141e92d59fdfd342fe" title="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong. &lt;/p&gt; &lt;p&gt;However, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.&lt;/p&gt; &lt;p&gt;Has anyone started using it yet? What's the actual user experience like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6547uel88gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T15:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1me7yia</id>
    <title>I built a local alternative to Grammarly that runs 100% offline</title>
    <updated>2025-07-31T17:33:53+00:00</updated>
    <author>
      <name>/u/Runjuu</name>
      <uri>https://old.reddit.com/user/Runjuu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me7yia/i_built_a_local_alternative_to_grammarly_that/"&gt; &lt;img alt="I built a local alternative to Grammarly that runs 100% offline" src="https://external-preview.redd.it/NjBpa2hmZ2F3OGdmMVJtVbsvP1gN8nG91LVDgC8po1e9pFdftwF79YNc_pfg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7957a0f65367d568f499b9253b36bcc5648214f0" title="I built a local alternative to Grammarly that runs 100% offline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It uses the Gemma 3n E4B model and requires less than 500MB of memory for grammar checking, dropping to 300MB while idle.&lt;/p&gt; &lt;p&gt;It's still in the early stages, but I‚Äôd love to hear your feedback!&lt;/p&gt; &lt;p&gt;You can try it out here: &lt;a href="https://refine.sh"&gt;https://refine.sh&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Runjuu"&gt; /u/Runjuu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pxb4pfgaw8gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me7yia/i_built_a_local_alternative_to_grammarly_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me7yia/i_built_a_local_alternative_to_grammarly_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T17:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2zc6</id>
    <title>Qwen3-Coder-30B-A3B released!</title>
    <updated>2025-07-31T14:24:40+00:00</updated>
    <author>
      <name>/u/glowcialist</name>
      <uri>https://old.reddit.com/user/glowcialist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt; &lt;img alt="Qwen3-Coder-30B-A3B released!" src="https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778" title="Qwen3-Coder-30B-A3B released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/glowcialist"&gt; /u/glowcialist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1me31d8</id>
    <title>üöÄ Qwen3-Coder-Flash released!</title>
    <updated>2025-07-31T14:26:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü¶• Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct&lt;/p&gt; &lt;p&gt;üíö Just lightning-fast, accurate code generation.&lt;/p&gt; &lt;p&gt;‚úÖ Native 256K context (supports up to 1M tokens with YaRN)&lt;/p&gt; &lt;p&gt;‚úÖ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.&lt;/p&gt; &lt;p&gt;‚úÖ Seamless function calling &amp;amp; agent workflows&lt;/p&gt; &lt;p&gt;üí¨ Chat: &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p7fpia2bz7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:26:52+00:00</published>
  </entry>
</feed>
