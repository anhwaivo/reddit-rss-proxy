<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-26T20:35:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k7tg8n</id>
    <title>GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)</title>
    <updated>2025-04-25T19:22:16+00:00</updated>
    <author>
      <name>/u/danihend</name>
      <uri>https://old.reddit.com/user/danihend</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt; &lt;img alt="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" src="https://external-preview.redd.it/M3Z4eDhhdmU3MXhlMYg3hh2y6NN7WC_nAJWjhF3jltCetUE7ORI41iUNIAJC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45713b47f9ba53d37e0b87a41e271222a2364c40" title="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title pretty much says it but just to clarify - it wasn't one-shot. It was prompt-&amp;gt;response-&amp;gt;error, then this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here is an error after running the sim: &amp;lt;error&amp;gt; Exception in Tkinter callback Traceback (most recent call last): File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 1967, in call return self.func(*args) ^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 861, in callit func(*args) File &amp;quot;c:\Users\username\VSCodeProjects\model_tests\balls\GLM49B_Q5KL_balls.py&amp;quot;, line 140, in update current_time_ms = float(current_time) ^^^^^^^^^^^^^^^^^^^ ValueError: could not convert string to float: 'after#2' &amp;lt;/error&amp;gt; Now think as hard as you can about why this is happening. Look at the entire script and consider how the parts work together. You are free to think as long as you need if you use thinking tags like this: &amp;lt;think&amp;gt;thoughts here&amp;lt;/think&amp;gt;. Once finished thinking, just provide the patch to the code. No need to rewrite it all. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then I applied the fix, got another error, replaced the original Assistant code block with the new code and presented the new error as if it were the 1st error by editing my message. I think that resulted in the working version.&lt;/p&gt; &lt;p&gt;So TL;DR - couple of prompts to get it working.&lt;/p&gt; &lt;p&gt;Simply pasting error after error did not work, but structured prompting with a bit of thinking seems to bring out some more potential.&lt;/p&gt; &lt;p&gt;Just thought I'd share in case it helps people with prompting it and just to show that it is not a bad model for it's size. The result is very similar to the 32B version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danihend"&gt; /u/danihend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zrjvo8ve71xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7krlm</id>
    <title>Gemma 3 fakes (and ignores) the system prompt</title>
    <updated>2025-04-25T13:20:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt; &lt;img alt="Gemma 3 fakes (and ignores) the system prompt" src="https://preview.redd.it/xuycbwnk4zwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fba119d92fca9059223ac136a22602c0f3b43b8" title="Gemma 3 fakes (and ignores) the system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screenshot shows what Gemma 3 said when I pointed out that it wasn't following its system prompt properly. &amp;quot;Who reads the fine print? 😉&amp;quot; - really, seriously, WTF?&lt;/p&gt; &lt;p&gt;At first I thought it may be an issue with the format/quant, an inference engine bug or just my settings or prompt. But digging deeper, I realized I had been fooled: While the [Gemma 3 chat template](&lt;a href="https://huggingface.co/google/gemma-3-27b-it/blob/main/chat%5C_template.json"&gt;https://huggingface.co/google/gemma-3-27b-it/blob/main/chat\_template.json&lt;/a&gt;) *does* support a system role, all it *really* does is dump the system prompt into the first user message. That's both ugly *and* unreliable - doesn't even use any special tokens, so there's no way for the model to differentiate between what the system (platform/dev) specified as general instructions and what the (possibly untrusted) user said. 🙈&lt;/p&gt; &lt;p&gt;Sure, the model still follows instructions like any other user input - but it never learned to treat them as higher-level system rules, so they're basically &amp;quot;optional&amp;quot;, which is why it ignored mine like &amp;quot;fine print&amp;quot;. That makes Gemma 3 utterly unreliable - so I'm switching to Mistral Small 3.1 24B Instruct 2503 which has proper system prompt support.&lt;/p&gt; &lt;p&gt;Hopefully Google will provide *real* system prompt support in Gemma 4 - or the community will deliver a better finetune in the meantime. For now, I'm hoping Mistral's vision capability gets wider support, since that's one feature I'll miss from Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xuycbwnk4zwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uxxk</id>
    <title>LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released</title>
    <updated>2025-04-25T20:25:39+00:00</updated>
    <author>
      <name>/u/ispolin</name>
      <uri>https://old.reddit.com/user/ispolin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt; &lt;img alt="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" src="https://b.thumbs.redditmedia.com/zPNTVgAISYkoCxdv3xYABX-74BRXUsT6QERGjZPbVto.jpg" title="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1"&gt;https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ispolin"&gt; /u/ispolin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rgyv</id>
    <title>Tiny Agents: a MCP-powered agent in 50 lines of code</title>
    <updated>2025-04-25T18:00:23+00:00</updated>
    <author>
      <name>/u/julien_c</name>
      <uri>https://old.reddit.com/user/julien_c</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt; &lt;img alt="Tiny Agents: a MCP-powered agent in 50 lines of code" src="https://external-preview.redd.it/fCTs8gI7KvvOKk5o8AQ0g6EQWi7h5KkDI0MBs8uNyiw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09d0dce63a6cd60077b6242bb1e5e6a8b6411b5f" title="Tiny Agents: a MCP-powered agent in 50 lines of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm a co-founder of HuggingFace and a big &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; fan.&lt;/p&gt; &lt;p&gt;Today I'm dropping Tiny Agents, a 50 lines-of-code Agent in Javascript 🔥&lt;/p&gt; &lt;p&gt;I spent the last few weeks diving into MCP (Model Context Protocol) to understand what the hype was about.&lt;/p&gt; &lt;p&gt;It is fairly simple, but still quite useful as a standard API to expose sets of Tools that can be hooked to LLMs.&lt;/p&gt; &lt;p&gt;But while implementing it I came to my second realization:&lt;/p&gt; &lt;p&gt;Once you have a MCP Client, an Agent is literally just a while loop on top of it. 🤯&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/tiny-agents"&gt;https://huggingface.co/blog/tiny-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5"&gt;https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/julien_c"&gt; /u/julien_c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k85izg</id>
    <title>5tps with Llama 4 Scout via Ollama and Unsloth dynamic quants, CPU only</title>
    <updated>2025-04-26T05:26:45+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that the llama 4 branch was just merged into ollama main, so I updated ollama and grabbed the 2.71 bit unsloth dynamic quant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ollama run --verbose hf.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF:Q2_K_XL&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It works!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;total duration: 2m7.090132071s&lt;/p&gt; &lt;p&gt;load duration: 45.646389ms&lt;/p&gt; &lt;p&gt;prompt eval count: 91 token(s)&lt;/p&gt; &lt;p&gt;prompt eval duration: 4.847635243s&lt;/p&gt; &lt;p&gt;prompt eval rate: 18.77 tokens/s&lt;/p&gt; &lt;p&gt;eval count: 584 token(s)&lt;/p&gt; &lt;p&gt;eval duration: 2m2.195920773s&lt;/p&gt; &lt;p&gt;eval rate: 4.78 tokens/s&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here's a tokens-per-second simulator to get an idea if this would be accceptable for your use case: &lt;a href="https://tokens-per-second-visualizer.tiiny.site/"&gt;https://tokens-per-second-visualizer.tiiny.site/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;42GB is the size of the 2.71Q model on disk, and it is much faster (of course) than equivalent 70B Q4 (which is also 42GB on disc)&lt;/p&gt; &lt;p&gt;CPU is 64GB Ryzen 7.&lt;/p&gt; &lt;p&gt;Feels lightning fast for CPU only compared to 70B and even 27-32B dense models. &lt;/p&gt; &lt;p&gt;First test questions worked great. &lt;/p&gt; &lt;p&gt;Looking forward to using this; I've been hoping for a large MoE with small experts for a while, very excited.&lt;/p&gt; &lt;p&gt;Next will be Maverick on the AI server (500GB RAM, 24GB VRAM)...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T05:26:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uvpm</id>
    <title>Qwen introduces their mobile app</title>
    <updated>2025-04-25T20:22:54+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt; &lt;img alt="Qwen introduces their mobile app" src="https://preview.redd.it/ewjq8s2ei1xe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe3b4f5cf5c69932dece355c02addb1e439cdd0" title="Qwen introduces their mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewjq8s2ei1xe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k89gaa</id>
    <title>Handling Mid-Sentence Pauses in Voice Conversations?</title>
    <updated>2025-04-26T09:58:37+00:00</updated>
    <author>
      <name>/u/_lindt_</name>
      <uri>https://old.reddit.com/user/_lindt_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t think this is an LLM/ML problem — it feels more like an algorithmic issue. Current systems don’t handle natural pauses well. If you pause mid-sentence to think, the model often responds prematurely based only on what’s been said so far, which disrupts the conversation’s flow. Has anyone found or implemented a solution for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_lindt_"&gt; /u/_lindt_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k89gaa/handling_midsentence_pauses_in_voice_conversations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k89gaa/handling_midsentence_pauses_in_voice_conversations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k89gaa/handling_midsentence_pauses_in_voice_conversations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T09:58:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k88k0h</id>
    <title>System Prompt vs. User Prompt</title>
    <updated>2025-04-26T08:53:10+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. What difference does it make, if I split my instructions into a system and user prompt, compared to just writing everything in the user prompt and keeping the system prompt empty or the generic &amp;quot;You are a helpful assistant&amp;quot;? &lt;/p&gt; &lt;p&gt;Assume the instruction is composed of an almost constant part (e.g. here is the data), and a more variable part (the question about the data). Is there any tangible difference in correctness, consistency etc?&lt;/p&gt; &lt;p&gt;And given that OpenAI API allows multiple user messages in the same request (does it?), will it have any benefit to separate a message into multiple user messages?&lt;/p&gt; &lt;p&gt;It's not an interactive scenario, so jailbreaking is not an issue. And for paid models, the tokens are anyways counted for the whole payload at the same rate, right?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T08:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8csdj</id>
    <title>A simple CLI tool for managing and running llama-server</title>
    <updated>2025-04-26T13:19:51+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I mostly made this tool to manage and run my local models and their parameters, mostly for my own use but I share it in case it is useful for someone else. I wish I had a tool like this when I started with local models, so I hope it is helpful!&lt;/p&gt; &lt;p&gt;The purpose of the tool it be very simple to use. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Install the pip packages &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Simply place the llama-server-cli.py file next to your llama-server executable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run it. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Use the interface to point it at the gguf file and start the server, this will use the default parameters. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It will run the server in the background and any changes made to the settings while the server is running will restart the server automatically with the new settings.&lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://github.com/R-Dson/llama-server-cli.py"&gt;https://github.com/R-Dson/llama-server-cli.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8csdj/a_simple_cli_tool_for_managing_and_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8csdj/a_simple_cli_tool_for_managing_and_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8csdj/a_simple_cli_tool_for_managing_and_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8m5x9</id>
    <title>AB^N×Judge(s) - Test models, generate data, etc.</title>
    <updated>2025-04-26T20:18:21+00:00</updated>
    <author>
      <name>/u/Accomplished_Mode170</name>
      <uri>https://old.reddit.com/user/Accomplished_Mode170</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8m5x9/abnjudges_test_models_generate_data_etc/"&gt; &lt;img alt="AB^N×Judge(s) - Test models, generate data, etc." src="https://external-preview.redd.it/eDdmNGdpN3dsOHhlMV8F268F1FG7hL8eI18HZCWuM36dgxTfIURYHTKezRhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc04b7e06087a0fbbd1385fc3305f5d1e240006c" title="AB^N×Judge(s) - Test models, generate data, etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://github.com/rabbidave/ZeroDay.Tools/blob/main/ABxJudge.py"&gt;AB^N×Judge(s)&lt;/a&gt; - Test models, generate data, etc.&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Self-Installing Python VENV &amp;amp; Dependency Management&lt;/li&gt; &lt;li&gt;N-Endpoint (Local and/or Distributed) Pairwise AI Testing &amp;amp; Auto-Evaluation&lt;/li&gt; &lt;li&gt;UI/CLI support for K/V &amp;amp; (optional) multimodal reference input&lt;/li&gt; &lt;li&gt;It's really fun to watch it describe &lt;a href="https://huggingface.co/datasets/TheFusion21/PokemonCards"&gt;different generations of Pokémon card schemas &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;spoiler: Gemma 3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Mode170"&gt; /u/Accomplished_Mode170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8gxu9i7wl8xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8m5x9/abnjudges_test_models_generate_data_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8m5x9/abnjudges_test_models_generate_data_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T20:18:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k89s1u</id>
    <title>Lmarena hard auto benchmark v2 results.</title>
    <updated>2025-04-26T10:21:38+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lmarena/arena-hard-auto"&gt;https://github.com/lmarena/arena-hard-auto&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Hard Prompt, Style Control, and Gemini-2.5 as Judge)&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Model Scores (%) CI (%) 0 o3-2025-04-16 86.1 (-1.1 / +1.1) 1 gemini-2.5 79.3 (-1.5 / +1.9) 2 o4-mini-2025-04-16-high 79.2 (-1.2 / +1.5) 3 o4-mini-2025-04-16 74.8 (-1.4 / +1.4) 4 gemini-2.5-flash 69.0 (-1.3 / +1.9) 5 o3-mini-2025-01-31-high 66.5 (-1.9 / +1.4) 6 claude-3-7-sonnet-20250219-thinking-16k 61.1 (-2.1 / +1.5) 7 o1-2024-12-17-high 61.0 (-1.6 / +1.8) 8 deepseek-r1 57.9 (-2.4 / +2.3) 9 o1-2024-12-17 56.0 (-1.7 / +2.0) 10 gpt-4.5-preview 50.7 (-1.8 / +1.7) 11 gpt-4.1 50.7 (-2.3 / +1.9) 12 o3-mini-2025-01-31 50.0 (-0.0 / +0.0) 13 gpt-4.1-mini 47.2 (-1.9 / +2.6) 14 QwQ-32B 43.7 (-2.4 / +2.1) 15 claude-3-5-sonnet-20241022 33.6 (-1.9 / +1.7) 16 s1.1-32B 22.2 (-1.6 / +1.6) 17 llama4-maverick-instruct-basic 17.5 (-1.4 / +1.6) 18 Athene-V2-Chat 16.5 (-1.0 / +1.5) 19 gemma-3-27b-it 14.8 (-1.3 / +0.9) 20 gpt-4.1-nano 14.1 (-1.3 / +1.0) 21 Llama-3.1-Nemotron-70B-Instruct-HF 10.1 (-0.9 / +0.8) 22 Qwen2.5-72B-Instruct 10.1 (-0.8 / +1.3) 23 OpenThinker2-32B 3.1 (-0.2 / +0.4) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Interesting tidbits that apply also on the lmarena benchmark. Emphasis is mine. For example on the part that simple prompts - that could be common in LMarena (check the lmarena explorer) - make two models similar though the models could be vastly different.&lt;/p&gt; &lt;p&gt;Of course LLM judges may be biased as well (there are some papers on this), but I think they are trying to limit the bias as much as they can.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;V2.0 contains 500 fresh, challenging real-world user queries (open-ended software engineering problems, math questions, etc) and 250 creative writing queries sourced from Chatbot Arena. We employs automatic judges, GPT-4.1 and Gemini-2.5, as a cheaper and faster approximator to human preference.&lt;/p&gt; &lt;p&gt;Following the newly introduced Style Control on Chatbot Arena, we release Style Control on Arena Hard Auto! We employ the same Style Control methods as proposed in the blogpost. Please refer to the blogpost for methodology and technical background. (&lt;a href="https://lmsys.org/blog/2024-08-28-style-control/"&gt;https://lmsys.org/blog/2024-08-28-style-control/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;We outline two key properties that the benchmark aiming to approximate human preference should possess to provide meaningful comparisons between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Separability: the benchmark should separate models with high confidence.&lt;/li&gt; &lt;li&gt;Alignment with Human Preference: the benchmark should agree with human preference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While previous works have focused on alignment, separability is also a crucial consideration when comparing models of similar quality (e.g., different checkpoints from the same training run). However, achieving high-confidence separability is challenging due to limitations in prompt design and inherent variances in LLM evaluations. &lt;strong&gt;Overly simplistic prompts fail to distinguish between models&lt;/strong&gt;, while the randomness in human and LLM judgments leads to inconsistent predictions. As a result, it is often difficult to confidently determine if a model’s apparent performance reflects a genuine difference in capability or merely noisy observations, highlighting a need for methods to verify whether a benchmark can reliably separate similar models.&lt;/p&gt; &lt;p&gt;Statistical measures like Pearson (Pearson, 1895) and Spearman Correlations (Spearman, 1961), commonly used in benchmarks such as AlpacaEval (Li et al., 2023) to measure correlation to human preference ranking, may fail to adequately address model separability and ranking instability. In addition, these measures only provide a coarse signal of ranking correlation without quantifying the magnitude of performance differences between model pairs. To address these shortcomings, we develop three novel metrics: Separability with Confidence, Agreement with Confidence, and Pair Rank Brier Score.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k89s1u/lmarena_hard_auto_benchmark_v2_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k89s1u/lmarena_hard_auto_benchmark_v2_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k89s1u/lmarena_hard_auto_benchmark_v2_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T10:21:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k801ba</id>
    <title>It's been a while since we had new Qwen &amp; Qwen Coder models...</title>
    <updated>2025-04-26T00:18:43+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saying... 😉&lt;/p&gt; &lt;p&gt;In all seriousness if they need to cook further - let them cook.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T00:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8ivb5</id>
    <title>Split MoE GGUFs for modular quants?</title>
    <updated>2025-04-26T17:52:15+00:00</updated>
    <author>
      <name>/u/Aerikh</name>
      <uri>https://old.reddit.com/user/Aerikh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the optimizations happening around MoE models such as in Ktransformers and Llama.cpp with custom layer offloading overrides, I was thinking that it would be nice if there were GGUFs where the static parts of the model (the layers that are active every token, which for Llama 4 would be the dense layers and the 1 &amp;quot;shared&amp;quot; expert) are stored in a different file from the non-static parts (the routed experts). This would allow a user to mix and match to optimize for their hardware. Someone with a 12 GB GPU and 96 GB RAM for instance would be able to get a big quant of the static layers, while someone else with a 8 GB GPU but the same RAM could choose a smaller quant of the static, but still get the benefit of the big quant for the non-static layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aerikh"&gt; /u/Aerikh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T17:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o89n</id>
    <title>We compress any BF16 model to ~70% size during inference, while keeping the output LOSSLESS so that you can fit in more ERP context or run larger models.</title>
    <updated>2025-04-25T15:47:29+00:00</updated>
    <author>
      <name>/u/choHZ</name>
      <uri>https://old.reddit.com/user/choHZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glad to share another interesting piece of work from us: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;&lt;strong&gt;70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DF11)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tl;dr of this work is super simple. We — and several prior works — noticed that while &lt;strong&gt;BF16&lt;/strong&gt; is often promoted as a “more range, less precision” alternative to FP16 (especially to avoid value overflow/underflow during training), &lt;strong&gt;its range part (exponent bits) ends up being pretty redundant once the model is trained.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, although BF16 as a data format can represent a wide range of numbers, most trained models' exponents are plenty sparse. In practice, the exponent bits carry around 2.6 bits of actual information on average — far from the full 8 bits they're assigned.&lt;/p&gt; &lt;p&gt;This opens the door for classic Huffman coding — where shorter bit sequences are assigned to more frequent values — to &lt;strong&gt;compress the model weights&lt;/strong&gt; into a new data format we call &lt;strong&gt;DFloat11/DF11&lt;/strong&gt;, resulting in a &lt;strong&gt;LOSSLESS compression down to ~11 bits&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;But isn’t this just Zip?&lt;/h1&gt; &lt;p&gt;Not exactly. It is true that tools like Zip also leverage Huffman coding, but the tricky part here is &lt;strong&gt;making it memory efficient during inference&lt;/strong&gt;, as end users are probably not gonna be too trilled if it just makes model checkpoint downloads a bit faster (in all fairness, smaller chekpoints means a lot when training at scale, but that's not a problem for everyday users).&lt;/p&gt; &lt;p&gt;What does matter to everyday users is &lt;strong&gt;making the memory footprint smaller during GPU inference, which requires nontrivial efforts.&lt;/strong&gt; But we have figured it out, and we’ve open-sourced the code.&lt;/p&gt; &lt;p&gt;So now you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run models that previously didn’t fit into your GPU memory.&lt;/li&gt; &lt;li&gt;Or run the same model with &lt;strong&gt;larger batch sizes and/or longer sequences&lt;/strong&gt; (very handy for those lengthy ERPs, or so I have heard).&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPU Type&lt;/th&gt; &lt;th align="left"&gt;Method&lt;/th&gt; &lt;th align="left"&gt;Successfully Run?&lt;/th&gt; &lt;th align="left"&gt;Required Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-405B-Instruct&lt;/td&gt; &lt;td align="left"&gt;8×H100-80G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;811.71 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;551.22 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×H200-141G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;141.11 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;96.14 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×A6000-48G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;65.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;45.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-Distill-Llama-8B&lt;/td&gt; &lt;td align="left"&gt;1×RTX 5080-16G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;16.06 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;11.23 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some research promo posts try to surgercoat their weakness or tradeoff, thats not us. So here's are some honest FAQs:&lt;/p&gt; &lt;h1&gt;What’s the catch?&lt;/h1&gt; &lt;p&gt;Like all compression work, there’s a cost to decompressing. And here are some efficiency reports.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On an A100 with batch size 128, DF11 is &lt;strong&gt;basically just as fast&lt;/strong&gt; as BF16 (1.02x difference, assuming both version fits in the GPUs with the same batch size). See Figure 9.&lt;/li&gt; &lt;li&gt;It is up to &lt;strong&gt;38.8x faster&lt;/strong&gt; than CPU offloading, so if you have a model that can't be run on your GPU in BF16, but can in DF11, there are plenty sweet performance gains over CPU offloading — one of the other popular way to run larger-than-capacity models. See Figure 3.&lt;/li&gt; &lt;li&gt;With the model weight being compressed, you can use the saved real estate for larger batch size or longer context length. This is expecially significant if the model is already tightly fitted in GPU. See Figure 4.&lt;/li&gt; &lt;li&gt;What about batch size 1 latency when both versions (DF11 &amp;amp; BF16) can fit in a single GPU? This is where DF11 is the weakest — we observe &lt;strong&gt;~40% slower&lt;/strong&gt; (2k/100 tokens for in/out). So there is not much motivation in using DF11 if you are not trying to run larger model/bigger batch size/longer sequence length.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why not just (lossy) quantize to 8-bit?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The short answer is you should totally do that if you are satisfied with the output lossy 8-bit quantization with respect to your task. But how do you really know it is always good?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many benchmark literature suggest that compressing a model (weight-only or otherwise) to 8-bit-ish is typically a safe operation, even though it's technically lossy. What we found, however, is that while this claim is often made in quantization papers, their benchmarks tend to focus on general tasks like MMLU and Commonsense Reasoning; which do not present a comprehensive picture of model capability.&lt;/p&gt; &lt;p&gt;More challenging benchmarks — such as those involving complex reasoning — and real-world user preferences often reveal noticeable differences. One good example is Chatbot Arena indicates the 8-bit (though it is W8A8 where DF11 is weight only, so it is not 100% apple-to-apple) and 16-bit Llama 3.1 405b tend to behave quite differently on some categories of tasks (e.g., Math and Coding).&lt;/p&gt; &lt;p&gt;Although the broader question: &lt;em&gt;“Which specific task, on which model, using which quantization technique, under what conditions, will lead to a noticeable drop compared to FP16/BF16?”&lt;/em&gt; is likely to remain open-ended simply due to the sheer amount of potential combinations and definition of “noticable.” &lt;strong&gt;It is fair to say that lossy quantization introduces complexities that some end-users would prefer to avoid, since it creates uncontrolled variables that must be empirically stress-tested for each deployment scenario.&lt;/strong&gt; DF11 offeres an alternative that avoids this concern 100%.&lt;/p&gt; &lt;h1&gt;What about finetuning?&lt;/h1&gt; &lt;p&gt;Our method could potentially pair well with PEFT methods like LoRA, where the base weights are frozen. But since we compress block-wise, we can’t just apply it naively without breaking gradients. We're actively exploring this direction. If it works, if would potentially become a QLoRA alternative where you can lossly LoRA finetune a model with reduced memory footprint.&lt;/p&gt; &lt;p&gt;(As always, happy to answer questions or chat until my advisor notices I’m doomscrolling socials during work hours :&amp;gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;https://arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/LeanModels/DFloat11"&gt;https://github.com/LeanModels/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choHZ"&gt; /u/choHZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8jymm</id>
    <title>End-to-end conversation projects? Dia, Sesame, etc</title>
    <updated>2025-04-26T18:39:34+00:00</updated>
    <author>
      <name>/u/Kep0a</name>
      <uri>https://old.reddit.com/user/Kep0a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past month we've had some pretty amazing voice models. After talking with the Sesame demo, I'm wondering, has anyone made an easy streaming end-to-end, conversation project yet? I want to run these but combining things seamlessly is outside my skillset. I need my 'Her' moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kep0a"&gt; /u/Kep0a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T18:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8kh94</id>
    <title>[Open Source] QA for cursor - Make sure it only gives you correct code.</title>
    <updated>2025-04-26T19:02:30+00:00</updated>
    <author>
      <name>/u/Cheap_Concert168no</name>
      <uri>https://old.reddit.com/user/Cheap_Concert168no</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"&gt; &lt;img alt="[Open Source] QA for cursor - Make sure it only gives you correct code." src="https://external-preview.redd.it/dWp1NWc2em42OHhlMUtddb3oYk3YHCqLnAUEgID_4mfN2VhQqz8ywXIoHXVz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b47ad65b6d6631fd64a9f0fc219c95cbec0702f" title="[Open Source] QA for cursor - Make sure it only gives you correct code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a MCP server that allows cursor(,etc) to test out the code before delivering it to you. If test fails it gets the exact logical error/console errors/screenshots directly resulting in a feedback loop until it gets it right. This makes the agent get as close to your requirements as possible before delivering it to you. Particularly, improving the coding experience with smaller/open coding models&lt;/p&gt; &lt;p&gt;It also tests in regression (test old features) so that new developments don't break working features which is a very common problem with these agents. It also has a mode to discover new test flows just by crawling a website, but that is trash for now. &lt;/p&gt; &lt;p&gt;You can use any LLM for this but I am using free gemini-2.0-flash and it works like a charm. It works a looot faster on gemini-2.0-flash-lite but I am happy to trade off time for accuracy (demo is sped up, check github for full length demo). A testing integration is inevitable for cursor/windsurf so until then I will keep working on this. Any feedback is welcome :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ilikepizza2/QA-MCP"&gt;QA-MCP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Concert168no"&gt; /u/Cheap_Concert168no &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1p5tt1zn68xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T19:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8d8a3</id>
    <title>5090 prices in Switzerland normalizing, looking good for local AI?</title>
    <updated>2025-04-26T13:41:47+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been checking 5090 prices in Switzerland. Found offers as low as CHF 1950.- although sold out very quickly and not up for order, but offer still online. The next one that's available, although with a 28 day lead time is at CHF 2291.-&lt;/p&gt; &lt;p&gt;Do you guys see this as a response to the harsh competition by AMD? Do you see similar trends in your country?&lt;/p&gt; &lt;p&gt;2291.- offer was found on nalda.ch&lt;/p&gt; &lt;p&gt;1950.- offer (they used the 5080 package in the image, but the stats mention the 5090) was found on conrad.ch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8d8a3/5090_prices_in_switzerland_normalizing_looking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8d8a3/5090_prices_in_switzerland_normalizing_looking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8d8a3/5090_prices_in_switzerland_normalizing_looking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8bodt</id>
    <title>Llama 3.3 70B Q40: eval 7.2 tok/s, pred 3.3 tok/s on 4 x NVIDIA RTX 3060 12 GB (GPU cost: $1516)</title>
    <updated>2025-04-26T12:20:59+00:00</updated>
    <author>
      <name>/u/b4rtaz</name>
      <uri>https://old.reddit.com/user/b4rtaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8bodt/llama_33_70b_q40_eval_72_toks_pred_33_toks_on_4_x/"&gt; &lt;img alt="Llama 3.3 70B Q40: eval 7.2 tok/s, pred 3.3 tok/s on 4 x NVIDIA RTX 3060 12 GB (GPU cost: $1516)" src="https://external-preview.redd.it/B1TF2IQo6iquhmu5K16ixy7w2XYeK22BYOFGGiutRMc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c09552066bc18c0bbffa03e596104499ea5c504" title="Llama 3.3 70B Q40: eval 7.2 tok/s, pred 3.3 tok/s on 4 x NVIDIA RTX 3060 12 GB (GPU cost: $1516)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b4rtaz"&gt; /u/b4rtaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/205"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8bodt/llama_33_70b_q40_eval_72_toks_pred_33_toks_on_4_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8bodt/llama_33_70b_q40_eval_72_toks_pred_33_toks_on_4_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T12:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k88v0p</id>
    <title>Newelle 0.9.5 Released: Internet Access, Improved Document Reading</title>
    <updated>2025-04-26T09:15:18+00:00</updated>
    <author>
      <name>/u/iTzSilver_YT</name>
      <uri>https://old.reddit.com/user/iTzSilver_YT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"&gt; &lt;img alt="Newelle 0.9.5 Released: Internet Access, Improved Document Reading" src="https://preview.redd.it/6n7tbbk5c5xe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=c94def82cef2aab0509b503092cef40a3a4c19f3" title="Newelle 0.9.5 Released: Internet Access, Improved Document Reading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newelle 0.9.5 Released! Newelle is an advanced AI assistant for Linux supporting any LLM (Local or Online), voice commands, extensions and much more!&lt;/p&gt; &lt;p&gt;🔎 Implemented Web Search with SearXNG, DuckDuckGo, and Tavily&lt;br /&gt; 🌐 Website Reading: ask questions about websites (Write #url to embed it)&lt;br /&gt; 🔢 Improved inline LaTeX support&lt;br /&gt; 🗣 New empty chat placeholder&lt;br /&gt; 📎 Improved Document reading: semantic search will only be done if the document is too long&lt;br /&gt; 💭 New thinking widget&lt;br /&gt; 🧠 Add vision support for llama4 on Groq and possibility to choose provider on OpenRouter&lt;br /&gt; 🌍 New translations (Traditional Chinese, Bengali, Hindi)&lt;br /&gt; 🐞 Various bug fixes&lt;/p&gt; &lt;p&gt;Source Code: &lt;a href="https://github.com/qwersyk/Newelle/"&gt;https://github.com/qwersyk/Newelle/&lt;/a&gt;&lt;br /&gt; Flathub: &lt;a href="https://flathub.org/apps/io.github.qwersyk.Newelle"&gt;https://flathub.org/apps/io.github.qwersyk.Newelle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTzSilver_YT"&gt; /u/iTzSilver_YT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6n7tbbk5c5xe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T09:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8dqa7</id>
    <title>It's really cool now to have an idea, and few hours later you have a working app</title>
    <updated>2025-04-26T14:05:42+00:00</updated>
    <author>
      <name>/u/Nyao</name>
      <uri>https://old.reddit.com/user/Nyao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"&gt; &lt;img alt="It's really cool now to have an idea, and few hours later you have a working app" src="https://external-preview.redd.it/MzQ1cGE3N2FxNnhlMe_3z2US61k9w-e99dI3sh4KfvfwKeGQ6lAD-f6G97nN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d39f3e4229afc94ed424a14e9084252b056a258" title="It's really cool now to have an idea, and few hours later you have a working app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rarely do web development, and without the help of LLMs it would have taken me days to build the frontend and these animations. But after one morning, I already have a cool result.&lt;/p&gt; &lt;p&gt;The idea and the app themselves aren't very original or complex, but here's the source code in case anyone is interested: &lt;a href="https://github.com/YofarDev/chapitre"&gt;https://github.com/YofarDev/chapitre&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nyao"&gt; /u/Nyao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vgz7967aq6xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T14:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8601g</id>
    <title>Qwen AI - My most used LLM!</title>
    <updated>2025-04-26T05:56:59+00:00</updated>
    <author>
      <name>/u/Glittering-Cancel-25</name>
      <uri>https://old.reddit.com/user/Glittering-Cancel-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Qwen, DeepSeek, paid ChatGPT, and paid Claude. I must say, i find myself using Qwen the most often. It's great, especially for a free model! &lt;/p&gt; &lt;p&gt;I use all of the LLMs for general and professional work. E.g., writing, planning, management, self-help, idea generation, etc. For most of those things, i just find that Qwen produces the best results and requires the least rework, follow ups, etc. I've tested all of the LLMs by putting in the exact same prompt (i've probably done this a couple dozen times) and overall (but not always), Qwen produces the best result for me. I absolutely can't wait until they release Qwen3 Max! I also have a feeling DeepSeek is gonna go with with R2... &lt;/p&gt; &lt;p&gt;Id love to know what LLM you find yourself using the most, what you use them for (that makes a big difference), and why you think that one is the best. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Cancel-25"&gt; /u/Glittering-Cancel-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T05:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8aput</id>
    <title>LangoTango - A local language model powered language learning partner</title>
    <updated>2025-04-26T11:23:55+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"&gt; &lt;img alt="LangoTango - A local language model powered language learning partner" src="https://b.thumbs.redditmedia.com/lyZDij5G9j7z2Yk-cHHcn18tC7IFPbWsmBtOUr5HdYs.jpg" title="LangoTango - A local language model powered language learning partner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Put this together over the week. It's a fork of another app I made called Dillon, but in this case I optimised it for language learning. It can be forked for all sorts of different hobbies. You could make a fork for personal recipe books or exercise diaries for example.&lt;/p&gt; &lt;p&gt;Here's the repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/shokuninstudio/LangoTango"&gt;https://github.com/shokuninstudio/LangoTango&lt;/a&gt;&lt;/p&gt; &lt;p&gt;macOS and Windows binaries are ready to download.&lt;/p&gt; &lt;p&gt;If you want to build it for Linux it's easy with pyinstaller and should work. I have not been able to test on Linux as I only have VMs at the moment. I need some drivers (not available) to run Linux native on my laptop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k8aput"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T11:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8f38v</id>
    <title>Dia-1.6B in Jax to generate audio from text from any machine</title>
    <updated>2025-04-26T15:07:47+00:00</updated>
    <author>
      <name>/u/Due-Yoghurt2093</name>
      <uri>https://old.reddit.com/user/Due-Yoghurt2093</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"&gt; &lt;img alt="Dia-1.6B in Jax to generate audio from text from any machine" src="https://external-preview.redd.it/OUXDxyqECR3Zwafh-pl-0Sio26N3Ne4UW5G-7VkXL1o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=868d202201bf917cbb597d31bb847a1ff2f6b446" title="Dia-1.6B in Jax to generate audio from text from any machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a JAX port of Dia, the 1.6B parameter text-to-speech model to generate voice from any machine, and would love to get any feedback. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due-Yoghurt2093"&gt; /u/Due-Yoghurt2093 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jaco-bro/diajax"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T15:07:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8hob9</id>
    <title>My AI dev prompt playbook that actually works (saves me 10+ hrs/week)</title>
    <updated>2025-04-26T17:00:42+00:00</updated>
    <author>
      <name>/u/namanyayg</name>
      <uri>https://old.reddit.com/user/namanyayg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been using AI tools to speed up my dev workflow for about 2 years now, and I've finally got a system that doesn't suck. Thought I'd share my prompt playbook since it's helped me ship way faster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix the root cause&lt;/strong&gt;: when debugging, AI usually tries to patch the end result instead of understanding the root cause. Use this prompt for that case:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyze this error: [bug details] Don't just fix the immediate issue. Identify the underlying root cause by: - Examining potential architectural problems - Considering edge cases - Suggesting a comprehensive solution that prevents similar issues &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ask for explanations:&lt;/strong&gt; Here's another one that's saved my ass repeatedly - the &amp;quot;explain what you just generated&amp;quot; prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Can you explain what you generated in detail: 1. What is the purpose of this section? 2. How does it work step-by-step? 3. What alternatives did you consider and why did you choose this one? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Forcing myself to understand ALL code before implementation has eliminated so many headaches down the road.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My personal favorite:&lt;/strong&gt; what I call the &amp;quot;rage prompt&amp;quot; (I usually have more swear words lol):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This code is DRIVING ME CRAZY. It should be doing [expected] but instead it's [actual]. PLEASE help me figure out what's wrong with it: [code] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works way better than it should! Sometimes being direct cuts through the BS and gets you answers faster.&lt;/p&gt; &lt;p&gt;The main thing I've learned is that AI is like any other tool - it's all about HOW you use it.&lt;/p&gt; &lt;p&gt;Good prompts = good results. Bad prompts = garbage.&lt;/p&gt; &lt;p&gt;What prompts have y'all found useful? I'm always looking to improve my workflow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/namanyayg"&gt; /u/namanyayg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T17:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8fe14</id>
    <title>Hot Take: Gemini 2.5 Pro Makes Too Many Assumptions About Your Code</title>
    <updated>2025-04-26T15:21:19+00:00</updated>
    <author>
      <name>/u/HideLord</name>
      <uri>https://old.reddit.com/user/HideLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini 2.5 Pro is probably the smartest model that is publicly available at the moment. But it makes TOO fucking many assumptions about your code that often outright break functionality. Not only that, but it's overly verbose and boilerplate-y. Google really needs to tone it down. &lt;/p&gt; &lt;p&gt;I'll give an example: I had a function which extracts a score from a given string. The correct format is 1-10/10. Gemini randomly decides that this is a bug and modifies the regex to also accept 0/10. &lt;/p&gt; &lt;p&gt;The query was to use the result from the function to calculate the MSE. Nowhere did I specify it to modify the get_score function. Sonnet/DeepSeek do not have that issue by the way. &lt;/p&gt; &lt;p&gt;Thanks for coming to my TED talk. I just needed to vent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HideLord"&gt; /u/HideLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T15:21:19+00:00</published>
  </entry>
</feed>
