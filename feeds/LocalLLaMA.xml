<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-30T23:35:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lo5uz6</id>
    <title>Has anyone tried running 2 AMD Ryzen™ AI Max+ 395 in parallel?</title>
    <updated>2025-06-30T12:09:31+00:00</updated>
    <author>
      <name>/u/orkutmuratyilmaz</name>
      <uri>https://old.reddit.com/user/orkutmuratyilmaz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Some models require more VRAM to run. I was thinking of getting 2 AMD Ryzen™ AI Max+ 395 and trying to run them in parallel. I wonder if anyone has tried this? Does anyone have any information?&lt;/p&gt; &lt;p&gt;Have a nice one:)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orkutmuratyilmaz"&gt; /u/orkutmuratyilmaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lomilz</id>
    <title>[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀</title>
    <updated>2025-06-30T23:22:23+00:00</updated>
    <author>
      <name>/u/Awkward-Dare-1127</name>
      <uri>https://old.reddit.com/user/Awkward-Dare-1127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt; &lt;img alt="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" src="https://external-preview.redd.it/4LxPZe7g9FzOelbXz3aOGVDeWd4mgBA3OWlLsXGi_Bc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61aa76d902ab96a1963a6d4338aa8b21a38657e" title="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Copy one portable&lt;/em&gt; &lt;code&gt;.exe&lt;/code&gt; &lt;em&gt;+ a&lt;/em&gt; &lt;code&gt;.gguf&lt;/code&gt; &lt;em&gt;model to a flash drive → double-click on any Windows PC → start chatting offline in seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;GitHub ▶︎ &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad"&gt;&lt;strong&gt;https://github.com/runzhouye/Local_LLM_Notepad&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8"&gt;https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lz6e4zmpd5af1.gif"&gt;https://i.redd.it/lz6e4zmpd5af1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;30-second Quick-Start&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Grab &lt;strong&gt;Local_LLM_Notepad-portable.exe&lt;/strong&gt; from the &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad/releases"&gt;latest release&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download a small CPU model like &lt;strong&gt;gemma-3-1b-it-Q4_K_M.gguf&lt;/strong&gt; (≈0.8 GB) from &lt;a href="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/tree/main"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Copy both files onto a USB stick.&lt;/li&gt; &lt;li&gt;Double-click the EXE on any Windows box → first run loads the model.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;✅&lt;/th&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What it means&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Plug-and-play&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Single 45 MB EXE runs without admin rights&lt;/td&gt; &lt;td align="left"&gt;Run on any computer—no install needed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Source-word highlighting&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Bold-underlines every word/number from your prompt&lt;/td&gt; &lt;td align="left"&gt;Ctrl-click to trace facts &amp;amp; tables for quick fact-checking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hotkeys&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Ctrl + SCtrl + ZCtrl + FCtrl + X&lt;/code&gt; send, stop, search, clear, etc.&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Portable chat logs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;One-click JSON export&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Dare-1127"&gt; /u/Awkward-Dare-1127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojgxl</id>
    <title>OpenSource CLI Agent with Local models.</title>
    <updated>2025-06-30T21:14:27+00:00</updated>
    <author>
      <name>/u/x8ko_dev</name>
      <uri>https://old.reddit.com/user/x8ko_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm building this CLI coding agent right now. My big goal is to turn it into a fully autonomous bot that runs on a server, handles error reports, crash logs, and random issues, then tracks them down and fixes everything on its own.&lt;/p&gt; &lt;p&gt;For the moment, it's just a basic CLI tool packed with features for dealing with files, GitHub, general docs, and a bunch more.If you could test it out on your projects and hit me with some feedback or suggestions for improvements, that'd be super helpful.&lt;/p&gt; &lt;p&gt;Im struggling to find any edge cases that arent UI/Command related in my personal usage currently so i think its time to get a little real world responses.&lt;/p&gt; &lt;p&gt;I currently support LMStudio, Requesty and OpenRouter.&lt;br /&gt; So far our testing of local models (devstral, qwen and alike) are working really well. I'd love to hear your feedback, the worse the better. i want to know every issue, minor details and alike, im not here to get my ass kissed like ive seen from others.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/xyOz-dev/LogiQCLI/"&gt;https://github.com/xyOz-dev/LogiQCLI/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x8ko_dev"&gt; /u/x8ko_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojgxl/opensource_cli_agent_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lompd5</id>
    <title>I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax</title>
    <updated>2025-06-30T23:31:00+00:00</updated>
    <author>
      <name>/u/sbuswell</name>
      <uri>https://old.reddit.com/user/sbuswell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).&lt;/p&gt; &lt;p&gt;Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.&lt;/p&gt; &lt;p&gt;This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.&lt;/p&gt; &lt;p&gt;I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!&lt;/p&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/elevanaltd/octave"&gt;https://github.com/elevanaltd/octave&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbuswell"&gt; /u/sbuswell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1loal9v</id>
    <title>Drafted Llama as an enhanced parser for interactive fiction puzzles/games</title>
    <updated>2025-06-30T15:32:42+00:00</updated>
    <author>
      <name>/u/Fit-Lengthiness-4747</name>
      <uri>https://old.reddit.com/user/Fit-Lengthiness-4747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt; &lt;img alt="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" src="https://preview.redd.it/wg741dwa23af1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91fd5c66c0a9e6f241558b7edd95831e8c284bc3" title="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Llama as a way to expand the types of games that can be played within interactive fiction, such as creating non-deterministic rubrics to grade puzzle solutions, allowing building/crafting with a wide range of objects.combinatorial possibilities, and enabling sentiment and emotion-based responses with NPCs as a way of getting game information. try is here: &lt;a href="https://thoughtauction.itch.io/last-audit-of-the-damned"&gt;https://thoughtauction.itch.io/last-audit-of-the-damned&lt;/a&gt; And if you like, please vote for us in the ParserComp 2025 contest, as well as play some of the other entries. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Lengthiness-4747"&gt; /u/Fit-Lengthiness-4747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wg741dwa23af1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojd3e</id>
    <title>Gemma-3n VRAM usage</title>
    <updated>2025-06-30T21:10:17+00:00</updated>
    <author>
      <name>/u/el_pr3sid3nt3</name>
      <uri>https://old.reddit.com/user/el_pr3sid3nt3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello fellow redditors,&lt;/p&gt; &lt;p&gt;I am trying to run Gemma-3n-E2B and E4B advertised as 2gb-3gb VRAM models. However, I couldn't run E4B due to torch outOfMemory, but when I ran E2B it took 10gbs and after few requests I went out of memory.&lt;/p&gt; &lt;p&gt;I am trying to understand, is there a way to run these models really on 2gb-3gb VRAM, and if yes how so, and what I missed?&lt;/p&gt; &lt;p&gt;Thank you all&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el_pr3sid3nt3"&gt; /u/el_pr3sid3nt3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobzkr</id>
    <title>n8n ,proxmox ,docker and Google API.</title>
    <updated>2025-06-30T16:26:28+00:00</updated>
    <author>
      <name>/u/Able-Consequence8872</name>
      <uri>https://old.reddit.com/user/Able-Consequence8872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt; &lt;img alt="n8n ,proxmox ,docker and Google API." src="https://preview.redd.it/02pteeydc3af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc69450af42416d14cf7491d093e484565716a06" title="n8n ,proxmox ,docker and Google API." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, trying to use Google API in 8n8 (in a PROXMOX container ) and LMstudio (another machine in the same LAN) but it won't take my LAN ip adresse.n8n gives the localhost value by default. I know there is a trick with docker, like &lt;a href="https://local.docker/v1"&gt;https://local.docker/v1&lt;/a&gt;, but it works only if both n8n and LMstudio work on the same machine. n8n is on a different machine on the LAN.&lt;/p&gt; &lt;p&gt;how can I fix this? I want to run everything locally, with 2 different machines on the LAN, using Google workspace with my assistant in 8n8, and Mistral as a local AI in LMstudio.&lt;/p&gt; &lt;p&gt;thx..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Able-Consequence8872"&gt; /u/Able-Consequence8872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/02pteeydc3af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo84yj</id>
    <title>[2506.21734] Hierarchical Reasoning Model</title>
    <updated>2025-06-30T13:54:40+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract:&lt;/p&gt; &lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T13:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lohzzj</id>
    <title>How to run Hunyuan-A13B on a RTX 5090 / Blackwell ?</title>
    <updated>2025-06-30T20:16:23+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;Since the launch of Hunyuan-A13B, I’ve been struggling to get it running on an RTX 5090 with 32 GB of RAM. The official Docker images from Tencent don’t seem to be compatible with the Blackwell architecture. I even tried building vLLM from source via &lt;code&gt;git clone&lt;/code&gt;, but no luck either.&lt;/p&gt; &lt;p&gt;Any hints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T20:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5vnf</id>
    <title>What is the current best local coding model with &lt;= 4B parameters?</title>
    <updated>2025-06-30T12:10:32+00:00</updated>
    <author>
      <name>/u/Wooden-Key751</name>
      <uri>https://old.reddit.com/user/Wooden-Key751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking for &amp;lt;= 4B coding models. I realize that none of these will be practical for now just looking for some to do experiments.&lt;/p&gt; &lt;p&gt;Here is what i found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Menlo / Jan-nano — 4.02 B (Not really coding but I expect it to be better than others)&lt;/li&gt; &lt;li&gt;Gemma — 4 B / 2 B&lt;/li&gt; &lt;li&gt;Qwen 3 — 4 B / 0.6 B&lt;/li&gt; &lt;li&gt;Phi-4 Mini — 3.8 B&lt;/li&gt; &lt;li&gt;Phi-3.5 Mini — 3.5 B&lt;/li&gt; &lt;li&gt;Llama-3.2 — 3.2 B&lt;/li&gt; &lt;li&gt;Starcoder — 3 B / 1 B&lt;/li&gt; &lt;li&gt;Starcoder 2 — 3 B&lt;/li&gt; &lt;li&gt;Stable-Code — 3 B&lt;/li&gt; &lt;li&gt;Granite — 3 B / 2.53 B&lt;/li&gt; &lt;li&gt;Cogito — 3 B&lt;/li&gt; &lt;li&gt;DeepSeek Coder — 2.6 B / 1.3 B&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill (Qwen-tuned) — 1.78 B&lt;/li&gt; &lt;li&gt;Qwen 2.5 — 1.5 B / 0.5 B&lt;/li&gt; &lt;li&gt;Yi-Coder — 1.5 B&lt;/li&gt; &lt;li&gt;Deepscaler — 1.5 B&lt;/li&gt; &lt;li&gt;Deepcoder — 1.5 B&lt;/li&gt; &lt;li&gt;CodeGen2 — 1 B&lt;/li&gt; &lt;li&gt;BitNet-B1.58 — 0.85 B&lt;/li&gt; &lt;li&gt;ERNIE-4.5 — 0.36 B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone tried any of these or compared &amp;lt;= 4B models on coding tasks? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Key751"&gt; /u/Wooden-Key751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnrd1t</id>
    <title>You can just RL a model to beat any "AI detectors"</title>
    <updated>2025-06-29T22:22:55+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt; &lt;img alt="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d"&gt;https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd"&gt;https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baseline&lt;br /&gt; • Model: Llama-3.1 8B-Instruct&lt;br /&gt; • Prompt: plain &amp;quot;Write an essay about X&amp;quot;&lt;br /&gt; • Detector: ZeroGPT&lt;br /&gt; Result: 100 % AI-written&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d"&gt;https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Data&lt;br /&gt; • Synthetic dataset of 150 school-style prompts (history, literature, tech). Nothing fancy, just json lines + system prompt &amp;quot;You are a human essay writer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e"&gt;https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First training run&lt;br /&gt; After ~30 GRPO steps on a single A100:&lt;br /&gt; • ZeroGPT score drops from 100 → 42 %&lt;br /&gt; The model learned:&lt;br /&gt; Write a coherent intro&lt;br /&gt; Stuff one line of high-entropy junk&lt;br /&gt; Finish normally&lt;br /&gt; Average &amp;quot;human-ness&amp;quot; skyrockets because detector averages per-sentence scores&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75"&gt;https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch #1&lt;br /&gt; Added a gibberish classifier (tiny DistilRoBERTa) and multiplied reward by its minimum &amp;quot;clean&amp;quot; score. Junk lines now tank reward → behaviour disappears. GRPO’s beta ≈ how harshly to penalize incoherence. Set β = 0.4 and reward curve stabilized; no more oscillation between genius &amp;amp; garbage. Removed reasoning (memory constraints).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252"&gt;https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny models crush it&lt;br /&gt; Swapped in Qwen 0.5B LoRA rank 8, upped num_generations → 64.&lt;br /&gt; Result after 7 steps: best sample already at 28 % &amp;quot;human&amp;quot;. Smaller vocab seems to help leak less LM &amp;quot;signature&amp;quot; (the model learned to use lots of proper nouns to trick the detector).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2"&gt;https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;Detector bug?&lt;br /&gt; ZeroGPT sometimes marks the first half AI, second half human for the same paragraph. The RL agent locks onto that gradient and exploits it. Classifier clearly over-fits surface patterns rather than semantics&lt;/p&gt; &lt;p&gt;Single scalar feedback is enough for LMs to reverse-engineer public detectors &lt;/p&gt; &lt;p&gt;Add even a tiny auxiliary reward (gibberish, length) to stop obvious failure modes &lt;/p&gt; &lt;p&gt;Public &amp;quot;AI/Not-AI&amp;quot; classifiers are security-through-obscurity&lt;/p&gt; &lt;p&gt;Reward function: &lt;a href="https://codefile.io/f/R4O9IdGEhg"&gt;https://codefile.io/f/R4O9IdGEhg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnlxp1</id>
    <title>4x 4090 48GB inference box (I may have overdone it)</title>
    <updated>2025-06-29T18:33:40+00:00</updated>
    <author>
      <name>/u/101m4n</name>
      <uri>https://old.reddit.com/user/101m4n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt; &lt;img alt="4x 4090 48GB inference box (I may have overdone it)" src="https://external-preview.redd.it/o67J1SHcLKrQAlXicnfT20w0glJr7s4wb4-c1GOwiA8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=936572d5a67f4298cbb8ecc135d737e991ade403" title="4x 4090 48GB inference box (I may have overdone it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I discovered that 48GB 4090s were starting to show up on the western market in large numbers. I didn't think much of it at the time, but then I got my payout from the mt.gox bankruptcy filing (which has been ongoing for over 10 years now), and decided to blow a chunk of it on an inference box for local machine learning experiments.&lt;/p&gt; &lt;p&gt;After a delay receiving some of the parts (and admittedly some procrastination on my end), I've finally found the time to put the whole machine together!&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock romed8-2t motherboard (SP3)&lt;/li&gt; &lt;li&gt;32 core epyc&lt;/li&gt; &lt;li&gt;256GB 2666V memory&lt;/li&gt; &lt;li&gt;4x &amp;quot;tronizm&amp;quot; rtx 4090D 48GB modded GPUs from china&lt;/li&gt; &lt;li&gt;2x 1tb nvme (striped) for OS and local model storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The cards are very well built. I have no doubts as to their quality whatsoever. They were heavy, the heatsinks made contact with all the board level components and the shrouds were all-metal and very solid. It was almost a shame to take them apart! They were however incredibly loud. At idle, the fan sits at 30%, and at that level they are already as loud as the loudest blower cards for gaming. At full load, they are truly deafening and definitely not something you want to share space with. Hence the water-cooling.&lt;/p&gt; &lt;p&gt;There are however no full-cover waterblocks for these GPUs (they use a custom PCB), so to cool them I had to get a little creative. Corsair makes a (kinda) &lt;a href="https://www.corsair.com/uk/en/p/custom-liquid-cooling/cx-9025001-ww/icue-link-xg3-rgb-hybrid-gpu-water-block-4090-4080-cx-9025001-ww?srsltid=AfmBOopBdweqKN5Wpj6wHKLSR9SEYZmNpOpOyaFZTLLdld7hLBrg1iCg"&gt;generic block&lt;/a&gt; called the xg3. The product itself is a bit rubbish, requiring corsairs proprietary i-cue system to run the fan which is supposed to cool the components not covered by the coldplate. It's also overpriced. However these are more or less the only option here. As a side note, these &amp;quot;generic&amp;quot; blocks only work work because the mounting hole and memory layout around the core is actually standardized to some extent, something I learned during my research.&lt;/p&gt; &lt;p&gt;The cold-plate on these blocks turned out to foul one of the components near the core, so I had to modify them a bit. I also couldn't run the aforementioned fan without corsairs i-cue link nonsense and the fan and shroud were too thick anyway and would have blocked the next GPU anyway. So I removed the plastic shroud and fabricated a frame + heatsink arrangement to add some support and cooling for the VRMs and other non-core components.&lt;/p&gt; &lt;p&gt;As another side note, the marketing material for the xg3 claims that the block contains a built-in temperature sensor. However I saw no indication of a sensor anywhere when disassembling the thing. Go figure.&lt;/p&gt; &lt;p&gt;Lastly there's the case. I couldn't find a case that I liked the look of that would support three 480mm radiators, so I built something out of pine furniture board. Not the easiest or most time efficient approach, but it was fun and it does the job (fire hazard notwithstanding).&lt;/p&gt; &lt;p&gt;As for what I'll be using it for, I'll be hosting an LLM for local day-to-day usage, but I also have some more unique project ideas, some of which may show up here in time. Now that such projects won't take up resources on my regular desktop, I can afford to do a lot of things I previously couldn't!&lt;/p&gt; &lt;p&gt;P.S. If anyone has any questions or wants to replicate any of what I did here, feel free to DM me with any questions, I'm glad to help any way I can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/101m4n"&gt; /u/101m4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnlxp1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1loj134</id>
    <title>arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent</title>
    <updated>2025-06-30T20:57:06+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt; &lt;img alt="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" src="https://preview.redd.it/rak71t31n4af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcce9df76f1ce564209c9bfa33c8883ba2b4cbc5" title="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all been there, spend a morning setting up to find out it's not gonna work for your application.&lt;/p&gt; &lt;p&gt;From &lt;a href="https://arxiv.org/pdf/2409.07440"&gt;SUPER&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;As a recent study shows (Storks et al., 2023), both novice and advanced researchers find the challenge of &amp;quot;setting up the code base&amp;quot; to be the most difficult part of reproducing experiments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I'm sharing auto-generated Docker images for papers my agent recommends based on what I'm building.&lt;/p&gt; &lt;p&gt;Today's recommendation: &lt;strong&gt;LLaVA-Scissor&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull remyxai/2506.21862v1:latest docker run --gpus all -it remyxai/2506.21862v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More on &lt;a href="https://remyxai.substack.com/p/the-experimentops-agent"&gt;ExperimentOps&lt;/a&gt; and computational reproducibility. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rak71t31n4af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T20:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokkpc</id>
    <title>A Meta-Framework for Self-Improving LLMs with Transparent Reasoning</title>
    <updated>2025-06-30T21:59:34+00:00</updated>
    <author>
      <name>/u/henryb213</name>
      <uri>https://old.reddit.com/user/henryb213</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt; &lt;img alt="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" src="https://external-preview.redd.it/GF7LOLNV1EkT3j_WQj3wN6pKRBc62ktaNGoxeqmHjug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31f4c15b33f9e40cd80aee5e1468225b045437e8" title="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Framework overview:&lt;/strong&gt; LLMs iteratively refine their own outputs—typically through a three‑phase cycle &lt;strong&gt;draft → critique → revision&lt;/strong&gt;, repeat until convergence (all phases &amp;amp; stop rules are configurable). I started coding three weeks ago after an eight‑year break and zero professional dev experience.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;The classes work as Python callables with built in observability: instances are callable -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Python,tabs=4 from recursive_companion.base import MarketingCompanion agent = MarketingCompanion() answer = agent(&amp;quot;question or problem…&amp;quot;) # final refined output print(answer) print(agent.run_log) # list[dict] of every draft, critique &amp;amp; revision &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Why it stays clean &amp;amp; modular&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Templates are plain text files (system prompts, user prompts, protocol). &lt;em&gt;Swap harsh critiques for creative ones by swapping files.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;build_templates()&lt;/code&gt; lets you compose any combination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol injection&lt;/strong&gt; cleanly separates reasoning patterns from implementation.&lt;/li&gt; &lt;li&gt;New agents in &lt;strong&gt;3 lines&lt;/strong&gt;—just inherit from &lt;code&gt;BaseCompanion&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Convergence uses &lt;strong&gt;embedding‑based cosine similarity&lt;/strong&gt; by default, but the metric is fully pluggable.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How it came together&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The design emerged from recursive dialogues with multiple LLMs—the same iterative process the framework now automates. No legacy assumptions meant every piece became independent: swap models, add phases, change convergence logic—no rewiring required.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streamlit app&lt;/strong&gt; shows the thinking live as it happens.&lt;/li&gt; &lt;li&gt;Demos cover raw orchestration &lt;em&gt;and&lt;/em&gt; LangGraph integration (agents as graph nodes).&lt;/li&gt; &lt;li&gt;Full architecture docs, comprehensive docstrings, commenting, and worked examples included.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT)&lt;/strong&gt; &lt;a href="https://github.com/hankbesser/recursive-companion"&gt;https://github.com/hankbesser/recursive-companion&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built by questioning everything. Learning by building, built for learning.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for reading and really looking for any feedback and open to contributors, no question or discussion is too big or small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryb213"&gt; /u/henryb213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hankbesser/recursive-companion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxo8y</id>
    <title>Major AI platforms will eventually have ads</title>
    <updated>2025-06-30T03:40:35+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this as a huge reason to continue advancement of local LLMs. OpenAI, Google, Microsoft, Anthropic, etc. all the big players have investors to answer to, and will eventually need to stop burning money. They will get pressured into a sustainable business model. I think Google has already lost a lot of traffic to AI search that they will try to win back. Right now, they are giving LLM access in exchange for data to train on. Eventually they will have enough that it won’t be worth it anymore. &lt;/p&gt; &lt;p&gt;Anyone else see this coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobyx5</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-06-30T16:25:45+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on past threads from this sub, I see that below coding models are coming.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3 Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;Recent thread&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deep Cogito - Preview models there&lt;/li&gt; &lt;li&gt;Polaris - Preview models there&lt;/li&gt; &lt;li&gt;Granite releasing any new coding models? Preview (General) models there for upcoming Version 4. How good is their existing coding models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What other coding models coming apart from above ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1load8a</id>
    <title>[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?</title>
    <updated>2025-06-30T15:23:52+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" src="https://b.thumbs.redditmedia.com/QrXwS0MMtdu4-LCvZnP-VTv25rOcYvXpPucHJrkYiSQ.jpg" title="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7"&gt;https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’ve ever peeked inside models like GPT or BERT and wondered &lt;em&gt;how&lt;/em&gt; they understand the &lt;em&gt;order&lt;/em&gt; of words, the secret sauce is something called positional embedding.&lt;/p&gt; &lt;p&gt;Without it, a language model can’t tell the difference between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The cat sat on the mat”&lt;/li&gt; &lt;li&gt;“The mat sat on the cat”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem: Transformers Don’t Understand Word Order&lt;/h1&gt; &lt;p&gt;Transformers process all tokens at once, which is great for speed, but unlike RNNs, they don’t read text sequentially. That means they don’t naturally know the order of words.&lt;/p&gt; &lt;p&gt;To a plain Transformer, “I love AI” could mean the same as “AI love I.”&lt;/p&gt; &lt;h1&gt;The Solution: Positional Embeddings&lt;/h1&gt; &lt;p&gt;To fix this, we add a second layer of information: positional embeddings. These vectors tell the model &lt;em&gt;where&lt;/em&gt; each word appears in the input sequence.&lt;/p&gt; &lt;p&gt;So instead of just using word embeddings, we do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Final Input = Word Embedding + Positional Embedding &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the model knows both the meaning of each word and its position in the sentence.&lt;/p&gt; &lt;h1&gt;Why Not Let the Model Learn Position on Its Own?&lt;/h1&gt; &lt;p&gt;In theory, a large model &lt;em&gt;could&lt;/em&gt; infer word order from patterns. But in practice, that’s inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.&lt;/p&gt; &lt;h1&gt;Two Common Types of Positional Embeddings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in the original Transformer paper&lt;/li&gt; &lt;li&gt;Not learned, uses sine and cosine functions&lt;/li&gt; &lt;li&gt;Good for generalizing to longer sequences&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learned Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in models like BERT&lt;/li&gt; &lt;li&gt;Learned during training, like word embeddings&lt;/li&gt; &lt;li&gt;Flexible, but may not generalize well to unseen sequence lengths&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Real Example: Why It Matters&lt;/h1&gt; &lt;p&gt;Compare:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The dog chased the cat.”&lt;/li&gt; &lt;li&gt;“The cat chased the dog”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Same words, totally different meaning. Without positional embeddings, the model can’t tell which animal is doing the chasing.&lt;/p&gt; &lt;h1&gt;What’s New: Rotary Positional Embeddings (RoPE)&lt;/h1&gt; &lt;p&gt;Modern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It’s more efficient for long sequences and performs better in certain settings.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Positional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.&lt;/p&gt; &lt;p&gt;👉 Tomorrow, we’re going to code positional embeddings from scratch—so stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnu4zl</id>
    <title>Baidu releases ERNIE 4.5 models on huggingface</title>
    <updated>2025-06-30T00:34:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt; &lt;img alt="Baidu releases ERNIE 4.5 models on huggingface" src="https://external-preview.redd.it/Wyzo5BvQjbbXvCrrpLypEcj3XicuXWigLyl_Acs2b5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33f879948e7c84df63582ea3398eb078c0298c8e" title="Baidu releases ERNIE 4.5 models on huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp support for ERNIE 4.5 0.3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;https://github.com/ggml-org/llama.cpp/pull/14408&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vllm Ernie4.5 and Ernie4.5MoE Model Support&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20220"&gt;https://github.com/vllm-project/vllm/pull/20220&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
</feed>
