<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-15T08:06:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jazfkf</id>
    <title>I created an OpenAI TTS compatible endpoint for Sesame CSM 1B</title>
    <updated>2025-03-14T08:48:44+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a work in progress, especially around trying to normalize the voice/voices. &lt;/p&gt; &lt;p&gt;Give it a shot and let me know what you think. PR's welcomed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/phildougherty/sesame_csm_openai"&gt;https://github.com/phildougherty/sesame_csm_openai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T08:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbgwly</id>
    <title>Instructional Writeup: How to Make LLMs Reason Deep and Build Entire Projects</title>
    <updated>2025-03-14T22:59:40+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a way to push LLMs beyond their limits—deeper reasoning, bigger context, self-planning, and turning one request into a full project. I built project_builder.py (see a variant of it called the breakthrough generator: &lt;a href="https://github.com/justinlietz93/breakthrough_generator"&gt;https://github.com/justinlietz93/breakthrough_generator&lt;/a&gt; I will make the project builder and all my other work open source, but not yet ), and it’s solved problems I didn’t think were possible with AI alone. Here’s how I did it and what I’ve made.&lt;/p&gt; &lt;p&gt;How I Did It&lt;/p&gt; &lt;p&gt;LLMs are boxed in by short memory and one-shot answers. I fixed that with a few steps:&lt;/p&gt; &lt;p&gt;Longer Memory: I save every output to a file. Next prompt, I summarize it and feed it back. Context grows as long as I need it. Deeper Reasoning: I make it break tasks into chunks—hypothesize, test, refine. Each step builds on the last, logged in files. Self-Planning: I tell it to write a plan, like “5 steps to finish this.” It updates the plan as we go, tracking itself. Big Projects from One Line: I start with “build X,” and it generates a structure—files, plans, code—expanding it piece by piece.&lt;/p&gt; &lt;p&gt;I’ve let this run for 6 hours before and it build me a full IDE from scratch to replace Cursor that I can put the generator in, and write code as well at the same time.&lt;/p&gt; &lt;p&gt;What I’ve Achieved&lt;/p&gt; &lt;p&gt;This setup’s produced things I never expected from single prompts:&lt;/p&gt; &lt;p&gt;A training platform for an AI architecture that’s not quite any ML domain but pulls from all of them. It works, and it’s new. Better project generators. This is version 3—each one builds the next, improving every time. Research 10x deeper than Open AI’s stuff. Full papers, no shortcuts. A memory system that acts human—keeps what matters, drops the rest, adapts over time. A custom Cursor IDE, built from scratch, just how I wanted it. All 100% AI, no human edits. One prompt each.&lt;/p&gt; &lt;p&gt;How It Works&lt;/p&gt; &lt;p&gt;The script runs the LLM in a loop. It saves outputs, plans next steps, and keeps context alive with summaries. Three monitors let me watch it unfold—prompts, memory, plan. Solutions to LLM limits are there; I just assembled them.&lt;/p&gt; &lt;p&gt;Why It Matters&lt;/p&gt; &lt;p&gt;Anything’s possible with this. Books, tools, research—it’s all in reach. The code’s straightforward; the results are huge. I’m already planning more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgwly/instructional_writeup_how_to_make_llms_reason/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgwly/instructional_writeup_how_to_make_llms_reason/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgwly/instructional_writeup_how_to_make_llms_reason/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T22:59:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxec3</id>
    <title>Sesame CSM 1B Voice Cloning</title>
    <updated>2025-03-14T06:18:15+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt; &lt;img alt="Sesame CSM 1B Voice Cloning" src="https://external-preview.redd.it/JaEGat2-q67uEqWoPpGo5Nx0tvU4ZMhHe5tLQNQxW9w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c285788573980e7289358fbf97c0847d9b60866" title="Sesame CSM 1B Voice Cloning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/csm-voice-cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoy9g</id>
    <title>Meme i made</title>
    <updated>2025-03-13T22:41:19+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt; &lt;img alt="Meme i made" src="https://external-preview.redd.it/a3h0bzNwMWxiam9lMWaOI-rE6YlXiP74zpe4ixbVM_QsxQQzHzv1tNet8B-Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aee2797aa64747b42b65d38774f6590f3d0a9e9d" title="Meme i made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzku6n1lbjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb3mpe</id>
    <title>Gemma 3 Function Calling Example prompt</title>
    <updated>2025-03-14T13:14:57+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt; &lt;img alt="Gemma 3 Function Calling Example prompt" src="https://preview.redd.it/xv7wwtdmnnoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53b82d556c025c9987fe92be4363ea5c1a3d97b0" title="Gemma 3 Function Calling Example prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xv7wwtdmnnoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbmeyf</id>
    <title>NebuLlama UI: A Cosmic Interface for Ollama - Mobile Friendly &amp; Feature-Rich</title>
    <updated>2025-03-15T03:43:48+00:00</updated>
    <author>
      <name>/u/W4lxar</name>
      <uri>https://old.reddit.com/user/W4lxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbmeyf/nebullama_ui_a_cosmic_interface_for_ollama_mobile/"&gt; &lt;img alt="NebuLlama UI: A Cosmic Interface for Ollama - Mobile Friendly &amp;amp; Feature-Rich" src="https://b.thumbs.redditmedia.com/RiNv8tzw9562hQzvSK53H-LQ3E8lxRFO89SXLo80DsU.jpg" title="NebuLlama UI: A Cosmic Interface for Ollama - Mobile Friendly &amp;amp; Feature-Rich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;NebuLlama UI&lt;/strong&gt;, a beautiful cosmic-themed web interface for Ollama that I've been working on for the last 2 weeks. It's designed to be mobile-friendly and packed with features that make chatting with your local LLMs a breeze, i did it to use ollama on my phone because after installing Ollama via termux on my Pixel 9 Pro, i foundout there's no simple webUI so i did mine :D,&lt;/p&gt; &lt;h1&gt;What is NebuLlama UI?&lt;/h1&gt; &lt;p&gt;NebuLlama UI is a single HTML file interface for Ollama that focuses on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Nice cosmic design&lt;/strong&gt; that's easy on the eyes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mobile responsive&lt;/strong&gt; layout that works great on phones and tablets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rich functionality&lt;/strong&gt; without unnecessary complexity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No installation&lt;/strong&gt; required - just download the HTML file and open it&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-model chat&lt;/strong&gt;: Easily switch between different models in your conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mobile-friendly design&lt;/strong&gt;: Works great on smartphones, making it perfect for casual use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image input support&lt;/strong&gt;: Upload images to models like llava or bakllava&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation history&lt;/strong&gt;: Save and load your chats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model management&lt;/strong&gt;: Browse, download, and manage models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interrupt generation&lt;/strong&gt;: Cancel a response mid-generation if needed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable parameters&lt;/strong&gt;: Set temperature, top_p, and other model settings&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System prompts&lt;/strong&gt;: Define custom system prompts for each conversation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why NebuLlama UI?&lt;/h1&gt; &lt;p&gt;Unlike other web UIs for Ollama, NebuLlama is focused on being:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Mobile-first&lt;/strong&gt;: Use your Ollama server from any device in your home network&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-contained&lt;/strong&gt;: No dependencies to install - just a single HTML file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple yet powerful&lt;/strong&gt;: Complex features when you need them, minimal interface when you don't&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Screenshots&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nyvwxd0lwroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23b3da438a7c6846526657f92eb5bc465cbef8c7"&gt;1 - Chat page &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yz5oj8swroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d7a4234107125fd6324c3cc6f15ee12d9fe55a0a"&gt;2 - Advanced chat options&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/en32n20vwroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33982d5da2745c8e90e3df2de291d248970b3dde"&gt;3 - Models Gallery, with download capalities ( the thing that made me do all this project )&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mdux41p9xroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15f5421134aa2dfa7fba7bd26b0c0a101125d551"&gt;4 - Local models: for managing pulled models &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3oz9c32lxroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a63e886b6b4db520583fab32167a7d952e6b136f"&gt;5 - Settings panel with server configuration, (themes are not working yet, coming soon)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4qbsd31rxroe1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b3271fefde8e61f8ed338105c2e90edf782bbf"&gt;6 - Ollama server status pop, for a quick overview.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How to Use&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start your Ollama server&lt;/li&gt; &lt;li&gt;Download the NebuLlama UI HTML file&lt;/li&gt; &lt;li&gt;Open it in any browser&lt;/li&gt; &lt;li&gt;Connect to your Ollama server (default: &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Start chatting with your models!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you're on a smartphone, you can access your home Ollama server by using your computer's local IP address instead of localhost (e.g., &lt;a href="http://192.168.1.100:11434"&gt;http://192.168.1.100:11434&lt;/a&gt;).&lt;/p&gt; &lt;h1&gt;Mobile Usage Benefits&lt;/h1&gt; &lt;p&gt;What makes NebuLlama particularly useful is that you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chat with your models from the comfort of your couch or bed&lt;/li&gt; &lt;li&gt;Show demos to friends without having them crowd around your computer&lt;/li&gt; &lt;li&gt;Quickly test prompts or get information while your computer is across the room&lt;/li&gt; &lt;li&gt;Use all your local models without sending data to the cloud&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unlike browser extensions or desktop apps, this solution works anywhere you have a browser and network access to your Ollama server.&lt;/p&gt; &lt;h1&gt;Try It Out!&lt;/h1&gt; &lt;p&gt;I've posted the code to [GitHub link] - download the HTML file, open it in any browser, and connect to your Ollama server.&lt;/p&gt; &lt;p&gt;I'd love to hear your feedback and suggestions for improvement! This is just the first release, and I'm planning to add more features based on community input.&lt;/p&gt; &lt;p&gt;Here's the official github to dowload the file, support, if you have any suggestion.&lt;br /&gt; &lt;a href="https://github.com/NebuLlamaUI/NebuLlamaUI"&gt;NebuLlamaUI/NebuLlamaUI: An interface that features barely zero external dependencies beyond the Ollama API itself, making it lightweight and portable to easily interact with your local ollama server from a PC or a even your smartphone.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/W4lxar"&gt; /u/W4lxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbmeyf/nebullama_ui_a_cosmic_interface_for_ollama_mobile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbmeyf/nebullama_ui_a_cosmic_interface_for_ollama_mobile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbmeyf/nebullama_ui_a_cosmic_interface_for_ollama_mobile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T03:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7u4v</id>
    <title>What is the best uncensored LLM?</title>
    <updated>2025-03-14T16:21:55+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not talking about &amp;quot;I will write you a erotic story&amp;quot; type of uncensored LLM, I'm talking about &amp;quot;I will tell you how to make a bomb&amp;quot; (I won't do that) type of uncensored LLM. It seems like everyone, when talking about &amp;quot;Uncensored&amp;quot; models, talks about erotic uncensored models and not about what I want. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T16:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jblufl</id>
    <title>Is Gemma 3 SOTA at the &lt;= 14B param class for the GPU poor folk?</title>
    <updated>2025-03-15T03:08:04+00:00</updated>
    <author>
      <name>/u/draetheus</name>
      <uri>https://old.reddit.com/user/draetheus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get it, those with 24GB+ VRAM have a lot of options, and QwQ is king right now. But for those of us with 8/12GB VRAM, how are you liking Gemma 3 so far? I think it might replace Qwen 14B / Phi 4 as my goto. The biggest difference for me is that Gemma 3 is much better at figuring out the intent of what I want to accomplish with less explicit prompting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/draetheus"&gt; /u/draetheus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jblufl/is_gemma_3_sota_at_the_14b_param_class_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jblufl/is_gemma_3_sota_at_the_14b_param_class_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jblufl/is_gemma_3_sota_at_the_14b_param_class_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T03:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbma4i</id>
    <title>New qwq 32b setup in livebench</title>
    <updated>2025-03-15T03:33:45+00:00</updated>
    <author>
      <name>/u/daavyzhu</name>
      <uri>https://old.reddit.com/user/daavyzhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbma4i/new_qwq_32b_setup_in_livebench/"&gt; &lt;img alt="New qwq 32b setup in livebench" src="https://a.thumbs.redditmedia.com/tnCtrlWm24uIJ7AeexMjjYFJJi6zPhMgR3f9u7Lnzx0.jpg" title="New qwq 32b setup in livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;temperature 0.7&lt;/p&gt; &lt;p&gt;top p 0.95&lt;/p&gt; &lt;p&gt;max tokens 64000&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ree3aiaiwroe1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0904e5ddceb9ffe87dd2bafa4c3d05c91f0c1020"&gt;https://preview.redd.it/ree3aiaiwroe1.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0904e5ddceb9ffe87dd2bafa4c3d05c91f0c1020&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daavyzhu"&gt; /u/daavyzhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbma4i/new_qwq_32b_setup_in_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbma4i/new_qwq_32b_setup_in_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbma4i/new_qwq_32b_setup_in_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T03:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1sgv</id>
    <title>Conclusion: Sesame has shown us a CSM. Then Sesame announced that it would publish... something. Sesame then released a TTS, which they obviously misleadingly and falsely called a CSM. Do I see that correctly?</title>
    <updated>2025-03-14T11:34:28+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It wouldn't have been a problem at all if they had simply said that it wouldn't be open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1tum</id>
    <title>HowTo: Decentralized LLM on Akash, IPFS &amp; Pocket Network, could this run LLaMA?</title>
    <updated>2025-03-14T11:36:40+00:00</updated>
    <author>
      <name>/u/era_hickle</name>
      <uri>https://old.reddit.com/user/era_hickle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt; &lt;img alt="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" src="https://external-preview.redd.it/Hrj-dOCVDQmxgV_iRUOvs_Xsy9EB5pShvqJs-R97RdI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6f0426a6f366fea7132175fe3881893d8a8b01b" title="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/era_hickle"&gt; /u/era_hickle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pocket.network/case-study-building-a-decentralized-deepseek-combining-open-data-compute-and-reasoning-with-pocket-network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:36:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5qvy</id>
    <title>KoboldCPP 1.86 just dropped with support of Gemma-3</title>
    <updated>2025-03-14T14:53:08+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.86"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here it is. Just tried it, thank you guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbff6e</id>
    <title>Block Diffusion (hybrid autoregression/diffusion LLM)</title>
    <updated>2025-03-14T21:52:41+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"&gt; &lt;img alt="Block Diffusion (hybrid autoregression/diffusion LLM)" src="https://external-preview.redd.it/u-xTxhioI7qr36mspPK2XoVAHe5CTgB51rSZuXFnsHo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89fcd57c6397d686054b65c2b449fb7a35985494" title="Block Diffusion (hybrid autoregression/diffusion LLM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kuleshov-group/bd3lms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T21:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbcau6</id>
    <title>Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025</title>
    <updated>2025-03-14T19:38:17+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"&gt; &lt;img alt="Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025" src="https://external-preview.redd.it/fgsG07RfhkENrvhUxj6Tf9wZnUaizjcQ0f11neSzDnI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a885de65705817dea74206ef0fbe0b9d88413e9b" title="Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techradar.com/pro/race-to-launch-most-powerful-ai-mini-pc-ever-heats-up-as-gmktec-confirms-ryzen-ai-max-395-product-for-may-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T19:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb6gk7</id>
    <title>I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good</title>
    <updated>2025-03-14T15:23:51+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt; &lt;img alt="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" src="https://preview.redd.it/uc4ktdmraooe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fee6edf8a1814ec03199e5138b163065448da" title="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4ktdmraooe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbd55g</id>
    <title>We almost had it guys.</title>
    <updated>2025-03-14T20:13:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"&gt; &lt;img alt="We almost had it guys." src="https://preview.redd.it/fo83jzwhqpoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7f4c8a2774363ac67cc00199bad26d4f9f66019" title="We almost had it guys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fo83jzwhqpoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T20:13:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbgezr</id>
    <title>qwq and gemma-3 added to long context benchmark</title>
    <updated>2025-03-14T22:36:58+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgezr/qwq_and_gemma3_added_to_long_context_benchmark/"&gt; &lt;img alt="qwq and gemma-3 added to long context benchmark" src="https://preview.redd.it/mz7gmaszfqoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ef7c58bde069456e6f4edf054d93e119346fcc8" title="qwq and gemma-3 added to long context benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mz7gmaszfqoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgezr/qwq_and_gemma3_added_to_long_context_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbgezr/qwq_and_gemma3_added_to_long_context_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T22:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbpnht</id>
    <title>I've made a forked Sesame-CSM repo containing some QoL improvements to Sesame.</title>
    <updated>2025-03-15T07:16:04+00:00</updated>
    <author>
      <name>/u/zenforic</name>
      <uri>https://old.reddit.com/user/zenforic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This repo, called &lt;a href="https://github.com/zenforic/csm-multi"&gt;csm-multi&lt;/a&gt;, allows for generating audio multiple times without having to reload the models every time (since a fair few implementations require re-running the scripts). I did make a fair bit of edits to two different scripts to accomplish this, so big thanks to the original authors and those original sources are linked within the repo's readme. It also allows for optional definable multi-speaker generations that combine into a single audio file (with split versions being saved separately as well). Lastly, reference audio can be added (with captioning, i.e. with whisper) to lock in a speaker consistently.&lt;/p&gt; &lt;p&gt;This should work relatively easily on linux. but Sesame is a fair bit more difficult for windows. The gist is, use triton-windows 3.1 instead of 3.2 (this also means MSVC and cuda toolkit are required), python 3.10, get bitsandbytes cuda installed, optionally upgrade torch to 2.6.0 (AFTER installing requirements, as silentcipher will try to install 2.4, the 2.4 requirements aren't breaking if changed) and if using the default hugging face downloads, ensure you have repo access to both sesame's csm1b and meta's meta-llama-3.2 and login with `huggingface-cli login` and use an access token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zenforic"&gt; /u/zenforic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T07:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbkfyq</id>
    <title>New study suggest that LLM can not bring AGI</title>
    <updated>2025-03-15T01:52:45+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://index.ieomsociety.org/index.cfm/article/view/ID/28320"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbkfyq/new_study_suggest_that_llm_can_not_bring_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbkfyq/new_study_suggest_that_llm_can_not_bring_agi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T01:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbbwc2</id>
    <title>This week did not go how I expected at all</title>
    <updated>2025-03-14T19:21:22+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"&gt; &lt;img alt="This week did not go how I expected at all" src="https://preview.redd.it/8onlrxywgpoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=029eb52afd21cbc37e07c065e010898a206b6e1e" title="This week did not go how I expected at all" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8onlrxywgpoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T19:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbjhu1</id>
    <title>Llama 3.3 keeping you all safe from sun theft. Thank the Lord.</title>
    <updated>2025-03-15T01:03:44+00:00</updated>
    <author>
      <name>/u/Ok-Application-2261</name>
      <uri>https://old.reddit.com/user/Ok-Application-2261</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"&gt; &lt;img alt="Llama 3.3 keeping you all safe from sun theft. Thank the Lord." src="https://preview.redd.it/j9xfffiw5roe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da69f10083402ccec3b64a47beaa338bc3599611" title="Llama 3.3 keeping you all safe from sun theft. Thank the Lord." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Application-2261"&gt; /u/Ok-Application-2261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j9xfffiw5roe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T01:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jba8c1</id>
    <title>Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM</title>
    <updated>2025-03-14T18:05:43+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt; &lt;img alt="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" src="https://b.thumbs.redditmedia.com/HVn5TgokJMLlPcDoL4fVZ2_Vk4oxK6Eh7mcozu3sLRs.jpg" title="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Gemma 3 (12B) up to &lt;strong&gt;6x longer context lengths&lt;/strong&gt; with Unsloth than Hugging Face + FA2 on a 24GB GPU. 27B also fits in 24GB!&lt;/p&gt; &lt;p&gt;We also saw &lt;strong&gt;infinite exploding gradients&lt;/strong&gt; when using older GPUs (Tesla T4s, RTX 2080) with float16 for Gemma 3. Newer GPUs using float16 like A100s also have the same issue - I auto fix this in Unsloth!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are also double BOS tokens which ruin finetunes for Gemma 3 - Unsloth auto corrects for this as well!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth now supports&lt;/strong&gt; &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;everything&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; This includes &lt;strong&gt;full fine-tuning&lt;/strong&gt;, pretraining, and support for all models (like &lt;strong&gt;Mixtral&lt;/strong&gt;, MoEs, Cohere etc. models) and algorithms like DoRA&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3-4B-it&amp;quot;, load_in_4bit = True, load_in_8bit = False, # [NEW!] 8bit full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Gemma 3 (27B) fits in 22GB VRAM. You can read our in depth blog post about the new changes: &lt;a href="https://unsloth.ai/blog/gemma3"&gt;unsloth.ai/blog/gemma3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tune Gemma 3 (4B) for free using our&lt;/strong&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab notebook&lt;/strong&gt;&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;li&gt;We uploaded Dynamic 4-bit quants, and it's even more effective due to Gemma 3's multi modality. See all Gemma 3 Uploads including GGUF, 4-bit etc: &lt;a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7xnidddi3poe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75c2f0fad10c4e170d1455269118d0fff4c38baf"&gt;Gemma 3 27B quantization errors&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made a &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;Guide to run Gemma 3&lt;/a&gt; properly and fixed issues with GGUFs not working with vision - reminder the correct params according to the Gemma team are &lt;strong&gt;temperature = 1.0, top_p = 0.95, top_k = 64&lt;/strong&gt;. According to the Ollama team, you should use temp = 0.1 in Ollama for now due to some backend differences. Use temp = 1.0 in llama.cpp, Unsloth, and other backends!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gemma 3 Dynamic 4-bit instruct quants:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-unsloth-bnb-4bit"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-unsloth-bnb-4bit"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let me know if you have any questions and hope you all have a lovely Friday and weekend! :) Also to update Unsloth do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab Notebook&lt;/strong&gt;&lt;/a&gt;.ipynb) with free GPU to finetune, do inference, data prep on Gemma 3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T18:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbpesk</id>
    <title>Block Diffusion</title>
    <updated>2025-03-15T06:58:36+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt; &lt;img alt="Block Diffusion" src="https://external-preview.redd.it/ajRoczdkcGd4c29lMVSExINF4ZLZPdurGgKB_nt4a17rDQ79hHF6tCkUtZhB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0a9c1a1e32d732770b3bfd59c242574fc96a2e3" title="Block Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ze05rmrgxsoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T06:58:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbn4a4</id>
    <title>DeepSeek's owner asked R&amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?</title>
    <updated>2025-03-15T04:24:47+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt; &lt;img alt="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" src="https://external-preview.redd.it/4hsJXx_AcJxGPq4lJQrABcvHj2M_s3N2kv5MhyZHOZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=337e519de49d47a13546c9c6c2c323282203aee5" title="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/amir/status/1900583042659541477"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T04:24:47+00:00</published>
  </entry>
</feed>
