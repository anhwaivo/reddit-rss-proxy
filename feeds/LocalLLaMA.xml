<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-11T23:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hydavt</id>
    <title>New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b</title>
    <updated>2025-01-10T19:56:16+00:00</updated>
    <author>
      <name>/u/iamephemeral</name>
      <uri>https://old.reddit.com/user/iamephemeral</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt; &lt;img alt="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" src="https://external-preview.redd.it/r4CGqgcRPLr1eA9JfvNHSBaN_-4tgT5j575hGH0pgUU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239946d045e3a552b2d863b9157de34884befd7f" title="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamephemeral"&gt; /u/iamephemeral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Goodfire/Llama-3.3-70B-Instruct-SAE-l50"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T19:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz1u6k</id>
    <title>Models for music?</title>
    <updated>2025-01-11T18:07:39+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone use LLMs for writing music and have leads of frameworks and models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1u6k/models_for_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1u6k/models_for_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1u6k/models_for_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:07:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz4arl</id>
    <title>Vision for the Future</title>
    <updated>2025-01-11T19:56:06+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With NVIDIA's release of their Project Digits, previously the Jetson Super and several other major players putting resources into local AI hardware, what is the vision for the future when it comes to local AI use? &lt;/p&gt; &lt;p&gt;In the limit, do we envision every household equipped with local AI inference? If that's the case, what is the scenario for when you're not in proximity to these devices and need access to the AI? &lt;/p&gt; &lt;p&gt;Are they only for use at home or do we envision a world where people's devices are accessible from outside the home using something like dynamic DNS for external accessibility? &lt;/p&gt; &lt;p&gt;Or we do we envision people using current cloud infrastructure when away from these AI devices?&lt;/p&gt; &lt;p&gt;If the future is local but local as in you do inference on your phone, your laptop etc... (without a dedicated AI machine) how does personal context gets synced between devices?&lt;/p&gt; &lt;p&gt;TLDR: what's your long term vision for local AI use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4arl/vision_for_the_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4arl/vision_for_the_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4arl/vision_for_the_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T19:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz2dj1</id>
    <title>Good docker compose for vision LLM that fits in 24gb vram?</title>
    <updated>2025-01-11T18:31:01+00:00</updated>
    <author>
      <name>/u/LightBrightLeftRight</name>
      <uri>https://old.reddit.com/user/LightBrightLeftRight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing around with a 3090 server I have at my house, trying to have a vision-capable and tool-capable model to integrate with home assistant, cameras etc. My homelab setup is a lot of docker-compose containers, and as a medium-competent person I've been trying to just use prebuild docker images.&lt;/p&gt; &lt;p&gt;I've had a hell of a time getting any of the vision models to work on VLLM, Aphrodite. Ollama works but I want a model to stay in the VRAM rather than loading/unloading, I don't like how hacky it is getting that to work with Ollama.&lt;/p&gt; &lt;p&gt;Does anybody have a docker compose I could gank from you that would give me an open-ai vision/tool model that would fit in a 24gb graphics card? Would be greatly appreciated.&lt;/p&gt; &lt;p&gt;OK based on the docker command from &lt;a href="/u/DeltaSqueezer"&gt;u/DeltaSqueezer&lt;/a&gt; I played around with some settings and got this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; services: vllm-openai: runtime: nvidia deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: - gpu environment: - VLLM_RPC_TIMEOUT=999000 - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True volumes: - /home/root/.cache/huggingface:/root/.cache/huggingface ports: - 18888:18888 image: vllm/vllm-openai:latest command: --model Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8 --host 0.0.0.0 --port 18888 --max-model-len 16384 --gpu-memory-utilization 0.85 --disable-custom-all-reduce --swap-space 1 --max-num-seqs 6 --dtype half --enforce-eager --enable-prefix-caching &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I needed to decrease the gpu memory utilization or there wasn't enough to process new requests. I had this before, and this doesn't feel like the right way to go about tuning this but I'm not sure what a better way would be. I also changed the max model length to handle some complaints I was getting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightBrightLeftRight"&gt; /u/LightBrightLeftRight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2dj1/good_docker_compose_for_vision_llm_that_fits_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2dj1/good_docker_compose_for_vision_llm_that_fits_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2dj1/good_docker_compose_for_vision_llm_that_fits_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyf1pf</id>
    <title>Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro?</title>
    <updated>2025-01-10T21:09:12+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt; &lt;img alt="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjoau</id>
    <title>This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)</title>
    <updated>2025-01-11T00:38:10+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt; &lt;img alt="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" src="https://b.thumbs.redditmedia.com/niNscGOj9hur8A-QVwFzrElx4sAsFt-GLXQ2A5RCLGw.jpg" title="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hyjoau"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz1g2z</id>
    <title>Why are YC companies bullish on AI Workflows?</title>
    <updated>2025-01-11T17:51:05+00:00</updated>
    <author>
      <name>/u/Sam_Tech1</name>
      <uri>https://old.reddit.com/user/Sam_Tech1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen a lot of YC Companies building AI workflows but I haven't seen any real life use case for it yet. They are very bullish on it but are any companies or individuals using it? If yes, what are the use cases? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sam_Tech1"&gt; /u/Sam_Tech1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1g2z/why_are_yc_companies_bullish_on_ai_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1g2z/why_are_yc_companies_bullish_on_ai_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1g2z/why_are_yc_companies_bullish_on_ai_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyomxu</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push</title>
    <updated>2025-01-11T05:04:42+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push?leadSource=reddit_wall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz1a01</id>
    <title>48gb vs 96gb VRAM for fine-tuning</title>
    <updated>2025-01-11T17:43:37+00:00</updated>
    <author>
      <name>/u/salec65</name>
      <uri>https://old.reddit.com/user/salec65</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A popular configuration for local hosting is 2x 24gb GPUs (3090's for example). This would let me access LLama3.3 Q4 which I find myself using often. In order to go beyond this, one either has to add more consumer GPUs, which gets tricky in standard desktops or rackmount cases, or switch to workstation/server GPUs where they can be more efficiently packed in.&lt;/p&gt; &lt;p&gt;For someone that is about to start really getting into fine-tuning models but hasn't quite understood when to use QLoRA/LoRA/FFT as well as starting to use larger prompts, I am curious whether it'd be worth it (or necessary) to go the extra mile and get myself setup for 96gb or more.&lt;/p&gt; &lt;p&gt;Some of my goals include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Code/Data generation - Generating documents w/ a specific syntax (xml-ish) based on prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Domain specific Q&amp;amp;A&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creative personas and characters for unique dialog&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While I plan to spin up instances of 2x 3090 and 2x A6000 to see for myself, I am very interested to hear from others with experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salec65"&gt; /u/salec65 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyukc2</id>
    <title>GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;D</title>
    <updated>2025-01-11T12:09:03+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt; &lt;img alt="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" src="https://external-preview.redd.it/2MaaUSNtf5DLbq6ZpF876OWYQdcOtASsj6e_pAKWpKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec156925c4109c28028bb52b1517ca8eb977cd5a" title="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tegridydev/dnd-llm-game?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz50c0</id>
    <title>Denser Reward for RLHF PPO Training</title>
    <updated>2025-01-11T20:27:42+00:00</updated>
    <author>
      <name>/u/Leading-Contract7979</name>
      <uri>https://old.reddit.com/user/Leading-Contract7979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to share our recent work &amp;quot;Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model&amp;quot;! &lt;/p&gt; &lt;p&gt;In this paper, &lt;strong&gt;we study the granularity of action space in RLHF PPO training&lt;/strong&gt;, assuming only binary preference labels. Our proposal is to &lt;strong&gt;assign reward to each semantically complete text segment&lt;/strong&gt;, rather than per-token (maybe over-granular) or bandit reward (sparse). We further &lt;strong&gt;design techniques to ensure the effectiveness and stability of RLHF PPO training under the denser {segment, token}-level rewards&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our Segment-level RLHF PPO and its Token-level PPO variant outperform bandit PPO&lt;/strong&gt; across AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks under various backbone LLMs (Llama Series, Phi Series).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2501.02790"&gt;https://arxiv.org/pdf/2501.02790&lt;/a&gt; &lt;ol&gt; &lt;li&gt;Benckmark results are available at: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO?tab=readme-ov-file#benckmark-results--released-models"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO?tab=readme-ov-file#benckmark-results--released-models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Method illustration at: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO/blob/main/method.png"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO/blob/main/method.png&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Prior work on token-level reward model for RLHF: &lt;a href="https://arxiv.org/abs/2306.00398"&gt;https://arxiv.org/abs/2306.00398&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading-Contract7979"&gt; /u/Leading-Contract7979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyrml</id>
    <title>[Mini Rant] Are LLMs trapped in English and the assistant paradigms?</title>
    <updated>2025-01-11T15:52:33+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;br /&gt; It feels like we’re trapped in two mainstream paradigms, and it’s starting to get on my nerves. Let me explain:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLMs (too) focused on English&lt;/strong&gt;&lt;br /&gt; We’re seeing more and more models—Qwen, Mistral, Llama 3.x, etc.—that claim “multilingual” abilities. And if you look closely, everyone approaches the problem differently. However, my empirical scenarios often fail to deliver a good experience with those LLMs, even at a 70B scale.&lt;br /&gt; Yes, I understand English reaches the largest audience, but by focusing everything on English, we’re limiting the nuanced cultural and stylistic richness of other languages (French, Spanish, Italian, etc.).&lt;br /&gt; As a result, we rarely see new “styles” or modes of reasoning outside of English.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The “assistant” obsession&lt;/strong&gt;&lt;br /&gt; Everyone wants to build a conversation assistant. Sure, it’s a popular use case,&lt;br /&gt; but it kind of locks us into a single format: a Q&amp;amp;A flow with a polite, self-censored style.&lt;br /&gt; We forget these are token generators that could be tweaked for creative text manipulation or other forms of generation.&lt;br /&gt; I really wish we’d explore more diverse use cases: scenario generation, data-to-text, or other conversation protocols that aren’t so uniform.&lt;/p&gt; &lt;p&gt;I understand that model publishers invest significant resources into performing benchmarks and enhancing multilingual capabilities. For instance, Aya Expanse by Cohere For AI represents a notable advancement in this area. Despite these efforts, in real-world scenarios, I’ve never been able to achieve the same level of performance in French as in English with open-source models. Conversely, closed-source models maintain a more consistent performance across languages, which is frustrating because I’d prefer using open-source models.&lt;/p&gt; &lt;p&gt;Am I the only one who feels we’re stuck between “big English-only LLMs” and “conversation assistant” paradigms? I think there’s so much potential out there for better multilingual support and more interesting use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvfjq</id>
    <title>What do you think of AI employees?</title>
    <updated>2025-01-11T13:03:35+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seeing a surge in start-ups and large enterprises building AI employees.&lt;/p&gt; &lt;p&gt;A good number of well-funded start-ups are building AI SDRs, SWEs, marketing agents, and Customer success agents. Even Salesforce is working on AgentForce to create no-code salesforce automation agents.&lt;/p&gt; &lt;p&gt;This trend is growing faster than I thought; dozens of start-ups are probably in YC this year.&lt;/p&gt; &lt;p&gt;I’m not sure if any of them are in production doing the jobs in the real world, and also, these agents may require a dozen integrations to be anywhere close to being functional.&lt;/p&gt; &lt;p&gt;As much as I like LLMs, they still don’t seem capable of handling edge cases in real-world jobs. They may be suitable for building automated pipelines for tightly scoped tasks, but replacing humans seems far-fetched.&lt;/p&gt; &lt;p&gt;Salesforce Chairman Mark Benioff even commented on not hiring human employees anymore; though it could be their sneaky marketing, it shows their intent.&lt;/p&gt; &lt;p&gt;What do you think of this AI employee in general the present and future? I would love to hear your thoughts if you’re building something simillar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz97my</id>
    <title>they don’t know how good gaze detection is on moondream</title>
    <updated>2025-01-11T23:38:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt; &lt;img alt="they don’t know how good gaze detection is on moondream" src="https://external-preview.redd.it/anBia3RnaGhhZ2NlMSTi0DO1FtxEm4mYFQVOtZR8uuj4lv59wjB_E-Pc4Mjr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3206ad0d968f5e06525f4113c574566e35551fb1" title="they don’t know how good gaze detection is on moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xgysp5nhagce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T23:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu2dh</id>
    <title>LocalGLaDOS - running on a real LLM-rig</title>
    <updated>2025-01-11T11:34:21+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt; &lt;img alt="LocalGLaDOS - running on a real LLM-rig" src="https://external-preview.redd.it/EfE2n_bbhcmfaS9RbA5FtQq7jGIahU2UIGm8g-a1Uag.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ce4ca891cbd89dfa15f29ba5ffa968064f42e85" title="LocalGLaDOS - running on a real LLM-rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-GHKTocDF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz254t</id>
    <title>New finetune Negative_LLAMA_70B</title>
    <updated>2025-01-11T18:20:50+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's January 2025, and still, there are very few models out there that have successfully tackled LLM's positivity bias. &lt;strong&gt;LLAMA 3.3&lt;/strong&gt; was received in the community with mixed feelings. It is an exceptional assistant, and superb at instruction following (&lt;strong&gt;highest IFEVAL&lt;/strong&gt; to date, and by a large margin too.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem-&lt;/strong&gt; it is very predictable, dry, and of course, plaugued with positivity bias like all other LLMs. &lt;strong&gt;Negative_LLAMA_70B&lt;/strong&gt; is &lt;strong&gt;not&lt;/strong&gt; an unalignment-focused model (even though it's pretty uncensored), but it is my attempt to address positivity bias while keeping the exceptional intelligence of the &lt;strong&gt;LLAMA 3.3 70B&lt;/strong&gt; base model. Is the base 3.3 smarter than my finetune? I'm pretty sure it is, however, Negative_LLAMA_70B is still pretty damn smart.&lt;/p&gt; &lt;p&gt;The model was &lt;strong&gt;NOT&lt;/strong&gt; overcooked with unalignment, so it won't straight up throw morbid or depressing stuff at you, but if you were to ask it to write a story, or engage in an RP, you would notice &lt;strong&gt;slightly&lt;/strong&gt; darker undertones. In a long trip, the character takes in a story- their legs will be hurt and would feel tired, in &lt;strong&gt;Roleplay&lt;/strong&gt; when you seriously piss off a character- it might hit you (without the need to explicitly prompt such behavior in the character card).&lt;/p&gt; &lt;p&gt;Also, &lt;strong&gt;toxic-dpo&lt;/strong&gt; and other morbid unalignment datasets were &lt;strong&gt;not&lt;/strong&gt; used. I did include a private dataset that should allow total freedom in both &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt;, and quite a lot of various assistant-oriented tasks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B?not-for-all-audiences=true#tldr"&gt;&lt;/a&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less positivity bias&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;exceptionally good&lt;/strong&gt; at following the character card.&lt;/li&gt; &lt;li&gt;Characters feel more &lt;strong&gt;'alive'&lt;/strong&gt;, and will occasionally &lt;strong&gt;initiate stuff on their own&lt;/strong&gt; (without being prompted to, but fitting to their character).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong ability&lt;/strong&gt; to comprehend and roleplay &lt;strong&gt;uncommon physical and mental characteristics&lt;/strong&gt;. TL;DR Strong Roleplay &amp;amp; Creative writing abilities. Less positivity bias. Very smart assistant with low refusals. exceptionally good at following the character card. Characters feel more 'alive', and will occasionally initiate stuff on their own (without being prompted to, but fitting to their character). Strong ability to comprehend and roleplay uncommon physical and mental characteristics.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyils</id>
    <title>Nvidia 50x0 cards are not better than their 40x0 equivalents</title>
    <updated>2025-01-11T15:40:50+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking closely at the specs, I found 40x0 equivalents for the new 50x0 cards except for 5090. Interestingly, all 50x0 cards are not as energy efficient as the 40x0 cards. Obviously, GDDR7 is the big reason for the significant boost in memory bandwidth for 50x0.&lt;/p&gt; &lt;p&gt;Unless you really need FP4 and DLSS4, there are not that strong a reason to buy the new cards. For the 4070Super/5070 pair, the former can be 15% faster in prompt processing and the latter is 33% faster in inference. If you value prompt processing, it might even make sense to buy the 4070S.&lt;/p&gt; &lt;p&gt;As I mentioned in another thread, this gen is more about memory upgrade than the actual GPU upgrade.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;4070 Super&lt;/th&gt; &lt;th align="left"&gt;5070&lt;/th&gt; &lt;th align="left"&gt;4070Ti Super&lt;/th&gt; &lt;th align="left"&gt;5070Ti&lt;/th&gt; &lt;th align="left"&gt;4080 Super&lt;/th&gt; &lt;th align="left"&gt;5080&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;141.93&lt;/td&gt; &lt;td align="left"&gt;123.37&lt;/td&gt; &lt;td align="left"&gt;176.39&lt;/td&gt; &lt;td align="left"&gt;175.62&lt;/td&gt; &lt;td align="left"&gt;208.9&lt;/td&gt; &lt;td align="left"&gt;225.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;220&lt;/td&gt; &lt;td align="left"&gt;250&lt;/td&gt; &lt;td align="left"&gt;285&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;320&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;656.12&lt;/td&gt; &lt;td align="left"&gt;493.49&lt;/td&gt; &lt;td align="left"&gt;618.93&lt;/td&gt; &lt;td align="left"&gt;585.39&lt;/td&gt; &lt;td align="left"&gt;652.8&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GB/s&lt;/td&gt; &lt;td align="left"&gt;504&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;896&lt;/td&gt; &lt;td align="left"&gt;736&lt;/td&gt; &lt;td align="left"&gt;960&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Price at Launch&lt;/td&gt; &lt;td align="left"&gt;$599&lt;/td&gt; &lt;td align="left"&gt;$549&lt;/td&gt; &lt;td align="left"&gt;$799&lt;/td&gt; &lt;td align="left"&gt;$749&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz7l7d</id>
    <title>OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model</title>
    <updated>2025-01-11T22:23:27+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt; &lt;img alt="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" src="https://preview.redd.it/nhsep8z3xfce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b460c09bdabd5d02e8e8e46757749c4711b75f" title="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhsep8z3xfce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T22:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz4vgm</id>
    <title>I lost track of how good deepfakes are getting</title>
    <updated>2025-01-11T20:21:28+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4vgm/i_lost_track_of_how_good_deepfakes_are_getting/"&gt; &lt;img alt="I lost track of how good deepfakes are getting" src="https://external-preview.redd.it/NWkzYjZ2MGNiZmNlMb2jfsb5QnWJTtG7eE9LYo3ISJ62g2ejiuiPOrO5k-S6.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce3eb3743b8a47035799598b6123366e88fd6de0" title="I lost track of how good deepfakes are getting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6easpphcbfce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4vgm/i_lost_track_of_how_good_deepfakes_are_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz4vgm/i_lost_track_of_how_good_deepfakes_are_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X: &lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt;hf: &lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview&lt;/a&gt; blog: &lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;https://novasky-ai.github.io/posts/sky-t1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz2rar</id>
    <title>Why we don't know researchers behind DeepSeek?</title>
    <updated>2025-01-11T18:48:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero interviews, zero social activity. Zero group photos, none about us page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz0n8c</id>
    <title>GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</title>
    <updated>2025-01-11T17:15:58+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt; &lt;img alt="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" src="https://external-preview.redd.it/fWekNX9cjJo2NgR6zTyYnqvItoILS5GvTDAQC2foz30.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13f97a5793ce6881c43646a9bce53d9dbbf16b98" title="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz5caf</id>
    <title>Tutorial: Run Moondream 2b's new gaze detection on any video</title>
    <updated>2025-01-11T20:42:31+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt; &lt;img alt="Tutorial: Run Moondream 2b's new gaze detection on any video" src="https://external-preview.redd.it/a2VmczhmdHllZmNlMTF40J1mEmizgXzWsZQRgxJwv14NVEzVGBQqF-uixs9J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6d357b58a592f06ed596b1615e185d70bfedfdf" title="Tutorial: Run Moondream 2b's new gaze detection on any video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9ofbftyefce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz28ld</id>
    <title>Bro whaaaat?</title>
    <updated>2025-01-11T18:24:57+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt; &lt;img alt="Bro whaaaat?" src="https://preview.redd.it/cwi5l2ziqece1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6895d12163dd294798940a5c5b6368da7f91b2f" title="Bro whaaaat?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwi5l2ziqece1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:24:57+00:00</published>
  </entry>
</feed>
