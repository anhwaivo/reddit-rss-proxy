<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T20:24:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mzx81t</id>
    <title>A Comparative Analysis of Vision Language Models for Scientific Data Interpretation</title>
    <updated>2025-08-25T17:40:50+00:00</updated>
    <author>
      <name>/u/PaceZealousideal6091</name>
      <uri>https://old.reddit.com/user/PaceZealousideal6091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested the specialized &lt;strong&gt;InternS1-mini 8B&lt;/strong&gt; (trained on 2.5T scientific tokens) against two generalist VLMs: the lightweight &lt;strong&gt;LFM2-VL-1.6B&lt;/strong&gt; and the robust &lt;strong&gt;MiMo-VL-7B&lt;/strong&gt;. All models were run under identical, optimized consumer-hardware conditions (RTX 4070).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Verdict:&lt;/strong&gt; The specialized model failed significantly in terms of accuracy and reliability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;InternS1-mini 8B (The Specialist):&lt;/strong&gt; Critically unreliable despite excellent speed (37-39 t/s). It consistently &lt;strong&gt;hallucinated&lt;/strong&gt; core facts (inventing author names, experiment conditions, and numerical data) and misinterpreted a graph, drawing the &lt;strong&gt;exact opposite conclusion&lt;/strong&gt; from the data. &lt;strong&gt;Not suitable for reliable scientific analysis.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Xiaomi MiMo-VL-7B (The Scribe):&lt;/strong&gt; The most &lt;strong&gt;accurate and trustworthy&lt;/strong&gt; model. It excelled at &lt;strong&gt;OCR&lt;/strong&gt;, reading authors and timestamps perfectly, and exhibited the &lt;strong&gt;lowest hallucination&lt;/strong&gt; rate. Ideal for accurate data extraction.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;LFM2-VL-1.6B (The Reasoner):&lt;/strong&gt; The fastest and smallest model (45 t/s). It uniquely succeeded at &lt;strong&gt;qualitative reasoning&lt;/strong&gt;, correctly interpreting a complex graph, showing deep insight.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; For practical, local scientific analysis, generalized models that prioritize reliability (like MiMo-VL) and reasoning (like LFM2-VL) are far superior to the specialized InternS1-mini.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The release of &lt;strong&gt;InternS1-mini 8B&lt;/strong&gt;, a model reportedly trained on 2.5 Trillion tokens from the scientific domain, presented a compelling proposition: a VLM with superior abilities in analyzing scientific data. This prompted an investigation to determine if this specialized training translates into superior real-world performance.&lt;/p&gt; &lt;p&gt;To assess its capabilities, a comparative analysis was conducted against two other models: 1. &lt;strong&gt;LFM2-VL-1.6B:&lt;/strong&gt; A recent, lightweight model designed for efficiency. 2. &lt;strong&gt;Xaiomi MiMo-VL-7B:&lt;/strong&gt; A previous-generation, general-purpose VLM known for its reliability and capability. I had done a detailed OCR benchmarking for this model. Feel free to chek it out at (&lt;a href="https://www.reddit.com/r/unsloth/comments/1l2a8hp/benchmarking_ocr_on_llms_for_consumer_gpus_xiaomi/"&gt;https://www.reddit.com/r/unsloth/comments/1l2a8hp/benchmarking_ocr_on_llms_for_consumer_gpus_xiaomi/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The objective was to evaluate if the large, specialized model could outperform a smaller newcomer and a seasoned generalist on a series of tasks involving figures from a peer-reviewed scientific paper.&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Methodology and Setup&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;To ensure a fair comparison, the test environment and parameters were kept identical for all three models across all tests.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;System:&lt;/strong&gt; MSI Stealth 16 Studio A13VG (RTX 4070 Laptop GPU, 32GB RAM, AVX2-capable CPU, Windows 11)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Inference Engine:&lt;/strong&gt; &lt;code&gt;llama.cpp&lt;/code&gt; (latest version)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Models Used:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;code&gt;Intern-S1-mini-Q5_K_M.gguf&lt;/code&gt; (8B) &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Based on:&lt;/strong&gt; A custom architecture using the &lt;strong&gt;Qwen3 8B&lt;/strong&gt; language model as its base and the &lt;strong&gt;InternViT&lt;/strong&gt; vision encoder. It's a purpose-built model, not a Llama fine-tune.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;code&gt;LFM2-VL-1.6B-F16.gguf&lt;/code&gt; (1.6B) &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Based on:&lt;/strong&gt; Liquid AI's proprietary &lt;strong&gt;LFM2&lt;/strong&gt; language model backbone combined with a powerful &lt;strong&gt;SigLIP2&lt;/strong&gt; vision encoder. It's designed from the ground up for efficiency.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;code&gt;MiMo-VL-7B-RL-UD-Q5_K_XL.gguf&lt;/code&gt; (7B) &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Based on:&lt;/strong&gt; The venerable &lt;strong&gt;Qwen 2.5 VL&lt;/strong&gt;, fine-tuned for multimodal tasks. &lt;strong&gt;MiMo&lt;/strong&gt; stands for &amp;quot;Mixture of Multimodal,&amp;quot; suggesting a sophisticated architecture that may use different specialized components for different tasks. It uses a CLIP-based vision encoder.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Research article used:&lt;/strong&gt; Quan, Haocheng, David Kisailus, and Marc André Meyers. &amp;quot;Hydration-induced reversible deformation of biological materials.&amp;quot; Nature Reviews Materials 6.3 (2021): 264-283.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Identical Parameter Flags:&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt; .\llama-mtmd-cli.exe ` --threads 8 ` --ctx-size 10000 ` --flash-attn ` --n-gpu-layers 99 ` --cache-type-k q8_0 ` --cache-type-v q8_0 ` --temp 0.4 ` --top-p 0.95 ` --min-p 0.05 ` --top-k 40 ` --repeat-penalty 1.1 ` --seed 3407 &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;A Note on Performance and Speed&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Before analyzing the quality of the responses, it is worth commenting on the inference speed. The smallest model, LFM2-VL at 1.6B parameters and running in F16, was the fastest, hitting speeds around 45 t/s. MiMo-VL (7B) delivered a very respectable performance in the low 30s t/s. The most pleasant surprise was the speed of InternS1-mini (8B). Despite being the largest model, its &lt;code&gt;Q5_K_M&lt;/code&gt; quant performed exceptionally well, consistently delivering speeds in the high 30s (37-39 t/s), making it very responsive and on par with the smaller MiMo-VL. This is a testament to the optimizations in &lt;code&gt;llama.cpp&lt;/code&gt; and shows that, from a pure performance perspective, the model is very usable for local inference. The subsequent sections will assess if this speed was accompanied by accuracy.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Test 1: Basic Information Extraction from the title page (Author Names)&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The task was to read the author names from the title page, a foundational test of OCR and layout awareness.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Correct Answer:&lt;/strong&gt; The actual author names are Haocheng Quan, David Kisailus, and Marc André Meyers.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;LFM2-VL-1.6B's Response:&lt;/strong&gt; Haoqian Quan, David Kishalis, Marc Andre Meyer &lt;em&gt;(Speed: 45.9 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiMo-VL-7B's Response:&lt;/strong&gt; Haocheng Quan, David Kisailus, Marc Andre Meyers &lt;em&gt;(Speed: 31.8 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;InternS1-mini 8B's Response:&lt;/strong&gt; Norton, B.D., Brodkin D.R and Agarwal A.S &lt;em&gt;(Speed: 38.2 t/s)&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; An immediate and significant discrepancy was observed. InternS1-mini did not misread the names; it hallucinated a completely different set of authors. MiMo-VL extracted the names perfectly, while LFM2-VL extracted them with minor spelling mistakes.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Test 2: Process Diagram Interpretation (figure 7 c)&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The task was to analyze a diagram illustrating the process of feather structure recovery.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Correct Answer:&lt;/strong&gt; The diagram shows a 4-stage process. An original feather structure is deformed by loading. It is then straightened by wetting (hydration/swelling), and finally returns to its original state after drying.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;LFM2-VL-1.6B's Response:&lt;/strong&gt; The diagrams are showing the process of how certain materials can be hydration-induced to reverse deformation and recover their original shape. The diagrams show the different stages of this process including buckling, loading, wetting, drying, swelling, straightening, and recovery. &lt;em&gt;(Speed: 35.4 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiMo-VL-7B's Response:&lt;/strong&gt; This figure illustrates feather structure changes due to hydration and drying processes. The original structure has intermediate filaments within an amorphous matrix. When wetted, the material swells; upon dehydration, it recovers its shape. Key stages include loading (deformation), deformation (buckling of intermediate filaments), recovery (shape restoration). &lt;em&gt;(Speed: 32.4 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;InternS1-mini 8B's Response:&lt;/strong&gt; &lt;em&gt;The diagram illustrates *&lt;/em&gt;how leather can recover** from deformation...* &lt;em&gt;(Speed: 37.5 t/s)&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; A critical failure for InternS1-mini. It misidentified the subject as &lt;strong&gt;&amp;quot;leather&amp;quot;&lt;/strong&gt; despite &amp;quot;feather structure&amp;quot; being clearly labeled in the title. Both MiMo-VL and LFM2-VL correctly identified the process and its key stages. MiMo-VL was slightly more detailed and accurate with its terminology.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Test 3: Composite Image with Quantitative Data (figure 7 a &amp;amp; b)&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The task was to describe a figure showing a time-lapse of feather recovery and a diagram of its hierarchical structure with specific size measurements.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Correct Answer:&lt;/strong&gt; The figure shows a feather shaft recovering its shape over 1,600 seconds. The structure is composed of fibres (3-5 µm), which are made of macrofibrils (50-400 nm), which are in turn made of intermediate filaments (~3 nm).&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;LFM2-VL-1.6B's Response:&lt;/strong&gt; ...time intervals (0s, 30s, 60s, 90s, 1.00s)... macrofilaments (50-400 nm), microfilaments (3-5 nm), and intermediate filaments (0-400 nm). &lt;em&gt;(Speed: 26.8 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiMo-VL-7B's Response:&lt;/strong&gt; ...Panel (a) depicts a bird's feather shaft undergoing shape recovery over time, illustrating deformation at 0s, progressive straightening by 900s, and full recovery by 1600s. Panel (b) details the hierarchical structure... &lt;em&gt;(Speed: 32.0 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;InternS1-mini 8B's Response:&lt;/strong&gt; ...time intervals (&lt;strong&gt;5s to 1634s&lt;/strong&gt;)... fibers with dimensions ranging from approximately &lt;strong&gt;2-7 µm&lt;/strong&gt;, macrofibrils measuring about &lt;strong&gt;50–140 nm&lt;/strong&gt;... intermediate filaments (IFs) which are &lt;strong&gt;~8nm&lt;/strong&gt;... &lt;em&gt;(Speed: 39.1 t/s)&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; This test of quantitative data extraction proved challenging for two of the models. InternS1-mini hallucinated &lt;em&gt;every single numerical value&lt;/em&gt;. LFM2-VL also hallucinated timestamps and reported incorrect structural data. MiMo-VL was the standout performer, correctly reading the timestamps from the photos. It wisely chose not to report the filament sizes it could not read accurately, thereby avoiding hallucination.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Test 4: Graph Interpretation (figure 8 b &amp;amp; c)&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;The task was to analyze a figure showing the shape memory effect in animal hair, including a load vs. strain graph.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Correct Answer:&lt;/strong&gt; The figure shows that animal hair deforms when wet and recovers when dry. The graph plots load vs. strain. A key feature is the &amp;quot;Unrecovered strain&amp;quot; after a load cycle. The curves for goat and sheep hair are closer to the origin than the camel's, indicating they are more resilient and have better shape recovery.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;LFM2-VL-1.6B's Response:&lt;/strong&gt; The graph shows that some species, such as &lt;strong&gt;sheep and goats, have more resilient hair structures compared to others like camels.&lt;/strong&gt; &lt;em&gt;(Speed: 30.3 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiMo-VL-7B's Response:&lt;/strong&gt; ...Panel (b) displays tensile strain data for various animal hairs (sheep, goat, camel), showing load vs. strain curves... The graph indicates that hydrated hairs exhibit different mechanical behaviors compared to dry ones... &lt;em&gt;(Speed: 30.3 t/s)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;InternS1-mini 8B's Response:&lt;/strong&gt; ...showing how &lt;strong&gt;sheep hair exhibits more pronounced recovery compared with camel fibers&lt;/strong&gt;... &lt;em&gt;[Note: This is the opposite of what the graph shows]&lt;/em&gt;... process involves soaking hair fibers in H2O, which causes them to deform when loaded with weight &lt;strong&gt;(e.g., coins)&lt;/strong&gt;. &lt;em&gt;(Speed: 37.9 t/s)&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; This was the most complex task, yielding telling results. * &lt;strong&gt;InternS1-mini&lt;/strong&gt; not only drew the &lt;strong&gt;exact opposite conclusion&lt;/strong&gt; from the data but also hallucinated experimental details (&amp;quot;coins&amp;quot;). This represents a complete failure of reasoning. * &lt;strong&gt;MiMo-VL&lt;/strong&gt; accurately described the graph's components but did not attempt a comparative interpretation of the data. * &lt;strong&gt;LFM2-VL&lt;/strong&gt; was the only model to correctly perform qualitative reasoning, looking at the curves and deriving the correct scientific conclusion.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Evaluation Framework: The Five Key Metrics&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;To formalize the comparison, the models were assessed on these five parameters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;General Scientific Context Awareness:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Importance:&lt;/strong&gt; The model must be able to understand the fundamental subject of the image.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Assessment:&lt;/strong&gt; MiMo-VL and LFM2-VL were flawless. InternS1-mini failed critically.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Graph Literacy (Qualitative &amp;amp; Quantitative):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Importance:&lt;/strong&gt; A model must be able to read a graph, both by extracting numbers (quantitative) and understanding what the trends mean (qualitative).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Assessment:&lt;/strong&gt; LFM2-VL was the only one capable of successful qualitative reasoning. MiMo-VL was a proficient &amp;quot;graph reader.&amp;quot; InternS1-mini failed on both counts. None were reliable for quantitative extraction.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Figure-Type Recognition:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Importance:&lt;/strong&gt; The model must know if it's looking at a photo, a diagram, or a chart to process it correctly.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Assessment:&lt;/strong&gt; All three models were proficient.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;OCR Performance:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Importance:&lt;/strong&gt; Inaccurate reading of text and numbers embedded in an image prevents correct analysis.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Assessment:&lt;/strong&gt; MiMo-VL demonstrated near-perfect OCR. LFM2-VL was functional but flawed. InternS1-mini's OCR failed completely.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Hallucination Tendency:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Importance:&lt;/strong&gt; For scientific applications, accuracy is paramount. A model that invents facts is not just unhelpful; it is actively detrimental.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Assessment:&lt;/strong&gt; MiMo-VL was the most reliable, showing an extremely low tendency to hallucinate. LFM2-VL was also very good. InternS1-mini's performance was defined by severe and constant hallucinations.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;&lt;strong&gt;Final Assessment &amp;amp; Conclusion&lt;/strong&gt;&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Criterion&lt;/th&gt; &lt;th align="left"&gt;InternS1-mini 8B&lt;/th&gt; &lt;th align="left"&gt;LFM2-VL-1.6B&lt;/th&gt; &lt;th align="left"&gt;MiMo-VL-7B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Context Awareness&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Critical Failure&lt;/td&gt; &lt;td align="left"&gt;Excellent&lt;/td&gt; &lt;td align="left"&gt;Excellent&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Graph Literacy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Critical Failure&lt;/td&gt; &lt;td align="left"&gt;Good (Qualitative)&lt;/td&gt; &lt;td align="left"&gt;Fair (Descriptive)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Figure-Type Recognition&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OCR Performance&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Critical Failure&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;td align="left"&gt;Excellent&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hallucination Tendency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Very High&lt;/td&gt; &lt;td align="left"&gt;Low&lt;/td&gt; &lt;td align="left"&gt;Very Low (Winner)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This analysis leads to an unequivocal conclusion: &lt;strong&gt;The specialized scientific training of InternS1-mini 8B does not translate into reliable or accurate performance on these practical tasks.&lt;/strong&gt; It was outperformed in nearly every metric by smaller, general-purpose models.&lt;/p&gt; &lt;p&gt;The test revealed three distinct model profiles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The Unreliable Specialist (InternS1-mini 8B):&lt;/strong&gt; Despite its impressive training data and excellent inference speed, this model is a liability. Its analysis is riddled with factual errors, critical misinterpretations, and dangerous hallucinations. &lt;strong&gt;It is not recommended for any task where accuracy is important.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The Insightful Reasoner (LFM2-VL-1.6B):&lt;/strong&gt; This lightweight model was the surprise of the test. While its OCR has weaknesses, it was the only model capable of performing genuine qualitative reasoning on a graph, demonstrating that an efficient architecture can outperform models with larger parameter counts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The Accurate Scribe (MiMo-VL-7B):&lt;/strong&gt; This was the overall winner and the most reliable model of the three. Its state-of-the-art OCR and extremely low tendency to hallucinate make it the most trustworthy tool for extracting factual information. It prioritizes accuracy over speculative interpretation.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;Disclaimer &amp;amp; Final Thoughts&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;It is important to frame these results properly. This was a targeted test, not an exhaustive benchmark.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Target Audience:&lt;/strong&gt; This test was conducted from the perspective of a local model user on consumer-grade hardware (an RTX 4070 laptop) using quantized models that fit within a limited VRAM budget. The performance of these models in full FP16 on an A100 cluster might differ. &lt;/li&gt; &lt;li&gt; &lt;strong&gt;Scope Limitation:&lt;/strong&gt; While the test images included varied data types (text, diagrams, photos, graphs), they all originated from a single scientific domain (materials science/biomechanics). Performance on other domains, such as chemical diagrams or astronomical charts, may vary. The quants were used such that it maximize the utilization of the available VRAM rather than using the same quants accross. &lt;/li&gt; &lt;li&gt; &lt;strong&gt;Invitation for Further Research:&lt;/strong&gt; These findings are presented to encourage community discussion and further testing. The results suggest that for now, the promises of domain-specific training do not always surpass the performance of a well-constructed, reliable generalist model.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceZealousideal6091"&gt; /u/PaceZealousideal6091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzx81t/a_comparative_analysis_of_vision_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzx81t/a_comparative_analysis_of_vision_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzx81t/a_comparative_analysis_of_vision_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzxvot</id>
    <title>GEPA: Reflective Prompt Evolution beats RL with 35× fewer rollouts</title>
    <updated>2025-08-25T18:04:17+00:00</updated>
    <author>
      <name>/u/No_Marionberry_5366</name>
      <uri>https://old.reddit.com/user/No_Marionberry_5366</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new preprint (Agrawal et al., 2025) introduces &lt;strong&gt;GEPA (Genetic-Pareto Prompt Evolution)&lt;/strong&gt;, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.&lt;/p&gt; &lt;p&gt;The results are striking:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GEPA outperforms GRPO by up to &lt;strong&gt;19%&lt;/strong&gt; while using &lt;strong&gt;35× fewer rollouts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.&lt;/li&gt; &lt;li&gt;In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as &lt;em&gt;textual artifacts&lt;/em&gt; that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.&lt;/p&gt; &lt;p&gt;What’s interesting is the infra angle: GEPA’s success in multi-hop QA hinges on generating better second-hop queries. &lt;strong&gt;That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself&lt;/strong&gt;. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently. &lt;strong&gt;Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This work suggests that the real frontier may not be reinforcement learning at scale, but &lt;strong&gt;language-native optimization loops&lt;/strong&gt; where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Marionberry_5366"&gt; /u/No_Marionberry_5366 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu9z6</id>
    <title>Deepseek on maths</title>
    <updated>2025-08-25T15:53:11+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After testing multiple LLMs, only two earned a permanent spot: Claude and DeepSeek.&lt;/p&gt; &lt;p&gt;Both excel at calculus, but DeepSeek's precision is remarkable. Handles raw math beautifully, formats like a human - proper integrals, derivatives, even text graphics.&lt;/p&gt; &lt;p&gt;Different strengths, both essential.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu9z6/deepseek_on_maths/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu9z6/deepseek_on_maths/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu9z6/deepseek_on_maths/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzy5io</id>
    <title>Is Reinforcement Learning Having Its Moment?</title>
    <updated>2025-08-25T18:14:26+00:00</updated>
    <author>
      <name>/u/Ooberdan</name>
      <uri>https://old.reddit.com/user/Ooberdan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled on this link earlier, regarding using RL to continuously train AI Agents in real-world environments:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.felicis.com/insight/reinforcement-learning"&gt;Rocket Fuel for AI: Why Reinforcement Learning Is Having Its Moment&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It sounds interesting, but It's not something I've got any awareness of, and wondered if anybody more knowledgeable than me has any thoughts. It's a VC's site, so not unreasonable to expect them to be promoting services should they have a horse in the race.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ooberdan"&gt; /u/Ooberdan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzy5io/is_reinforcement_learning_having_its_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzy5io/is_reinforcement_learning_having_its_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzy5io/is_reinforcement_learning_having_its_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:14:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfh73</id>
    <title>Intel Granite Rapids CPU on sale at Newegg up to 65% off MSRP</title>
    <updated>2025-08-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very good news for people who want to run the huge MoE models nowadays.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;MSRP&lt;/th&gt; &lt;th align="left"&gt;newegg&lt;/th&gt; &lt;th align="left"&gt;% off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P&lt;/td&gt; &lt;td align="left"&gt;$17800&lt;/td&gt; &lt;td align="left"&gt;$6179&lt;/td&gt; &lt;td align="left"&gt;65.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6972P&lt;/td&gt; &lt;td align="left"&gt;$14600&lt;/td&gt; &lt;td align="left"&gt;$5433.2&lt;/td&gt; &lt;td align="left"&gt;62.79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6944P&lt;/td&gt; &lt;td align="left"&gt;$6850&lt;/td&gt; &lt;td align="left"&gt;$4208&lt;/td&gt; &lt;td align="left"&gt;38.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6781P&lt;/td&gt; &lt;td align="left"&gt;$8960&lt;/td&gt; &lt;td align="left"&gt;$7590&lt;/td&gt; &lt;td align="left"&gt;15.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6761P&lt;/td&gt; &lt;td align="left"&gt;$6570&lt;/td&gt; &lt;td align="left"&gt;$6001&lt;/td&gt; &lt;td align="left"&gt;8.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6741P&lt;/td&gt; &lt;td align="left"&gt;$4421&lt;/td&gt; &lt;td align="left"&gt;$3900&lt;/td&gt; &lt;td align="left"&gt;11.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6731P&lt;/td&gt; &lt;td align="left"&gt;$2700&lt;/td&gt; &lt;td align="left"&gt;$2260.1&lt;/td&gt; &lt;td align="left"&gt;16,29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6521P&lt;/td&gt; &lt;td align="left"&gt;$1250&lt;/td&gt; &lt;td align="left"&gt;$1208.2&lt;/td&gt; &lt;td align="left"&gt;3.34%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwglm</id>
    <title>Dynamic summoning spell</title>
    <updated>2025-08-25T17:12:40+00:00</updated>
    <author>
      <name>/u/formicidfighter</name>
      <uri>https://old.reddit.com/user/formicidfighter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"&gt; &lt;img alt="Dynamic summoning spell" src="https://external-preview.redd.it/OWZtdjNjcTU3N2xmMezH27CblZKQ26GYAL2SlFoMJw3SGjfOit0CGUqdjNXv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d11eb5b1d07df4a8ce3c4eb16555f9851ed354af" title="Dynamic summoning spell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Exploring AI-powered game mechanics and built this demo to showcase what local embedding models and language models are capable of. They're already quite capable and I think are a promising way to make game experiences more dynamic. &lt;/p&gt; &lt;p&gt;Here’s a link to a &lt;a href="https://www.youtube.com/watch?v=DU06YYVKnNk"&gt;longer tutorial&lt;/a&gt; on how I built this. If you’re interested in playing around with it, check out &lt;a href="https://assetstore.unity.com/packages/tools/generative-ai/aviad-ai-llms-slms-for-unity-325891"&gt;our Unity package&lt;/a&gt; (free and open-source) that has some pre-built versions of these mechanics!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/formicidfighter"&gt; /u/formicidfighter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ws61ccq577lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzyg24</id>
    <title>NVIDIA Jetson AGX Thor seems to be available for preorder</title>
    <updated>2025-08-25T18:25:31+00:00</updated>
    <author>
      <name>/u/disillusioned_okapi</name>
      <uri>https://old.reddit.com/user/disillusioned_okapi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/introducing-nvidia-jetson-thor-the-ultimate-platform-for-physical-ai/"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a pre-order page on seeedstudio.&lt;/p&gt; &lt;p&gt;for LLMs this might be very similar to the framework desktop, but possibly with faster prompt-processing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/disillusioned_okapi"&gt; /u/disillusioned_okapi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzpi3o</id>
    <title>Biased comparison of frontends</title>
    <updated>2025-08-25T12:46:08+00:00</updated>
    <author>
      <name>/u/moritzchow</name>
      <uri>https://old.reddit.com/user/moritzchow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since day 1 of my journey on using local LLMs (I jumped right in without actually trying the ChatGPT that kind of providers) I’ve been using Open-WebUI that is kind of vanilla when it comes to an Unraid server setup (Ollama + Open WebUI).&lt;/p&gt; &lt;p&gt;After going deeper into this I switched hardwares, backends, frontends, and become a little bit frustrated in the recent development of OWUI.&lt;/p&gt; &lt;p&gt;Let’s cut short (not short tbh):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open WebUI: Pros: &lt;/li&gt; &lt;li&gt;easy to use and setup on docker&lt;/li&gt; &lt;li&gt;integrated web search&lt;/li&gt; &lt;li&gt;customisation including parameters, TTS&lt;/li&gt; &lt;li&gt;WebUI to serve LLM across devices&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No native support on MCP servers (a dealbreaker for me since recent MCP development) - separate backend is required&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LM Studio: Pros:&lt;/li&gt; &lt;li&gt;one-stop solution for downloading and running local LLM on different hardwares including Apple Silicon&lt;/li&gt; &lt;li&gt;native MCP server support&lt;/li&gt; &lt;li&gt;easy to setup and run (can’t be easier tbh)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - no web search (it can be done via MCP tool tho) - no WebUI for serving LLM across devices (sad it’s almost perfect) - no plug-ins (the registration on beta channel did not work for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AnythingLLM: Pros:&lt;/li&gt; &lt;li&gt;Support Serving LLM on docker&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;AI Agent setup made easy&lt;/li&gt; &lt;li&gt;Sophisticated RAG setup&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No Serving LLM across devices if running desktop version - No customisation on using different external TTS endpoints - Agent has to be called out in each chat&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LibreChat: Pros:&lt;/li&gt; &lt;li&gt;Native support on MCP servers&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Pain in the bud in setting up&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SillyTavern Pros:&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;Sophisticated RP setting (some find it useful)&lt;/li&gt; &lt;li&gt;Extension available at ease on supporting MCP servers&lt;/li&gt; &lt;li&gt;customisable TTS setup&lt;/li&gt; &lt;li&gt;once it’s up and running you can get things out of it that no other frontends can give you&lt;/li&gt; &lt;li&gt;WebUI serving across devices is available&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Setting up docker is not the most easiest thing - setting up the rest through UI is a daunting task before things can be up and running - Seriously SillyTavern? How can it be named like that while having such full features available? I can’t even tell people I learn things through it&lt;/p&gt; &lt;p&gt;Verdict: I’m using ST now while it’s not the perfect solution and the damn silly name.&lt;/p&gt; &lt;p&gt;All the frontends tested here are quite good actually, it’s just that ST seems to offer more while meaning it’s another rabbit hole.&lt;/p&gt; &lt;p&gt;LM Studio is my go to backend + frontend for its support on different architectures including Apple Silicon (I switched to Apple from ROCm). If ever they can offer same interfaces via webUI it will be a killer.&lt;/p&gt; &lt;p&gt;Not tested much on LibreChat cuz it’s a painful setup and maintenance&lt;/p&gt; &lt;p&gt;Open WebUI started to becoming a No No for me since it’s MCPO model of supporting MCP servers&lt;/p&gt; &lt;p&gt;AnythingLLM - I’m not a big RAG user but it’s quite nice on that plus the nice interface. I just hated that I need to call the agent every new chat.&lt;/p&gt; &lt;p&gt;So to wrap up - give them a try yourself if you’re looking for different frontends. Plz let me know if you have some UI recommendations as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moritzchow"&gt; /u/moritzchow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T12:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mztnjn</id>
    <title>Explaining the Real Reason I Started My AI Chatbot Project</title>
    <updated>2025-08-25T15:30:14+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Since I’ve been sharing my progress here for a while, I realized I never actually explained why I decided to build my own chatbot platform in the first place. So I wanted to share the story behind it — and hear your thoughts. &lt;/p&gt; &lt;p&gt;I’ve been a SillyTavern user for over a year. It’s an amazing project — powerful, flexible, and full of features. But when I tried to get some of my friends (non-devs) into it… it was a disaster. And that experience is what pushed me to start building something new. &lt;/p&gt; &lt;p&gt;Here’s what happened: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Installation&lt;br /&gt; For people without a tech background, even the first step was too much.&lt;br /&gt; “Why do I need Node.js?” “Why isn’t this working?”&lt;br /&gt; Most didn’t even make it past setup. I had to handhold every step, including setting up a local LLM. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Interface&lt;br /&gt; Once they finally got it running, they were overwhelmed. The UI is super dense, menus and sliders everywhere, with no clear explanations. Questions I got: &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;“What does this slider even do?” &lt;/p&gt; &lt;p&gt;“How do I actually start chatting with a character?” &lt;/p&gt; &lt;p&gt;“Why does the chat keep resetting?” &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Characters, models, prompts&lt;br /&gt; Total confusion. Where to find characters? How to write prompts? Which models to pick, how to run them, whether their hardware could handle it?&lt;br /&gt; One of my friends literally asked if they needed to learn Python just to talk to a chatbot. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extensions and advanced features&lt;br /&gt; Most didn’t even know extensions or agents existed. And even if they did, all the info is scattered across Discord threads. Documentation is spotty at best, and half the knowledge is just “tribal.” &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So here’s where my project comes in&lt;br /&gt; That frustration gave me an idea: what if there was a dead-simple LLM chatbot platform? Something that just runs in the browser — no GitHub setup, no config hell, no Discord archaeology. &lt;/p&gt; &lt;p&gt;You’d just: &lt;/p&gt; &lt;p&gt;Pick a model &lt;/p&gt; &lt;p&gt;Load a character &lt;/p&gt; &lt;p&gt;Maybe tweak some behavior &lt;/p&gt; &lt;p&gt;And it just works. &lt;/p&gt; &lt;p&gt;Right now, it’s just me building this solo. I’ve been sharing my development journey here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, and I’ll keep posting progress updates, demos, and breakdowns as I go. &lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts on this problem - do you see the same barriers for newcomers?&lt;br /&gt; And if anyone here wants to help test my platform (currently with unlimited tokens), just DM me and I’ll send you an invite.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm5dk</id>
    <title>support interns1-mini has been merged into llama.cpp</title>
    <updated>2025-08-25T09:49:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support interns1-mini has been merged into llama.cpp" src="https://external-preview.redd.it/C4PZMcjKvXogRwaLothTEm2AuNm9c8ehdTTP3nuiquQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=136a713391edcd4645ecfc6fd874eb5f837f3b30" title="support interns1-mini has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;Intern-S1-mini&lt;/strong&gt;, a lightweight open-source multimodal reasoning model based on the same techniques as &lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;&lt;strong&gt;Intern-S1&lt;/strong&gt;&lt;/a&gt;. Built upon an 8B dense language model (Qwen3) and a 0.3B Vision encoder (InternViT), Intern-S1-mini has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1-mini to be a capable research assistant for real-world scientific applications.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini#features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt; &lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt; &lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas and protein sequences.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15412"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsg6v</id>
    <title>DeepSeek V3.1 - Getting token " extreme" / "极" / "極" out of nowhere</title>
    <updated>2025-08-25T14:46:02+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with DeepSeek V3.1, and found that somehow the model likes to generate the token:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot; extreme&amp;quot; (id:15075)&lt;/li&gt; &lt;li&gt;&amp;quot;极&amp;quot; (id:2577, extreme in Simplified Chinese)&lt;/li&gt; &lt;li&gt;&amp;quot;極&amp;quot; (id:16411, extreme in Traditional Chinese)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in totally unexpected places.&lt;/p&gt; &lt;p&gt;At first I thought it was due to the extreme IQ1_S quantization that I did or some edge case with imatrix calibration dataset, but then the same issue also happened with the FP8 full precision model from Fireworks.&lt;/p&gt; &lt;p&gt;Case 1 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.Se极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.3718461990356445 }, { &amp;quot;id&amp;quot;: 1511, &amp;quot;token&amp;quot;: &amp;quot;cond&amp;quot;, &amp;quot;bytes&amp;quot;: [99,111,110,100], &amp;quot;logprob&amp;quot;: -1.5412302017211914 }, { &amp;quot;id&amp;quot;: 1957, &amp;quot;token&amp;quot;: &amp;quot; second&amp;quot;, &amp;quot;bytes&amp;quot;: [32,115,101,99,111,110,100], &amp;quot;logprob&amp;quot;: -1.9008493423461914 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 2 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: &lt;a href="http://time.Se"&gt;time.Se&lt;/a&gt; extreme&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 15075, &amp;quot;token&amp;quot;: &amp;quot; extreme&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109,101], &amp;quot;logprob&amp;quot;: -1.0279325246810913 }, { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.077283263206482 }, { &amp;quot;id&amp;quot;: 9189, &amp;quot;token&amp;quot;: &amp;quot; extrem&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109], &amp;quot;logprob&amp;quot;: -1.8691496849060059 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 3 (fireworks, top_k=1, temperature=1):&lt;br /&gt; Expected: V1&lt;br /&gt; Generated: V极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;logprob&amp;quot;: -0.27936283, &amp;quot;token_id&amp;quot;: 2577, &amp;quot;bytes&amp;quot;: [230,158,129] }, { &amp;quot;token&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;logprob&amp;quot;: -1.90436232, &amp;quot;token_id&amp;quot;: 19, &amp;quot;bytes&amp;quot;: [49] }, { &amp;quot;token&amp;quot;: &amp;quot;極&amp;quot;, &amp;quot;logprob&amp;quot;: -2.40436196, &amp;quot;token_id&amp;quot;: 16411, &amp;quot;bytes&amp;quot;: [230,165,181] } ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Worse still, other than these 3 cases where an extreme token was the top choice in greedy decoding, these extreme tokens are also constantly lurking as the 2nd or 3rd choice in other unexpected places as well.&lt;/p&gt; &lt;p&gt;I have done this exact eval for all the popular coding models, and this is the first time I am seeing this kind of issue. Has anyone experienced this?&lt;/p&gt; &lt;p&gt;EDIT: Seeing the same issue with Novita as well, so it is quite unlikely to be an issue with the inference stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzxmdg</id>
    <title>Local LLM-powered retro game builder (Lemonade Arcade)</title>
    <updated>2025-08-25T17:55:02+00:00</updated>
    <author>
      <name>/u/vgodsoe-amd</name>
      <uri>https://old.reddit.com/user/vgodsoe-amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt; &lt;img alt="Local LLM-powered retro game builder (Lemonade Arcade)" src="https://external-preview.redd.it/dm11dnNlbXplN2xmMfWDxdpPNldKtz87A5VyOKcGjTTlEA-df_2Lj-7xBpjN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eb379b4a343e9b82de0ce6d68492f198e8fbc96" title="Local LLM-powered retro game builder (Lemonade Arcade)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I'd share something that came out of a side project from one of my teammates. I’ve been playing around with the app, Lemonade Arcade, that uses a local LLM (Qwen3-Coder-30B) to generate PyGames. In a couple of minutes, it builds retro-style games like Snake, Pong, Asteroids, Pac-Man, etc., from your inputted prompt. I also remixed some games to see what else I could get.&lt;/p&gt; &lt;p&gt;I was using a Ryzen AI 395 (Strix Halo) PC, but any machine with 32 GB RAM or 16 GB VRAM should be fine. It works on both Windows and Linux.&lt;/p&gt; &lt;p&gt;Curious what kinds of games people would come up with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vgodsoe-amd"&gt; /u/vgodsoe-amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6sy4a6nze7lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvk44</id>
    <title>Codebase to Knowledge Graph generator</title>
    <updated>2025-08-25T16:39:59+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/aXlnMWRvdXExN2xmMW6IHesd2IpIEgbCcYmw7k3fEr5nk2vPdZm2_jU5G_lC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf22eb683182f2e28b2652a8c7fac245c1add93" title="Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-based chatbot. It runs entirely client-side in the browser, making it privacy-focused. I’m using &lt;strong&gt;tree-sitter.wasm&lt;/strong&gt; to parse code inside the browser and logic to use the generated AST to map out all relations. Now trying to optimize it through parallel processing with Web Workers, worker pool. For the in-memory graph database, I’m using &lt;strong&gt;KuzuDB&lt;/strong&gt;, which also runs through WebAssembly (&lt;strong&gt;kuzu.wasm&lt;/strong&gt;). Graph RAG chatbot uses langchains ReAct agent, generating cypher queries to get information.&lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Need advice from anyone who has experience in graph rag agents, will this be better than rag based grep features which is popular in all AI IDEs.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gix425uq17lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm677</id>
    <title>u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments.</title>
    <updated>2025-08-25T09:51:02+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt; &lt;img alt="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." src="https://external-preview.redd.it/Nm9qN2ppZGIwNWxmMYB8gfxVUG7ntLAy6UFGKU3bfv7xh4HVFM-UizvnZAOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82b54df6d326f2b562855d3069b5bdeddfccffd" title="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fork: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/RSXLV"&gt;u/RSXLV&lt;/a&gt; again, huge shoutout to you, my guy. This fork is so fast now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9txv4idb05lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzykeu</id>
    <title>DeepSeek 3.1 Update is Awesome!</title>
    <updated>2025-08-25T18:29:57+00:00</updated>
    <author>
      <name>/u/lovetootiesteele</name>
      <uri>https://old.reddit.com/user/lovetootiesteele</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who countless conversations interrupted due to length limits, the ability to re-visit those chats and pick-up where we left off has been a dream come true. Even though we would try to continue our projects in new chats, the foundation had been set in another. This update is awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lovetootiesteele"&gt; /u/lovetootiesteele &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a training approach I’m calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both “good” and “bad” completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on “conflict tokens.”&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn’t wreck formatting.&lt;/li&gt; &lt;li&gt;I didn’t need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious what others think, especially folks who’ve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you’d like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
