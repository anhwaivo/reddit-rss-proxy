<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-06T03:00:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kfo0hx</id>
    <title>Where to buy workstation GPUs?</title>
    <updated>2025-05-05T21:49:04+00:00</updated>
    <author>
      <name>/u/Prestigious_Thing797</name>
      <uri>https://old.reddit.com/user/Prestigious_Thing797</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've bought some used ones in the past from Ebay, but looking at the RTX Pro 6000 and can't find places to buy an individual card. Anyone know where to look?&lt;/p&gt; &lt;p&gt;I've been bouncing around the Nvidia Partners link (&lt;a href="https://www.nvidia.com/en-us/design-visualization/where-to-buy/"&gt;https://www.nvidia.com/en-us/design-visualization/where-to-buy/&lt;/a&gt;) but haven't found individual cards for sale. Microcenter doesn't list anything near me either.&lt;/p&gt; &lt;p&gt;Edit : Looking to purchase in the US.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious_Thing797"&gt; /u/Prestigious_Thing797 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfo0hx/where_to_buy_workstation_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfo0hx/where_to_buy_workstation_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfo0hx/where_to_buy_workstation_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T21:49:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kftu3s</id>
    <title>Draft Model Compatible With unsloth/Qwen3-235B-A22B-GGUF?</title>
    <updated>2025-05-06T02:28:20+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have installed unsloth/Qwen3-235B-A22B-GGUF and while it runs, it's only about 4 t/sec. I was hoping to speed it up a bit with a draft model such as unsloth/Qwen3-16B-A3B-GGUF or unsloth/Qwen3-8B-GGUF but the smaller models are not &amp;quot;compatible&amp;quot;. &lt;/p&gt; &lt;p&gt;I've used draft models with Llama with no problems. I don't know enough about draft models to know what makes them compatible other than they have to be in the same family. Example, I don't know if it's possible to use draft models of an MoE model. Is it possible at all with Qwen3?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kftu3s/draft_model_compatible_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kftu3s/draft_model_compatible_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kftu3s/draft_model_compatible_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T02:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kexdgy</id>
    <title>What do I test out / run first?</title>
    <updated>2025-05-04T23:21:02+00:00</updated>
    <author>
      <name>/u/Recurrents</name>
      <uri>https://old.reddit.com/user/Recurrents</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt; &lt;img alt="What do I test out / run first?" src="https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4" title="What do I test out / run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got her in the mail. Haven't had a chance to put her in yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recurrents"&gt; /u/Recurrents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kexdgy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T23:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfq4q5</id>
    <title>Some Benchmarks of Qwen/Qwen3-32B-AWQ</title>
    <updated>2025-05-05T23:24:15+00:00</updated>
    <author>
      <name>/u/Specific-Rub-7250</name>
      <uri>https://old.reddit.com/user/Specific-Rub-7250</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfq4q5/some_benchmarks_of_qwenqwen332bawq/"&gt; &lt;img alt="Some Benchmarks of Qwen/Qwen3-32B-AWQ" src="https://b.thumbs.redditmedia.com/eMVYtlEV1KSdm19y92s_NNigff-BhaX1ExeVbSi248M.jpg" title="Some Benchmarks of Qwen/Qwen3-32B-AWQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran some benchmarks locally for the AWQ version of Qwen3-32B using vLLM and evalscope (38K context size without rope scaling)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Default thinking mode: temperature=0.6,top_p=0.95,top_k=20,presence_penalty=1.5&lt;/li&gt; &lt;li&gt;/no_think: temperature=0.7,top_p=0.8,top_k=20,presence_penalty=1.5&lt;/li&gt; &lt;li&gt;live code bench only 30 samples: &amp;quot;2024-10-01&amp;quot; to &amp;quot;2025-02-28&amp;quot;&lt;/li&gt; &lt;li&gt;all were few_shot_num: 0&lt;/li&gt; &lt;li&gt;statistically not super sound, but good enough for my personal evaluation&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specific-Rub-7250"&gt; /u/Specific-Rub-7250 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kfq4q5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfq4q5/some_benchmarks_of_qwenqwen332bawq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfq4q5/some_benchmarks_of_qwenqwen332bawq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T23:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfn6qh</id>
    <title>What benchmarks/scores do you trust to give a good idea of a models performance?</title>
    <updated>2025-05-05T21:14:42+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just looking for some advice on how i can quickly look up a models actual performance compared to others.&lt;/p&gt; &lt;p&gt;The benchmarks used seem to change alot and seeing every single model on huggingface have themselves at the very top or competing just under like OpenAI at 30b params just seems unreal.&lt;/p&gt; &lt;p&gt;(I'm not saying anybody is lying it just seems like companies are choosy with the numbers they share)&lt;/p&gt; &lt;p&gt;Where would you recommend I look for scores that are atleast somewhat accurate and unbiased?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfn6qh/what_benchmarksscores_do_you_trust_to_give_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfn6qh/what_benchmarksscores_do_you_trust_to_give_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfn6qh/what_benchmarksscores_do_you_trust_to_give_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T21:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kff36y</id>
    <title>Experimental Quant (DWQ) of Qwen3-A30B</title>
    <updated>2025-05-05T15:54:01+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt; &lt;img alt="Experimental Quant (DWQ) of Qwen3-A30B" src="https://external-preview.redd.it/LCswmSPGlLeg2uEXPrDVWNpr9PBgA-O2GA2zpqtkfFQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2a9b92fb8daa878a2d33d37bea5b5e7acbb4797" title="Experimental Quant (DWQ) of Qwen3-A30B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used a novel technique - details &lt;a href="https://x.com/N8Programs/status/1919283193892540850"&gt;here&lt;/a&gt; - to quantize Qwen3-30B-A3B into 4.5bpw in MLX. As shown in the image, the perplexity is now on par with a 6-bit quant at no storage cost:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87znt1c8jzye1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e238bdbbc9f02e8be52c3a49393e195c77d5ca7"&gt;Graph showing the superiority of the DWQ technique.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The way the technique works is distilling the logits of the 6bit into the 4bit, treating the quant biases + scales as learnable parameters.&lt;/p&gt; &lt;p&gt;Get the model here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Should theoretically feel like a 6bit in a 4bit quant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfs61t</id>
    <title>Advice: Wanting to create a Claude.ai server on my LAN for personal use</title>
    <updated>2025-05-06T01:03:15+00:00</updated>
    <author>
      <name>/u/phIIX</name>
      <uri>https://old.reddit.com/user/phIIX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am Super new to all this LLM stuff, and y'all will probably be frustrated at my lack of knowledge. Appologies in advanced. If there is a better place to post this, please delete and repost to the proper forum or tell me.&lt;/p&gt; &lt;p&gt;I have been using Claude.ai and having had a blast. I've been using the free version to help me with Commodore Basic 7.0 code, and it's been so much fun! I hit the limits of usage whenever I consult it. So what I would like to do is build a computer to put on my LAN so I don't have the limitations (if it's even possible) of the number of tokens or whatever it is that it has. Again, I am not sure if that is possible, but it can't hurt to ask, right? I have a bunch of computer parts that I could cobble something together. I understand it won't be near as fast/responsive as Claude.ai - BUT that is ok. I just want something I could have locally without the limtations, or not have to spend $20/month I was looking at this: &lt;a href="https://www.kdnuggets.com/using-claude-3-7-locally"&gt;https://www.kdnuggets.com/using-claude-3-7-locally&lt;/a&gt; &lt;/p&gt; &lt;p&gt;As far as hardware goes, I have an i7 and willing to purchase a minimum graphics card and memory (like a 4060 8g for &amp;lt;%500 [I realize 16gb is prefered] - or maybe the 3060 12gb for &amp;lt; $400).&lt;/p&gt; &lt;p&gt;So, is this realistic, or am I (probably) just not understanding all of what's involved? Feel free to flame me or whatever, I realize I don't know much about this and just want a Claude.ai on my LAN.&lt;/p&gt; &lt;p&gt;And after following that tutorial, not sure how I would access it over the LAN. But baby steps. I'm semi-Tech-savy, so I hope I could figure it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phIIX"&gt; /u/phIIX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfs61t/advice_wanting_to_create_a_claudeai_server_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfs61t/advice_wanting_to_create_a_claudeai_server_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfs61t/advice_wanting_to_create_a_claudeai_server_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T01:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfhr8t</id>
    <title>128GB GMKtec EVO-X2 AI Mini PC AMD Ryzen Al Max+ 395 is $800 off at Amazon for $1800.</title>
    <updated>2025-05-05T17:39:08+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my stop. Amazon has the GMK X2 for $1800 after a $800 coupon. That's price of just the Framework MB. This is a fully spec'ed computer with a 2TB SSD. Also, since it's through the Amazon Marketplace all tariffs have been included in the price. No surprise $2,600 bill from CBP. And needless to say, Amazon has your back with the A-Z guarantee.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amazon.com/dp/B0F53MLYQ6"&gt;https://www.amazon.com/dp/B0F53MLYQ6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhr8t/128gb_gmktec_evox2_ai_mini_pc_amd_ryzen_al_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhr8t/128gb_gmktec_evox2_ai_mini_pc_amd_ryzen_al_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhr8t/128gb_gmktec_evox2_ai_mini_pc_amd_ryzen_al_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfdbdi</id>
    <title>is elevenlabs still unbeatable for tts? or good locall options</title>
    <updated>2025-05-05T14:42:21+00:00</updated>
    <author>
      <name>/u/sandwich_stevens</name>
      <uri>https://old.reddit.com/user/sandwich_stevens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is a common one, but surely due to the progress of these models, by now something would have changed with the TTS landscape, and we have some clean sounding local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandwich_stevens"&gt; /u/sandwich_stevens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfk3tw</id>
    <title>A step-by-step guide for fine-tuning the Qwen3-32B model on the medical reasoning dataset within an hour.</title>
    <updated>2025-05-05T19:11:55+00:00</updated>
    <author>
      <name>/u/kingabzpro</name>
      <uri>https://old.reddit.com/user/kingabzpro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building on the success of QwQ and Qwen2.5, Qwen3 represents a major leap forward in reasoning, creativity, and conversational capabilities. With open access to both dense and Mixture-of-Experts (MoE) models, ranging from 0.6B to 235B-A22B parameters, Qwen3 is designed to excel in a wide array of tasks.&lt;/p&gt; &lt;p&gt;In this tutorial, we will fine-tune the Qwen3-32B model on a medical reasoning dataset. The goal is to optimize the model's ability to reason and respond accurately to patient queries, ensuring it adopts a precise and efficient approach to medical question-answering.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kingabzpro"&gt; /u/kingabzpro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.datacamp.com/tutorial/fine-tuning-qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfk3tw/a_stepbystep_guide_for_finetuning_the_qwen332b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfk3tw/a_stepbystep_guide_for_finetuning_the_qwen332b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T19:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfmic3</id>
    <title>Ollama 0.6.8 released, stating performance improvements for Qwen 3 MoE models (30b-a3b and 235b-a22b) on NVIDIA and AMD GPUs.</title>
    <updated>2025-05-05T20:47:33+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmic3/ollama_068_released_stating_performance/"&gt; &lt;img alt="Ollama 0.6.8 released, stating performance improvements for Qwen 3 MoE models (30b-a3b and 235b-a22b) on NVIDIA and AMD GPUs." src="https://external-preview.redd.it/0lw8qn5OefTTlWMq6-CTI2D_wJSi67bbux5PetB3scY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59a2d6daceec4522a308302ffb73397db28bdd3d" title="Ollama 0.6.8 released, stating performance improvements for Qwen 3 MoE models (30b-a3b and 235b-a22b) on NVIDIA and AMD GPUs." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The update also includes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fixed &lt;code&gt;GGML_ASSERT(tensor-&amp;gt;op == GGML_OP_UNARY) failed&lt;/code&gt; issue caused by conflicting installations&lt;/p&gt; &lt;p&gt;Fixed a memory leak that occurred when providing images as input&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama show&lt;/code&gt; will now correctly label older vision models such as &lt;code&gt;llava&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Reduced out of memory errors by improving worst-case memory estimations&lt;/p&gt; &lt;p&gt;Fix issue that resulted in a &lt;code&gt;context canceled&lt;/code&gt; error&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.8"&gt;https://github.com/ollama/ollama/releases/tag/v0.6.8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmic3/ollama_068_released_stating_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmic3/ollama_068_released_stating_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T20:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfhmdq</id>
    <title>EQ-Bench gets a proper update today. Targeting emotional intelligence in challenging multi-turn roleplays.</title>
    <updated>2025-05-05T17:33:48+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Leaderboard: &lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample outputs: &lt;a href="https://eqbench.com/results/eqbench3_reports/o3.html"&gt;https://eqbench.com/results/eqbench3_reports/o3.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/EQ-bench/eqbench3"&gt;https://github.com/EQ-bench/eqbench3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lots more to read about the benchmark:&lt;br /&gt; &lt;a href="https://eqbench.com/about.html#long"&gt;https://eqbench.com/about.html#long&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://eqbench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhmdq/eqbench_gets_a_proper_update_today_targeting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhmdq/eqbench_gets_a_proper_update_today_targeting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf5ry6</id>
    <title>JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality.</title>
    <updated>2025-05-05T07:31:25+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt; &lt;img alt="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Primary link is for Ollama but here is the creator's model card on HF:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanna say this model has replaced my older Abliterated models. I genuinely think this Josie model is better than the stock model. It adhears to instructions better and is not dry in its responses at all. Running at Q8 myself and it definitely punches above its weight class. Using it primarily in a online RAG system. &lt;/p&gt; &lt;p&gt;Hoping for a 30B A3B Josie finetune in the future! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/goekdenizguelmez/JOSIEFIED-Qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfrcul</id>
    <title>Qwen 3 Small Models: 0.6B, 1.7B &amp; 4B compared with Gemma 3</title>
    <updated>2025-05-06T00:22:44+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtube.com/watch?v=v8fBtLdvaBM&amp;amp;si=L_xzVrmeAjcmOKLK"&gt;https://youtube.com/watch?v=v8fBtLdvaBM&amp;amp;si=L_xzVrmeAjcmOKLK&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I compare the performance of smaller Qwen 3 models (0.6B, 1.7B, and 4B) against Gemma 3 models on various tests. &lt;/p&gt; &lt;p&gt;TLDR: Qwen 3 4b outperforms Gemma 3 12B on 2 of the tests and comes in close on 2. It outperforms Gemma 3 4b on all tests. These tests were done without reasoning, for an apples to apples with Gemma. &lt;/p&gt; &lt;p&gt;This is the first time I have seen a 4B model actually acheive a respectable score on many of the tests. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;0.6B Model&lt;/th&gt; &lt;th align="left"&gt;1.7B Model&lt;/th&gt; &lt;th align="left"&gt;4B Model&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Harmful Question Detection&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Named Entity Recognition&lt;/td&gt; &lt;td align="left"&gt;Did not perform well&lt;/td&gt; &lt;td align="left"&gt;45%&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SQL Code Generation&lt;/td&gt; &lt;td align="left"&gt;45%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Retrieval Augmented Generation&lt;/td&gt; &lt;td align="left"&gt;37%&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;83%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfrcul/qwen_3_small_models_06b_17b_4b_compared_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T00:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfi8xh</id>
    <title>[Benchmark] Quick‑and‑dirty test of 5 models on a Mac Studio M3 Ultra 512 GB (LM Studio) – Qwen3 runs away with it</title>
    <updated>2025-05-05T17:58:51+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA!&lt;/p&gt; &lt;p&gt;I’m a former university physics lecturer (taught for five years) and—one month after buying a Mac Studio (M3 Ultra, 128 CPU / 80 GPU cores, 512 GB unified RAM)—I threw a very simple benchmark at a few LLMs inside &lt;strong&gt;LM Studio&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt (intentional typo):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Explain to me why sky is blue at an physiscist Level PhD. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Raw numbers&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Quant. / RAM footprint&lt;/th&gt; &lt;th align="left"&gt;Speed (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Tokens out&lt;/th&gt; &lt;th align="left"&gt;1st‑token latency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX deepseek‑V3‑0324‑4bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;355.95 GB&lt;/td&gt; &lt;td align="left"&gt;19.34&lt;/td&gt; &lt;td align="left"&gt; 755&lt;/td&gt; &lt;td align="left"&gt;17.29 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Gemma‑3‑27b‑it‑bf16&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; 52.57 GB&lt;/td&gt; &lt;td align="left"&gt;11.19&lt;/td&gt; &lt;td align="left"&gt; 1 317&lt;/td&gt; &lt;td align="left"&gt; 1.72 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Deepseek‑R1‑4bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;402.17 GB&lt;/td&gt; &lt;td align="left"&gt;16.55&lt;/td&gt; &lt;td align="left"&gt; 2 062&lt;/td&gt; &lt;td align="left"&gt; 15.01 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Qwen3‑235‑A22B‑8bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;233.79 GB&lt;/td&gt; &lt;td align="left"&gt;18.86&lt;/td&gt; &lt;td align="left"&gt; 3 096&lt;/td&gt; &lt;td align="left"&gt; 9.02 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GGFU Qwen3‑235‑A22B‑8bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; 233.72 GB&lt;/td&gt; &lt;td align="left"&gt;14.35&lt;/td&gt; &lt;td align="left"&gt; 2 883&lt;/td&gt; &lt;td align="left"&gt; 4.47 s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;&lt;strong&gt;Teacher’s impressions&lt;/strong&gt;&lt;/h1&gt; &lt;h1&gt;1. Reasoning speed&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;R1 &amp;gt; Qwen3 &amp;gt; Gemma3&lt;/strong&gt;.&lt;br /&gt; The “thinking time” (pre‑generation) is roughly half of total generation time. If I had to re‑prompt twice to get a good answer, I’d simply pick a model with better reasoning instead of chasing seconds.&lt;/p&gt; &lt;h1&gt;2. Generation speed&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;V3 ≈ MLX‑Qwen3 &amp;gt; R1 &amp;gt; GGFU‑Qwen3 &amp;gt; Gemma3&lt;/strong&gt;.&lt;br /&gt; No surprise: token‑width + unified‑memory bandwidth rule here. The Mac’s 890 GB/s is great for a compact workstation, but it’s nowhere near the monster discrete GPUs you guys already know—so throughput drops once the model starts chugging serious tokens.&lt;/p&gt; &lt;h1&gt;3. Output quality (grading as if these were my students)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3 &amp;gt;&amp;gt;&amp;gt; R1 &amp;gt; Gemma3 &amp;gt; V3&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;deepseek‑V3&lt;/strong&gt; – trivial answer, would fail the course.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deepseek‑R1&lt;/strong&gt; – solid undergrad level.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma‑3&lt;/strong&gt; – punchy for its size, respectable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; – in a league of its own: clear, creative, concise, high‑depth. If the others were bachelor’s level, Qwen3 was PhD defending a job talk.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Bottom line: for text‑to‑text tasks balancing quality and speed, &lt;strong&gt;Qwen3‑8bit (MLX)&lt;/strong&gt; is my daily driver.&lt;/p&gt; &lt;h1&gt;One month with the Mac Studio – worth it?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Why I don’t regret it&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Stellar build &amp;amp; design.&lt;/li&gt; &lt;li&gt;Makes sense if a computer &amp;gt; a car for you (I do bio‑informatics), you live in an apartment (space is luxury, no room for a noisy server), and noise destroys you (I’m neurodivergent; the Mac is silent even at 100 %).&lt;/li&gt; &lt;li&gt;Power draw peaks &amp;lt; 250 W.&lt;/li&gt; &lt;li&gt;Ridiculously small footprint, light enough to slip in a backpack.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why you might pass&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You game heavily on PC.&lt;/li&gt; &lt;li&gt;You hate macOS learning curves.&lt;/li&gt; &lt;li&gt;You want constant hardware upgrades.&lt;/li&gt; &lt;li&gt;You can wait 2–3 years for LLM‑focused hardware to get cheap.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Money‑saving tips&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stick with the 1 TB SSD—Thunderbolt + a fast NVMe enclosure covers the rest.&lt;/li&gt; &lt;li&gt;Skip Apple’s monitor &amp;amp; peripherals; third‑party is way cheaper.&lt;/li&gt; &lt;li&gt;Grab one before any Trump‑era import tariffs jack up Apple prices again.&lt;/li&gt; &lt;li&gt;I would not buy the 256 Gb over the 512 Gb, of course is double the price, but it opens more opportunities at least for me. With it I can run an bioinformatics analysis while using Qwen3, and even if Qwen3 fits (tightly) in the 256 Gb, this won't let you with a large margin of maneuver for other tasks. Finally, who knows what would be the next generation of models and how much memory it will get.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen3‑8bit&lt;/strong&gt; dominates – PhD‑level answers, fast enough, reasoning quick.&lt;/li&gt; &lt;li&gt;Thinking time isn’t the bottleneck; quantization + memory bandwidth are (if any expert wants to correct or improve this please do so).&lt;/li&gt; &lt;li&gt;Mac Studio M3 Ultra is a silence‑loving, power‑sipping, tiny beast—just not the rig for GPU fiends or upgrade addicts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask away if you want more details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfjhlv</id>
    <title>This is how small models single-handedly beat all the big ones in benchmarks...</title>
    <updated>2025-05-05T18:47:34+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"&gt; &lt;img alt="This is how small models single-handedly beat all the big ones in benchmarks..." src="https://preview.redd.it/kammsi5ce0ze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8bb724a8000281ed4503c39eeec9365cc1bf41c" title="This is how small models single-handedly beat all the big ones in benchmarks..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you ever wondered how do the small models always beat the big models in the benchmarks, this is how...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kammsi5ce0ze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfjhlv/this_is_how_small_models_singlehandedly_beat_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T18:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffj42</id>
    <title>New Qwen3-32B-AWQ (Activation-aware Weight Quantization)</title>
    <updated>2025-05-05T16:11:31+00:00</updated>
    <author>
      <name>/u/jbaenaxd</name>
      <uri>https://old.reddit.com/user/jbaenaxd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt; &lt;img alt="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" src="https://external-preview.redd.it/-aaTUrK8hOTrBZTDUSYGah4_Rjpn4rU7szaPy5gCq8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69449e796c503e08a80bc47f50ab28640b3a7384" title="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10"&gt;https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen released this 3 days ago and no one noticed. These new models look great for running in local. This technique was used in Gemma 3 and it was great. Waiting for someone to add them to Ollama, so we can easily try them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1918353505074725363"&gt;https://x.com/Alibaba_Qwen/status/1918353505074725363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbaenaxd"&gt; /u/jbaenaxd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kft806</id>
    <title>Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L</title>
    <updated>2025-05-06T01:57:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt; &lt;img alt="Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L" src="https://external-preview.redd.it/Yy-F4J1DZLB-ka7GXhD-N9ToOmxtqP5_TS_ElH4PISA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=536123bdc72e4ce9ed8c25a5831a4d5645fa049d" title="Qwen3-32B-Q4 GGUFs MMLU-PRO benchmark comparison - IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-32B-IQ4_XS / Q4_K_M / UD-Q4_K_XL / Q4_K_L&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;&lt;em&gt;12 hours 17 minutes and 53 seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Observation: IQ4_XS is the most efficient Q4 quant for 32B, the quality difference is minimum&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d97vf2l9l2ze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb36de1aa23008cd2574d4a71141b741c06f4fef"&gt;https://preview.redd.it/d97vf2l9l2ze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb36de1aa23008cd2574d4a71141b741c06f4fef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g5fwhfeai2ze1.png?width=1420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b1e705703af19fbdbab465029ff9a3921616eb5"&gt;https://preview.redd.it/g5fwhfeai2ze1.png?width=1420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b1e705703af19fbdbab465029ff9a3921616eb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2tgyiu3al2ze1.png?width=2188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d80bf13dfb43de8147b3b75aa2957629fedec3e"&gt;https://preview.redd.it/2tgyiu3al2ze1.png?width=2188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d80bf13dfb43de8147b3b75aa2957629fedec3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is listing the score of Qwen3 base model instead of instruct, that's why these q4 quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;gguf source:&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kft806/qwen332bq4_ggufs_mmlupro_benchmark_comparison_iq4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T01:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfcdll</id>
    <title>We fit 50+ LLMs on 2 GPUs — cold starts under 2s. Here’s how.</title>
    <updated>2025-05-05T14:02:10+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been experimenting with multi-model orchestration and ran into the usual wall: cold starts, bloated memory, and inefficient GPU usage. Everyone talks about inference, but very few go below the HTTP layer.&lt;/p&gt; &lt;p&gt;So we built our own runtime that snapshots the entire model execution state , attention caches, memory layout, everything , and restores it directly on the GPU. Result?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;•50+ models running on 2× A4000s •Cold starts consistently under 2 seconds •90%+ GPU utilization •No persistent bloating or overprovisioning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It feels like an OS for inference , instead of restarting a process, we just resume it. If you’re running agents, RAG pipelines, or multi-model setups locally, this might be useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfor2i</id>
    <title>RTX PRO 6000 now available at €9000</title>
    <updated>2025-05-05T22:20:52+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-gpus-now-available-starting-at-e9000"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfor2i/rtx_pro_6000_now_available_at_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfor2i/rtx_pro_6000_now_available_at_9000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf9i52</id>
    <title>RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI</title>
    <updated>2025-05-05T11:42:02+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt; &lt;img alt="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" src="https://a.thumbs.redditmedia.com/ZP_Pe9inInMfd_-rxiw6xfzYHLp5iOazS6Ztzjzk5U4.jpg" title="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently grabbed an RTX 5060 Ti 16GB for “just” $499 - while it’s no one’s first choice for gaming (reviews are pretty harsh), for AI workloads? This card might be a hidden gem.&lt;/p&gt; &lt;p&gt;I mainly wanted those 16GB of VRAM to fit bigger models, and it actually worked out. Ran LightRAG to ingest this beefy PDF: &lt;a href="https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf"&gt;https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared it with a 12GB GPU (RTX 3060 Ti 12GB) - and I’ve attached Grafana charts showing GPU utilization for both runs.&lt;/p&gt; &lt;p&gt;🟢 16GB card: finished in 3 min 29 sec (green line) 🟡 12GB card: took 8 min 52 sec (yellow line)&lt;/p&gt; &lt;p&gt;Logs showed the 16GB card could load all 41 layers, while the 12GB one only managed 31. The rest had to be constantly swapped in and out - crushing performance by 2x and leading to underutilizing the GPU (as clearly seen in the Grafana metrics).&lt;/p&gt; &lt;p&gt;LightRAG uses “Mistral Nemo Instruct 12B”, served via Ollama, if you’re curious.&lt;/p&gt; &lt;p&gt;TL;DR: 16GB+ VRAM saves serious time.&lt;/p&gt; &lt;p&gt;Bonus: the card is noticeably shorter than others — it has 2 coolers instead of the usual 3, thanks to using PCIe x8 instead of x16. Great for small form factor builds or neat home AI setups. I’m planning one myself (please share yours if you’re building something similar!).&lt;/p&gt; &lt;p&gt;And yep - I had written a full guide earlier on how to go from clean bare metal to fully functional LightRAG setup in minutes. Fully automated, just follow the steps: 👉 &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you try this setup or run into issues - happy to help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kf9i52"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfmoyx</id>
    <title>Qwen3 235b pairs EXTREMELY well with a MacBook</title>
    <updated>2025-05-05T20:55:14+00:00</updated>
    <author>
      <name>/u/Ashefromapex</name>
      <uri>https://old.reddit.com/user/Ashefromapex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried the new Qwen3 MoEs on my MacBook m4 max 128gb, and I was expecting speedy inference but I was blown out off the water. On the smaller MoE at q8 I get approx. 75 tok/s on the mlx version which is insane compared to &amp;quot;only&amp;quot; 15 on a 32b dense model. &lt;/p&gt; &lt;p&gt;Not expecting great results tbh, I loaded a q3 quant of the 235b version, eating up 100 gigs of ram. And to my surprise it got almost 30 (!!) tok/s. &lt;/p&gt; &lt;p&gt;That is actually extremely usable, especially for coding tasks, where it seems to be performing great. &lt;/p&gt; &lt;p&gt;This model might actually be the perfect match for apple silicon and especially the 128gb MacBooks. It brings decent knowledge but at INSANE speeds compared to dense models. Also 100 gb of ram usage is a pretty big hit, but it leaves enough room for an IDE and background apps which is mind blowing.&lt;/p&gt; &lt;p&gt;In the next days I will look at doing more in depth benchmarks once I find the time, but for the time being I thought this would be of interest since I haven't heard much about Owen3 on apple silicon yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashefromapex"&gt; /u/Ashefromapex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfmoyx/qwen3_235b_pairs_extremely_well_with_a_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T20:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfebga</id>
    <title>Open WebUI license change : no longer OSI approved ?</title>
    <updated>2025-05-05T15:23:12+00:00</updated>
    <author>
      <name>/u/CroquetteLauncher</name>
      <uri>https://old.reddit.com/user/CroquetteLauncher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Open WebUI has proved an excellent tool, with a permissive license, I have noticed the new release do not seem to use an &lt;a href="https://opensource.org/licenses"&gt;OSI approved license&lt;/a&gt; and require a contributor license agreement.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openwebui.com/license/"&gt;https://docs.openwebui.com/license/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I understand the reasoning, but i wish they could find other way to enforce contribution, without moving away from an open source license. Some OSI approved license enforce even more sharing back for service providers (AGPL).&lt;/p&gt; &lt;p&gt;The FAQ &amp;quot;6. Does this mean Open WebUI is “no longer open source”? -&amp;gt; No, not at all.&amp;quot; is missing the point. Even if you have good and fair reasons to restrict usage, it does not mean that you can claim to still be open source. I asked Gemini pro 2.5 preview, Mistral 3.1 and Gemma 3 and they tell me that no, the new license is not opensource / freesoftware.&lt;/p&gt; &lt;p&gt;For now it's totally reasonable, but If there are some other good reasons to add restrictions in the future, and a CLA that say &amp;quot;we can add any restriction to your code&amp;quot;, it worry me a bit.&lt;/p&gt; &lt;p&gt;I'm still a fan of the project, but a bit more worried than before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CroquetteLauncher"&gt; /u/CroquetteLauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffq2u</id>
    <title>Qwen 3 235b gets high score in LiveCodeBench</title>
    <updated>2025-05-05T16:19:04+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt; &lt;img alt="Qwen 3 235b gets high score in LiveCodeBench" src="https://preview.redd.it/px3okqrznzye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c94891166c70bc3a59e886ae5359d04bdf3d33af" title="Qwen 3 235b gets high score in LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/px3okqrznzye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfkg29</id>
    <title>Claude full system prompt with all tools is now ~25k tokens.</title>
    <updated>2025-05-05T19:25:13+00:00</updated>
    <author>
      <name>/u/StableSable</name>
      <uri>https://old.reddit.com/user/StableSable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"&gt; &lt;img alt="Claude full system prompt with all tools is now ~25k tokens." src="https://external-preview.redd.it/-gpcWCAltY66ZM29aKxaxLCWgfV5wmtUemjqB6JURhI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ac3f0d5a570d0bb7b8c1d30d521411d0c135796" title="Claude full system prompt with all tools is now ~25k tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableSable"&gt; /u/StableSable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/claude.txt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfkg29/claude_full_system_prompt_with_all_tools_is_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T19:25:13+00:00</published>
  </entry>
</feed>
