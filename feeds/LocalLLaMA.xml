<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-08T08:39:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jth72b</id>
    <title>OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages</title>
    <updated>2025-04-07T09:30:12+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"&gt; &lt;img alt="OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages" src="https://external-preview.redd.it/Z2xta3YzNmRzZHRlMQNkNwCzMCfIUGDv1ghIC3hRjQCjH8UUjqUCJCni6W9u.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df30707fd170e60bfac0344bd18a6f9e6c533729" title="OuteTTS 1.0: Upgrades in Quality, Cloning, and 20 Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bw9ii56dsdte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jth72b/outetts_10_upgrades_in_quality_cloning_and_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T09:30:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju3pjb</id>
    <title>Weird new livebench.ai coding scores</title>
    <updated>2025-04-08T03:00:58+00:00</updated>
    <author>
      <name>/u/SandboChang</name>
      <uri>https://old.reddit.com/user/SandboChang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3pjb/weird_new_livebenchai_coding_scores/"&gt; &lt;img alt="Weird new livebench.ai coding scores" src="https://b.thumbs.redditmedia.com/9ly2jkzLDcfv4_ZINd8MbM00syRF5SbgPFq1oD3TvUI.jpg" title="Weird new livebench.ai coding scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It uses to align with aider's leaderboard relatively well, but these new scores just did not make any sense to me. Sonnet 3.7 Thinking cannot be worse than R1 Distilled models, for example.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandboChang"&gt; /u/SandboChang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ju3pjb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3pjb/weird_new_livebenchai_coding_scores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3pjb/weird_new_livebenchai_coding_scores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T03:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtwcdo</id>
    <title>Guide for quickly setting up aider, QwQ and Qwen Coder</title>
    <updated>2025-04-07T21:06:01+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a guide for setting up a a 100% local coding co-pilot setup with QwQ as as an architect model and qwen Coder as the editor. The focus for the guide is on the trickiest part which is configuring everything to work together.&lt;/p&gt; &lt;p&gt;This guide uses QwQ and qwen Coder 32B as those can fit in a 24GB GPU. This guide uses llama-swap so QwQ and Qwen Coder are swapped in and our during aider's architect or editing phases. The guide also has settings for dual 24GB GPUs where both models can be used without swapping.&lt;/p&gt; &lt;p&gt;The original version is here: &lt;a href="https://github.com/mostlygeek/llama-swap/tree/main/examples/aider-qwq-coder"&gt;https://github.com/mostlygeek/llama-swap/tree/main/examples/aider-qwq-coder&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Here's what you you need:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;aider - &lt;a href="https://aider.chat/docs/install.html"&gt;installation docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama-server - &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;download latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama-swap - &lt;a href="https://github.com/mostlygeek/llama-swap/releases"&gt;download latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF"&gt;QwQ 32B&lt;/a&gt; and &lt;a href="https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF"&gt;Qwen Coder 2.5 32B&lt;/a&gt; models&lt;/li&gt; &lt;li&gt;24GB VRAM video card&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Running aider&lt;/h2&gt; &lt;p&gt;The goal is getting this command line to work:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh aider --architect \ --no-show-model-warnings \ --model openai/QwQ \ --editor-model openai/qwen-coder-32B \ --model-settings-file aider.model.settings.yml \ --openai-api-key &amp;quot;sk-na&amp;quot; \ --openai-api-base &amp;quot;http://10.0.1.24:8080/v1&amp;quot; \ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Set &lt;code&gt;--openai-api-base&lt;/code&gt; to the IP and port where your llama-swap is running.&lt;/p&gt; &lt;h2&gt;Create an aider model settings file&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;aider.model.settings.yml&lt;/h1&gt; &lt;h1&gt;!!! important: model names must match llama-swap configuration names !!!&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/QwQ&amp;quot; edit_format: diff extra_params: max_tokens: 16384 top_p: 0.95 top_k: 40 presence_penalty: 0.1 repetition_penalty: 1 num_ctx: 16384 use_temperature: 0.6 reasoning_tag: think weak_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot; editor_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/qwen-coder-32B&amp;quot; edit_format: diff extra_params: max_tokens: 16384 top_p: 0.8 top_k: 20 repetition_penalty: 1.05 use_temperature: 0.6 reasoning_tag: think editor_edit_format: editor-diff editor_model_name: &amp;quot;openai/qwen-coder-32B&amp;quot; ```&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;llama-swap configuration&lt;/h2&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;config.yaml&lt;/h1&gt; &lt;h1&gt;The parameters are tweaked to fit model+context into 24GB VRAM GPUs&lt;/h1&gt; &lt;p&gt;models: &amp;quot;qwen-coder-32B&amp;quot;: proxy: &amp;quot;&lt;a href="http://127.0.0.1:8999"&gt;http://127.0.0.1:8999&lt;/a&gt;&amp;quot; cmd: &amp;gt; /path/to/llama-server --host 127.0.0.1 --port 8999 --flash-attn --slots --ctx-size 16000 --cache-type-k q8_0 --cache-type-v q8_0 -ngl 99 --model /path/to/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf&lt;/p&gt; &lt;p&gt;&amp;quot;QwQ&amp;quot;: proxy: &amp;quot;&lt;a href="http://127.0.0.1:9503"&gt;http://127.0.0.1:9503&lt;/a&gt;&amp;quot; cmd: &amp;gt; /path/to/llama-server --host 127.0.0.1 --port 9503 --flash-attn --metrics--slots --cache-type-k q8_0 --cache-type-v q8_0 --ctx-size 32000 --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 --min-p 0.01 --top-k 40 --top-p 0.95 -ngl 99 --model /mnt/nvme/models/bartowski/Qwen_QwQ-32B-Q4_K_M.gguf ```&lt;/p&gt; &lt;h2&gt;Advanced, Dual GPU Configuration&lt;/h2&gt; &lt;p&gt;If you have &lt;em&gt;dual 24GB GPUs&lt;/em&gt; you can use llama-swap profiles to avoid swapping between QwQ and Qwen Coder.&lt;/p&gt; &lt;p&gt;In llama-swap's configuration file:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;add a &lt;code&gt;profiles&lt;/code&gt; section with &lt;code&gt;aider&lt;/code&gt; as the profile name&lt;/li&gt; &lt;li&gt;using the &lt;code&gt;env&lt;/code&gt; field to specify the GPU IDs for each model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;config.yaml&lt;/h1&gt; &lt;h1&gt;Add a profile for aider&lt;/h1&gt; &lt;p&gt;profiles: aider: - qwen-coder-32B - QwQ&lt;/p&gt; &lt;p&gt;models: &amp;quot;qwen-coder-32B&amp;quot;: # manually set the GPU to run on env: - &amp;quot;CUDA_VISIBLE_DEVICES=0&amp;quot; proxy: &amp;quot;&lt;a href="http://127.0.0.1:8999"&gt;http://127.0.0.1:8999&lt;/a&gt;&amp;quot; cmd: /path/to/llama-server ...&lt;/p&gt; &lt;p&gt;&amp;quot;QwQ&amp;quot;: # manually set the GPU to run on env: - &amp;quot;CUDA_VISIBLE_DEVICES=1&amp;quot; proxy: &amp;quot;&lt;a href="http://127.0.0.1:9503"&gt;http://127.0.0.1:9503&lt;/a&gt;&amp;quot; cmd: /path/to/llama-server ... ```&lt;/p&gt; &lt;p&gt;Append the profile tag, &lt;code&gt;aider:&lt;/code&gt;, to the model names in the model settings file&lt;/p&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;aider.model.settings.yml&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/aider:QwQ&amp;quot; weak_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot; editor_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;name: &amp;quot;openai/aider:qwen-coder-32B&amp;quot; editor_model_name: &amp;quot;openai/aider:qwen-coder-32B-aider&amp;quot; ```&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run aider with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh $ aider --architect \ --no-show-model-warnings \ --model openai/aider:QwQ \ --editor-model openai/aider:qwen-coder-32B \ --config aider.conf.yml \ --model-settings-file aider.model.settings.yml --openai-api-key &amp;quot;sk-na&amp;quot; \ --openai-api-base &amp;quot;http://10.0.1.24:8080/v1&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_up_aider_qwq_and_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T21:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtm56c</id>
    <title>"10m context window" Well, doesn't look good for Llama 4.</title>
    <updated>2025-04-07T14:09:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"&gt; &lt;img alt="&amp;quot;10m context window&amp;quot; Well, doesn't look good for Llama 4." src="https://preview.redd.it/xbucodud7fte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a0b3770d582753b384f5d1576fa77960022bb4f" title="&amp;quot;10m context window&amp;quot; Well, doesn't look good for Llama 4." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hmmm😢😢&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xbucodud7fte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtm56c/10m_context_window_well_doesnt_look_good_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtlymx</id>
    <title>Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models</title>
    <updated>2025-04-07T14:01:34+00:00</updated>
    <author>
      <name>/u/babydriver808</name>
      <uri>https://old.reddit.com/user/babydriver808</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"&gt; &lt;img alt="Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models" src="https://b.thumbs.redditmedia.com/VPjvvVejK3-Q7SfRk1d98G-LK9eBxXjm5dqQe5ogFqw.jpg" title="Neural Graffiti - A Neuroplasticity Drop-In Layer For Transformers Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liquid neural networks are awesome - they change how that &amp;quot;neuron black box&amp;quot; connects over time given its past experiences, emulating the human brain in relating concepts and how it changes our perspective. &lt;/p&gt; &lt;p&gt;They are great at time series forecasting like weather and analytics, however the idea is to do it on a transformers model, making it acquire neuroplasticity at token prediction - and as we know its very expensive to train a whole model from scratch. &lt;/p&gt; &lt;p&gt;I figured we could splice in a new neuron layer inside the model's networks right between the transformers layer and the output projection layer that actually predicts the tokens. This way the thought would have &amp;quot;influences&amp;quot; of past experiences for every token generated aka. during the entire line of thinking, making the model acquire a &amp;quot;personality in behavior&amp;quot; over time. &lt;/p&gt; &lt;p&gt;The vector embeddings from the transformers layer are mean-pooled and &amp;quot;sprayed&amp;quot; with past memories changing the way each token is generated, influencing the meaning and therefore choice of words in the vocab space. This neural “Spray Layer” also remembers the paths it took before, blending new input with previous ones and gradually evolving its internal understanding of concepts over time. &lt;/p&gt; &lt;p&gt;It won’t guarantee exact word outputs, but it will make the model lean into certain concepts the more it interacts. For example: Tell it you love dogs, and over time, the model will start leaning toward dog-related kindness, loyalty, and fuzziness in its tone and direction. More teste are yet to be done and I know there is a cold start problem, finding the sweet spot is key. &lt;/p&gt; &lt;p&gt;This is quite fascinating, especially because we don't know exactly what happen at the model's transformer neuron level and how it makes the connections, but hacking it like this is interesting to watch. &lt;/p&gt; &lt;p&gt;I called this technique &amp;quot;Neural Graffiti&amp;quot;, and it is free and open for everyone.&lt;/p&gt; &lt;p&gt;Try the demo and give it a star on the github repo! - &lt;a href="https://github.com/babycommando/neuralgraffiti"&gt;babycommando/neuralgraffiti&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babydriver808"&gt; /u/babydriver808 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jtlymx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtlymx/neural_graffiti_a_neuroplasticity_dropin_layer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6fa1</id>
    <title>MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations</title>
    <updated>2025-04-08T05:45:13+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6fa1/mathperturb_benchmarking_llms_math_reasoning/"&gt; &lt;img alt="MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations" src="https://a.thumbs.redditmedia.com/aZorfN1xCIDqdO9FZqUtfo5pYzqv5qhNi7niohnU9e0.jpg" title="MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://math-perturb.github.io/"&gt;https://math-perturb.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR by QwQ:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The study investigates whether large language models' success on complex math problems stems from true reasoning or memorization by creating two datasets, MATH-P-Simple and MATH-P-Hard, each with 279 modified problems from the MATH dataset's hardest level. MATH-P-Simple includes minor, non-essential changes that preserve the original solution method, while MATH-P-Hard involves fundamental alterations requiring new strategies and deeper understanding. Models showed significant performance drops on MATH-P-Hard, suggesting reliance on memorized methods. The authors highlight a concerning &amp;quot;blind memorization&amp;quot; issue where models apply learned techniques without assessing their relevance to modified contexts, especially when trained with original problems. This underscores the need for research to develop more adaptable and robust reasoning models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Leaderboard&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oa3hc69dsjte1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78653cfb0648bccae51b79d790c4cb8da943562d"&gt;https://preview.redd.it/oa3hc69dsjte1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78653cfb0648bccae51b79d790c4cb8da943562d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Observation:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Reasoning models, even small models without RL like R1-14B, performs very well compare to base models.&lt;/li&gt; &lt;li&gt;LLama4 &amp;amp; gpt-4o flopped extra hard, even when compare to small &amp;amp; cheap base models like gemini-2-flash, it's still really bad&lt;/li&gt; &lt;li&gt;Gemini reasoning models are less resistant to perturbations compare to QwQ, R1 and O3-mini&lt;/li&gt; &lt;li&gt;R1-Qwen-14B is a bit more resistant to perturbations compare to R1-Llama-70B&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o9bq2z02yjte1.png?width=1430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98b460a60b2dd9e28c8f37cd6877e0b0e6e74c96"&gt;https://preview.redd.it/o9bq2z02yjte1.png?width=1430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98b460a60b2dd9e28c8f37cd6877e0b0e6e74c96&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6fa1/mathperturb_benchmarking_llms_math_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6fa1/mathperturb_benchmarking_llms_math_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6fa1/mathperturb_benchmarking_llms_math_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T05:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtmy7p</id>
    <title>Qwen3/Qwen3MoE support merged to vLLM</title>
    <updated>2025-04-07T14:44:12+00:00</updated>
    <author>
      <name>/u/tkon3</name>
      <uri>https://old.reddit.com/user/tkon3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vLLM &lt;a href="https://github.com/vllm-project/vllm/pull/15289"&gt;merged&lt;/a&gt; two Qwen3 architectures today.&lt;/p&gt; &lt;p&gt;You can find a mention to &lt;code&gt;Qwen/Qwen3-8B&lt;/code&gt; and &lt;code&gt;Qwen/Qwen3-MoE-15B-A2B&lt;/code&gt;at this &lt;a href="https://github.com/YamPengLi/vllm/blob/25e4a80fe2ef7e2c6fa4c43bfc0402c17303b589/docs/source/models/supported_models.md"&gt;page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Interesting week in perspective.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tkon3"&gt; /u/tkon3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtmy7p/qwen3qwen3moe_support_merged_to_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T14:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtzue8</id>
    <title>Why we may be wrong about Llama 4 . . .</title>
    <updated>2025-04-07T23:44:09+00:00</updated>
    <author>
      <name>/u/dionysio211</name>
      <uri>https://old.reddit.com/user/dionysio211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe a lot has been lost in the discussion over the problematic roll out of the Llama 4 models. What we are seeing in these recent releases is a lot more novelty in LLM design with trends to multi-modality, new versions of reasoning and non-reasoning logic, different types of MoE's, etc which is causing the &amp;quot;first impression&amp;quot; of the average user to become misaligned with the progress being made. Gemma 3, particularly the multi-modal functionality, had a terrible rollout which has still not entirely been fixed in popular local LLM platforms like LM Studio, Ollama, Kobold CPP, etc. I mean if you think about it, it makes a lot of sense. To squeeze better performance out of current consumer technology and get these models out to the public, there's a whole lot of variables, not the least of which is a reliance on open source platforms to anticipate or somehow know what is going to happen when the model is released. If every new model came out with the same architecture supported by these platforms, how could there even be innovation? None of them are handling audio inputs in some standardized way so how are they going to roll out the &amp;quot;omni&amp;quot; models coming out? I haven't seen the omni version of Phi-4 supported by anyone so far. vLLM stands apart from most of these, even llama cpp, because it is a production level system actively deployed for serving models efficiently because of superior support for concurrency, throughput, etc. The Gemma team worked with vLLM and Llama CPP on theirs before releasing the model and they STILL had a bad rollout. Qwen 2.5 VL has been out forever, and it's still not even supported on most local inference platforms.&lt;/p&gt; &lt;p&gt;Since Mixtral at least, any novel architecture in the model has seen hiccups like this so we should all be used to it now without jumping to conclusions about the model until it is running properly. If you look at what has been posted about results derived from Meta's own inferencing, you can see the models clearly perform better across the board than some guy on X that got it to run on his stuff. It's all part of the ride and we should wait for support before deciding the dudes making the models have no idea what they are doing, which we all know just is not the case. I think what we will find is that this is actually the future of local LLMs, models like this. They get around the gigantic issues of memory transfer speeds by creating highly performant MoE's that can potentially run on a CPU, or at least platforms like AMD AI, Apple, etc. In fact, Qwen is set to release a very, very similar model imminently and it appears they are working with vLLM on that today. I believe this model and the new Qwen 3 MoE are going to redefine what can be done since information density has gotten so good that 3b models are doing what 24b models were doing a year and a half ago, at speeds superior to hosted solutions. It's one of the only known ways currently to get over 20 tokens a second on something that performs on par with with Sonnet 3.5, GPT 4, etc and it may guide hardware developers to focus on adding memory channels, not to match VRAM which is not going to happen, but to get to speeds which run things like this super fast, fast enough to code, do research at home, etc.&lt;/p&gt; &lt;p&gt;For those who are curious, you can view the commits up on vLLM today regarding the problems with LLama 4. Here's a summary from QwQ about the large commit made about 5 hours ago as to what was wrong:&lt;/p&gt; &lt;p&gt;### **Summary of Root Causes**&lt;/p&gt; &lt;p&gt;The original vLLM implementation struggled with Llama4 primarily because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Its MoE architecture introduced new configuration parameters and attention patterns not accounted for in prior code.&lt;/li&gt; &lt;li&gt;Flash Attention required modifications to handle local blocks, chunked sequences, and block tables for expert routing.&lt;/li&gt; &lt;li&gt;Initialization logic failed due to differing model class names or parameter naming conventions (e.g., `text_config`).&lt;/li&gt; &lt;li&gt;Memory management lacked support for MoE’s parallelism requirements, necessitating changes in how batches are split and processed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The commits address these by adding specialized handling for Llama4's architecture, reworking attention kernels, and adjusting configurations to match Meta’s implementation details.&lt;/p&gt; &lt;p&gt;### **End of Summary**&lt;/p&gt; &lt;p&gt;(If anyone wants the fully analysis, I will paste it below since I ran all the diffs into QwQ)&lt;/p&gt; &lt;p&gt;From that, you can see, at the very least, there were a number of issues affecting experts in the MoE system, flash attention was probably not working at all, memory issues galore, etc. Can it code the hexagon stuff eventually or score a 9 on your personal creative fiction benchmark? We don't know yet but for all our sakes, something like this is a brighter path forward. What about MoE's underperforming dense models because of some unnamed law of inference? Well, this is a new type of fused MoE, so we will have to see. Changes have to be made to get us closer to AGI on affordable consumer computers and all that growth is going to come with some pains. Soon the models will be able to make their own adaptations to these inference platforms to get out into the world less painfully but until then we are where we are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionysio211"&gt; /u/dionysio211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtzue8/why_we_may_be_wrong_about_llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtzue8/why_we_may_be_wrong_about_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtzue8/why_we_may_be_wrong_about_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T23:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtkb3p</id>
    <title>So what happened to Llama 4, which trained on 100,000 H100 GPUs?</title>
    <updated>2025-04-07T12:42:24+00:00</updated>
    <author>
      <name>/u/sunshinecheung</name>
      <uri>https://old.reddit.com/user/sunshinecheung</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt; &lt;img alt="So what happened to Llama 4, which trained on 100,000 H100 GPUs?" src="https://a.thumbs.redditmedia.com/8BEqw6mD4ZzZP0i0wXUPQrGPrNe0wHwiWVuao_uM-w8.jpg" title="So what happened to Llama 4, which trained on 100,000 H100 GPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zue0vixknete1.png?width=1040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9950ec242a827a86d5a2cc1d01da439839edd464"&gt;https://preview.redd.it/zue0vixknete1.png?width=1040&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9950ec242a827a86d5a2cc1d01da439839edd464&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 4 was trained using 100,000 H100 GPUs. However, even though Deepseek does not have as so much data and GPUs as Meta, it could manage to achieve a better performance (like DeepSeek-V3-0324)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fvq6jcysnete1.png?width=1107&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa1cdf1e955d208d2250e706d94efade06ae942"&gt;https://preview.redd.it/fvq6jcysnete1.png?width=1107&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa1cdf1e955d208d2250e706d94efade06ae942&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yann LeCun: FAIR is working on the next generation of AI architectures beyond Auto-Regressive LLMs. &lt;/p&gt; &lt;p&gt;But now, it seems that Meta's leading edge is diminishing, and smaller open-source model have been surpassed by Qwen.(Qwen3 is coming...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunshinecheung"&gt; /u/sunshinecheung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtkb3p/so_what_happened_to_llama_4_which_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T12:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jttq00</id>
    <title>Dream 7B (the diffusion reasoning model) no longer has a blank GitHub.</title>
    <updated>2025-04-07T19:19:35+00:00</updated>
    <author>
      <name>/u/Creative-robot</name>
      <uri>https://old.reddit.com/user/Creative-robot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/HKUNLP/Dream"&gt;https://github.com/HKUNLP/Dream&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanted to provide this because some people were disappointed that the code wasn’t available. It appears to be available now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-robot"&gt; /u/Creative-robot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jttq00/dream_7b_the_diffusion_reasoning_model_no_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T19:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju2po9</id>
    <title>Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion</title>
    <updated>2025-04-08T02:08:33+00:00</updated>
    <author>
      <name>/u/Cheap_Ship6400</name>
      <uri>https://old.reddit.com/user/Cheap_Ship6400</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt; &lt;img alt="Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion" src="https://external-preview.redd.it/xysnssK0wWdIRckvWVwaBSbIhMo96eApOHbJ846j7qQ.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd1045517eda93c2aaafc19130bea85c7466318" title="Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uejdgej8qite1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc73f72076b01e82f3d46aa5b89f26373b080bb0"&gt;https://preview.redd.it/uejdgej8qite1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc73f72076b01e82f3d46aa5b89f26373b080bb0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://karpathy.bearblog.dev/power-to-the-people/"&gt;https://karpathy.bearblog.dev/power-to-the-people/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;If you go back through various sci-fi you'll see that very few would have predicted that the AI revolution would feature this progression. It was supposed to be a top secret government megabrain project wielded by the generals, not ChatGPT appearing basically overnight and for free on a device already in everyone's pocket.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Karpathy has argued that we are at a unique historical moment where technological (AI) power is being diffused to the general public in an astonishing and unprecedented way, which is very different from past experiences and science fiction predictions. That is a manifestation of &amp;quot;power to the people.&amp;quot; &lt;/p&gt; &lt;p&gt;I do think the LocalLLaMA community helps a lot in this paradigm shift.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Ship6400"&gt; /u/Cheap_Ship6400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtudz4</id>
    <title>Benchmark update: Llama 4 is now the top open source OCR model</title>
    <updated>2025-04-07T19:45:51+00:00</updated>
    <author>
      <name>/u/Tylernator</name>
      <uri>https://old.reddit.com/user/Tylernator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tylernator"&gt; /u/Tylernator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtudz4/benchmark_update_llama_4_is_now_the_top_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtudz4/benchmark_update_llama_4_is_now_the_top_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T19:45:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtweei</id>
    <title>Llama4 support is merged into llama.cpp!</title>
    <updated>2025-04-07T21:08:20+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"&gt; &lt;img alt="Llama4 support is merged into llama.cpp!" src="https://external-preview.redd.it/3YQJt4uj8I_2zhsxnK4qdxOjQqnMLiZv2IVMx9xShfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=553ce6c7a84f765dccbcc4f23b18ea664ca13750" title="Llama4 support is merged into llama.cpp!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12791"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtweei/llama4_support_is_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T21:08:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju044y</id>
    <title>Llama-4-Scout-17B-16E on single 3090 - 6 t/s</title>
    <updated>2025-04-07T23:57:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju044y/llama4scout17b16e_on_single_3090_6_ts/"&gt; &lt;img alt="Llama-4-Scout-17B-16E on single 3090 - 6 t/s" src="https://preview.redd.it/xjzq4t774ite1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33480ec505cfb3066d70e4cdb8885118b97b3ce2" title="Llama-4-Scout-17B-16E on single 3090 - 6 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xjzq4t774ite1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju044y/llama4scout17b16e_on_single_3090_6_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju044y/llama4scout17b16e_on_single_3090_6_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T23:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtslj9</id>
    <title>Official statement from meta</title>
    <updated>2025-04-07T18:34:05+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"&gt; &lt;img alt="Official statement from meta" src="https://preview.redd.it/4beb8fwkigte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcac364db46647580264300f0e485fe1826ca23c" title="Official statement from meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4beb8fwkigte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtslj9/official_statement_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jts2hq</id>
    <title>"...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in..."</title>
    <updated>2025-04-07T18:12:50+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"&gt; &lt;img alt="&amp;quot;...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in...&amp;quot;" src="https://external-preview.redd.it/Ux91NoXxBQFaUsZ3lSBPCQvGxPEHWAd01W7Kjotd_O8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=860e92dd619c1a16dfb6d356a8fb6e2167fe5e31" title="&amp;quot;...we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in...&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We're glad to start getting Llama 4 in all your hands. We're already hearing lots of great results people are getting with these models. &lt;/p&gt; &lt;p&gt;That said, we're also hearing some reports of mixed quality across different services. Since we dropped the models as soon as they were ready, we expect it'll take several days for all the public implementations to get dialed in. We'll keep working through our bug fixes and onboarding partners. &lt;/p&gt; &lt;p&gt;We've also heard claims that we trained on test sets -- that's simply not true and we would never do that. Our best understanding is that the variable quality people are seeing is due to needing to stabilize implementations. &lt;/p&gt; &lt;p&gt;We believe the Llama 4 models are a significant advancement and we're looking forward to working with the community to unlock their value.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ahmad_Al_Dahle/status/1909302532306092107"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jts2hq/were_also_hearing_some_reports_of_mixed_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T18:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju5aux</id>
    <title>lmarena.ai confirms that meta cheated</title>
    <updated>2025-04-08T04:32:53+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They provided a model that is optimized for human preferences, which is different then other hosted models. :(&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;https://x.com/lmarena_ai/status/1909397817434816562&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T04:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju7r63</id>
    <title>Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?</title>
    <updated>2025-04-08T07:18:47+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"&gt; &lt;img alt="Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?" src="https://preview.redd.it/clznuueqakte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2facfbcd182f06991c15e3a654ff2bebadec08e" title="Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/clznuueqakte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T07:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6sm1</id>
    <title>nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face</title>
    <updated>2025-04-08T06:10:27+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face" src="https://external-preview.redd.it/3UxngnIkjlXfR7MJ8ohQbkyRtFJzuTypVV_aoc8_Tmk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e441532f8217ac64c401c9352ae767ec98103b56" title="nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning model derived from Llama 3 405B, 128k context length. Llama-3 license. See model card for more info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T06:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju3dtg</id>
    <title>Llama 4 Computer Use Agent</title>
    <updated>2025-04-08T02:43:23+00:00</updated>
    <author>
      <name>/u/unforseen-anomalies</name>
      <uri>https://old.reddit.com/user/unforseen-anomalies</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"&gt; &lt;img alt="Llama 4 Computer Use Agent" src="https://external-preview.redd.it/mInt-jX9Z334TG_hOLgbkFELfN5NFbX9_ugIlxIbW_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=069a1c0127a866a1240e9a0eab175a0db2c1edd9" title="Llama 4 Computer Use Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I experimented with a computer use agent powered by Meta Llama 4 Maverick and it performed better than expected (given the recent feedback on Llama 4 😬) - in my testing it could browse the web archive, compress an image and solve a grammar quiz. And it's certainly much cheaper than other computer use agents.&lt;/p&gt; &lt;p&gt;Check out interaction trajectories here: &lt;a href="https://llama4.pages.dev/"&gt;https://llama4.pages.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please star it if you find it interesting :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unforseen-anomalies"&gt; /u/unforseen-anomalies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/TheoLeeCJ/llama4-computer-use"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtnryp</id>
    <title>Must have 5–8+ years experience with ChatGPT and Microsoft Copilot</title>
    <updated>2025-04-07T15:18:03+00:00</updated>
    <author>
      <name>/u/Leading-Leading6718</name>
      <uri>https://old.reddit.com/user/Leading-Leading6718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt; &lt;img alt="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" src="https://preview.redd.it/v4w6g5cohfte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0731325c97d9402fe56370d94bfeb59e80729e9c" title="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ah yes, the classic requirement:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;ChatGPT dropped in late 2022.&lt;br /&gt; Copilot showed up in 2023.&lt;br /&gt; APIs? Even newer.&lt;/p&gt; &lt;p&gt;But sure, let me just fire up the time machine real quick.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading-Leading6718"&gt; /u/Leading-Leading6718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4w6g5cohfte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T15:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju0nd6</id>
    <title>LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a "customized model to optimize for human preference"</title>
    <updated>2025-04-08T00:23:00+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"&gt; &lt;img alt="LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a &amp;quot;customized model to optimize for human preference&amp;quot;" src="https://external-preview.redd.it/Sg2foySeNKtSftpVRS-DvTZSfTyNGrVywN1v0vjvasA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0bc2190330f7558e229144dd8c588556bdeaf22" title="LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a &amp;quot;customized model to optimize for human preference&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T00:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju37gh</id>
    <title>Meta submitted customized llama4 to lmarena without providing clarification beforehand</title>
    <updated>2025-04-08T02:34:03+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"&gt; &lt;img alt="Meta submitted customized llama4 to lmarena without providing clarification beforehand" src="https://preview.redd.it/cl1e4af7wite1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3b61a0b1cca0493b9eb3ac029029dcf56706a46" title="Meta submitted customized llama4 to lmarena without providing clarification beforehand" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Meta should have made it clearer that “Llama-4-Maverick-03-26-Experimental” was a customized model to optimize for human preference&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;https://x.com/lmarena_ai/status/1909397817434816562&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cl1e4af7wite1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju4xjl</id>
    <title>1.58bit Llama 4 - Unsloth Dynamic GGUFs</title>
    <updated>2025-04-08T04:10:36+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Llama 4 is here &amp;amp; we uploaded &lt;strong&gt;imatrix&lt;/strong&gt; Dynamic GGUF formats so you can run them locally. All GGUFs are at: &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently text only. For our dynamic GGUFs, to ensure the best tradeoff between accuracy and size, we do not to quantize all layers, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit. Fine-tuning support coming in a few hours.&lt;/p&gt; &lt;p&gt;According to the official Llama-4 Github page, and other sources, use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.6 top_p = 0.9 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This time, &lt;strong&gt;all our GGUF uploads are quantized using imatrix&lt;/strong&gt;, which has improved accuracy over standard quantization. We intend to improve our imatrix quants even more with benchmarks (most likely when Qwen3 gets released). Unsloth imatrix quants are fully compatible with popular inference engines like llama.cpp, Ollama, Open WebUI etc.&lt;/p&gt; &lt;p&gt;We utilized DeepSeek R1, V3 and other LLMs to create a large calibration dataset.&lt;/p&gt; &lt;p&gt;Read our guide for running Llama 4 (with correct settings etc): &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Unsloth Dynamic Llama-4-Scout uploads with optimal configs:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.78bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;33.8GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_S.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Ok&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.93bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;35.4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_M.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.42-bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;38.6GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.71-bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.2GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-Q2_K_XL.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Suggested&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;52.9GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-IQ3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Great&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.6GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-IQ4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;* Originally we had a 1.58bit version was that still uploading, but we decided to remove it since it didn't seem to do well on further testing - the lowest quant is the 1.78bit version.&lt;/p&gt; &lt;p&gt;Let us know how it goes!&lt;/p&gt; &lt;p&gt;In terms of testing, unfortunately we can't make the full BF16 version (ie regardless of quantization or not) complete the Flappy Bird game nor the Heptagon test appropriately. We tried Groq, using imatrix or not, used other people's quants, and used normal Hugging Face inference, and this issue persists.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T04:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju1qtt</id>
    <title>Llama 4 (Scout) GGUFs are here! (and hopefully are final!) (and hopefully better optimized!)</title>
    <updated>2025-04-08T01:19:10+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;TEXT ONLY&lt;/em&gt; forgot to mention in title :')&lt;/p&gt; &lt;p&gt;Quants seem coherent, conversion seems to match original model's output, things look good thanks to Son over on llama.cpp putting great effort into it for the past 2 days :) Super appreciate his work!&lt;/p&gt; &lt;p&gt;Static quants of Q8_0, Q6_K, Q4_K_M, and Q3_K_L are up on the lmstudio-community page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(If you want to run in LM Studio make sure you update to the latest beta release)&lt;/p&gt; &lt;p&gt;Imatrix (and smaller sizes) are up on my own page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/meta-llama_Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/bartowski/meta-llama_Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One small note, if you've been following along over on the llama.cpp GitHub, you may have seen me working on some updates to DeepSeek here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12727"&gt;https://github.com/ggml-org/llama.cpp/pull/12727&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These changes though also affect MoE models in general, and so Scout is similarly affected.. I decided to make these quants WITH my changes, so they should perform better, similar to how Unsloth's DeekSeek releases were better, albeit at the cost of some size.&lt;/p&gt; &lt;p&gt;IQ2_XXS for instance is about 6% bigger with my changes (30.17GB versus 28.6GB), but I'm hoping that the quality difference will be big. I know some may be upset at larger file sizes, but my hope is that even IQ1_M is better than IQ2_XXS was.&lt;/p&gt; &lt;p&gt;Q4_K_M for reference is about 3.4% bigger (65.36 vs 67.55)&lt;/p&gt; &lt;p&gt;I'm running some PPL measurements for Scout (you can see the numbers from DeepSeek for some sizes in the listed PR above, for example IQ2_XXS got 3% bigger but PPL improved by 20%, 5.47 to 4.38) so I'll be reporting those when I have them. Note both lmstudio and my own quants were made with my PR.&lt;/p&gt; &lt;p&gt;In the mean time, enjoy!&lt;/p&gt; &lt;p&gt;Edit for PPL results:&lt;/p&gt; &lt;p&gt;Did not expect such awful PPL results from IQ2_XXS, but maybe that's what it's meant to be for this size model at this level of quant.. But for direct comparison, should still be useful?&lt;/p&gt; &lt;p&gt;Anyways, here's some numbers, will update as I have more:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;quant&lt;/th&gt; &lt;th&gt;size (master)&lt;/th&gt; &lt;th&gt;ppl (master)&lt;/th&gt; &lt;th&gt;size (branch)&lt;/th&gt; &lt;th&gt;ppl (branch)&lt;/th&gt; &lt;th&gt;size increase&lt;/th&gt; &lt;th&gt;PPL improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M&lt;/td&gt; &lt;td&gt;65.36GB&lt;/td&gt; &lt;td&gt;9.1284 +/- 0.07558&lt;/td&gt; &lt;td&gt;67.55GB&lt;/td&gt; &lt;td&gt;pending&lt;/td&gt; &lt;td&gt;2.19GB (3.4%)&lt;/td&gt; &lt;td&gt;pending&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ2_XXS&lt;/td&gt; &lt;td&gt;28.56GB&lt;/td&gt; &lt;td&gt;12.0353 +/- 0.09845&lt;/td&gt; &lt;td&gt;30.17GB&lt;/td&gt; &lt;td&gt;10.9130 +/- 0.08976&lt;/td&gt; &lt;td&gt;1.61GB (6%)&lt;/td&gt; &lt;td&gt;-1.12 9.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ1_M&lt;/td&gt; &lt;td&gt;24.57GB&lt;/td&gt; &lt;td&gt;14.1847 +/- 0.11599&lt;/td&gt; &lt;td&gt;26.32GB&lt;/td&gt; &lt;td&gt;12.1686 +/- 0.09829&lt;/td&gt; &lt;td&gt;1.75GB (7%)&lt;/td&gt; &lt;td&gt;-2.02 (14.2%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;(another edit, Q4_K_M is up at 9.1..? these are very strange PPL numbers.. still crunching of course)&lt;/p&gt; &lt;p&gt;As suspected, IQ1_M with my branch shows similar PPL to IQ2_XXS from master with 2GB less size.. Hopefully that means successful experiment..?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T01:19:10+00:00</published>
  </entry>
</feed>
