<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-03T05:42:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mfaigh</id>
    <title>We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)</title>
    <updated>2025-08-01T22:40:00+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Organization&lt;/th&gt; &lt;th align="left"&gt;HuggingFace Link&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Modality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;dots.ocr&lt;/td&gt; &lt;td align="left"&gt;REDnote Hilab&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5-Air&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;480B-A35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Base&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intern S1&lt;/td&gt; &lt;td align="left"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;241B-A22B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;49B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 7B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 14B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 32B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;step3&lt;/td&gt; &lt;td align="left"&gt;StepFun&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/stepfun-ai/step3"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;321B-A38B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B-A0.6B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X Instruct-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X PPO-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Small 24B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Mini 3B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B Base&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ling lite-1.5 2506&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-lite-1.5-2506"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ming Lite Omni-1.5&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;20.3B&lt;/td&gt; &lt;td align="left"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 32B 0727&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 4B 0729&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 8B&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;command a vision 07-2025&lt;/td&gt; &lt;td align="left"&gt;Cohere&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-vision-07-2025"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;112B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT V1 40B&lt;/td&gt; &lt;td align="left"&gt;Kwaipilot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;40B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 2B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;671B-A37B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-405B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;405B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;109B-A17B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-70B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 4.0 VL Light&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-4.0-VL-Light"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 3.1&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-3.1"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmOCR 7B 0725&lt;/td&gt; &lt;td align="left"&gt;AllenAI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5v 3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 7B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-7B"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 21B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-21B"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 70B preview SFT&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt; &lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt; &lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;[EDIT] &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyone could publish a new model—with some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:40:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgatd6</id>
    <title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title>
    <updated>2025-08-03T05:00:24+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm. CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100. The capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.14111v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgatd6/cudal1_improving_cuda_optimization_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgatd6/cudal1_improving_cuda_optimization_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfnq2r</id>
    <title>Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090</title>
    <updated>2025-08-02T10:57:41+00:00</updated>
    <author>
      <name>/u/kargafe</name>
      <uri>https://old.reddit.com/user/kargafe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt; &lt;img alt="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" src="https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f" title="Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt; &lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. I used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison. I'm also planning to run some fine-tuning benchmarks later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kargafe"&gt; /u/kargafe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erib4a6t7lgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T10:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg5scj</id>
    <title>Thinking or Instruct?</title>
    <updated>2025-08-03T00:34:17+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br /&gt; It's something I've never been clear about.&lt;br /&gt; For example, when I've used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn't add much, but I haven't noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don't.&amp;quot;&lt;br /&gt; Could someone clarify this for me?&lt;/p&gt; &lt;p&gt;I'm thinking of downloading this model:&lt;br /&gt; &lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T00:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgadmz</id>
    <title>Seeking a way to implement Low-Maintenance, Fully Local RAG Stack for a 16GB VRAM Setup (36k Arabic epub Docs)</title>
    <updated>2025-08-03T04:35:19+00:00</updated>
    <author>
      <name>/u/rfiraz</name>
      <uri>https://old.reddit.com/user/rfiraz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm looking for advice on building a robust, self-hosted RAG system with a strong emphasis on &lt;strong&gt;long-term, low-maintenance operation&lt;/strong&gt;. My goal is to create a powerful knowledge engine that I can &amp;quot;set and forget&amp;quot; as much as possible, without needing constant daily troubleshooting.&lt;/p&gt; &lt;p&gt;The entire system must run &lt;strong&gt;100% locally&lt;/strong&gt; on a single machine with a &lt;strong&gt;16GB VRAM GPU&lt;/strong&gt; (RTX 5070 Ti).&lt;/p&gt; &lt;p&gt;My knowledge base is unique and large: &lt;strong&gt;36,000+ ePub files, all in Arabic&lt;/strong&gt;. The system needs to handle multilingual queries (&lt;strong&gt;Indonesian, English, Arabic&lt;/strong&gt;) and provide accurate, cited answers.&lt;/p&gt; &lt;p&gt;To achieve low maintenance, my core idea is a &lt;strong&gt;decoupled architecture&lt;/strong&gt;, where each component runs independently (e.g., in separate containers). My reasoning is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If the UI (Open WebUI) breaks, the backend is unaffected.&lt;/li&gt; &lt;li&gt;If I want to swap the LLM in Ollama, I don't need to touch the RAG logic code.&lt;/li&gt; &lt;li&gt;Most importantly, re-indexing the entire 36k ePub corpus (a massive background task) shouldn't take down the live Q&amp;amp;A service.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given the focus on stability and a 16GB VRAM limit, I'd love your recommendations on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vector Database:&lt;/strong&gt; Which vector store offers the &lt;strong&gt;easiest management, backup, and recovery process&lt;/strong&gt; for a local setup? I need something that &amp;quot;just works&amp;quot; without constant administration. Are &lt;strong&gt;ChromaDB&lt;/strong&gt;, &lt;strong&gt;LanceDB&lt;/strong&gt;, or a simple file-based &lt;strong&gt;FAISS&lt;/strong&gt; index the most reliable choices here?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Ingestion Pipeline:&lt;/strong&gt; What is the most &lt;strong&gt;resilient and automated&lt;/strong&gt; way to build the ingestion pipeline for the 36k ePubs? My plan is a separate, scheduled script that processes new/updated files. Is this more maintainable than building it into the main API?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stable Models (Embeddings &amp;amp; LLM):&lt;/strong&gt; Beyond pure performance, which &lt;strong&gt;embedding and LLM models&lt;/strong&gt; are known for their stability and good long-term support? I want to avoid using a &amp;quot;flavor-of-the-month&amp;quot; model that might be abandoned. The models must handle Arabic, Indonesian, and English well and fit within the VRAM budget.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Budgeting:&lt;/strong&gt; How do you wisely allocate a &lt;strong&gt;16GB VRAM budget&lt;/strong&gt; between the LLM, embedding model, and a potential re-ranker to ensure system stability and avoid &amp;quot;out of memory&amp;quot; errors during peak use?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliable Cross-Lingual Flow:&lt;/strong&gt; For handling Indonesian/English queries against Arabic text, what's the most &lt;em&gt;reliable&lt;/em&gt; method? Is translating queries first more robust in the long run than relying solely on a multilingual embedding space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help or suggestions would be greatly appreciated! I'd like to hear more about the setups you all use and what's worked best for you.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rfiraz"&gt; /u/rfiraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T04:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfrunn</id>
    <title>It's time to run your own R1, Kimi ... and split the cost of it</title>
    <updated>2025-08-02T14:26:51+00:00</updated>
    <author>
      <name>/u/HammerSpb</name>
      <uri>https://old.reddit.com/user/HammerSpb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HammerSpb"&gt; /u/HammerSpb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg82el</id>
    <title>ccproxy - Route Claude Code requests to any LLM while keeping your MAX plan</title>
    <updated>2025-08-03T02:31:07+00:00</updated>
    <author>
      <name>/u/_kintsu</name>
      <uri>https://old.reddit.com/user/_kintsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup. Large context requests would hit Claude's limits, and running compaction so often and having Claude lose important context was a frustrating experience.&lt;/p&gt; &lt;p&gt;So I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.&lt;/p&gt; &lt;p&gt;What it actually does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Routes requests to different providers while keeping your Claude Code client unchanged&lt;/li&gt; &lt;li&gt;Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash&lt;/li&gt; &lt;li&gt;Define rules based on token count, model name, tool usage, or any request property&lt;/li&gt; &lt;li&gt;Everything else defaults to your Claude MAX plan&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current limitations&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cross-provider context caching is coming but not ready yet&lt;/li&gt; &lt;li&gt;Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it's using LiteLLM I expect it to work with most setups.&lt;/li&gt; &lt;li&gt;No fancy UI - it's YAML config for now&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Who this helps: If you're already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic. It's particularly useful if you're hitting context limits or want to use cheaper models for simple tasks.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/starbased-co/ccproxy"&gt;https://github.com/starbased-co/ccproxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feedback. What routing patterns would be most useful for your workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_kintsu"&gt; /u/_kintsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T02:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg1evr</id>
    <title>GNOME AI Virtual Assistant "Newelle" Reaches Version 1.0 Milestone</title>
    <updated>2025-08-02T21:11:21+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/GNOME-AI-Assistant-1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T21:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg4lxw</id>
    <title>How do I get Qwen 3 to stop asking terrible questions?</title>
    <updated>2025-08-02T23:36:38+00:00</updated>
    <author>
      <name>/u/TastesLikeOwlbear</name>
      <uri>https://old.reddit.com/user/TastesLikeOwlbear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt; &lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt; &lt;p&gt;Sometimes, both sides are the same. (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt; &lt;p&gt;Sometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt; &lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt; &lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don't...&amp;quot; instructions) I've tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt; &lt;p&gt;And if I bring it up in chat, I get Qwen3's usual long obsequious apology (&amp;quot;You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.&lt;/p&gt; &lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt; &lt;p&gt;Thanks for any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastesLikeOwlbear"&gt; /u/TastesLikeOwlbear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T23:36:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg12k4</id>
    <title>Convert your ChatGTP exported conversations to something that Open-WebUI can import</title>
    <updated>2025-08-02T20:56:15+00:00</updated>
    <author>
      <name>/u/scubanarc</name>
      <uri>https://old.reddit.com/user/scubanarc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/"&gt; &lt;img alt="Convert your ChatGTP exported conversations to something that Open-WebUI can import" src="https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b17fc3d36981f925574075a8743bee7f8626049a" title="Convert your ChatGTP exported conversations to something that Open-WebUI can import" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.&lt;/p&gt; &lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt; &lt;p&gt;Specifically, this fixes the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chat dates are maintained&lt;/li&gt; &lt;li&gt;Chat hierarchy is preserved&lt;/li&gt; &lt;li&gt;Empty conversations are skipped&lt;/li&gt; &lt;li&gt;Parent-child relationships are maintained&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;You can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.&lt;/p&gt; &lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt; &lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scubanarc"&gt; /u/scubanarc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/scubanarc/chatgpt-to-open-webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T20:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfwckf</id>
    <title>100+ AI Benchmarks list</title>
    <updated>2025-08-02T17:34:52+00:00</updated>
    <author>
      <name>/u/panilyaU</name>
      <uri>https://old.reddit.com/user/panilyaU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt; &lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href="https://github.com/sindresorhus/awesome"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href="https://aibenchmarks.net/"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href="https://github.com/panilya/awesome-ai-benchmarks"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panilyaU"&gt; /u/panilyaU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T17:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mga3ox</id>
    <title>I made a prebuilt windows binary for ik_llama.cpp</title>
    <updated>2025-08-03T04:19:52+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/X5R/ik_llama.cpp"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T04:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfxas1</id>
    <title>Qwen moe in C</title>
    <updated>2025-08-02T18:14:18+00:00</updated>
    <author>
      <name>/u/1Hesham</name>
      <uri>https://old.reddit.com/user/1Hesham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped something I'm really excited about! 🚀 I was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo. That got me thinking... what if I could bring this to pure C? 🤔 Inspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file. The result? Qwen_MOE_C - a complete inference engine that: ✅ Handles sparse MoE computation (only 8 out of 128 experts active) ✅ Supports Grouped Query Attention with proper head ratios ✅ Uses memory mapping for efficiency (~30GB models) ✅ Zero external dependencies (just libc + libm) The beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency. Huge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems. Sometimes the best way to truly understand something is to build it from scratch. 🛠️ Link to the project: &lt;a href="https://github.com/h9-tec/Qwen_MOE_C"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Hesham"&gt; /u/1Hesham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T18:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg1e80</id>
    <title>Any news about the open source models that OpenAI promised to release ?</title>
    <updated>2025-08-02T21:10:32+00:00</updated>
    <author>
      <name>/u/NeedleworkerDull7886</name>
      <uri>https://old.reddit.com/user/NeedleworkerDull7886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeedleworkerDull7886"&gt; /u/NeedleworkerDull7886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T21:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfs9qn</id>
    <title>[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations</title>
    <updated>2025-08-02T14:44:41+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use &lt;code&gt;q5_1&lt;/code&gt; for a good balance of memory usage and performance. It works well in PPL tests and in practice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You're bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt; &lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\.(1[6-9]|[2-4][0-9])\.ffn_.*._=CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt; &lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt; &lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt; &lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt; &lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;code&gt;q5_1&lt;/code&gt; or even &lt;code&gt;q8_0&lt;/code&gt; for the draft model KV cache for much better performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt; &lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3d62</id>
    <title>Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct</title>
    <updated>2025-08-02T22:38:22+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt; &lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt; &lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfvxdo</id>
    <title>What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5</title>
    <updated>2025-08-02T17:17:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15026"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt; &lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt; &lt;p&gt;Disclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt; &lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T17:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7qpa</id>
    <title>Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends</title>
    <updated>2025-08-03T02:14:39+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt; &lt;img alt="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" src="https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg" title="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt; &lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt; &lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt; &lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt; &lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt; &lt;p&gt;The attempt to unify across providers wasn't as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt; &lt;p&gt;Would love to know your thoughts. &lt;/p&gt; &lt;p&gt;Olla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mg7qpa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T02:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg6xia</id>
    <title>Any news on updated Qwen3-8B/14B versions?</title>
    <updated>2025-08-03T01:32:51+00:00</updated>
    <author>
      <name>/u/zyxwvu54321</name>
      <uri>https://old.reddit.com/user/zyxwvu54321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zyxwvu54321"&gt; /u/zyxwvu54321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T01:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgj0g</id>
    <title>all I need....</title>
    <updated>2025-08-02T03:34:51+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt; &lt;img alt="all I need...." src="https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd" title="all I need...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggc3dzhr0jgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T03:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqejn</id>
    <title>Open-source model that is as intelligent as Claude Sonnet 4</title>
    <updated>2025-08-02T13:21:11+00:00</updated>
    <author>
      <name>/u/vishwa1238</name>
      <uri>https://old.reddit.com/user/vishwa1238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt; &lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt; &lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishwa1238"&gt; /u/vishwa1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T13:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfuiri</id>
    <title>Qwen Code + Qwen Coder 30b 3A is insane</title>
    <updated>2025-08-02T16:17:55+00:00</updated>
    <author>
      <name>/u/Flashy_Management962</name>
      <uri>https://old.reddit.com/user/Flashy_Management962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a little remark that if you haven't you definitely should try qwen code &lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br /&gt; I use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt; &lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt; &lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy_Management962"&gt; /u/Flashy_Management962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7abc</id>
    <title>Mac + Blackwell 👀</title>
    <updated>2025-08-03T01:51:15+00:00</updated>
    <author>
      <name>/u/Accomplished_Ad9530</name>
      <uri>https://old.reddit.com/user/Accomplished_Ad9530</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt; &lt;img alt="Mac + Blackwell 👀" src="https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=161c4ac9304218f08618c511e09178e7a7c08931" title="Mac + Blackwell 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/anemll/status/1951307167417639101"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Ad9530"&gt; /u/Accomplished_Ad9530 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u2mr83o6npgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T01:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg5xlb</id>
    <title>I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system</title>
    <updated>2025-08-03T00:41:41+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt; &lt;p&gt;🧠 Features:&lt;/p&gt; &lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt; &lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt; &lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt; &lt;p&gt;- Cross-platform and fully tested&lt;/p&gt; &lt;p&gt;- Open source and modular&lt;/p&gt; &lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/savantskie/persistent-ai-memory"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T00:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3i48</id>
    <title>HRM solved thinking more than current "thinking" models (this needs more hype)</title>
    <updated>2025-08-02T22:44:39+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article: &lt;a href="https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context:&lt;/p&gt; &lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href="https://x.com/VictorTaelin/status/1950512015899840768"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt; &lt;a href="https://github.com/sapientinc/HRM/issues/12"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:44:39+00:00</published>
  </entry>
</feed>
