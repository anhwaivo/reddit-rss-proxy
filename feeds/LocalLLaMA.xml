<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-18T01:11:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1komb56</id>
    <title>Pivotal Token Search (PTS): Optimizing LLMs by targeting the tokens that actually matter</title>
    <updated>2025-05-17T06:21:59+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;Pivotal Token Search (PTS)&lt;/strong&gt;, a technique for identifying and targeting critical decision points in language model generations that I've just open-sourced.&lt;/p&gt; &lt;h1&gt;What is PTS and why should you care?&lt;/h1&gt; &lt;p&gt;Have you ever noticed that when an LLM solves a problem, there are usually just a few key decision points where it either stays on track or goes completely off the rails? That's what PTS addresses.&lt;/p&gt; &lt;p&gt;Inspired by the recent &lt;a href="https://arxiv.org/abs/2412.08905v1"&gt;Phi-4 paper from Microsoft&lt;/a&gt;, PTS identifies &amp;quot;pivotal tokens&amp;quot; - specific points in a generation where the next token dramatically shifts the probability of a successful outcome.&lt;/p&gt; &lt;p&gt;Traditional DPO treats all tokens equally, but in reality, a tiny fraction of tokens are responsible for most of the success or failure. By targeting these, we can get more efficient training and better results.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;PTS uses a binary search algorithm to find tokens that cause significant shifts in solution success probability:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We take a model's solution to a problem with a known ground truth&lt;/li&gt; &lt;li&gt;We sample completions from different points in the solution to estimate success probability&lt;/li&gt; &lt;li&gt;We identify where adding a single token causes a large jump in this probability&lt;/li&gt; &lt;li&gt;We then create DPO pairs focused &lt;em&gt;specifically&lt;/em&gt; on these pivotal decision points&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example, in a math solution, choosing &amp;quot;cross-multiplying&amp;quot; vs &amp;quot;multiplying both sides&amp;quot; might dramatically affect the probability of reaching the correct answer, even though both are valid operations.&lt;/p&gt; &lt;h1&gt;What's included in the repo&lt;/h1&gt; &lt;p&gt;The &lt;a href="https://github.com/codelion/pts"&gt;GitHub repository&lt;/a&gt; contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complete implementation of the PTS algorithm&lt;/li&gt; &lt;li&gt;Data generation pipelines&lt;/li&gt; &lt;li&gt;Examples and usage guides&lt;/li&gt; &lt;li&gt;Evaluation tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, we've released:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets?other=pts"&gt;Pre-generated datasets&lt;/a&gt; for multiple domains&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/models?other=pts"&gt;Pre-trained models&lt;/a&gt; fine-tuned with PTS-generated preference pairs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/codelion/pts"&gt;https://github.com/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Datasets: &lt;a href="https://huggingface.co/datasets?other=pts"&gt;https://huggingface.co/datasets?other=pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Models: &lt;a href="https://huggingface.co/models?other=pts"&gt;https://huggingface.co/models?other=pts&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear about your experiences if you try it out! What other applications can you think of for this approach? Any suggestions for improvements or extensions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T06:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1koylpl</id>
    <title>Training Models</title>
    <updated>2025-05-17T17:38:03+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to fine-tune an AI model to essentially write like I would as a test. I have a bunch of.txt documents with things that I have typed. It looks like the first step is to convert it into a compatible format for training, which I can't figure out how to do. If you have done this before, could you give me help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koylpl/training_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koylpl/training_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koylpl/training_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T17:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp5mbu</id>
    <title>What models do yaâ€™ll recommend from Arli Ai?</title>
    <updated>2025-05-17T22:55:45+00:00</updated>
    <author>
      <name>/u/Melodyblue11</name>
      <uri>https://old.reddit.com/user/Melodyblue11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using Arli Ai for a couple of days now. I really like the huge variety of models on there. But I still canâ€™t seem to find the right model that sticks with me. I was wondering what models do yaâ€™ll mostly use for text roleplay?&lt;/p&gt; &lt;p&gt;Iâ€™m looking for a model thatâ€™s creative, doesnâ€™t need me to hold its hand to get things moving along, and is good with erp. &lt;/p&gt; &lt;p&gt;I mainly use Janitor Ai with my iPhone for text roleplay. I wish I could get silly tavern on iPhone ðŸ˜­.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Melodyblue11"&gt; /u/Melodyblue11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5mbu/what_models_do_yall_recommend_from_arli_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5mbu/what_models_do_yall_recommend_from_arli_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5mbu/what_models_do_yall_recommend_from_arli_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp1cuu</id>
    <title>Usecases for delayed,yet much cheaper inference?</title>
    <updated>2025-05-17T19:37:42+00:00</updated>
    <author>
      <name>/u/Maleficent-Tone6316</name>
      <uri>https://old.reddit.com/user/Maleficent-Tone6316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a project which hosts an open source LLM. The sell is that the cost is much cheaper (about 50-70%) as compared to current inference api costs. However the catch is that the output is generated later (delayed). I want to know the use cases for something like this. An example we thought of was async agentic systems which are scheduled daily. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Tone6316"&gt; /u/Maleficent-Tone6316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp1cuu/usecases_for_delayedyet_much_cheaper_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp1cuu/usecases_for_delayedyet_much_cheaper_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp1cuu/usecases_for_delayedyet_much_cheaper_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T19:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kojtwd</id>
    <title>Qwen is about to release a new model?</title>
    <updated>2025-05-17T03:47:46+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.10527"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kojtwd/qwen_is_about_to_release_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kojtwd/qwen_is_about_to_release_a_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T03:47:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp7vba</id>
    <title>Qwen3+ MCP</title>
    <updated>2025-05-18T00:51:36+00:00</updated>
    <author>
      <name>/u/OGScottingham</name>
      <uri>https://old.reddit.com/user/OGScottingham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to workshop a capable local rig, the latest buzz is MCP... Right?&lt;/p&gt; &lt;p&gt;Can Qwen3(or the latest sota 32b model) be fine tuned to use it well or does the model itself have to be trained on how to use it from the start?&lt;/p&gt; &lt;p&gt;Rig context: I just got a 3090 and was able to keep my 3060 in the same setup. I also have 128gb of ddr4 that I use to hot swap models with a mounted ram disk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OGScottingham"&gt; /u/OGScottingham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp7vba/qwen3_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp7vba/qwen3_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp7vba/qwen3_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T00:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kovobp</id>
    <title>What to do with extra PC</title>
    <updated>2025-05-17T15:29:42+00:00</updated>
    <author>
      <name>/u/PickleSavings1626</name>
      <uri>https://old.reddit.com/user/PickleSavings1626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Work gives me $200/months stipend to buy whatever I want, mainly for happiness (they are big on mental health). Not knowing what to buy, I now have a maxed out mac mini and a 6750 XT GPU rig. They both just sit there. I usually use LM Studio on my Macbook Pro. Any suggestions on what to do with these? I donâ€™t think I can link them up for faster LLM work or higher context windows. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PickleSavings1626"&gt; /u/PickleSavings1626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T15:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp5tur</id>
    <title>Thoughts on build? This is phase I. Open to all advice and opinions.</title>
    <updated>2025-05-17T23:05:53+00:00</updated>
    <author>
      <name>/u/Substantial_Cut_9418</name>
      <uri>https://old.reddit.com/user/Substantial_Cut_9418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Category Part Key specs / notes CPU AMD Ryzen 9 7950X3D 16 C / 32 T, 128 MB 3D V-Cache Motherboard ASUS ROG Crosshair X870E Hero AM5, PCIe 5.0 x16 / x8 + x8 Memory 4 Ã— 48 GB Corsair Vengeance DDR5-6000 CL30 192 GB total GPUs 2 Ã— NVIDIA RTX 5090 32 GB GDDR7 each, Blackwell Storage 2 Ã— Samsung 990 Pro 2 TB NVMe Gen-4 Ã—4 Case Phanteks Enthoo Pro II (Server Edition) SSI-EEB, 15 fan mounts, dual-PSU bay PSU Corsair TX-1600 (1600 W Platinum) Two native 12 VHPWR per GPU CPU cooler Corsair Nautilus 360 RS ARGB 360 mm AIO System fans 9 Ã— Corsair AF120 RGB Elite Front &amp;amp; bottom intake, top exhaust Fan / RGB hub Corsair iCUE Commander Core XT Ports 1-3 front, 4-6 bottom Thermal paste Thermal Grizzly Kryonaut Extreme â€” Extras Inland 4-port USB-C 3.2 Gen 1 hub Desk convenience&lt;/p&gt; &lt;p&gt;This is phase I. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cut_9418"&gt; /u/Substantial_Cut_9418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5tur/thoughts_on_build_this_is_phase_i_open_to_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5tur/thoughts_on_build_this_is_phase_i_open_to_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5tur/thoughts_on_build_this_is_phase_i_open_to_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T23:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kozggz</id>
    <title>Help me decide DGX Spark vs M2 Max 96GB</title>
    <updated>2025-05-17T18:15:21+00:00</updated>
    <author>
      <name>/u/Vegetable_Mix6629</name>
      <uri>https://old.reddit.com/user/Vegetable_Mix6629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to run a local LLM + RAG. Ideally 70B+ I am not sure if the DGX Spark is going to be significantly better than this MacBook Pro:&lt;/p&gt; &lt;p&gt;2023 M2 | 16.2&amp;quot; M2 Max 12-Core CPU | 38-Core GPU | 96 GB | 2 TB SSD&lt;/p&gt; &lt;p&gt;Can you guys please help me decide? Any advice, insights, and thoughts would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Mix6629"&gt; /u/Vegetable_Mix6629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozggz/help_me_decide_dgx_spark_vs_m2_max_96gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozggz/help_me_decide_dgx_spark_vs_m2_max_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kozggz/help_me_decide_dgx_spark_vs_m2_max_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T18:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kony6o</id>
    <title>Orpheus-TTS is now supported by chatllm.cpp</title>
    <updated>2025-05-17T08:14:32+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"&gt; &lt;img alt="Orpheus-TTS is now supported by chatllm.cpp" src="https://external-preview.redd.it/d2R5dTV2NnV2YTFmMbi8x691ZBFKYQvO7W9KNJH0CgcVBTuUP81YP-JSjSnu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4df49a2ab27c9a554035f635b2cd548a552a97be" title="Orpheus-TTS is now supported by chatllm.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy to share that &lt;a href="https://github.com/foldl/chatllm.cpp"&gt;chatllm.cpp&lt;/a&gt; now supports Orpheus-TTS models.&lt;/p&gt; &lt;p&gt;The demo audio is generated with this prompt: &lt;/p&gt; &lt;p&gt;```sh&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;build-vulkan\bin\Release\main.exe -m quantized\orpheus-tts-en-3b.bin -i --max&lt;em&gt;length 1000 _&lt;/em&gt;______ __ __ __ __ ___ / _&lt;strong&gt;&lt;em&gt;/ /&lt;/em&gt; __&lt;/strong&gt; &lt;em&gt;/ /&lt;/em&gt;/ / / / / |/ /_________ ____ / / / __ / __ `/ &lt;strong&gt;/ / / / / /|&lt;em&gt;/ // _&lt;/em&gt;&lt;em&gt;/ _&lt;/em&gt; / __ \ / /&lt;/strong&gt;&lt;em&gt;/ / / / /&lt;/em&gt;/ / /&lt;em&gt;/ /&lt;/em&gt;&lt;strong&gt;/ /&lt;/strong&gt;&lt;em&gt;/ / / // /&lt;/em&gt;&lt;em&gt;/ /&lt;/em&gt;/ / /&lt;em&gt;/ / \&lt;/em&gt;&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;/ /_/\&lt;/strong&gt;,&lt;em&gt;/\&lt;/em&gt;&lt;em&gt;/&lt;/em&gt;_&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;/&lt;em&gt;/ /&lt;/em&gt;(_)&lt;/strong&gt;&lt;em&gt;/ .&lt;/em&gt;&lt;strong&gt;/ .&lt;/strong&gt;&lt;em&gt;/ You are served by Orpheus-TTS, /&lt;/em&gt;/ /_/ with 3300867072 (3.3B) parameters.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Input &amp;gt; Orpheus-TTS is now supported by chatllm.cpp. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3lyipv6uva1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T08:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp5thx</id>
    <title>Multi-Source RAG with Hybrid Search and Re-ranking in OpenWebUI - Step-by-Step Guide</title>
    <updated>2025-05-17T23:05:23+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I created a DETAILED step-by-step hybrid RAG implementation guide for OpenWebUI -&lt;/p&gt; &lt;p&gt;&lt;a href="https://productiv-ai.guide/start/multi-source-rag-openwebui/"&gt;https://productiv-ai.guide/start/multi-source-rag-openwebui/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think. I couldn't find any other online sources that are as detailed as what I put together. I even managed to include external re-ranking steps which was a feature just added a couple weeks ago.&lt;br /&gt; I've seen all kinds of questions on how up-to-date guides on how to set up a RAG pipeline, so I wanted to contribute. Hope it helps some folks out there!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5thx/multisource_rag_with_hybrid_search_and_reranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5thx/multisource_rag_with_hybrid_search_and_reranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp5thx/multisource_rag_with_hybrid_search_and_reranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T23:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp23kw</id>
    <title>is it worth running fp16?</title>
    <updated>2025-05-17T20:10:35+00:00</updated>
    <author>
      <name>/u/kweglinski</name>
      <uri>https://old.reddit.com/user/kweglinski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm getting mixed responses from search. Answers are literally all over the place. Ranging from absolute difference, through zero difference to even - better results at q8. &lt;/p&gt; &lt;p&gt;I'm currently testing qwen3 30a3 at fp16 as it still has decent throughput (~45t/s) and for many tasks I don't need ~80t/s, especially if I'd get some quality gains. Since it's weekend and I'm spending much less time at computer I can't really put it through real trail by fire. Hence asking the question - is it going to improve anything or is it just burning ram? &lt;/p&gt; &lt;p&gt;Also note - I'm finding 32b (and higher) too slow for some of my tasks, especially if they are reasoning models, so I'd rather stick to moe.&lt;/p&gt; &lt;p&gt;edit: it did get couple obscure-ish factual questions correct which q8 didn't but that could be just lucky shot and also simple qa is not that important to me (though I do it as well)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kweglinski"&gt; /u/kweglinski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp23kw/is_it_worth_running_fp16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp23kw/is_it_worth_running_fp16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp23kw/is_it_worth_running_fp16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T20:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kooyfx</id>
    <title>llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)</title>
    <updated>2025-05-17T09:27:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"&gt; &lt;img alt="llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)" src="https://b.thumbs.redditmedia.com/aDTETHCO_H8_4IRBitddp52LLLB3EW99KQXDlvzStXo.jpg" title="llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Building a LocalLlama Machine â€“ Episode 4:&lt;/strong&gt; &lt;strong&gt;I think I am done (for now!)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I added a second RTX 3090 and replaced 64GB of slower RAM with 128GB of faster RAM.&lt;br /&gt; I think my build is complete for now (unless we get new models in 40B - 120B range!). &lt;/p&gt; &lt;p&gt;GPU Prices:&lt;br /&gt; - 2x RTX 3090 - 6000 PLN&lt;br /&gt; - 2x RTX 3060 - 2500 PLN&lt;br /&gt; - for comparison: single RTX 5090 costs between 12,000 and 15,000 PLN &lt;/p&gt; &lt;p&gt;Here are benchmarks of my system: &lt;/p&gt; &lt;p&gt;Qwen2.5-72B-Instruct-Q6_K - 9.14 t/s&lt;br /&gt; &lt;strong&gt;Qwen3-235B-A22B-Q3_K_M&lt;/strong&gt; - &lt;strong&gt;10.41 t/s (maybe I should try Q4)&lt;/strong&gt;&lt;br /&gt; Llama-3.3-70B-Instruct-Q6_K_L - 11.03 t/s&lt;br /&gt; Qwen3-235B-A22B-Q2_K - 14.77 t/s&lt;br /&gt; nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q8_0 - 15.09 t/s&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q8_0 - 15.1 t/s&lt;br /&gt; &lt;strong&gt;Llama-3.3-70B-Instruct-Q4_K_M&lt;/strong&gt; - &lt;strong&gt;17.4 t/s (important big dense model family)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q6_K&lt;/strong&gt; - &lt;strong&gt;17.84 t/s (kind of improved 70B)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Qwen_Qwen3-32B-Q8_0&lt;/strong&gt; - &lt;strong&gt;22.2 t/s (my fav general model)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;google_gemma-3-27b-it-Q8_0&lt;/strong&gt; - &lt;strong&gt;25.08 t/s (complements Qwen 32B)&lt;/strong&gt;&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q5_K_M - 29.78 t/s&lt;br /&gt; google_gemma-3-12b-it-Q8_0 - 30.68 t/s&lt;br /&gt; &lt;strong&gt;mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q8_0&lt;/strong&gt; - &lt;strong&gt;32.09 t/s (lots of finetunes)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Llama-4-Scout-17B-16E-Instruct-Q4_K_M&lt;/strong&gt; - &lt;strong&gt;38.75 t/s (fast, very underrated)&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-14B-Q8_0 - 49.47 t/s&lt;br /&gt; microsoft_Phi-4-reasoning-plus-Q8_0 - 50.16 t/s&lt;br /&gt; &lt;strong&gt;Mistral-Nemo-Instruct-2407-Q8_0&lt;/strong&gt; - &lt;strong&gt;59.12 t/s (most finetuned model ever?)&lt;/strong&gt;&lt;br /&gt; granite-3.3-8b-instruct-Q8_0 - 78.09 t/s&lt;br /&gt; Qwen_Qwen3-8B-Q8_0 - 83.13 t/s&lt;br /&gt; Meta-Llama-3.1-8B-Instruct-Q8_0 - 87.76 t/s&lt;br /&gt; Qwen_Qwen3-30B-A3B-Q8_0 - 90.43 t/s&lt;br /&gt; Qwen_Qwen3-4B-Q8_0 - 126.92 t/s &lt;/p&gt; &lt;p&gt;Please look at screenshots to understand how I run these benchmarks, it's not always obvious:&lt;br /&gt; - if you want to use RAM with MoE models, you need to learn how to use the &lt;strong&gt;--override-tensor&lt;/strong&gt; option&lt;br /&gt; - if you want to use different GPUs like I do, you'll need to get familiar with the &lt;strong&gt;--tensor-split&lt;/strong&gt; option &lt;/p&gt; &lt;p&gt;Depending on the model, I use different configurations:&lt;br /&gt; - Single 3090&lt;br /&gt; - Both 3090s&lt;br /&gt; - Both 3090s + one 3060&lt;br /&gt; - Both 3090s + both 3060s&lt;br /&gt; - Both 3090s + both 3060s + RAM/CPU &lt;/p&gt; &lt;p&gt;In my opinion &lt;strong&gt;Llama 4 Scout&lt;/strong&gt; is extremely underrated â€” it's fast and surprisingly knowledgeable. Maverick is too big for me.&lt;br /&gt; I hope weâ€™ll see some finetunes or variants of this model eventually. I hope Meta will release a 4.1 Scout at some point. &lt;/p&gt; &lt;p&gt;Qwen3 models are awesome, but in general, Qwen tends to lack knowledge about Western culture (movies, music, etc). In that area, Llamas, Mistrals, and Nemotrons perform much better. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please post your benchmarks&lt;/strong&gt; so we could compare different setups &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kooyfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T09:27:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1koqlmm</id>
    <title>Best model for upcoming 128GB unified memory machines?</title>
    <updated>2025-05-17T11:19:22+00:00</updated>
    <author>
      <name>/u/woahdudee2a</name>
      <uri>https://old.reddit.com/user/woahdudee2a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen-3 32B at Q8 is likely the best local option for now at just 34 GB, but surely we can do better?&lt;/p&gt; &lt;p&gt;Maybe the Qwen-3 235B-A22B at Q3 is possible, though it seems quite sensitive to quantization, so Q3 might be too aggressive.&lt;/p&gt; &lt;p&gt;Isn't there a more balanced 70B-class model that would fit this machine better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woahdudee2a"&gt; /u/woahdudee2a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T11:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kompbk</id>
    <title>New New Qwen</title>
    <updated>2025-05-17T06:48:29+00:00</updated>
    <author>
      <name>/u/bobby-chan</name>
      <uri>https://old.reddit.com/user/bobby-chan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"&gt; &lt;img alt="New New Qwen" src="https://external-preview.redd.it/KnKOyLV6zthvubjnKd-6Nrxq-GYIVyUyXITDw76dq6k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18734097bc18deaa13d0e68101249a248ed7e211" title="New New Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobby-chan"&gt; /u/bobby-chan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/WorldPM-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T06:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp558b</id>
    <title>RAG embeddings survey - What are your chunking / embedding settings?</title>
    <updated>2025-05-17T22:31:55+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp558b/rag_embeddings_survey_what_are_your_chunking/"&gt; &lt;img alt="RAG embeddings survey - What are your chunking / embedding settings?" src="https://preview.redd.it/z0sfv55h5f1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=642324c1171a53af3487212ca4182ab4c419f02e" title="RAG embeddings survey - What are your chunking / embedding settings?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been working with RAG for over a year now and it honestly seems like a bit of a dark art. I havenâ€™t really found the perfect settings for my use case yet. Iâ€™m dealing with several hundred policy documents as well as spreadsheets that contain number codes that link to specific products and services. Itâ€™s very important that these codes be associated with the correct product or service. Unfortunately I get a lot of hallucinations when it comes to the code lookup tasks. The policy PDFs are usually 100 pages or more. The larger chunk size seems to help with the policy PDFs but not so much with the specific code lookups in the spreadsheets&lt;/p&gt; &lt;p&gt;After a lot of experimenting over months and months. The following settings seem to work best for me (at least for the policy PDFs). &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document ingestion = Docling&lt;/li&gt; &lt;li&gt;Vector Storage = ChromaDB (built into Open WebUI)&lt;/li&gt; &lt;li&gt;Embedding Model = Nomic-embed-large&lt;/li&gt; &lt;li&gt;Hybrid Search Model (reranker) = BAAI/bge-reranker-v2-m3&lt;/li&gt; &lt;li&gt;Chunk size = 2000&lt;/li&gt; &lt;li&gt;Overlap size = 500&lt;/li&gt; &lt;li&gt;Top K = 10&lt;/li&gt; &lt;li&gt;Top K reranker = 10&lt;/li&gt; &lt;li&gt;Relevance Threshold = 0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are your use cases and what settings have you found works best for them? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z0sfv55h5f1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp558b/rag_embeddings_survey_what_are_your_chunking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp558b/rag_embeddings_survey_what_are_your_chunking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:31:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp64ro</id>
    <title>UQLM: Uncertainty Quantification for Language Models</title>
    <updated>2025-05-17T23:21:01+00:00</updated>
    <author>
      <name>/u/Opposite_Answer_287</name>
      <uri>https://old.reddit.com/user/Opposite_Answer_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a new open source Python package for generation time, zero-resource hallucination detection called UQLM. It leverages state-of-the-art uncertainty quantification techniques from the academic literature to compute response-level confidence scores based on response consistency (in multiple responses to the same prompt), token probabilities, LLM-as-a-Judge, or ensembles of these. Check it out, share feedback if you have any, and reach out if you want to contribute!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cvs-health/uqlm"&gt;https://github.com/cvs-health/uqlm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Opposite_Answer_287"&gt; /u/Opposite_Answer_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp64ro/uqlm_uncertainty_quantification_for_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp64ro/uqlm_uncertainty_quantification_for_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp64ro/uqlm_uncertainty_quantification_for_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T23:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp6gdv</id>
    <title>ROCm 6.4 + current unsloth working</title>
    <updated>2025-05-17T23:37:19+00:00</updated>
    <author>
      <name>/u/Ok_Ocelot2268</name>
      <uri>https://old.reddit.com/user/Ok_Ocelot2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here a working ROCm unsloth docker setup:&lt;/p&gt; &lt;p&gt;Dockerfile (for gfx1100)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM rocm/pytorch:rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.6.0 WORKDIR /root RUN git clone -b rocm_enabled_multi_backend https://github.com/ROCm/bitsandbytes.git RUN cd bitsandbytes/ &amp;amp;&amp;amp; cmake -DGPU_TARGETS=&amp;quot;gfx1100&amp;quot; -DBNB_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; -DCOMPUTE_BACKEND=hip -S . &amp;amp;&amp;amp; make &amp;amp;&amp;amp; pip install -e . RUN pip install unsloth_zoo&amp;gt;=2025.5.7 RUN pip install datasets&amp;gt;=3.4.1 sentencepiece&amp;gt;=0.2.0 tqdm psutil wheel&amp;gt;=0.42.0 RUN pip install accelerate&amp;gt;=0.34.1 RUN pip install peft&amp;gt;=0.7.1,!=0.11.0 WORKDIR /root RUN git clone https://github.com/ROCm/xformers.git RUN cd xformers/ &amp;amp;&amp;amp; git submodule update --init --recursive &amp;amp;&amp;amp; git checkout 13c93f3 &amp;amp;&amp;amp; PYTORCH_ROCM_ARCH=gfx1100 python setup.py install ENV FLASH_ATTENTION_TRITON_AMD_ENABLE=&amp;quot;TRUE&amp;quot; WORKDIR /root RUN git clone https://github.com/ROCm/flash-attention.git RUN cd flash-attention &amp;amp;&amp;amp; git checkout main_perf &amp;amp;&amp;amp; python setup.py install WORKDIR /root RUN git clone https://github.com/unslothai/unsloth.git RUN cd unsloth &amp;amp;&amp;amp; pip install . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;docker-compose.yml&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3' services: unsloth: container_name: unsloth devices: - /dev/kfd:/dev/kfd - /dev/dri:/dev/dri image: unsloth volumes: - ./data:/data - ./hf:/root/.cache/huggingface environment: - 'HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION-11.0.0}' command: sleep infinity &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;python -m bitsandbytes says &amp;quot;PyTorch settings found: ROCM_VERSION=64&amp;quot; but also tracebacks with &lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/root/bitsandbytes/bitsandbytes/backends/__init__.py&amp;quot;, line 15, in ensure_backend_is_available raise NotImplementedError(f&amp;quot;Device backend for {device_type} is currently not supported.&amp;quot;) NotImplementedError: Device backend for cuda is currently not supported. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;python -m xformers.info&lt;/p&gt; &lt;pre&gt;&lt;code&gt;xFormers 0.0.30+13c93f39.d20250517 memory_efficient_attention.ckF: available memory_efficient_attention.ckB: available memory_efficient_attention.ck_decoderF: available memory_efficient_attention.ck_splitKF: available memory_efficient_attention.cutlassF-pt: unavailable memory_efficient_attention.cutlassB-pt: unavailable memory_efficient_attention.fa2F@2.7.4.post1: available memory_efficient_attention.fa2B@2.7.4.post1: available memory_efficient_attention.fa3F@0.0.0: unavailable memory_efficient_attention.fa3B@0.0.0: unavailable memory_efficient_attention.triton_splitKF: available indexing.scaled_index_addF: available indexing.scaled_index_addB: available indexing.index_select: available sp24.sparse24_sparsify_both_ways: available sp24.sparse24_apply: available sp24.sparse24_apply_dense_output: available sp24._sparse24_gemm: available sp24._cslt_sparse_mm_search@0.0.0: available sp24._cslt_sparse_mm@0.0.0: available swiglu.dual_gemm_silu: available swiglu.gemm_fused_operand_sum: available swiglu.fused.p.cpp: available is_triton_available: True pytorch.version: 2.6.0+git45896ac pytorch.cuda: available gpu.compute_capability: 11.0 gpu.name: AMD Radeon PRO W7900 dcgm_profiler: unavailable build.info: available build.cuda_version: None build.hip_version: None build.python_version: 3.10.16 build.torch_version: 2.6.0+git45896ac build.env.TORCH_CUDA_ARCH_LIST: None build.env.PYTORCH_ROCM_ARCH: gfx1100 build.env.XFORMERS_BUILD_TYPE: None build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS: None build.env.NVCC_FLAGS: None build.env.XFORMERS_PACKAGE_FROM: None source.privacy: open source &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B"&gt;This&lt;/a&gt;-Reasoning-Conversational.ipynb) Notebook on a W7900 48GB: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;... {'loss': 0.3836, 'grad_norm': 25.887989044189453, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01} {'loss': 0.4308, 'grad_norm': 1.1072479486465454, 'learning_rate': 2.4e-05, 'epoch': 0.01} {'loss': 0.3695, 'grad_norm': 0.22923792898654938, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01} {'loss': 0.4119, 'grad_norm': 1.4164329767227173, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01} 17.4 minutes used for training. Peak reserved memory = 14.551 GB. Peak reserved memory for training = 0.483 GB. Peak reserved memory % of max memory = 32.347 %. Peak reserved memory for training % of max memory = 1.074 %. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ocelot2268"&gt; /u/Ok_Ocelot2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp6gdv/rocm_64_current_unsloth_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T23:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kosz97</id>
    <title>Orin Nano finally arrived in the mail. What should I do with it?</title>
    <updated>2025-05-17T13:26:49+00:00</updated>
    <author>
      <name>/u/miltonthecat</name>
      <uri>https://old.reddit.com/user/miltonthecat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"&gt; &lt;img alt="Orin Nano finally arrived in the mail. What should I do with it?" src="https://b.thumbs.redditmedia.com/kqIcM98T4EAIxbeoEQuku-Ke2-Up08JpbmnrUxjlTfI.jpg" title="Orin Nano finally arrived in the mail. What should I do with it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking of running home assistant with a local voice model or something like that. Open to any and all suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/miltonthecat"&gt; /u/miltonthecat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kosz97"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T13:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp2cok</id>
    <title>Visual reasoning still has a lot of room for improvement.</title>
    <updated>2025-05-17T20:21:57+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp2cok/visual_reasoning_still_has_a_lot_of_room_for/"&gt; &lt;img alt="Visual reasoning still has a lot of room for improvement." src="https://b.thumbs.redditmedia.com/NtdCi57QWJoTClzhdBLuaMUf88NNSJfg9Jsh48HhXKg.jpg" title="Visual reasoning still has a lot of room for improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was pretty surprised how poorly LLMs handle this question, so figured I would share it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/be4c6mx0fe1f1.png?width=1149&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1909a7872de046afcc355b8b726a8e0aed2b8a68"&gt;https://preview.redd.it/be4c6mx0fe1f1.png?width=1149&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1909a7872de046afcc355b8b726a8e0aed2b8a68&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What is DTS temp and why is it so much higher than my CPU temp?&lt;/p&gt; &lt;p&gt;Tried this on: Gemma 27b, Maverick, Scout, 2.5 PRO, Sonnet 3.7, 04-mini-high, grok 3.&lt;/p&gt; &lt;p&gt;Every single model gets it wrong at first.&lt;br /&gt; After following up with a little hint:&lt;/p&gt; &lt;p&gt;but look at the graphs&lt;/p&gt; &lt;p&gt;Sonnet 3.7 figures it out, but all the others still get it wrong. &lt;/p&gt; &lt;p&gt;If you aren't familiar with servers / overclocking CPUs this might not be obvious to you,&lt;br /&gt; The key thing here is those 2 temperature graphs are inverted.&lt;br /&gt; The DTS temperature here is actually showing a &amp;quot;Distance to maximum temperature&amp;quot; (high temperature number = colder cpu)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp2cok/visual_reasoning_still_has_a_lot_of_room_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp2cok/visual_reasoning_still_has_a_lot_of_room_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp2cok/visual_reasoning_still_has_a_lot_of_room_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T20:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kotssm</id>
    <title>I believe we're at a point where context is the main thing to improve on.</title>
    <updated>2025-05-17T14:05:40+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like language models have become incredibly smart in the last year or two. Hell even in the past couple months we've gotten Gemini 2.5 and Grok 3 and both are incredible in my opinion. This is where the problems lie though. If I send an LLM a well constructed message these days, it is very uncommon that it misunderstands me. Even the open source and small ones like Gemma 3 27b has understanding and instruction following abilities comparable to gemini but what I feel that every single one of these llms lack in is maintaining context over a long period of time. Even models like gemini that claim to support a 1M context window don't actually support a 1m context window coherently thats when they start screwing up and producing bugs in code that they can't solve no matter what etc. Even Llama 3.1 8b is a really good model and it's so small! Anyways I wanted to know what you guys think. I feel like maintaining context and staying on task without forgetting important parts of the conversation is the biggest shortcoming of llms right now and is where we should be putting our efforts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T14:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kosbyy</id>
    <title>GLaDOS has been updated for Parakeet 0.6B</title>
    <updated>2025-05-17T12:55:20+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt; &lt;img alt="GLaDOS has been updated for Parakeet 0.6B" src="https://preview.redd.it/8rtph8367c1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=412a76d5c943b2ae78ee168ac871cf7d6391f4e9" title="GLaDOS has been updated for Parakeet 0.6B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while, but I've had a chance to make &lt;a href="https://github.com/dnhkng/GLaDOS"&gt;a big update to GLaDOS&lt;/a&gt;: A much improved ASR model!&lt;/p&gt; &lt;p&gt;The new &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Nemo Parakeet 0.6B model&lt;/a&gt; is smashing the &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;Huggingface ASR Leaderboard&lt;/a&gt;, both in accuracy (#1!), and also speed (&amp;gt;10x faster then Whisper Large V3).&lt;/p&gt; &lt;p&gt;However, if you have been following the project, you will know I really dislike adding in more dependencies... and Nemo from Nvidia is a huge download. Its great; but its a library designed to be able to run hundreds of models. I just want to be able to run the very best or fastest 'good' model available.&lt;/p&gt; &lt;p&gt;So, I have refactored our all the audio pre-processing into &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/mel_spectrogram.py"&gt;one simple file&lt;/a&gt;, and the full &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/tdt_asr.py"&gt;Token-and-Duration Transducer (TDT)&lt;/a&gt; or &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/ctc_asr.py"&gt;FastConformer CTC model&lt;/a&gt; inference code as a file each. Minimal dependencies, maximal ease in doing ASR!&lt;/p&gt; &lt;p&gt;So now to can easily run either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt_ctc-110m"&gt;Parakeet-TDT_CTC-110M&lt;/a&gt; - solid performance, 5345.14 RTFx&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Parakeet-TDT-0.6B-v2&lt;/a&gt; - best performance, 3386.02 RTFx&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;just by using my python modules from the GLaDOS source. Installing GLaDOS will auto pull all the models you need, or you can download them directly from the &lt;a href="https://github.com/dnhkng/GLaDOS/releases/tag/0.1"&gt;releases section&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The TDT model is great, much better than Whisper too, give it a go! Give the project a Star to keep track, there's more cool stuff in development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rtph8367c1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kozpym</id>
    <title>Local models are starting to be able to do stuff on consumer grade hardware</title>
    <updated>2025-05-17T18:26:49+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is something that has a different threshold for people depending on exactly the hardware configuration they have, but I've actually crossed an important threshold today and I think this is representative of a larger trend. &lt;/p&gt; &lt;p&gt;For some time, I've really wanted to be able to use local models to &amp;quot;vibe code&amp;quot;. But not in the sense &amp;quot;one-shot generate a pong game&amp;quot;, but in the actual sense of creating and modifying some smallish application with meaningful functionality. There are some agentic frameworks that do that - out of those, I use Roo Code and Aider - and up until now, I've been relying solely on my free credits in enterprise models (Gemini, Openrouter, Mistral) to do the vibe-coding. It's mostly worked, but from time to time I tried some SOTA open models to see how they fare. &lt;/p&gt; &lt;p&gt;Well, up until a few weeks ago, this wasn't going anywhere. The models were either (a) unable to properly process bigger context sizes or (b) degenerating on output too quickly so that they weren't able to call tools properly or (c) simply too slow.&lt;/p&gt; &lt;p&gt;Imagine my surprise when I loaded up the yarn-patched 128k context version of Qwen14B. On IQ4_NL quants and 80k context, about the limit of what my PC, with 10 GB of VRAM and 24 GB of RAM can handle. Obviously, on the contexts that Roo handles (20k+), with all the KV cache offloaded to RAM, the processing is slow: the model can output over 20 t/s on an empty context, but with this cache size the throughput slows down to about 2 t/s, with thinking mode on. But on the other hand - the quality of edits is very good, its codebase cognition is very good, This is actually the first time that I've ever had a local model be able to handle Roo in a longer coding conversation, output a few meaningful code diffs and not get stuck.&lt;/p&gt; &lt;p&gt;Note that this is a function of not one development, but at least three. On one hand, the models are certainly getting better, this wouldn't have been possible without Qwen3, although earlier on GLM4 was already performing quite well, signaling a potential breakthrough. On the other hand, the tireless work of Llama.cpp developers and quant makers like Unsloth or Bartowski have made the quants higher quality and the processing faster. And finally, the tools like Roo are also getting better at handling different models and keeping their attention.&lt;/p&gt; &lt;p&gt;Obviously, this isn't the vibe-coding comfort of a Gemini Flash yet. Due to the slow speed, this is the stuff you can do while reading mails / writing posts etc. and having the agent run in the background. But it's only going to get better. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T18:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1konnx9</id>
    <title>Let's see how it goes</title>
    <updated>2025-05-17T07:54:06+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt; &lt;img alt="Let's see how it goes" src="https://preview.redd.it/ngy98tkusa1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=109911e4427c5bfba6bed05ca517063cd80c31ef" title="Let's see how it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ngy98tkusa1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T07:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp4scy</id>
    <title>AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!</title>
    <updated>2025-05-17T22:14:16+00:00</updated>
    <author>
      <name>/u/Huge-Designer-7825</name>
      <uri>https://old.reddit.com/user/Huge-Designer-7825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt; &lt;img alt="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" src="https://external-preview.redd.it/HyXeCsstmjpayPcbhebb2CfV3uo4aDIHFzZeJ7oDZps.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd4f6f0cce166ce310d3f28e00f2ea465bb8294" title="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind just dropped their AlphaEvolve paper (May 14th) on an AI that designs and evolves algorithms. Pretty groundbreaking.&lt;/p&gt; &lt;p&gt;Inspired, I immediately built OpenAlpha_Evolve â€“ an open-source Python framework so anyone can experiment with these concepts.&lt;/p&gt; &lt;p&gt;This was a rapid build to get a functional version out. Feedback, ideas for new agent challenges, or contributions to improve it are welcome. Let's explore this new frontier.&lt;/p&gt; &lt;p&gt;Imagine an agent that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand a complex problem description.&lt;/li&gt; &lt;li&gt;Generate initial algorithmic solutions.&lt;/li&gt; &lt;li&gt;Rigorously test its own code.&lt;/li&gt; &lt;li&gt;Learn from failures and successes.&lt;/li&gt; &lt;li&gt;Evolve increasingly sophisticated and efficient algorithms over time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub (All new code): &lt;a href="https://github.com/shyamsaktawat/OpenAlpha_Evolve"&gt;https://github.com/shyamsaktawat/OpenAlpha_Evolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40"&gt;https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;+---------------------+ +-----------------------+ +--------------------+ | Task Definition |-----&amp;gt;| Prompt Engineering |-----&amp;gt;| Code Generation | | (User Input) | | (PromptDesignerAgent) | | (LLM / Gemini) | +---------------------+ +-----------------------+ +--------------------+ ^ | | | | V +---------------------+ +-----------------------+ +--------------------+ | Select Survivors &amp;amp; |&amp;lt;-----| Fitness Evaluation |&amp;lt;-----| Execute &amp;amp; Test | | Next Generation | | (EvaluatorAgent) | | (EvaluatorAgent) | +---------------------+ +-----------------------+ +--------------------+ (Evolutionary Loop Continues) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Sources: DeepMind Blog - May 14, 2025: \&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Paper - &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Blogpost - &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge-Designer-7825"&gt; /u/Huge-Designer-7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:14:16+00:00</published>
  </entry>
</feed>
