<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-26T12:11:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n0dwjy</id>
    <title>Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025</title>
    <updated>2025-08-26T05:54:55+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"&gt; &lt;img alt="Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025" src="https://external-preview.redd.it/-JwExha6KVfoLydMnfzEw3abME2jLPqYjYEHLkxDPAk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9b2da8cd3912ed21dd7375c45a8e7c5c8636a0" title="Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The highlights for running LLMs are * 1300GB/s measured (real world) memory bandwidth in a dual socket configuration! * 576GB/s measured bandwidth between two sockets. * 96 PCIe Gen 5 lanes per socket, 32 of which support CXL, for an additional 128GB/s memory bandwidth per socket over CXL.&lt;/p&gt; &lt;p&gt;In a few years, when the industry moves to DDR6, these will hopefully become cheap enough to run big LLMs without or with only one GPU for prompt processing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.servethehome.com/intel-xeon-clearwater-forest-with-288-cores-on-intel-18a-at-hot-chips-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06ahz</id>
    <title>InternVL3_5 GGUF here</title>
    <updated>2025-08-25T23:33:37+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt; &lt;img alt="InternVL3_5 GGUF here" src="https://b.thumbs.redditmedia.com/EdxxIyVz6m8Y6DmR8UexDPcp-CYBhmw8ygu2dfJ8LCY.jpg" title="InternVL3_5 GGUF here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tested the &lt;a href="https://huggingface.co/collections/QuantStack/internvl3-5-ggufs-68acef206837c4f661a9b0a5"&gt;InternVL3_5&lt;/a&gt; 1b fp16 GGUF, it works&lt;br /&gt; (that's means the model architect is supported now in llama.cpp, I tested on LM studio) &lt;/p&gt; &lt;p&gt;every models now, just fp16,&lt;br /&gt; I think the QuantStack team is quantizing to different quants,&lt;br /&gt; if you want a quick try, just like and watch this repo, you may get surprised in few hours&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n06ahz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bbnx</id>
    <title>Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it.</title>
    <updated>2025-08-26T03:28:31+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt; &lt;img alt="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." src="https://external-preview.redd.it/IQUYk0UKH8LL_chH4rY6LMG-79G-lG-wrUjPogrDpXU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec62af39d7821e4b67b4c3bcde089ba116620d78" title="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to create a creative writing fine tune, but I've never done it and I don't know if to 3090's are up for the task for this model. So, I thought I'd start with inspiring those who have to take a look at this model if they missed it. While Seed-OSS was developed by ByteDance's Seed Team for reasoning and agent type stuff, it also has general capabilities and powerful long-context features. Also, this base doesn't have synthetic data in it. All sounds promising. What do you think? Does it have a shot at being a good base for creative models? Anyone attempting anything? Anyone up for helping me head down the fine tuning road? Is it even possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base-woSyn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0i0w0</id>
    <title>Which local LLM would use your phone the best?</title>
    <updated>2025-08-26T10:21:05+00:00</updated>
    <author>
      <name>/u/Salty-Bodybuilder179</name>
      <uri>https://old.reddit.com/user/Salty-Bodybuilder179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i0w0/which_local_llm_would_use_your_phone_the_best/"&gt; &lt;img alt="Which local LLM would use your phone the best?" src="https://preview.redd.it/3o99t20haclf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=353ea62d95af4d6d75a47e4b57dc168209fdd899" title="Which local LLM would use your phone the best?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing Android Automation Agent App with a few local models. Some handle multi-step instructions surprisingly well, some just‚Ä¶ rage-click everywhere üòÇ.&lt;/p&gt; &lt;p&gt;Task List comprises of rather simple tasks I collected from the people using the app.&lt;/p&gt; &lt;p&gt;Some of the results:&lt;br /&gt; Task: &amp;quot;Open WhatsApp ‚Üí Find contact ‚Üí Send 'Hello'&amp;quot;&lt;/p&gt; &lt;p&gt;LLaMA-3 8B | ‚úÖ Opens app | ‚úÖ Finds contact | ‚ùå Types gibberish&lt;/p&gt; &lt;p&gt;Mistral 7B | ‚úÖ Opens app | ‚ùå Stuck on search | ‚ùå Never sends&lt;/p&gt; &lt;p&gt;Qwen 7B | ‚úÖ Opens app | ‚úÖ Finds contact | ‚úÖ Sends properly&lt;/p&gt; &lt;p&gt;Phi-3 Mini | ‚ùå Confuses icons | ‚ùå Wrong tap | ‚ùå Fails task&lt;/p&gt; &lt;p&gt;agent I am testing on [gh link]: &lt;a href="https://github.com/Ayush0Chaudhary/blurr"&gt;https://github.com/Ayush0Chaudhary/blurr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm collecting results, but curious: what models would you recommend me for this kind of agentic use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Bodybuilder179"&gt; /u/Salty-Bodybuilder179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3o99t20haclf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i0w0/which_local_llm_would_use_your_phone_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i0w0/which_local_llm_would_use_your_phone_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dnp1</id>
    <title>A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed</title>
    <updated>2025-08-26T05:39:21+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"&gt; &lt;img alt="A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed" src="https://preview.redd.it/ut7b266wwalf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98224b002731e25972ece7ed84bf2f67e5ff05e1" title="A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i‚Äôm PSBigBig. i‚Äôve been publishing a plain-text, MIT repo that hit 600 stars in 60 days. the whole thing started as a rescue kit for teams who got stuck in ‚Äúit should work, but it doesn‚Äôt‚Äù RAG projects. today i‚Äôm sharing the part people asked for most often ‚Äî a Problem Map that turns fuzzy symptoms into numbered, auditable fixes.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;what it is&lt;/h2&gt; &lt;p&gt;a compact set of failure modes, each with a short checklist and a guardrail you can copy into your pipeline. no infra changes, no sdk lock-in. it behaves like a semantic firewall sitting beside your LLM flow. goal is simple. stop silent collapses before they poison your fine-tunes or your vector store.&lt;/p&gt; &lt;p&gt;example entries people keep hitting&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No.1 bootstrap ordering / empty ingestion looks fine in logs, then queries return air. root cause is ingestion windows happen out of order or pre-deploy triggers fire too early.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.5 semantic + embedding leakage adding special tokens seems to help then drift returns. mismatch across semantic layers, cosine won‚Äôt save you.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.8 vectorstore contamination mixed namespaces, re-index on write, faiss rebuild timing, stale shards. accuracy swings for no ‚Äúobvious‚Äù reason.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.12 chunking illusions you think it‚Äôs about chunk size. it‚Äôs usually mixed layout signals, lost anchors, or table regions pretending to be prose.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.16 agent loop collapse tool calls work, yet the agent never reaches goal state. usually caused by unguarded retries or an eval mirage.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;‚Äúyou think the problem is ‚Ä¶‚Äù vs ‚Äúthe real problem is ‚Ä¶‚Äù&lt;/h2&gt; &lt;p&gt;you think&lt;/p&gt; &lt;ul&gt; &lt;li&gt;our model is weak, so add more tokens, switch vector DBs, or ‚Äúupgrade embeddings‚Äù.&lt;/li&gt; &lt;li&gt;chunk sizes are wrong.&lt;/li&gt; &lt;li&gt;prompt is not strong enough.&lt;/li&gt; &lt;li&gt;we need LoRA or RAG-as-a-service.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the real thing&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you have a bootstrap timing fault. ingestion completed in the wrong order so the model ‚Äúnever saw‚Äù the data.&lt;/li&gt; &lt;li&gt;semantic drift happened across two layers, so cosine looks healthy while meaning has moved.&lt;/li&gt; &lt;li&gt;layout anchors were lost when converting PDF to markdown, tables became fake paragraphs.&lt;/li&gt; &lt;li&gt;a silent vector contamination mixed namespaces after a reindex, so you are retrieving ghosts from last week.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;each entry in the map is designed to be checked in minutes, not days. if you pass the checklist, you move on. if you fail, you apply the guardrail and re-run the same test.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;a small story, very real&lt;/h2&gt; &lt;p&gt;a team asked me why their ‚Äúfinance memos‚Äù bot kept citing the wrong quarter. they had already&lt;/p&gt; &lt;ul&gt; &lt;li&gt;switched from chroma to qdrant&lt;/li&gt; &lt;li&gt;tried three embedding families&lt;/li&gt; &lt;li&gt;doubled context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;looked like a vector db choice problem. it wasn‚Äôt. timeline showed uploads completing while a background re-index still held stale shards. the retriever was sober, the store was not. this matches No.8 in the map. fix was two lines of guardrail around ingestion gates and a post-commit verify. accuracy jumped, they never changed model or db again.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;why this seems to work&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;short checklists, not recipes. you can audit decisions later.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;all text-only, so you can paste it right into your LocalLLaMA notes or your agent‚Äôs system prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;model-agnostic. works with llama, claude, gpt, mistral, grok.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;free and MIT. keep it, fork it, print it, throw it at your teammates.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;coming next&lt;/h2&gt; &lt;p&gt;üõ† coming next: the Semantic Surgery Room and the Global Fix Map&lt;/p&gt; &lt;p&gt;this expands beyond RAG into n8n, GHL, Make.com and related automation stacks. think of it as a global AI clinic. we will publish real guardrails for orchestration loops, webhooks, field drift, file watchers, queue poisoning. target date by Sep 1. if your team runs ops on these tools, now is a good time to follow along.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;i‚Äôm posting here because LocalLLaMA folks often push close to the metal and need fast, real fixes. hope the map makes your next debug day shorter.&lt;/p&gt; &lt;p&gt;link: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ut7b266wwalf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0j56s</id>
    <title>support for Kimi VL model has been merged into llama.cpp (mtmd)</title>
    <updated>2025-08-26T11:23:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt; &lt;img alt="support for Kimi VL model has been merged into llama.cpp (mtmd)" src="https://external-preview.redd.it/cbkhZmPayjB8ku2nI_wAWqat0X8_NNQmx76ZV3jHgSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4760e99c4781aaef8c2381cf144ea12255d3b5e8" title="support for Kimi VL model has been merged into llama.cpp (mtmd)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We present &lt;strong&gt;Kimi-VL&lt;/strong&gt;, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers &lt;strong&gt;advanced multimodal reasoning, long-context understanding, and strong agent capabilities&lt;/strong&gt;‚Äîall while activating only &lt;strong&gt;2.8B&lt;/strong&gt; parameters in its language decoder (Kimi-VL-A3B).&lt;/p&gt; &lt;p&gt;(...)&lt;/p&gt; &lt;p&gt;This is an updated version of &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking"&gt;Kimi-VL-A3B-Thinking&lt;/a&gt;, with following improved abilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It Thinks Smarter while Consuming Less Tokens&lt;/strong&gt;: The 2506 version reaches better accuracy on multimodal reasoning benchmarks: 56.9 on MathVision (+20.1), 80.1 on MathVista (+8.4), 46.3 on MMMU-Pro (+3.3), 64.0 on MMMU (+2.1), while in average requires 20% reduced thinking length.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Sees Clearer with Thinking&lt;/strong&gt;: Unlike the previous version that specializes on thinking tasks, the 2506 version can also achieve the same or even better ability on general visual perception and understanding, e.g. MMBench-EN-v1.1 (84.4), MMStar (70.4), RealWorldQA (70.0), MMVet (78.4), surpassing or matching abilties of our non-thinking model (&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Video Scenarios&lt;/strong&gt;: The new 2506 version also improves on video reasoning and understanding benchmarks. It sets new state-of-the-art for open-source models on VideoMMMU (65.2), while also retains good ability on general video understanding (71.9 on Video-MME, matching &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Higher Resolution&lt;/strong&gt;: The new 2506 version supports 3.2 million total pixels in a single image, 4X compared to the previous version. This leads to non-trivial improvements on high-resolution perception and OS-agent grounding benchmarks: 83.2 on V* Benchmark (without extra tools), 52.8 on ScreenSpot-Pro, 52.5 on OSWorld-G (full set with refusal).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T11:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0jnsn</id>
    <title>multi-item tryon - qwen-edit</title>
    <updated>2025-08-26T11:49:51+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt; &lt;img alt="multi-item tryon - qwen-edit" src="https://b.thumbs.redditmedia.com/KlMjzSeLuGwUjogMzIosQV-Ldt0kqb3QFW98g1pD_Ro.jpg" title="multi-item tryon - qwen-edit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;today we release a early version of our multi item - tryon for qwen-edit&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FoxBaze/Try_On_Qwen_Edit_Lora_Alpha"&gt;https://huggingface.co/FoxBaze/Try_On_Qwen_Edit_Lora_Alpha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;given the early nature - we love to hear from you how you use it / and if something doesnt work ! find us on discord &lt;a href="https://discord.gg/UXN7zFuxbk"&gt;https://discord.gg/UXN7zFuxbk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s5c31qe4qclf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62dffbc3efbea228bd2fd938e1b45100724b7756"&gt;https://preview.redd.it/s5c31qe4qclf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62dffbc3efbea228bd2fd938e1b45100724b7756&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T11:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n09aof</id>
    <title>[2508.15884] Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
    <updated>2025-08-26T01:48:53+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.15884"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n09aof/250815884_jetnemotron_efficient_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n09aof/250815884_jetnemotron_efficient_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T01:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n02e7s</id>
    <title>How does huggingface make money?</title>
    <updated>2025-08-25T20:55:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I sure download from it a lot. What‚Äôs their way to bring profitably safe from shenanigans? Will it be stuff like GitHub? What‚Äôs the backup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T20:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0aijh</id>
    <title>GPT OSS 120B</title>
    <updated>2025-08-26T02:47:05+00:00</updated>
    <author>
      <name>/u/vinigrae</name>
      <uri>https://old.reddit.com/user/vinigrae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the best function calling model I‚Äôve used, don‚Äôt think twice, just use it. &lt;/p&gt; &lt;p&gt;We gave it a multi scenario difficulty 300 tool call test, where even 4o and GPT 5 mini performed poorly. &lt;/p&gt; &lt;p&gt;Ensure you format the system properly for it, you will find the model won‚Äôt even execute things that are actually done in a faulty manner and are detrimental to the pipeline.&lt;/p&gt; &lt;p&gt;I‚Äôm &lt;strong&gt;extremely&lt;/strong&gt; impressed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinigrae"&gt; /u/vinigrae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0c58h</id>
    <title>Update llama.cpp for a big speed boost with gpt-oss and cuda.</title>
    <updated>2025-08-26T04:11:45+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few cuda commits landed today that have made a big difference in performance. Testing with gpt-oss-120B, I saw a 14.5% increase in tokens per second with 2x3090 and 1xP40. It went from 51.6 tok/sec to 59.1 tok/sec. &lt;/p&gt; &lt;p&gt;With gptoss-20B I stayed at 130 tok/sec on a single 3090 power limited to 300W. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T04:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0i2ln</id>
    <title>Is there any way to run 100-120B MoE models at &gt;32k context at 30 tokens/second without spending a lot?</title>
    <updated>2025-08-26T10:23:55+00:00</updated>
    <author>
      <name>/u/vtkayaker</name>
      <uri>https://old.reddit.com/user/vtkayaker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 and a good AM5 socket system. With some tweaking, this is enough to run a 4-bit Qwen3-30B-A3B-Instruct-2507 as a coding model with 32k of context. It's no Claude Sonnet, but it's a cute toy and occasionally useful as a pair programmer.&lt;/p&gt; &lt;p&gt;I can also, with heroic effort and most of my 64GB of RAM, get GLM 4.5 Air to run painfully slowly with 32k context. Adding a &lt;a href="https://huggingface.co/jukofyork/GLM-4.5-DRAFT-0.6B-v3.0-GGUF/blob/main/README.md"&gt;draft model&lt;/a&gt; speeds up diff generation quite a bit, because even an 0.6B can accurately predict 16 tokens of unchanged diff context correctly.&lt;/p&gt; &lt;p&gt;But let's say I want to run a 4-bit quant of GLM 4.5 Air with 48-64k context at 30 tokens/second? What's the cheapest option?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An NVIDIA RTX PRO 6000 Blackwell 96GB costs around $8750. That would pay for &lt;em&gt;years&lt;/em&gt; of Claude MAX.&lt;/li&gt; &lt;li&gt;Lashing together 3 or 4 3090s requires both an EPYC motherboard and buying more 3090s.&lt;/li&gt; &lt;li&gt;Apple has some unified RAM systems. How fast are they &lt;em&gt;really&lt;/em&gt; for models like GLM 4.5 Air or GPT OSS 120B with 32-64k context and a 4-bit quant?&lt;/li&gt; &lt;li&gt;There's also the Ryzen AI MAX+ 395 with 128 GB of RAM, and dedicating 96 GB for the GPU. The few benchmarks I've seen are under 4k context, or not any better than 10 tokens/second.&lt;/li&gt; &lt;li&gt;NVIDIA has the DGX Spark coming out &lt;em&gt;sometime&lt;/em&gt; soon, but it looks like it will start at $3,000 and not actually be &lt;em&gt;that&lt;/em&gt; much better than the Ryzen AI MAX+ 395?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is there some clever setup that I'm missing? Does anyone have a 4-bit quant of GLM 4.5 Air running at 30 tokens/second with 48-64k context &lt;em&gt;without&lt;/em&gt; going all the way up to a RTX 6000 or 3-4 [345]090 cards and a server motherboard? I suspect the limiting factor here is RAM speed and PCIe lanes, even with the MoE?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vtkayaker"&gt; /u/vtkayaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0am1b</id>
    <title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title>
    <updated>2025-08-26T02:52:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;https://arxiv.org/abs/2508.18265v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05√ó inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks‚Äînarrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Models:&lt;/p&gt; &lt;p&gt;1B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-2B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-4B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-14B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;38B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-38B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-38B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;20BA4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30BA3B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;241BA28B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers&amp;quot;&lt;/li&gt; &lt;li&gt;Based on Qwen2.5-1.5B&lt;/li&gt; &lt;li&gt;7B variant &amp;quot;coming soon&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0f4hh</id>
    <title>Trying to run offline LLM+RAG feels impossible. What am I doing wrong?</title>
    <updated>2025-08-26T07:12:51+00:00</updated>
    <author>
      <name>/u/caprazli</name>
      <uri>https://old.reddit.com/user/caprazli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been banging my head against the wall trying to get a simple offline LLM+RAG setup running on my laptop (which is plenty powerful). The idea was just a proof of concept: local model + retrieval, able to handle MS Office docs, PDFs, &lt;strong&gt;and&lt;/strong&gt; (that's important) even .eml files.&lt;/p&gt; &lt;p&gt;Instead, it‚Äôs been an absolute nightmare. Nothing works out of the box. Every ‚Äúsolution‚Äù I try turns into endless code-patching across multiple platforms. Half the guides are outdated, half the repos are broken, and when I finally get something running, it chokes on the files I actually need.&lt;/p&gt; &lt;p&gt;I‚Äôm not a total beginner yet I‚Äôm definitely not an expert either. Still, I feel like the bar to entry here is ridiculously high. AI is fantastic for writing, summarizing, and all the fancy cloud-based stuff, but when it comes to coding and local setups, reliability is just‚Ä¶ not there yet.&lt;/p&gt; &lt;p&gt;Am I doing something completely wrong? Does anyone else have similar experiences? Because honestly, &lt;strong&gt;AI might be ‚Äútaking over the world,‚Äù but it‚Äôs definitely&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;taking over my computer&lt;/strong&gt;. It simply cannot.&lt;/p&gt; &lt;p&gt;Curious to hear from others. What‚Äôs your experience with local LLM+RAG setups? Any success stories or lessons learned?&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: U7-155H | 32G | 2T | Arc+NPU | W11: Should theoretically be enough to run local LLMs with big context, chew through Office/PDF/&lt;/em&gt;&lt;strong&gt;&lt;em&gt;.eml&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;docs, and push AI-native pipelines with NPU boost, yet...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caprazli"&gt; /u/caprazli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T07:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dl84</id>
    <title>Been working on something... A teaser</title>
    <updated>2025-08-26T05:35:04+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt; &lt;img alt="Been working on something... A teaser" src="https://b.thumbs.redditmedia.com/gJsXHozTMr8Q_MnsobeoZsA017tBJOzE3DqbO7Z1inw.jpg" title="Been working on something... A teaser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty excited about this project i have been working on lately, be back soon with more info, but in the meantime thought a teaser wouldn't hurt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n0dl84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0haub</id>
    <title>I pre-trained Gemma3 270m entirely from scratch</title>
    <updated>2025-08-26T09:36:43+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained Gemma3 270m entirely from scratch" src="https://external-preview.redd.it/BE2F9tVIKL9AN2T5zS4Z4ig6RgU9hM-QoHxWkSh5XTQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6fc22120dd0f86f8e67b629bd0ad915a09ad61" title="I pre-trained Gemma3 270m entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/9tmq5sa73clf1.gif"&gt;https://i.redd.it/9tmq5sa73clf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a video on this topic here: &lt;a href="https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB"&gt;https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in this video: &lt;/p&gt; &lt;p&gt;(1) Introduction&lt;/p&gt; &lt;p&gt;(2) Dataset loading&lt;/p&gt; &lt;p&gt;(3) Tokenisation&lt;/p&gt; &lt;p&gt;(4) Creating input-output pairs&lt;/p&gt; &lt;p&gt;(5) Building the Gemma 3 270M architecture&lt;/p&gt; &lt;p&gt;(6) Pre-training&lt;/p&gt; &lt;p&gt;(7) Inference&lt;/p&gt; &lt;p&gt;Attached is a GIF showing my lecture notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T09:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bhd7</id>
    <title>Microsoft VibeVoice TTS : Open-Sourced, Supports 90 minutes speech, 4 distinct speakers at a time</title>
    <updated>2025-08-26T03:36:48+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. &lt;/p&gt; &lt;p&gt;Demo Video : &lt;a href="https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ"&gt;https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0iho2</id>
    <title>LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA</title>
    <updated>2025-08-26T10:48:28+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt; &lt;img alt="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" src="https://preview.redd.it/g8lwztnlfclf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b45eb7eb720e8c27adcd24d4808bef43e5cb8dad" title="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source: &lt;a href="https://arxiv.org/pdf/2508.15884v1"&gt;https://arxiv.org/pdf/2508.15884v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g8lwztnlfclf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
