<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-09T10:51:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l6mg99</id>
    <title>I built an alternative chat client</title>
    <updated>2025-06-08T20:46:34+00:00</updated>
    <author>
      <name>/u/Electronic-Metal2391</name>
      <uri>https://old.reddit.com/user/Electronic-Metal2391</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hope you like it.&lt;br /&gt; &lt;a href="https://github.com/ialhabbal/Talk"&gt;ialhabbal/Talk: User-friendly visual chat story editor for writers, and roleplayers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Metal2391"&gt; /u/Electronic-Metal2391 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6mg99/i_built_an_alternative_chat_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6mg99/i_built_an_alternative_chat_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6mg99/i_built_an_alternative_chat_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T20:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6u6rw</id>
    <title>LMStudio and IPEX-LLM</title>
    <updated>2025-06-09T02:56:50+00:00</updated>
    <author>
      <name>/u/slowhandplaya</name>
      <uri>https://old.reddit.com/user/slowhandplaya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is my understanding correct that it's not possible to hook up the IPEX-LLM (Intel optimized llm) into LMStudio? I can't find any documentation that supports this, but some mention that LMStudio uses it's own build of llama.ccp so I can't just replace it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slowhandplaya"&gt; /u/slowhandplaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6u6rw/lmstudio_and_ipexllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6u6rw/lmstudio_and_ipexllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6u6rw/lmstudio_and_ipexllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T02:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l70h9t</id>
    <title>How I Cut Voice Chat Latency by 23% Using Parallel LLM API Calls</title>
    <updated>2025-06-09T09:36:56+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been optimizing my AI voice chat platform for months, and finally found a solution to the most frustrating problem: unpredictable LLM response times killing conversations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Latency Breakdown:&lt;/strong&gt; After analyzing 10,000+ conversations, here's where time actually goes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM API calls: 87.3% (Gemini/OpenAI)&lt;/li&gt; &lt;li&gt;STT (Fireworks AI): 7.2%&lt;/li&gt; &lt;li&gt;TTS (ElevenLabs): 5.5%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The killer insight: while STT and TTS are rock-solid reliable (99.7% within expected latency), LLM APIs are wild cards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reliability Problem (Real Data from My Tests):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I tested 6 different models extensively with my specific prompts (your results may vary based on your use case, but the overall trends and correlations should be similar):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Avg. latency (s)&lt;/th&gt; &lt;th align="left"&gt;Max latency (s)&lt;/th&gt; &lt;th align="left"&gt;Latency / char (s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="3"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.0-flash&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.99&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8.04&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00169&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4o-mini&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.42&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;9.94&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00529&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4o&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;5.94&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;23.72&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00988&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6.21&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;22.24&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00564&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6.10&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.79&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00457&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-pro&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;11.62&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;24.55&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.00876&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;My Production Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was using Gemini 2.5 Flash as my primary model - decent 6.10s average response time, but those 15.79s max latencies were conversation killers. Users don't care about your median response time when they're sitting there for 16 seconds waiting for a reply.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution: Adding GPT-4o in Parallel&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of switching models, I now fire requests to both Gemini 2.5 Flash AND GPT-4o simultaneously, returning whichever responds first.&lt;/p&gt; &lt;p&gt;The logic is simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 Flash: My workhorse, handles most requests&lt;/li&gt; &lt;li&gt;GPT-4o: Despite 5.94s average (slightly faster than Gemini 2.5), it provides redundancy and often beats Gemini on the tail latencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Average latency: 3.7s → 2.84s (23.2% improvement)&lt;/li&gt; &lt;li&gt;P95 latency: 24.7s → 7.8s (68% improvement!)&lt;/li&gt; &lt;li&gt;Responses over 10 seconds: 8.1% → 0.9%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The magic is in the tail - when Gemini 2.5 Flash decides to take 15+ seconds, GPT-4o has usually already responded in its typical 5-6 seconds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;But That Doubles Your Costs!&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yeah, I'm burning 2x tokens now - paying for both Gemini 2.5 Flash AND GPT-4o on every request. Here's why I don't care:&lt;/p&gt; &lt;p&gt;Token prices are in freefall. The LLM API market demonstrates clear price segmentation, with offerings ranging from highly economical models to premium-priced ones.&lt;/p&gt; &lt;p&gt;The real kicker? ElevenLabs TTS costs me 15-20x more per conversation than LLM tokens. I'm optimizing the wrong thing if I'm worried about doubling my cheapest cost component.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why This Works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Different failure modes&lt;/strong&gt;: Gemini and OpenAI rarely have latency spikes at the same time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Redundancy&lt;/strong&gt;: When OpenAI has an outage (3 times last month), Gemini picks up seamlessly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural load balancing&lt;/strong&gt;: Whichever service is less loaded responds faster&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Real Performance Data:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Based on my production metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 Flash wins ~55% of the time (when it's not having a latency spike)&lt;/li&gt; &lt;li&gt;GPT-4o wins ~45% of the time (consistent performer, saves the day during Gemini spikes)&lt;/li&gt; &lt;li&gt;Both models produce comparable quality for my use case&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Added GPT-4o in parallel to my existing Gemini 2.5 Flash setup. Cut latency by 23% and virtually eliminated those conversation-killing 15+ second waits. The 2x token cost is trivial compared to the user experience improvement - users remember the one terrible 24-second wait, not the 99 smooth responses.&lt;/p&gt; &lt;p&gt;Anyone else running parallel inference in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l70h9t/how_i_cut_voice_chat_latency_by_23_using_parallel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l70h9t/how_i_cut_voice_chat_latency_by_23_using_parallel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l70h9t/how_i_cut_voice_chat_latency_by_23_using_parallel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T09:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1l716f4</id>
    <title>5090 liquid cooled build optimization</title>
    <updated>2025-06-09T10:21:26+00:00</updated>
    <author>
      <name>/u/ElekDn</name>
      <uri>https://old.reddit.com/user/ElekDn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, i am building a new pc for me, primarily designed for ML and LLM tasks. I have all the components and would like to get some feedback, i did check if all things work with each other but maybe i missed something or you guys have improvement tips. This is the build:&lt;/p&gt; &lt;p&gt;|| || |AMD Ryzen™️ 9 9950X3D| |MSI GeForce RTX 5090 Suprim Liquid SOC | |NZXT Kraken Elite 420 RGB| |NZXT N9 X870E White AMD X870E| |64GB Kingston FURY Beast RGB weiß DDR5-6000| |2TB Samsung 990 PRO| |NZXT H9 Flow RGB (2025)| |NZXT F Series F120 RGB Core| |NZXT F120 RGB Core Triple Pack - 3 x 120mm| |NZXT C1500 PLATINUM Power Supply - 1500 Watt | ||&lt;/p&gt; &lt;p&gt;I really wanted to have a water cooled 5090 because of the high wattage. First i thought of doing a custom loop but i have no experience in that and it would add another 1000 euros to the build so i will not risk it, however i want to replace the original fans of the gpu radiator with the fans i have in the case. &lt;/p&gt; &lt;p&gt;My biggest worry is the motherboard, it is very expensive for what it is, i would like to stay with nzxt because i like the look and keep the ecosystem. I know they also make the 650E one but i did not find any sellers in EU for that. I am also worried about the pcie 4.0 in that. For gaming it does not really matter at all with just 1-4% fps difference, but for the bandwidth in ML tasks it does seem to matter. If i already have a 5090 with its insane bandwidth i might as well use it with the newer motherboard. &lt;/p&gt; &lt;p&gt;For the fans i will leave the 3 front fans as they are in the case, replace the rear one with the same colored and add the cpu cooler on top and gpu cooler on the bottom. &lt;/p&gt; &lt;p&gt;Thank you for any tips&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElekDn"&gt; /u/ElekDn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l716f4/5090_liquid_cooled_build_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l716f4/5090_liquid_cooled_build_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l716f4/5090_liquid_cooled_build_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T10:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l66b8a</id>
    <title>Apple's new research paper on the limitations of "thinking" models</title>
    <updated>2025-06-08T07:22:03+00:00</updated>
    <author>
      <name>/u/seasonedcurlies</name>
      <uri>https://old.reddit.com/user/seasonedcurlies</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l66b8a/apples_new_research_paper_on_the_limitations_of/"&gt; &lt;img alt="Apple's new research paper on the limitations of &amp;quot;thinking&amp;quot; models" src="https://external-preview.redd.it/5UeYdCFnJOGDfg-kwWIoUmPBZZuPLPogz2CSwjAzY08.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08a86235cd9b365c08749230f9302dd340fba50b" title="Apple's new research paper on the limitations of &amp;quot;thinking&amp;quot; models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seasonedcurlies"&gt; /u/seasonedcurlies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://machinelearning.apple.com/research/illusion-of-thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l66b8a/apples_new_research_paper_on_the_limitations_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l66b8a/apples_new_research_paper_on_the_limitations_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T07:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6t57v</id>
    <title>Do LLMs Reason? Opening the Pod Bay Doors with TiānshūBench 0.0.X</title>
    <updated>2025-06-09T02:02:16+00:00</updated>
    <author>
      <name>/u/JeepyTea</name>
      <uri>https://old.reddit.com/user/JeepyTea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6t57v/do_llms_reason_opening_the_pod_bay_doors_with/"&gt; &lt;img alt="Do LLMs Reason? Opening the Pod Bay Doors with TiānshūBench 0.0.X" src="https://external-preview.redd.it/Uk5moJ9QzjdIluOzdh0oAj___OrgGPUCFPPkE4_jM6M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8900aa9f4fd25153b453ae807e04b9b4f78494c" title="Do LLMs Reason? Opening the Pod Bay Doors with TiānshūBench 0.0.X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently released the results of &lt;a href="https://github.com/JeepyTea/TianShu"&gt;TiānshūBench (天书Bench)&lt;/a&gt; version 0.0.X. This benchmark attempts to measure reasoning and fluid intelligence in LLM systems through programming tasks. A brand new programming language is generated on each test run to help avoid data contamination and find out how well an AI system performs on unique tasks.&lt;/p&gt; &lt;p&gt;Posted the results of 0.0.0 of the test here a couple weeks back, but I've improved the benchmark suite in several ways since then, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;many more tests&lt;/li&gt; &lt;li&gt;multi-shot testing&lt;/li&gt; &lt;li&gt;new LLM models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the 0.0.X of the benchmark, DeepSeek-R1 takes the lead, but still stumbles on a number of pretty basic tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bbmow3pw6t5f1.png?width=2505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6658599a8e87de3e382386d0c1cae6d72c3750"&gt;https://preview.redd.it/bbmow3pw6t5f1.png?width=2505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6658599a8e87de3e382386d0c1cae6d72c3750&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jeepytea.github.io/general/update/2025/06/08/update00x.html"&gt;Read the blog post for an in-depth look at the latest TiānshūBench results.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeepyTea"&gt; /u/JeepyTea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6t57v/do_llms_reason_opening_the_pod_bay_doors_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6t57v/do_llms_reason_opening_the_pod_bay_doors_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6t57v/do_llms_reason_opening_the_pod_bay_doors_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T02:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5wxoa</id>
    <title>My 160GB local LLM rig</title>
    <updated>2025-06-07T22:26:06+00:00</updated>
    <author>
      <name>/u/TrifleHopeful5418</name>
      <uri>https://old.reddit.com/user/TrifleHopeful5418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l5wxoa/my_160gb_local_llm_rig/"&gt; &lt;img alt="My 160GB local LLM rig" src="https://preview.redd.it/qukd2c1lzk5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7a7aa1adf0ea1755b60c2abe92f454649a64f90" title="My 160GB local LLM rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built this monster with 4x V100 and 4x 3090, with the threadripper / 256 GB RAM and 4x PSU. One Psu for power everything in the machine and 3x PSU 1000w to feed the beasts. Used bifurcated PCIE raisers to split out x16 PCIE to 4x x4 PCIEs. Ask me anything, biggest model I was able to run on this beast was qwen3 235B Q4 at around ~15 tokens / sec. Regularly I am running Devstral, qwen3 32B, gamma 3-27B, qwen3 4b x 3….all in Q4 and use async to use all the models at the same time for different tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrifleHopeful5418"&gt; /u/TrifleHopeful5418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qukd2c1lzk5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l5wxoa/my_160gb_local_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l5wxoa/my_160gb_local_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-07T22:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6gc5o</id>
    <title>Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs</title>
    <updated>2025-06-08T16:30:47+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/"&gt; &lt;img alt="Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs" src="https://external-preview.redd.it/cSlpnM-F0ah5HqsHd87BsgMR5_Wvh1LjIwvfZJ0XrTw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2db108555c34df39bc4f1ce9bd54de50c02b280b" title="Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Ruminate: Taking Control of AI Reasoning Speed&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I ran 7,150 prompts through Qwen3-4B-AWQ to try to solve the &amp;quot;fast but wrong vs slow but unpredictable&amp;quot; problem with reasoning AI models and got fascinating results. Built a staged reasoning proxy that lets you dial in exactly the speed-accuracy tradeoff you need.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;Reasoning models like Qwen3 have a brutal tradeoff: turn reasoning off and get 27% accuracy (but fast), or turn it on and get 74% accuracy but completely unpredictable response times. Some requests take 200ms, others take 30+ seconds. That's unusable for production.&lt;/p&gt; &lt;h1&gt;The Solution: Staged Reasoning&lt;/h1&gt; &lt;p&gt;Instead of unlimited thinking time, give the AI a budget with gentle nudges:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Initial Think&lt;/strong&gt;: &amp;quot;Here's your ideal thinking time&amp;quot;&lt;br /&gt; &lt;strong&gt;Soft Warning&lt;/strong&gt;: &amp;quot;Time's getting short, stay focused&amp;quot;&lt;br /&gt; &lt;strong&gt;Hard Warning&lt;/strong&gt;: &amp;quot;Really need to wrap up now&amp;quot;&lt;br /&gt; &lt;strong&gt;Emergency Termination&lt;/strong&gt;: Force completion if all budgets exhausted&lt;/p&gt; &lt;h1&gt;What I Tested&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;4 reasoning tasks&lt;/strong&gt;: geometric shapes, boolean logic, dates, arithmetic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;11 different configurations&lt;/strong&gt; from quick-thinker to big-thinker&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proper statistics&lt;/strong&gt;: 95% confidence intervals to know which results are actually significant vs just noise&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CompletionCost metric&lt;/strong&gt;: tokens needed per 1% accuracy (efficiency tiebreaker)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zj0zzfnbbq5f1.png?width=3570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27e5f6b0732a01f77a0239e7902c53bf90f8f784"&gt;Open Run-time performance scaling: It's possible after all!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🎯 It works&lt;/strong&gt;: Staged reasoning successfully trades accuracy for predictability&lt;/p&gt; &lt;p&gt;&lt;strong&gt;📊 Big Thinker&lt;/strong&gt;: 77% accuracy, recovers 93% of full reasoning performance while cutting worst-case response time in half&lt;/p&gt; &lt;p&gt;&lt;strong&gt;⚡ Quick Thinker&lt;/strong&gt;: 59% accuracy, still 72% of full performance but 82% faster&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🤔 Budget allocation surprise&lt;/strong&gt;: How you split your token budget matters less than total budget size (confidence intervals overlap for most medium configs)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;📈 Task-specific patterns&lt;/strong&gt;: Boolean logic needs upfront thinking, arithmetic needs generous budgets, date problems are efficient across all configs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;❌ Hypothesis busted&lt;/strong&gt;: I thought termination rate would predict poor performance. Nope! The data completely disagreed with me - science is humbling.&lt;/p&gt; &lt;p&gt;Lots of additional details on the tasks, methodologies and results are in the mini-paper: &lt;a href="https://github.com/the-crypt-keeper/ChatBench/blob/main/ruminate/PAPER.md"&gt;https://github.com/the-crypt-keeper/ChatBench/blob/main/ruminate/PAPER.md&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Real Impact&lt;/h1&gt; &lt;p&gt;This transforms reasoning models from research toys into practical tools. Instead of &amp;quot;fast but wrong&amp;quot; or &amp;quot;accurate but unpredictable,&amp;quot; you get exactly the speed-accuracy tradeoff your app needs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical configs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time-critical: 72% of full performance, 82% speed boost&lt;/li&gt; &lt;li&gt;Balanced: 83% of performance, 60% speed boost&lt;/li&gt; &lt;li&gt;Accuracy-focused: 93% of performance, 50% speed boost&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implementation Detail&lt;/h1&gt; &lt;p&gt;The proxy accepts a &lt;code&gt;reason_control=[x,y,z]&lt;/code&gt; parameter controlling token budgets for Initial Think, Soft Warning, and Hard Warning stages respectively. It sits between your app and the model, making multiple completion calls and assembling responses transparently.&lt;/p&gt; &lt;h1&gt;Try It&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Full dataset, analysis, and experimental setup in the repo. Science works best when it's reproducible - replications welcome!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Code at &lt;a href="https://github.com/the-crypt-keeper/ChatBench/tree/main/ruminate"&gt;https://github.com/the-crypt-keeper/ChatBench/tree/main/ruminate&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full result dataset at &lt;a href="https://github.com/the-crypt-keeper/ChatBench/tree/main/ruminate/results"&gt;https://github.com/the-crypt-keeper/ChatBench/tree/main/ruminate/results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mini-paper analyzing the results at &lt;a href="https://github.com/the-crypt-keeper/ChatBench/blob/main/ruminate/PAPER.md"&gt;https://github.com/the-crypt-keeper/ChatBench/blob/main/ruminate/PAPER.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Experimental research code, subject to change!&lt;/p&gt; &lt;p&gt;Built this on dual RTX 3090s in my basement testing Qwen3-4B. Would love to see how patterns hold across different models and hardware&lt;strong&gt;. Everything is open source, these results can be reproduced on even a single 3060.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The beauty isn't just that staged reasoning works - it's that we can now systematically map the speed-accuracy tradeoff space with actual statistical rigor. No more guessing; we have confidence intervals and proper math backing every conclusion.&lt;/p&gt; &lt;h1&gt;Future Work&lt;/h1&gt; &lt;p&gt;More tasks, more samples (for better statistics), bigger models, Non-Qwen3 Reasoning Model Families the possibilities for exploration are endless. Hop into the GitHub and open an issue if you have interesting ideas or results to share!&lt;/p&gt; &lt;h1&gt;ChatBench&lt;/h1&gt; &lt;p&gt;I am the author of the Can-Ai-Code test suite and as you may have noticed, I am cooking up a new, cross-task test suite based on BigBenchHard that I'm calling &lt;a href="https://github.com/the-crypt-keeper/ChatBench/tree/main"&gt;ChatBench&lt;/a&gt;. This is just one of the many interesting outcomes from this work - stay tuned for more posts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T16:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6xo6e</id>
    <title>Low token per second on RTX5070Ti laptop with phi 4 reasoning plus</title>
    <updated>2025-06-09T06:26:22+00:00</updated>
    <author>
      <name>/u/PeaResponsible8685</name>
      <uri>https://old.reddit.com/user/PeaResponsible8685</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heya folks,&lt;/p&gt; &lt;p&gt;I'm running phi 4 reasoning plus and I'm encountering some issues. &lt;/p&gt; &lt;p&gt;Per the research that I did on the internet, generally rtx5070ti laptop gpu offers ~=150 tokens per second&lt;br /&gt; However mines only about 30ish token per second.&lt;/p&gt; &lt;p&gt;I've already maxed out the GPU offload option, so far no help.&lt;br /&gt; Any ideas on how to fix this would be appreciated, many thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeaResponsible8685"&gt; /u/PeaResponsible8685 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6xo6e/low_token_per_second_on_rtx5070ti_laptop_with_phi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6xo6e/low_token_per_second_on_rtx5070ti_laptop_with_phi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6xo6e/low_token_per_second_on_rtx5070ti_laptop_with_phi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T06:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l71iie</id>
    <title>Concept graph workflow in Open WebUI</title>
    <updated>2025-06-09T10:41:50+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"&gt; &lt;img alt="Concept graph workflow in Open WebUI" src="https://external-preview.redd.it/aXB5aHN0YTlydjVmMUDafYoOCCtYLjNpQgqDHTqQwNrdDTG86AmqQ0wIdIFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c2a7c9d31f1ad19416b2226d54eabeba40a3068" title="Concept graph workflow in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning workflow where LLM thinks about the concepts that are related to the User's query and then makes a final answer based on that&lt;/li&gt; &lt;li&gt;Workflow runs within OpenAI-compatible LLM proxy. It streams a special HTML artifact that connects back to the workflow and listens for events from it to display in the visualisation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/concept.py#L135"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dzeqvwa9rv5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l71iie/concept_graph_workflow_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T10:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6nof7</id>
    <title>Introducing llamate, a ollama-like tool to run and manage your local AI models easily</title>
    <updated>2025-06-08T21:40:04+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6nof7/introducing_llamate_a_ollamalike_tool_to_run_and/"&gt; &lt;img alt="Introducing llamate, a ollama-like tool to run and manage your local AI models easily" src="https://external-preview.redd.it/iaWMP1rzf1F_qp7nDaWaKocpdiNkgusjXw__ayLPW1A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdf0c30ddbcce0adbc95095f30c00d12a79c87b7" title="Introducing llamate, a ollama-like tool to run and manage your local AI models easily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am sharing my second iteration of a &amp;quot;ollama-like&amp;quot; tool, which is targeted at people like me and many others who like running the llama-server directly. This time I am building on the creation of llama-swap and llama.cpp, making it truly distributed and open source. It started with &lt;a href="https://github.com/R-Dson/llama-server-cli.py/tree/main"&gt;this&lt;/a&gt; tool, which worked okay-ish. However, after looking at llama-swap I thought it accomplished a lot of similar things, but it could become something more, so I started a discussion &lt;a href="https://github.com/mostlygeek/llama-swap/issues/153"&gt;here&lt;/a&gt; which was very useful and a lot of great points were brought up. After that I started this project instead, which manages all config files, model files and gguf files easily in the terminal.&lt;/p&gt; &lt;p&gt;Introducing &lt;a href="https://github.com/R-Dson/llamate"&gt;llamate&lt;/a&gt; (llama+mate), a simple &amp;quot;ollama-like&amp;quot; tool for managing and running GGUF language models from your terminal. It supports the typical API endpoints and ollama specific endpoints. If you know how to run ollama, you can most likely drop in replace this tool. Just make sure you got the drivers installed to run llama.cpp's llama-server. Currently, it only support Linux and Nvidia/CUDA by default. If you can compile llama-server for your own hardware, then you can simply replace the llama-server file.&lt;/p&gt; &lt;p&gt;Currently it works like this, I have set up two additional repos that the tool uses to manage the binaries:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/R-Dson/llama-server-compile"&gt;R-Dson/llama-server-compile&lt;/a&gt; is used to daily compile the CUDA version of llama-server.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/R-Dson/llama-swap"&gt;R-Dson/llama-swap&lt;/a&gt; is used to compile the llama-swap file with patches for ollama endpoint support.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These compiled binaries are used to run llama-swap and llama-server. This still need some testing and there will probably be bugs, but from my testing it seems to work fine so far.&lt;/p&gt; &lt;p&gt;To get start, it can be downloaded using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/R-Dson/llamate/main/install.sh | bash &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Feel free to read through the file first (as you should before running any script).&lt;/p&gt; &lt;p&gt;And the tool can be simply used like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Init the tool to download the binaries llamate init # Add and download a model llamate add llama3:8b llamate pull llama3:8b # To start llama-swap with your models automatically configured llamate serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can checkout &lt;a href="https://github.com/R-Dson/llamate/blob/main/llamate/data/model_aliases.py"&gt;this&lt;/a&gt; file for more aliases or checkout the repo for instructions of how to add a model from huggingface directly. I hope this tool will help with easily running models locally for your all!&lt;/p&gt; &lt;p&gt;Leave a comment or open an issue to start a discussion or leave feedback.&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/R-Dson/llamate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6nof7/introducing_llamate_a_ollamalike_tool_to_run_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6nof7/introducing_llamate_a_ollamalike_tool_to_run_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T21:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1l68m1m</id>
    <title>Confirmation that Qwen3-coder is in works</title>
    <updated>2025-06-08T10:02:00+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Junyang Lin from Qwen team &lt;a href="https://youtu.be/b0xlsQ_6wUQ?t=985"&gt;mentioned this here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l68m1m/confirmation_that_qwen3coder_is_in_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l68m1m/confirmation_that_qwen3coder_is_in_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l68m1m/confirmation_that_qwen3coder_is_in_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T10:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l67afp</id>
    <title>Rig upgraded to 8x3090</title>
    <updated>2025-06-08T08:29:06+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l67afp/rig_upgraded_to_8x3090/"&gt; &lt;img alt="Rig upgraded to 8x3090" src="https://preview.redd.it/7ios74ratn5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=246fae63e33b93df4bd41453232e98b5a716bad9" title="Rig upgraded to 8x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About 1 year ago I posted about a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1bqxfc0/another_4x3090_build/"&gt;4 x 3090 build&lt;/a&gt;. This machine has been great for learning to fine-tune LLMs and produce synthetic data-sets. However, even with deepspeed and 8B models, the maximum training full fine-tune context length was about 2560 tokens per conversation. Finally I decided to get some 16-&amp;gt;8x8 lane splitters, some more GPUs and some more RAM. Training Qwen/Qwen3-8B (full fine-tune) with 4K context length completed success fully and without pci errors, and I am happy with the build. The spec is like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock Rack EP2C622D16-2T&lt;/li&gt; &lt;li&gt;8xRTX 3090 FE (192 GB VRAM total)&lt;/li&gt; &lt;li&gt;Dual Intel Xeon 8175M&lt;/li&gt; &lt;li&gt;512 GB DDR4 2400&lt;/li&gt; &lt;li&gt;EZDIY-FAB PCIE Riser cables&lt;/li&gt; &lt;li&gt;Unbranded Alixpress PCIe-Bifurcation 16X to x8x8 &lt;/li&gt; &lt;li&gt;Unbranded Alixpress open chassis&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As the lanes are now split, each GPU has about half the bandwidth. Even if training takes a bit longer, being able to full fine tune to a longer context window is worth it in my opinion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7ios74ratn5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l67afp/rig_upgraded_to_8x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l67afp/rig_upgraded_to_8x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T08:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6zmxk</id>
    <title>UPDATE: Mission to make AI agents affordable - Tool Calling with DeepSeek-R1-0528 using LangChain/LangGraph is HERE!</title>
    <updated>2025-06-09T08:39:53+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've successfully implemented tool calling support for the newly released DeepSeek-R1-0528 model using my TAoT package with the LangChain/LangGraph frameworks! &lt;/p&gt; &lt;p&gt;What's New in This Implementation: As DeepSeek-R1-0528 has gotten smarter than its predecessor DeepSeek-R1, more concise prompt tweaking update was required to make my TAoT package work with DeepSeek-R1-0528 ➔ If you had previously downloaded my package, please perform an update&lt;/p&gt; &lt;p&gt;Why This Matters for Making AI Agents Affordable: ✅ Performance: DeepSeek-R1-0528 matches or slightly trails OpenAI's o4-mini (high) in benchmarks. ✅ Cost: 2x cheaper than OpenAI's o4-mini (high) - because why pay more for similar performance?&lt;/p&gt; &lt;p&gt;𝐼𝑓 𝑦𝑜𝑢𝑟 𝑝𝑙𝑎𝑡𝑓𝑜𝑟𝑚 𝑖𝑠𝑛'𝑡 𝑔𝑖𝑣𝑖𝑛𝑔 𝑐𝑢𝑠𝑡𝑜𝑚𝑒𝑟𝑠 𝑎𝑐𝑐𝑒𝑠𝑠 𝑡𝑜 𝐷𝑒𝑒𝑝𝑆𝑒𝑒𝑘-𝑅1-0528, 𝑦𝑜𝑢'𝑟𝑒 𝑚𝑖𝑠𝑠𝑖𝑛𝑔 𝑎 ℎ𝑢𝑔𝑒 𝑜𝑝𝑝𝑜𝑟𝑡𝑢𝑛𝑖𝑡𝑦 𝑡𝑜 𝑒𝑚𝑝𝑜𝑤𝑒𝑟 𝑡ℎ𝑒𝑚 𝑤𝑖𝑡ℎ 𝑎𝑓𝑓𝑜𝑟𝑑𝑎𝑏𝑙𝑒, 𝑐𝑢𝑡𝑡𝑖𝑛𝑔-𝑒𝑑𝑔𝑒 𝐴𝐼!&lt;/p&gt; &lt;p&gt;Check out my updated GitHub repos and please give them a star if this was helpful ⭐&lt;/p&gt; &lt;p&gt;Python TAoT package: &lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;JavaScript/TypeScript TAoT package: &lt;a href="https://github.com/leockl/tool-ahead-of-time-ts"&gt;https://github.com/leockl/tool-ahead-of-time-ts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6zmxk/update_mission_to_make_ai_agents_affordable_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6zmxk/update_mission_to_make_ai_agents_affordable_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6zmxk/update_mission_to_make_ai_agents_affordable_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T08:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6wxau</id>
    <title>Tokenizing research papers for Fine-tuning</title>
    <updated>2025-06-09T05:37:44+00:00</updated>
    <author>
      <name>/u/200ok-N1M0-found</name>
      <uri>https://old.reddit.com/user/200ok-N1M0-found</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a bunch of research papers of my field and want to use them to make a specific fine-tuned LLM for the domain.&lt;/p&gt; &lt;p&gt;How would i start tokenizing the research papers, as i would need to handle equations, tables and citations. (later planning to use the citations and references with RAG)&lt;/p&gt; &lt;p&gt;any help regarding this would be greatly appreciated !!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/200ok-N1M0-found"&gt; /u/200ok-N1M0-found &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6wxau/tokenizing_research_papers_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6wxau/tokenizing_research_papers_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6wxau/tokenizing_research_papers_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T05:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6w1wb</id>
    <title>I've built an AI agent that recursively decomposes a task and executes it, and I'm looking for suggestions.</title>
    <updated>2025-06-09T04:43:36+00:00</updated>
    <author>
      <name>/u/Pretend_Guava7322</name>
      <uri>https://old.reddit.com/user/Pretend_Guava7322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I've been working on a project I have temporarily named LLM Agent X, and I'm looking for feedback and ideas. The basic idea of the project is that it takes a task, and recursively splits it into smaller chunks, and eventually executes the tasks with an LLM and tools provided by the user. This is my first python project that I am making open source, so any suggestions are welcome. It currently uses LangChain, but if you have any other suggestions that make drop-in replacement of LLM's easy, I would love to hear them.&lt;/p&gt; &lt;p&gt;Here is the GitHub repo: &lt;a href="https://github.com/cvaz1306/llm_agent_x.git"&gt;https://github.com/cvaz1306/llm_agent_x.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear any of your ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretend_Guava7322"&gt; /u/Pretend_Guava7322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6w1wb/ive_built_an_ai_agent_that_recursively_decomposes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6w1wb/ive_built_an_ai_agent_that_recursively_decomposes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6w1wb/ive_built_an_ai_agent_that_recursively_decomposes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T04:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6ss2b</id>
    <title>Qwen3-Embedding-0.6B ONNX model with uint8 output</title>
    <updated>2025-06-09T01:43:40+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ss2b/qwen3embedding06b_onnx_model_with_uint8_output/"&gt; &lt;img alt="Qwen3-Embedding-0.6B ONNX model with uint8 output" src="https://external-preview.redd.it/MNsCzxHCPEDwtS2HUsD8JvnZOJLf50yfe6X06TlL_fA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd440f673ff4e766b0a429592c6b866869c5dc3b" title="Qwen3-Embedding-0.6B ONNX model with uint8 output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/electroglyph/Qwen3-Embedding-0.6B-onnx-uint8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ss2b/qwen3embedding06b_onnx_model_with_uint8_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ss2b/qwen3embedding06b_onnx_model_with_uint8_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T01:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6lp8x</id>
    <title>Llama3 is better than Llama4.. is this anyone else's experience?</title>
    <updated>2025-06-08T20:14:08+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend a lot of time using cheaper/faster LLMs when possible via paid inference API's. If I'm working on a microservice I'll gladly use Llama3.3 70B or Llama4 Maverick than the more expensive Deepseek. It generally goes very well.&lt;/p&gt; &lt;p&gt;And I came to an upsetting realization that, for all of my use cases, Llama3.3 70B and Llama3.1 405B perform better than Llama4 Maverick 400B. There are less bugs, less oversights, less silly mistakes, less editing-instruction-failures (Aider and Roo-Code, primarily). The benefit of Llama4 is that the MoE and smallish experts make it run at lightspeed, but the time savings are lost as soon as I need to figure out its silly mistakes.&lt;/p&gt; &lt;p&gt;Is anyone else having a similar experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6lp8x/llama3_is_better_than_llama4_is_this_anyone_elses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6lp8x/llama3_is_better_than_llama4_is_this_anyone_elses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6lp8x/llama3_is_better_than_llama4_is_this_anyone_elses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T20:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1l69w7i</id>
    <title>I Built 50 AI Personalities - Here's What Actually Made Them Feel Human</title>
    <updated>2025-06-08T11:25:12+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past 6 months, I've been obsessing over what makes AI personalities feel authentic vs robotic. After creating and testing 50 different personas for an AI audio platform I'm developing, here's what actually works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt; Each persona had unique voice, background, personality traits, and response patterns. Users could interrupt and chat with them during content delivery. Think podcast host that actually responds when you yell at them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Failed Spectacularly:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;❌ &lt;strong&gt;Over-engineered backstories&lt;/strong&gt; I wrote a 2,347-word biography for &amp;quot;Professor Williams&amp;quot; including his childhood dog's name, his favorite coffee shop in grad school, and his mother's maiden name. Users found him insufferable. Turns out, knowing too much makes characters feel scripted, not authentic.&lt;/p&gt; &lt;p&gt;❌ &lt;strong&gt;Perfect consistency&lt;/strong&gt; &amp;quot;Sarah the Life Coach&amp;quot; never forgot a detail, never contradicted herself, always remembered exactly what she said 3 conversations ago. Users said she felt like a &amp;quot;customer service bot with a name.&amp;quot; Humans aren't databases.&lt;/p&gt; &lt;p&gt;❌ &lt;strong&gt;Extreme personalities&lt;/strong&gt; &amp;quot;MAXIMUM DEREK&amp;quot; was always at 11/10 energy. &amp;quot;Nihilist Nancy&amp;quot; was perpetually depressed. Both had engagement drop to zero after about 8 minutes. One-note personalities are exhausting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Magic Formula That Emerged:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The 3-Layer Personality Stack&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Take &amp;quot;Marcus the Midnight Philosopher&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Core trait (40%)&lt;/strong&gt;: Analytical thinker&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modifier (35%)&lt;/strong&gt;: Expresses through food metaphors (former chef)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quirk (25%)&lt;/strong&gt;: Randomly quotes 90s R&amp;amp;B lyrics mid-explanation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This formula created depth without overwhelming complexity. Users remembered Marcus as &amp;quot;the chef guy who explains philosophy&amp;quot; not &amp;quot;the guy with 47 personality traits.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Imperfection Patterns&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The most &amp;quot;human&amp;quot; moment came when a history professor persona said: &amp;quot;The treaty was signed in... oh god, I always mix this up... 1918? No wait, 1919. Definitely 1919. I think.&amp;quot;&lt;/p&gt; &lt;p&gt;That single moment of uncertainty got more positive feedback than any perfectly delivered lecture.&lt;/p&gt; &lt;p&gt;Other imperfections that worked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Where was I going with this? Oh right...&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;That's a terrible analogy, let me try again&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;I might be wrong about this, but...&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. The Context Sweet Spot&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's the exact formula that worked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background (300-500 words):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 formative experiences: One positive (&amp;quot;won a science fair&amp;quot;), one challenging (&amp;quot;struggled with public speaking&amp;quot;)&lt;/li&gt; &lt;li&gt;Current passion: Something specific (&amp;quot;collects vintage synthesizers&amp;quot; not &amp;quot;likes music&amp;quot;)&lt;/li&gt; &lt;li&gt;1 vulnerability: Related to their expertise (&amp;quot;still gets nervous explaining quantum physics despite PhD&amp;quot;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example that worked: &amp;quot;Dr. Chen grew up in Seattle, where rainy days in her mother's bookshop sparked her love for sci-fi. Failed her first physics exam at MIT, almost quit, but her professor said 'failure is just data.' Now explains astrophysics through Star Wars references. Still can't parallel park despite understanding orbital mechanics.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why This Matters:&lt;/strong&gt; Users referenced these background details 73% of the time when asking follow-up questions. It gave them hooks for connection. &amp;quot;Wait, you can't parallel park either?&amp;quot;&lt;/p&gt; &lt;p&gt;The magic isn't in making perfect AI personalities. It's in making imperfect ones that feel genuinely flawed in specific, relatable ways.&lt;/p&gt; &lt;p&gt;Anyone else experimenting with AI personality design? What's your approach to the authenticity problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l69w7i/i_built_50_ai_personalities_heres_what_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l69w7i/i_built_50_ai_personalities_heres_what_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l69w7i/i_built_50_ai_personalities_heres_what_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T11:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6tnpl</id>
    <title>Kwaipilot/KwaiCoder-AutoThink-preview · Hugging Face</title>
    <updated>2025-06-09T02:28:57+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6tnpl/kwaipilotkwaicoderautothinkpreview_hugging_face/"&gt; &lt;img alt="Kwaipilot/KwaiCoder-AutoThink-preview · Hugging Face" src="https://external-preview.redd.it/38M7n8DVgopQFP924_Qhb022t0M5Y6Vl0qL8kqb9HPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c90ed2b61a8460b236c79f3c92809607183b0676" title="Kwaipilot/KwaiCoder-AutoThink-preview · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not tested yet. A notable feature:&lt;/p&gt; &lt;p&gt;&lt;em&gt;The model merges thinking and non‑thinking abilities into a single checkpoint and dynamically adjusts its reasoning depth based on the input’s difficulty.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-AutoThink-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6tnpl/kwaipilotkwaicoderautothinkpreview_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6tnpl/kwaipilotkwaicoderautothinkpreview_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T02:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6x91g</id>
    <title>Use Ollama to run agents that watch your screen! (100% Local and Open Source)</title>
    <updated>2025-06-09T05:58:30+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"&gt; &lt;img alt="Use Ollama to run agents that watch your screen! (100% Local and Open Source)" src="https://external-preview.redd.it/YjkwOGhtajRkdTVmMZ0cZOsTXi-ThTayE7iEfGGYXF4Z17hX-7dpetBO2beo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f5e81cd85f9cc73982ab5326f6dce727d4078f2" title="Use Ollama to run agents that watch your screen! (100% Local and Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tysofmj4du5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6x91g/use_ollama_to_run_agents_that_watch_your_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T05:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6urvw</id>
    <title>Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck...</title>
    <updated>2025-06-09T03:29:08+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"&gt; &lt;img alt="Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck..." src="https://external-preview.redd.it/cnVvdWR3c2RtdDVmMY0aYliuaUJ6RykjdFncok76V91JG_1sGT9Nkds3i_jF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58084eea8db99337bb3ecabbca254f7e62fa7193" title="Gemini 2.5 Flash plays Final Fantasy in real-time but gets stuck..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some more clips of frontier VLMs on games (gemini-2.5-flash-preview-04-17) on &lt;a href="https://www.vgbench.com/"&gt;VideoGameBench&lt;/a&gt;. Here is just unedited footage, where the model is able to defeat the first &amp;quot;mini-boss&amp;quot; with real-time combat but also gets stuck in the menu screens, despite having it in its prompt how to get out.&lt;/p&gt; &lt;p&gt;Generated from &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt; and recorded on OBS.&lt;/p&gt; &lt;p&gt;tldr; we're still pretty far from embodied intelligence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kun6x1tdmt5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6urvw/gemini_25_flash_plays_final_fantasy_in_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T03:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6vc8u</id>
    <title>I made the move and I'm in love. RTX Pro 6000 Workstation</title>
    <updated>2025-06-09T04:01:23+00:00</updated>
    <author>
      <name>/u/Demonicated</name>
      <uri>https://old.reddit.com/user/Demonicated</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"&gt; &lt;img alt="I made the move and I'm in love. RTX Pro 6000 Workstation" src="https://preview.redd.it/7uu5ooyast5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1dd757acde7feb8ae7c2f694103a8cab2dbaab8c" title="I made the move and I'm in love. RTX Pro 6000 Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're running a workload that's processing millions of records and analyzing using Magentic One (autogen) and the 4090 just want cutting it. With the way scalpers are preying on would be 5090 owners, it was much easier to pick one of these up. Plus significantly less wattage. Just posting cause I'm super excited. &lt;/p&gt; &lt;p&gt;What's the best tool model I can run with this bad boy? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Demonicated"&gt; /u/Demonicated &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7uu5ooyast5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6vc8u/i_made_the_move_and_im_in_love_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T04:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6v37m</id>
    <title>1.93bit Deepseek R1 0528 beats Claude Sonnet 4</title>
    <updated>2025-06-09T03:46:57+00:00</updated>
    <author>
      <name>/u/BumblebeeOk3281</name>
      <uri>https://old.reddit.com/user/BumblebeeOk3281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1.93bit Deepseek R1 0528 beats Claude Sonnet 4 (no think) on Aiders Polygot Benchmark. Unsloth's IQ1_M GGUF at 200GB fit with 65535 context into 224gb of VRAM and scored 60% which is over Claude 4's &amp;lt;no think&amp;gt; benchmark of 56.4%. Source: &lt;a href="https://aider.chat/docs/leaderboards/"&gt;https://aider.chat/docs/leaderboards/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;── tmp.benchmarks/2025-06-07-17-01-03--R1-0528-IQ1_M ─- dirname: 2025-06-07-17-01-03--R1-0528-IQ1_M&lt;/p&gt; &lt;p&gt;test_cases: 225&lt;/p&gt; &lt;p&gt;model: unsloth/DeepSeek-R1-0528-GGUF&lt;/p&gt; &lt;p&gt;edit_format: diff&lt;/p&gt; &lt;p&gt;commit_hash: 4c161f9&lt;/p&gt; &lt;p&gt;pass_rate_1: 25.8&lt;/p&gt; &lt;p&gt;pass_rate_2: 60.0&lt;/p&gt; &lt;p&gt;pass_num_1: 58&lt;/p&gt; &lt;p&gt;pass_num_2: 135&lt;/p&gt; &lt;p&gt;percent_cases_well_formed: 96.4&lt;/p&gt; &lt;p&gt;error_outputs: 9&lt;/p&gt; &lt;p&gt;num_malformed_responses: 9&lt;/p&gt; &lt;p&gt;num_with_malformed_responses: 8&lt;/p&gt; &lt;p&gt;user_asks: 104&lt;/p&gt; &lt;p&gt;lazy_comments: 0&lt;/p&gt; &lt;p&gt;syntax_errors: 0&lt;/p&gt; &lt;p&gt;indentation_errors: 0&lt;/p&gt; &lt;p&gt;exhausted_context_windows: 0&lt;/p&gt; &lt;p&gt;prompt_tokens: 2733132&lt;/p&gt; &lt;p&gt;completion_tokens: 2482855&lt;/p&gt; &lt;p&gt;test_timeouts: 6&lt;/p&gt; &lt;p&gt;total_tests: 225&lt;/p&gt; &lt;p&gt;command: aider --model unsloth/DeepSeek-R1-0528-GGUF&lt;/p&gt; &lt;p&gt;date: 2025-06-07&lt;/p&gt; &lt;p&gt;versions: &lt;a href="http://0.84.1.dev"&gt;0.84.1.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;seconds_per_case: 527.8&lt;/p&gt; &lt;p&gt;./build/bin/llama-server --model unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_M/DeepSeek-R1-0528-UD-IQ1_M-00001-of-00005.gguf --threads 16 --n-gpu-layers 507 --prio 3 --temp 0.6 --top_p 0.95 --min-p 0.01 --ctx-size 65535 --host 0.0.0.0 --host 0.0.0.0 --tensor-split 0.55,0.15,0.16,0.06,0.11,0.12 -fa&lt;/p&gt; &lt;p&gt;Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 1: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 2: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;Device 3: NVIDIA GeForce RTX 4080, compute capability 8.9, VMM: yes&lt;/p&gt; &lt;p&gt;Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/p&gt; &lt;p&gt;Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumblebeeOk3281"&gt; /u/BumblebeeOk3281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6v37m/193bit_deepseek_r1_0528_beats_claude_sonnet_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T03:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6ibwg</id>
    <title>When you figure out it’s all just math:</title>
    <updated>2025-06-08T17:53:48+00:00</updated>
    <author>
      <name>/u/Current-Ticket4214</name>
      <uri>https://old.reddit.com/user/Current-Ticket4214</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt; &lt;img alt="When you figure out it’s all just math:" src="https://preview.redd.it/t7ko9eywrq5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82581f5bc2e1251bb77594995cdd04eccde6717a" title="When you figure out it’s all just math:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Ticket4214"&gt; /u/Current-Ticket4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t7ko9eywrq5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-08T17:53:48+00:00</published>
  </entry>
</feed>
