<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-17T21:34:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jdle9r</id>
    <title>Self host Safely</title>
    <updated>2025-03-17T19:28:23+00:00</updated>
    <author>
      <name>/u/Old-Wind-6437</name>
      <uri>https://old.reddit.com/user/Old-Wind-6437</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is a safe beginner friendly way to host various AI models and tools interest in the ones mostly out of china that have browsers control and can execute task themselves. I dont know what I dont know and I worry about using the same computer for various bleeding edge AI and my taxes or logging into my bank account.&lt;/p&gt; &lt;p&gt;Should I use a different computer or set up a different user account on the computer? I tried using windows hyper-v but could not get my gpu to work in the VM i housed the AI. Thank you in advance for your thought on this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Wind-6437"&gt; /u/Old-Wind-6437 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdle9r/self_host_safely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdle9r/self_host_safely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdle9r/self_host_safely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T19:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdhbs1</id>
    <title>Why do "thinking" LLMs sound so schizophrenic?</title>
    <updated>2025-03-17T16:47:39+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whenever I try the Deepseek or QwQ models, I am very surprised about how haphazard the whole thinking process seems. This whole inner monologue approach doesn't make much sense to me and puts me off from using them and trusting them to produce solid results. &lt;/p&gt; &lt;p&gt;I understand that an LLM is pretty much like a person who can only think by speaking out loud, but I would imagine that these LLMs could produce a lot better results (and I'd definitely trust them a lot more) if their thinking was following some structure and logic instead of the random &amp;quot;But wait&amp;quot;s every couple of paragraphs. &lt;/p&gt; &lt;p&gt;Can someone point me to some explanations about why they work this way? If I understand correctly, the &amp;quot;thinking&amp;quot; part is a result of finetuning and I do not quite understand why would researchers not use more structured &amp;quot;thinking&amp;quot; data for this task. Are there any examples of LLMs that utilise more structure in their &amp;quot;thinking&amp;quot; part?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdhbs1/why_do_thinking_llms_sound_so_schizophrenic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdhbs1/why_do_thinking_llms_sound_so_schizophrenic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdhbs1/why_do_thinking_llms_sound_so_schizophrenic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdnjq9</id>
    <title>Improved realtime console with support for open-source speech-to-speech models</title>
    <updated>2025-03-17T20:55:11+00:00</updated>
    <author>
      <name>/u/heidihobo</name>
      <uri>https://old.reddit.com/user/heidihobo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! We’re a small dev team working on serving speech-to-speech models. Recently, we modified OpenAI’s realtime console to support more realtime speech models. We’ve added &lt;code&gt;miniCPM-O&lt;/code&gt; with support coming for more models in the future (suggestions welcome!). It already supports realtime API.&lt;/p&gt; &lt;p&gt;Check out here: &lt;a href="https://github.com/outspeed-ai/voice-devtools/"&gt;https://github.com/outspeed-ai/voice-devtools/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We added a few neat features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;cost calculation (since speech-to-speech models are still expensive)&lt;/li&gt; &lt;li&gt;session tracking (for models hosted by us)&lt;/li&gt; &lt;li&gt;Unlimited call duration&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We’re actively working on adding more capable open-source speech to speech models so devs can build on top of them.&lt;/p&gt; &lt;p&gt;Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heidihobo"&gt; /u/heidihobo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdnjq9/improved_realtime_console_with_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdnjq9/improved_realtime_console_with_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdnjq9/improved_realtime_console_with_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T20:55:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdjjgf</id>
    <title>Aider + QwQ-32b</title>
    <updated>2025-03-17T18:14:50+00:00</updated>
    <author>
      <name>/u/arivar</name>
      <uri>https://old.reddit.com/user/arivar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've been trying Aiden with QwQ-32b (GGUF Q6) and it is basically impossible to do anything. Every request, even the most simple, gets to &amp;quot;Model openai/qwq-32b-q6_k has hit a token limit!&amp;quot;. I am initializing QwQ with this prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./koboldcpp \ --model ~/.cache/huggingface/hub/models--Qwen--QwQ-32B-GGUF/snapshots/8728e66249190b78dee8404869827328527f6b3b/qwq-32b-q6_k.gguf \ --usecublas normal \ --gpulayers 4500 \ --tensor_split 0.6 0.4 \ --threads 8 \ --usemmap \ --flashattention &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;what am I missing here? How are people using this for coding? I also tried adding --contextsize 64000 or even 120k, but it doesn't really help.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;EDIT: I initialize aider with: aider --model openai/qwq-32b-q6_k&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arivar"&gt; /u/arivar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjjgf/aider_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjjgf/aider_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjjgf/aider_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T18:14:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdawqj</id>
    <title>What is the difference between an AI agent and a background job calling LLM API?</title>
    <updated>2025-03-17T11:58:44+00:00</updated>
    <author>
      <name>/u/superloser48</name>
      <uri>https://old.reddit.com/user/superloser48</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - I am a programmer and I use LLMs extensively for work. For coding and for data cleaning - I have found LLMs INSANELY helpful.&lt;/p&gt; &lt;p&gt;But I am struggling to understand the &lt;strong&gt;difference between using an AI agent vs calling the LLMs' API in a background job&lt;/strong&gt; (cron). My code currently runs in cron jobs and passes PDFs to LLMs' API to OCR for dirty PDFs. (eg. we have a lot of PDF submissions on our website).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is not a loaded question or a diss on AI agents.&lt;/strong&gt; Would love it if someone could point what can be done differently in a AI agent vs a background job. I am curious if I can reduce my codebase size for data cleaning.&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superloser48"&gt; /u/superloser48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdo67r</id>
    <title>DEEPSEEK R1 vs REKA</title>
    <updated>2025-03-17T21:20:13+00:00</updated>
    <author>
      <name>/u/unofficialUnknownman</name>
      <uri>https://old.reddit.com/user/unofficialUnknownman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one is best for reasoning, thinking, problem solving, Human interaction&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialUnknownman"&gt; /u/unofficialUnknownman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdo67r/deepseek_r1_vs_reka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdo67r/deepseek_r1_vs_reka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdo67r/deepseek_r1_vs_reka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T21:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd616a</id>
    <title>Why are audio (tts/stt) models so much smaller in size than general llms?</title>
    <updated>2025-03-17T06:07:10+00:00</updated>
    <author>
      <name>/u/Heybud221</name>
      <uri>https://old.reddit.com/user/Heybud221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs have possible outputs comprising of words(text) but speech models require words as well as phenomes. Shouldn't they be larger?&lt;/p&gt; &lt;p&gt;From what I think, it is because they don't have the understanding (technically, llms also don't &amp;quot;understand&amp;quot; words) as much as LLMs. Is that correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heybud221"&gt; /u/Heybud221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T06:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdf5ag</id>
    <title>New Paper by Yann LeCun (META) - Transformers without Normalization</title>
    <updated>2025-03-17T15:19:34+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://arxiv.org/abs/2503.10622"&gt;Transformers without Normalization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new AI paper by Yann LeCun (@ylecun), one of the fathers of Deep Learning, has been released, and it could bring a radical shift in the architecture of deep neural networks and LLMs.&lt;/p&gt; &lt;p&gt;The paper is called &lt;em&gt;&amp;quot;Transformers without Normalization&amp;quot;&lt;/em&gt; and introduces a surprisingly simple technique called Dynamic Tanh (DyT), which replaces traditional normalization layers (Layer Norm or RMSNorm) with a single operation:&lt;br /&gt; DyT(x) = tanh(αx)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jddh2e</id>
    <title>Do any of you have a "hidden gem" LLM that you use daily?</title>
    <updated>2025-03-17T14:07:31+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was common back in the Llama2 days when fine-tunes often out-performed the popular models. I don't see it quite as often, so I figured I'd ask.&lt;/p&gt; &lt;p&gt;For every major model (Mistral, Llama, Qwen, etc..) I'll try and download one community version of it to test out. Sometimes they're about &lt;em&gt;as&lt;/em&gt; good, sometimes they're slightly worse. Rarely are they better.&lt;/p&gt; &lt;p&gt;I'd say the &amp;quot;oddest&amp;quot; one I have is IBM-Granite-3.2-2B . Not exactly a community/small-time model, but it's managed to replace Llama 3B in certain use-cases for me. It performs exactly as well but is a fair bit smaller.&lt;/p&gt; &lt;p&gt;Are you using anything that you'd consider un/less common?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T14:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdjohr</id>
    <title>Charting and Navigating Hugging Face's Model Atlas</title>
    <updated>2025-03-17T18:20:20+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjohr/charting_and_navigating_hugging_faces_model_atlas/"&gt; &lt;img alt="Charting and Navigating Hugging Face's Model Atlas" src="https://external-preview.redd.it/Y3DEWDJvK_RDBl8tRwQHJl9_BO4NFRR9Nn-Pm5-6Df4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46cde4984ad9518bb1a1dbc8f2b8b0b5fbf1d124" title="Charting and Navigating Hugging Face's Model Atlas" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/Eliahu/Model-Atlas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjohr/charting_and_navigating_hugging_faces_model_atlas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjohr/charting_and_navigating_hugging_faces_model_atlas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T18:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdkir1</id>
    <title>Gemma 3 Text Finally working with MLX</title>
    <updated>2025-03-17T18:53:50+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you that tried running Gemma 3 text versions with MLX in lm studio or elsewhere you might probably had issues like it only generating &amp;lt;pad&amp;gt; tokens or endless &amp;lt;end\_of\_turn&amp;gt; or not loading at all. Now it seems they have fixed it, both on LM studio end with latest runtimes and on MLX end in a PR a few hours ago: &lt;a href="https://github.com/ml-explore/mlx-lm/pull/21"&gt;https://github.com/ml-explore/mlx-lm/pull/21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have tried gemma-3-text-4b-it and all versions of the 1B one which I have converted myself. They are converted with &amp;quot;--dtype bfloat16&amp;quot;, don't ask me what it is but fixed the issues. The new ones seem to follow the naming convention gemma-3-text-1B-8bit-mlx or similar, notice the -text. &lt;/p&gt; &lt;p&gt;Just for fun here are some benchmarks for gemma-3-text-1B-it-mlx on a base m4 mbp:&lt;/p&gt; &lt;p&gt;q3 - 125 tps&lt;/p&gt; &lt;p&gt;q4 - 110 tps&lt;/p&gt; &lt;p&gt;q6 - 86 tps&lt;/p&gt; &lt;p&gt;q8 - 66 tps&lt;/p&gt; &lt;p&gt;fp16 I think - 39 tps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdkir1/gemma_3_text_finally_working_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdkir1/gemma_3_text_finally_working_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdkir1/gemma_3_text_finally_working_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T18:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd87wv</id>
    <title>underwhelming MCP Vs hype</title>
    <updated>2025-03-17T08:58:04+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My early thoughts on MCPs :&lt;/p&gt; &lt;p&gt;As I see the current state of hype, the experience is underwhelming:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Confusing targeting — developers and non devs both.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For devs — it’s straightforward coding agent basically just llm.txt , so why would I use MCP isn’t clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For non devs — It’s like tools that can be published by anyone and some setup to add config etc. But the same stuff has been tried by ChatGPT GPTs as well last year where anyone can publish their tools as GPTs, which in my experience didn’t work well.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;There’s isn’t a good client so far and the clients UIs not being open source makes the experience limited as in our case, no client natively support video upload and playback.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Installing MCPs on local machines can have setup issues later with larger MCPs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I feel the hype isn’t organic and fuelled by Anthropic. I was expecting MCP ( being a protocol ) to have deeper developer value for agentic workflows and communication standards then just a wrapper over docker and config files.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let’s imagine a world with lots of MCPs — how would I choose which one to install and why, how would it rank similar servers? Are they imagining it like a ecosystem like App store where my main client doesn’t change but I am able to achieve any tasks that I do with a SaaS product.&lt;/p&gt; &lt;p&gt;We tried a simple task — &lt;code&gt;&amp;quot;take the latest video on Gdrive and give me a summary&amp;quot;&lt;/code&gt; For this the steps were not easy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Go through Gdrive MCP and setup documentation — Gdrive MCP has 11 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;VideoDB MCP has 1 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall 12, 13 step to do a basic task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdfdqz</id>
    <title>open source coding agent refact</title>
    <updated>2025-03-17T15:29:13+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"&gt; &lt;img alt="open source coding agent refact" src="https://preview.redd.it/vn8fcj32q9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f198892766072d65d46bc5c22c6a0a240669fddf" title="open source coding agent refact" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn8fcj32q9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdjzxw</id>
    <title>Mistral Small in Open WebUI via La Plateforme + Caveats</title>
    <updated>2025-03-17T18:32:59+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjzxw/mistral_small_in_open_webui_via_la_plateforme/"&gt; &lt;img alt="Mistral Small in Open WebUI via La Plateforme + Caveats" src="https://a.thumbs.redditmedia.com/g-aUfmxgQoY9tBia45qGbXcJsAWo_IhLAf5D7fMSDC0.jpg" title="Mistral Small in Open WebUI via La Plateforme + Caveats" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While we're waiting for Mistral 3.1 to be converted for local tooling - you can already start testing the model via Mistral's API with a free API key.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5tf7e72nape1.png?width=1243&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c21298604c2850620d35c46952dee063f22f03"&gt;Example misguided attention task where Mistral Small v3.1 behaves better than gpt-4o-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You'll need to provide your phone number to sign up for La Plateforme (they do it to avoid account abuse)&lt;/li&gt; &lt;li&gt;Open WebUI &lt;strong&gt;doesn't work with Mistral API out of the box&lt;/strong&gt;, you'll need to adjust the model settings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Guide&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sign Up for La Plateforme &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://console.mistral.ai/"&gt;https://console.mistral.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click &amp;quot;Sign Up&amp;quot;&lt;/li&gt; &lt;li&gt;Choose SSO or fill-in email details, click &amp;quot;Sign up&amp;quot;&lt;/li&gt; &lt;li&gt;Fill in Organization details and accept Mistral's Terms of Service, click &amp;quot;Create Organization&amp;quot;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Obtain La Plateforme API Key &lt;ol&gt; &lt;li&gt;In the sidebar, go to &amp;quot;La Plateforme&amp;quot; &amp;gt; &amp;quot;Subscription&amp;quot;: &lt;a href="https://admin.mistral.ai/plateforme/subscription"&gt;https://admin.mistral.ai/plateforme/subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click &amp;quot;Compare plans&amp;quot;&lt;/li&gt; &lt;li&gt;Choose &amp;quot;Experiment&amp;quot; plan &amp;gt; &amp;quot;Experiment for free&amp;quot;&lt;/li&gt; &lt;li&gt;Accept Mistral's Terms of Service for La Plateforme, click &amp;quot;Subscribe&amp;quot;&lt;/li&gt; &lt;li&gt;Provide a phone number, you'll receive SMS with the code that you'll need to type back in the form, once done click &amp;quot;Confirm code&amp;quot; &lt;ol&gt; &lt;li&gt;There's a limit to one organization per phone number, you won't be able to reuse the number for multiple account&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Once done, you'll be redirected to &lt;a href="https://console.mistral.ai/home"&gt;https://console.mistral.ai/home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;From there, go to &amp;quot;API Keys&amp;quot; page: &lt;a href="https://console.mistral.ai/api-keys"&gt;https://console.mistral.ai/api-keys&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click &amp;quot;Create new key&amp;quot;&lt;/li&gt; &lt;li&gt;Provide a key name and optionally an expiration date, click &amp;quot;Create new key&amp;quot;&lt;/li&gt; &lt;li&gt;You'll see &amp;quot;API key created&amp;quot; screen - &lt;strong&gt;this is your only chance to copy this key.&lt;/strong&gt; Copy the key - we'll need it later. If you didn't copy a key - don't worry, just generate a new one.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Add Mistral API to Open WebUI &lt;ol&gt; &lt;li&gt;Open your Open WebUI admin settings page. Should be on the &lt;a href="http://localhost:8080/admin/settings"&gt;http://localhost:8080/admin/settings&lt;/a&gt; for the default install.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Connections&amp;quot;&lt;/li&gt; &lt;li&gt;To the right from &amp;quot;Manage OpenAI Connections&amp;quot;, click &amp;quot;+&amp;quot; icon&lt;/li&gt; &lt;li&gt;In the &amp;quot;Add Connection&amp;quot; modal, provide &lt;a href="https://api.mistral.ai/v1"&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/a&gt; as API Base URL, paste copied key in the &amp;quot;API Key&amp;quot;, click &amp;quot;refresh&amp;quot; icon (Verify Connection) to the right of the URL - you should see a green toast message if everything is setup correctly&lt;/li&gt; &lt;li&gt;Click &amp;quot;Save&amp;quot; - you should see a green toast with &amp;quot;OpenAI Settings updated&amp;quot; message if everything is as expected&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disable &amp;quot;Usage&amp;quot; reporting&lt;/strong&gt; - not supported by Mistral's API streaming responses &lt;ol&gt; &lt;li&gt;From the same screen - click on &amp;quot;Models&amp;quot;. You should still be on the same URL as before, just in the &amp;quot;Models&amp;quot; tab. You should be able to see Mistral AI models in the list.&lt;/li&gt; &lt;li&gt;Locate &amp;quot;mistral-small-2503&amp;quot; model, click a pencil icon to the right from the model name&lt;/li&gt; &lt;li&gt;At the bottom of the page, just above &amp;quot;Save &amp;amp; Update&amp;quot; &lt;strong&gt;ensure that &amp;quot;Usage&amp;quot; is unchecked&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ensure &amp;quot;seed&amp;quot; setting is disabled/default&lt;/strong&gt; - not supported by Mistral's API &lt;ol&gt; &lt;li&gt;Click your Username &amp;gt; Settings&lt;/li&gt; &lt;li&gt;Click &amp;quot;General&amp;quot; &amp;gt; &amp;quot;Advanced Parameters&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Seed&amp;quot; (should be third from the top) - should be set to &amp;quot;Default&amp;quot; &lt;/li&gt; &lt;li&gt;It could be set for an individual chat - ensure to unset as well&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Done!&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjzxw/mistral_small_in_open_webui_via_la_plateforme/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjzxw/mistral_small_in_open_webui_via_la_plateforme/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdjzxw/mistral_small_in_open_webui_via_la_plateforme/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T18:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx69i</id>
    <title>Text an LLM at +61493035885</title>
    <updated>2025-03-16T22:10:18+00:00</updated>
    <author>
      <name>/u/benkaiser</name>
      <uri>https://old.reddit.com/user/benkaiser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a basic service running on an old Android phone + cheap prepaid SIM card to allow people to send a text and receive a response from Llama 3.1 8B. I felt the need when we recently lost internet access during a tropical cyclone but SMS was still working.&lt;/p&gt; &lt;p&gt;Full details in the blog post: &lt;a href="https://benkaiser.dev/text-an-llm/"&gt;https://benkaiser.dev/text-an-llm/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benkaiser"&gt; /u/benkaiser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdg29r</id>
    <title>AMD's Ryzen AI MAX+ 395 "Strix Halo" APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks</title>
    <updated>2025-03-17T15:56:46+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"&gt; &lt;img alt="AMD's Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks" src="https://external-preview.redd.it/3IcJjSUCxi122hiuXmFpDMPiHArK-sIOKdws7ZIS6y4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975ee513a9d290ecb7564adc784829a23a139908" title="AMD's Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-apu-over-3x-faster-rtx-5080-in-deepseek-benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdc0hq</id>
    <title>Mathematics for Machine Learning: 417 page pdf ebook</title>
    <updated>2025-03-17T12:58:19+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mml-book.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T12:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdfgx1</id>
    <title>QwQ 32B appears on LMSYS Arena Leaderboard</title>
    <updated>2025-03-17T15:32:48+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"&gt; &lt;img alt="QwQ 32B appears on LMSYS Arena Leaderboard" src="https://preview.redd.it/5zj3vxe1r9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9766a91861bd2ecad6af7013d5f14d8d3610d132" title="QwQ 32B appears on LMSYS Arena Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5zj3vxe1r9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdasng</id>
    <title>Heads up if you're using Gemma 3 vision</title>
    <updated>2025-03-17T11:52:09+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt; &lt;img alt="Heads up if you're using Gemma 3 vision" src="https://external-preview.redd.it/YXiABCYSLmR9qQ-LeZnYrdVC2XA7zhm-nWxrhAxjA3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f558a293cfea6675b1b3428038710d15adc358d8" title="Heads up if you're using Gemma 3 vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a quick heads up for anyone using Gemma 3 in &lt;strong&gt;LM Studio&lt;/strong&gt; or &lt;strong&gt;Koboldcpp&lt;/strong&gt;, its vision capabilities aren't fully functional within those interfaces, resulting in degraded quality. (I do not know about Open WebUI as I'm not using it).&lt;/p&gt; &lt;p&gt;I believe a lot of users potentially have used vision without realizing it has been more or less crippled, not showcasing Gemma 3's full potential. However, when you do &lt;strong&gt;not&lt;/strong&gt; use vision for details or texts, the degraded accuracy is often not noticeable and works quite good, for example with general artwork and landscapes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Koboldcpp&lt;/strong&gt; resizes images before being processed by Gemma 3, which particularly distorts details, perhaps most noticeable with smaller text. While Koboldcpp &lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.81.1"&gt;version 1.81&lt;/a&gt; (released January 7th) expanded supported resolutions and aspect ratios, the resizing still affects vision quality negatively, resulting in degraded accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; is behaving more odd, initial image input sent to Gemma 3 is relatively accurate (but still somewhat crippled, probably because it's doing re-scaling here as well), but subsequent regenerations using the same image or starting new chats with new images results in &lt;em&gt;significantly&lt;/em&gt; degraded output, most noticeable images with finer details such as characters in far distance or text.&lt;/p&gt; &lt;p&gt;When I send images to Gemma 3 directly (not through these UIs), its accuracy becomes much better, especially for details and texts.&lt;/p&gt; &lt;p&gt;Below is a collage (I can't upload multiple images on Reddit) demonstrating how vision quality degrades even more when doing a regeneration or starting a new chat in LM Studio.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb"&gt;https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9xjj</id>
    <title>Gemma 3 is now available for free on HuggingChat!</title>
    <updated>2025-03-17T10:59:36+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt; &lt;img alt="Gemma 3 is now available for free on HuggingChat!" src="https://external-preview.redd.it/fWy5OWPAOAEc-6PWhgSCmMIjuAf3liLshl-njJSXolI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc784c22ed0ab0a891d88c523a62c0663cf6cf" title="Gemma 3 is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/google/gemma-3-27b-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T10:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnh4</id>
    <title>Mistral Small 3.1 (24B)</title>
    <updated>2025-03-17T16:20:23+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt; &lt;img alt="Mistral Small 3.1 (24B)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Small 3.1 (24B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdaq7x</id>
    <title>3x RTX 5090 watercooled in one desktop</title>
    <updated>2025-03-17T11:48:07+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt; &lt;img alt="3x RTX 5090 watercooled in one desktop" src="https://preview.redd.it/zsu6kw5pm8pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313ede5ac5797a563ccfc2f875620a34ab784cbe" title="3x RTX 5090 watercooled in one desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zsu6kw5pm8pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgqcj</id>
    <title>NEW MISTRAL JUST DROPPED</title>
    <updated>2025-03-17T16:23:29+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt; &lt;img alt="NEW MISTRAL JUST DROPPED" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="NEW MISTRAL JUST DROPPED" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Outperforms&lt;/strong&gt; GPT-4o Mini, Claude-3.5 Haiku, and others in text, vision, and multilingual tasks.&lt;br /&gt; &lt;strong&gt;128k context window&lt;/strong&gt;, blazing &lt;strong&gt;150 tokens/sec speed&lt;/strong&gt;, and runs on a &lt;strong&gt;single RTX 4090&lt;/strong&gt; or &lt;strong&gt;Mac (32GB RAM)&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;—free to use, fine-tune, and deploy. Handles chatbots, docs, images, and coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;https://mistral.ai/fr/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"&gt;https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnw5</id>
    <title>Mistrall Small 3.1 released</title>
    <updated>2025-03-17T16:20:51+00:00</updated>
    <author>
      <name>/u/Dirky_</name>
      <uri>https://old.reddit.com/user/Dirky_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt; &lt;img alt="Mistrall Small 3.1 released" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistrall Small 3.1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dirky_"&gt; /u/Dirky_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdidoa</id>
    <title>Victory: My wife finally recognized my silly computer hobby as useful</title>
    <updated>2025-03-17T17:28:54+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a local LLM, LAN-accessible, with a vector database covering all tax regulations, labor laws, and compliance data. Now she sees the value. A small step for AI, a giant leap for household credibility.&lt;/p&gt; &lt;p&gt;Edit: Insane response! To everyone asking—yes, it’s just web scraping with correct layers (APIs help), embedding, and RAG. Not that hard if you structure it right. I might put together a simple guide later when i actually use a more advanced method.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T17:28:54+00:00</published>
  </entry>
</feed>
