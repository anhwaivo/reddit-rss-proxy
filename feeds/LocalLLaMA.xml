<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-25T00:50:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iwhfl5</id>
    <title>96GB modded RTX 4090 for $4.5k</title>
    <updated>2025-02-23T18:55:09+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt; &lt;img alt="96GB modded RTX 4090 for $4.5k" src="https://preview.redd.it/5rf8m3k1rxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d35cdb0e62ea887c4da38324fff2ccbbf226f9f" title="96GB modded RTX 4090 for $4.5k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rf8m3k1rxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwhfl5/96gb_modded_rtx_4090_for_45k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix11go</id>
    <title>aspen - Open-source voice assistant you can call, at only $0.01025/min!</title>
    <updated>2025-02-24T12:37:28+00:00</updated>
    <author>
      <name>/u/thooton</name>
      <uri>https://old.reddit.com/user/thooton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt; &lt;img alt="aspen - Open-source voice assistant you can call, at only $0.01025/min!" src="https://external-preview.redd.it/Y8cU497M8VmMsSvykiiACmZpJ9cu5NkYzryYit_2lHY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b00ce71267ae00f4c36a6263a4dd0cc5d7b9aee" title="aspen - Open-source voice assistant you can call, at only $0.01025/min!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player"&gt;https://reddit.com/link/1ix11go/video/ohkvv8g9z2le1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hi everyone, hope you're all doing great :) I thought I'd share a little project that I've been working on for the past few days. It's a voice assistant that uses Twilio's API to be accessible through a real phone number, so you can call it just like a person!&lt;/p&gt; &lt;p&gt;Using Groq's STT free tier and Google's TTS free tier, the only costs come from Twilio and Anthropic and add up to about $0.01025/min, which is a lot cheaper than the conversational agents from ElevenLabs or PlayAI which approach $0.10/min or $0.18/min respectively.&lt;/p&gt; &lt;p&gt;I wrote the code to be as modular as possible so it should be easy to modify it to use your own local LLM or whatever you like! all PRs are welcome :)&lt;/p&gt; &lt;p&gt;have an awesome day!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/thooton/aspen"&gt;https://github.com/thooton/aspen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thooton"&gt; /u/thooton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix11go/aspen_opensource_voice_assistant_you_can_call_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixflw6</id>
    <title>Model Tips &amp; Tricks - Instruct Formatting</title>
    <updated>2025-02-24T22:53:28+00:00</updated>
    <author>
      <name>/u/ParasiticRogue</name>
      <uri>https://old.reddit.com/user/ParasiticRogue</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings! I've decided to share some insight that I've accumulated over the few years I've been toying around with LLMs, and the intricacies of how to potentially make them run better for creative writing or roleplay as the focus, but it might also help with technical jobs too.&lt;/p&gt; &lt;p&gt;This is the first part of my general musings on what I've found, focusing more on the technical aspects, with more potentially coming soon in regards to model merging and system prompting, along with character and story prompting later, if people found this useful. These might not be applicable with every model or user case, nor would it guarantee the best possible response with every single swipe, but it should help increase the odds of getting better mileage out of your model and experience, even if slightly, and help you avoid some bad or misled advice, which I personally have had to put up with. Some of this will be retreading old ground if you are already privy, but I will try to include less obvious stuff as well. Remember, I still consider myself a novice in some areas, and am always open to improvement.&lt;/p&gt; &lt;p&gt;### What is the Instruct Template?&lt;/p&gt; &lt;p&gt;The Instruct Template/Format is probably the most important when it comes to getting a model to work properly, as it is what encloses the training data with token that were used for the model, and your chat with said model. Some of them are used in a more general sense and are not brand specific, such as ChatML or Alpaca, while others are stick to said brand, like Llama3 Instruct or Mistral Instruct. However not all models that are brand specific with their formatting will be trained with their own personal template.&lt;/p&gt; &lt;p&gt;Its important to find out what format/template a model uses before booting it up, and you can usually check to see which it is on the model page. If a format isn't directly listed on said page, then there is ways to check internally with the local files. Each model has a tokenizer_config file, and sometimes even a special_tokens file, inside the main folder. As an example of what to look for, If you see something like a Mistral brand model that has im_start/im_end inside those files, then chances are that the person who finetuned it used ChatML tokens in their training data. Familiarizing yourself with the popular tokens used in training will help you navigate models better internally, especially if a creator forgets to post a readme on how it's suppose to function.&lt;/p&gt; &lt;p&gt;### Is there any reason not to use the prescribed format/template?&lt;/p&gt; &lt;p&gt;Sticking to the prescribed format will give your model better odds of getting things correct, or even better prose quality. But there are *some* small benefits when straying from the model's original format, such as supposedly being less censored. However the trade-off when it comes to maximizing a model's intelligence is never really worth it, and there are better ways to get uncensored responses with better prompting, or even tricking the model by editing their response slightly and continuing from there.&lt;/p&gt; &lt;p&gt;From what I've found when testing models, if someone finetunes a model over the company's official Instruct focused model, instead of a base model, and doesn't use the underlining format that it was made with (such as ChatML over Mistral's 22B model as an example) then performance dips will kick in, giving less optimal responses then if it was instead using a unified format.&lt;/p&gt; &lt;p&gt;This does not factor other occurrences of poor performance or context degradation when choosing to train on top of official Instruct models which may occur, but if it uses the correct format, and/or is trained with DPO or one of its variance (this one is more anecdotal, but DPO/ORPO/Whatever-O seems moreto be a more stable method when it comes to training on top of per-existing Instruct models) then the model will perform better overall.&lt;/p&gt; &lt;p&gt;### What about models that list multiple formats/templates?&lt;/p&gt; &lt;p&gt;This one is more due to model merging or choosing to forgo an Instruct model's format in training, although some people will choose to train their models like this, for whatever reason. In such an instance, you kinda just have to pick one and see what works best, but the merging of formats, and possibly even models, might provide interesting results, but only if its agreeable with the clutter on how you prompt it yourself. What do I mean by this? Well, perhaps its better if I give you a couple anecdotes on how this might work in practice...&lt;/p&gt; &lt;p&gt;Nous-Capybara-limarpv3-34B is an older model at this point, but it has a unique feature that many models don't seem to implement; a Message Length Modifier. By adding small/medium/long at the end of the Assistant's Message Prefix, it will allow you to control how long the Bot's response is, which can be useful in curbing rambling, or enforcing more detail. Since Capybara, the underling model, uses the Vicuna format, its prompt typically looks like this:&lt;/p&gt; &lt;p&gt;System:&lt;/p&gt; &lt;p&gt;User:&lt;/p&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;p&gt;Meanwhile, the limarpv3 lora, which has the Message Length Modifier, was used on top of Capybara and chose to use Alpaca as its format:&lt;/p&gt; &lt;p&gt;### Instruction:&lt;/p&gt; &lt;p&gt;### Input:&lt;/p&gt; &lt;p&gt;### Response: (length = short/medium/long/etc)&lt;/p&gt; &lt;p&gt;Seems to be quite different, right? Well, it is, but we can also combine these two formats in a meaningful way and actually see tangible results. When using Nous-Capybara-limarpv3-34B with its underling Vicuna format and the Message Length Modifier together, the results don't come together, and you have basically 0 control on its length:&lt;/p&gt; &lt;p&gt;System:&lt;/p&gt; &lt;p&gt;User:&lt;/p&gt; &lt;p&gt;Assistant: (length = short/medium/long/etc)&lt;/p&gt; &lt;p&gt;The above example with Vicuna doesn't seem to work. However, by adding triple hashes to it, the modifier actually will take effect, making the messages shorter or longer on average depending on how you prompt it.&lt;/p&gt; &lt;p&gt;### System:&lt;/p&gt; &lt;p&gt;### User:&lt;/p&gt; &lt;p&gt;### Assistant: (length = short/medium/long/etc)&lt;/p&gt; &lt;p&gt;This is an example of where both formats can work together in a meaningful way.&lt;/p&gt; &lt;p&gt;Another example is merging a Vicuna model with a ChatML one and incorporating the stop tokens from it, like with RP-Stew-v4. For reference, ChatML looks like this:&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;system&lt;/p&gt; &lt;p&gt;System prompt&amp;lt;|im_end|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;user&lt;/p&gt; &lt;p&gt;User prompt&amp;lt;|im_end|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt; &lt;p&gt;Bot response&amp;lt;|im_end|&amp;gt;&lt;/p&gt; &lt;p&gt;One thing to note is that, unlike Alpaca, the ChatML template has System/User/Assistant inside it, making it vaguely similar to Vicuna. Vicuna itself doesn't have stop tokens, but if we add them like so:&lt;/p&gt; &lt;p&gt;SYSTEM: system prompt&amp;lt;|end|&amp;gt;&lt;/p&gt; &lt;p&gt;USER: user prompt&amp;lt;|end|&amp;gt;&lt;/p&gt; &lt;p&gt;ASSISTANT: assistant output&amp;lt;|end|&amp;gt;&lt;/p&gt; &lt;p&gt;Then it will actually help prevent RP-Stew from rambling or repeating itself within the same message, and also lowering the chances of your bot speaking as the user. When merging models I find it best to keep to one format in order to keep its performance high, but there can be rare cases where mixing them could work.&lt;/p&gt; &lt;p&gt;### Are stop tokens necessary?&lt;/p&gt; &lt;p&gt;In my opinion, models work best when it has stop tokens built into them. Like with RP-Stew, the decrease in repetitive message length was about 25~33% on average, give or take from what I remember, when these &amp;lt;|end|&amp;gt; tokens are added. That's one case where the usefulness is obvious. Formats that use stop tokens tend to be more stable on average when it comes to creative back-and-forths with the bot, since it gives it a structure that's easier for it to understand when to end things, and inform better on who is talking.&lt;/p&gt; &lt;p&gt;If you like your models to be unhinged and ramble on forever (aka; bad) then by all means, experiment by not using them. It might surprise you if you tweak it. But as like before, the intelligence hit is usually never worth it. Remember to make separate instances when experimenting with prompts, or be sure to put your tokens back in their original place. Otherwise you might end up with something dumb, like putting the stop token before the User in the User prefix.&lt;/p&gt; &lt;p&gt;I will leave that here for now. Next time I might talk about how to merge models, or creative prompting, idk. Let me know if you found this useful and if there is anything you'd like to see next, or if there is anything you'd like expanded on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParasiticRogue"&gt; /u/ParasiticRogue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixflw6/model_tips_tricks_instruct_formatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixflw6/model_tips_tricks_instruct_formatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixflw6/model_tips_tricks_instruct_formatting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixf88r</id>
    <title>New QwQ-max is great but not SOTA on livecodebench</title>
    <updated>2025-02-24T22:37:29+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://livecodebench.github.io/leaderboard.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixf88r/new_qwqmax_is_great_but_not_sota_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixf88r/new_qwqmax_is_great_but_not_sota_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5vgm</id>
    <title>TIP: Open WebUI "Overview" mode</title>
    <updated>2025-02-24T16:19:18+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt; &lt;img alt="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" src="https://a.thumbs.redditmedia.com/xhTnipJlQp9kwvDNpjx8F2VXJDkn8RgN7RNScLB8_00.jpg" title="TIP: Open WebUI &amp;quot;Overview&amp;quot; mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As Google added &lt;a href="https://nitter.net/OfficialLoganK/status/1894049802557456669"&gt;branching support&lt;/a&gt; for its AI Studio product, I think the crown in terms of implementation is still held by the Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfdla1lq34le1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70419e34b5c6474913c5d005bf6a5125561d8302"&gt;Overview mode&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;To activate: click &amp;quot;...&amp;quot; at the top right and select &amp;quot;Overview&amp;quot; in the menu&lt;/li&gt; &lt;li&gt;Clicking any leaf node in the graph will update the chat state accordingly&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5vgm/tip_open_webui_overview_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix6bjw</id>
    <title>Is there any image models coming out?</title>
    <updated>2025-02-24T16:37:38+00:00</updated>
    <author>
      <name>/u/hoja_nasredin</name>
      <uri>https://old.reddit.com/user/hoja_nasredin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were extremely spoiled this summer with Flux and SD3.1 coming out. But was anything else have been released since? Flux cannot be trained in a serious way apparently since it is distilled, and SD3 is hated by the community (or it might have some other issues I'm not aware). &lt;/p&gt; &lt;p&gt;What is happening with the image models right now? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hoja_nasredin"&gt; /u/hoja_nasredin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix6bjw/is_there_any_image_models_coming_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvo4b</id>
    <title>An Open-Source Implementation of Deep Research using Gemini Flash 2.0</title>
    <updated>2025-02-24T06:32:56+00:00</updated>
    <author>
      <name>/u/CarpetNo5579</name>
      <uri>https://old.reddit.com/user/CarpetNo5579</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open source version of deep research using Gemini Flash 2.0!&lt;/p&gt; &lt;p&gt;Feed it any topic and it'll explore it thoroughly, building and displaying a research tree in real-time as it works. &lt;/p&gt; &lt;p&gt;This implementation has three research modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast (1-3min): Quick surface research, perfect for initial exploration&lt;/li&gt; &lt;li&gt;Balanced (3-6min): Moderate depth, explores main concepts and relationships&lt;/li&gt; &lt;li&gt;Comprehensive (5-12min): Deep recursive research, builds query trees, explores counter-arguments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The coolest part is watching it think - it prints out the research tree as it explores, so you can see exactly how it's approaching your topic.&lt;/p&gt; &lt;p&gt;I built this because I haven't seen any implementation that uses Gemini and its built in search tool and thought others might find it useful too.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/eRuaro/open-gemini-deep-research"&gt;https://github.com/eRuaro/open-gemini-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarpetNo5579"&gt; /u/CarpetNo5579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvo4b/an_opensource_implementation_of_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixcaur</id>
    <title>New Deepseek integation repo</title>
    <updated>2025-02-24T20:37:59+00:00</updated>
    <author>
      <name>/u/lucitatecapacita</name>
      <uri>https://old.reddit.com/user/lucitatecapacita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like DeepSeek has released a repo with new integrations with several frameworks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/deepseek-ai/awesome-deepseek-integration"&gt;https://github.com/deepseek-ai/awesome-deepseek-integration&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucitatecapacita"&gt; /u/lucitatecapacita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcaur/new_deepseek_integation_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T20:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix3gao</id>
    <title>nvidia / Evo 2 Protein Design</title>
    <updated>2025-02-24T14:35:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt; &lt;img alt="nvidia / Evo 2 Protein Design" src="https://preview.redd.it/fp2o6r9ql3le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=174a8acebfd2e90b81df658a8e8c6f3c7d031293" title="nvidia / Evo 2 Protein Design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard"&gt;https://build.nvidia.com/nvidia/evo2-protein-design/blueprintcard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fp2o6r9ql3le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix3gao/nvidia_evo_2_protein_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T14:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix1ddj</id>
    <title>ragit 0.3.0 released</title>
    <updated>2025-02-24T12:55:49+00:00</updated>
    <author>
      <name>/u/baehyunsol</name>
      <uri>https://old.reddit.com/user/baehyunsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt; &lt;img alt="ragit 0.3.0 released" src="https://external-preview.redd.it/TipJWadkvg51FCh2k5yn7L-J4VOuk7fkeumbFTDL_OM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ff6bfc615eb15181fb1437f1d20a7a4e5656c6" title="ragit 0.3.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this open source RAG solution for a while.&lt;/p&gt; &lt;p&gt;It gives you a simple CLI for local rag, without any need for writing code!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baehyunsol"&gt; /u/baehyunsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/baehyunsol/ragit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix1ddj/ragit_030_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T12:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtl7f</id>
    <title>Most people are worried about LLM's executing code. Then theres me...... 😂</title>
    <updated>2025-02-24T04:24:24+00:00</updated>
    <author>
      <name>/u/DataScientist305</name>
      <uri>https://old.reddit.com/user/DataScientist305</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt; &lt;img alt="Most people are worried about LLM's executing code. Then theres me...... 😂" src="https://preview.redd.it/92abn3ekk0le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09547c1d04e4bb052014aac1d2d58fba8d76d0ee" title="Most people are worried about LLM's executing code. Then theres me...... 😂" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientist305"&gt; /u/DataScientist305 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92abn3ekk0le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwtl7f/most_people_are_worried_about_llms_executing_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5np0</id>
    <title>I built OLLAMA GUI in next.js how do you like it?</title>
    <updated>2025-02-24T16:10:21+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt; &lt;img alt="I built OLLAMA GUI in next.js how do you like it?" src="https://preview.redd.it/f0j99mmn24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=494ec50651dc68222e262f195d12282a270ea7e0" title="I built OLLAMA GUI in next.js how do you like it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!😀&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0j99mmn24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix5np0/i_built_ollama_gui_in_nextjs_how_do_you_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T16:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix0d4z</id>
    <title>Polish Ministry of Digital Affairs shared PLLuM model family on HF</title>
    <updated>2025-02-24T11:57:56+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CYFRAGOVPL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix0d4z/polish_ministry_of_digital_affairs_shared_pllum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T11:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixd0er</id>
    <title>QwQ Max Preview Published</title>
    <updated>2025-02-24T21:06:52+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwenlm.github.io/blog/qwq-max-preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixd0er/qwq_max_preview_published/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixd0er/qwq_max_preview_published/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvvmy</id>
    <title>Qwen is releasing something tonight!</title>
    <updated>2025-02-24T06:46:53+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt; &lt;img alt="Qwen is releasing something tonight!" src="https://external-preview.redd.it/vArUV2h82u8EtPauQRu5bQrqvRa1QZ1C_bg0wPIoH5o.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263b5e9873bf302385907f40a338f7412dc9b280" title="Qwen is releasing something tonight!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://twitter.com/Alibaba_Qwen/status/1893907569724281088"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwvvmy/qwen_is_releasing_something_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T06:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwqf3z</id>
    <title>FlashMLA - Day 1 of OpenSourceWeek</title>
    <updated>2025-02-24T01:37:17+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt; &lt;img alt="FlashMLA - Day 1 of OpenSourceWeek" src="https://preview.redd.it/to631nzvqzke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3551b21c98cfb01ba242529b337443a5c85b4481" title="FlashMLA - Day 1 of OpenSourceWeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/FlashMLA"&gt;https://github.com/deepseek-ai/FlashMLA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/to631nzvqzke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwqf3z/flashmla_day_1_of_opensourceweek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T01:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixcygz</id>
    <title>Qwq max preview released</title>
    <updated>2025-02-24T21:04:42+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1894130603513319842"&gt;https://x.com/Alibaba_Qwen/status/1894130603513319842&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixcygz/qwq_max_preview_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixe6yo</id>
    <title>Great announcement today. Heres how we already made it better months ago</title>
    <updated>2025-02-24T21:55:08+00:00</updated>
    <author>
      <name>/u/bmlattimer</name>
      <uri>https://old.reddit.com/user/bmlattimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt; &lt;img alt="Great announcement today. Heres how we already made it better months ago" src="https://external-preview.redd.it/grYs-3O6uZipD0Sj50ba5RJGLP9auRDlnYN5RIEw2ug.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=908d238bcf53c77c3b923be9dffa7d805c8338db" title="Great announcement today. Heres how we already made it better months ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;JOSH: Self-Improving LLMs for Tool Use Without Human Feedback&lt;/h1&gt; &lt;p&gt;Our team released a paper a few months ago introducing JOSH (Juxtaposed Outcomes for Simulation Harvesting), a self-alignment algorithm that enables LLMs to autonomously improve their tool-using capabilities without human feedback including notably on τ-bench. We also have introduced an agentic tool calling dataset ToolWOZ derived from MultiWOZ. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzfdhfkkq5le1.png?width=1906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35804ee77ec38267881cc116304f953b5f350341"&gt;JOSH uses methods similar to Test Time Scaling to generate training data&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What JOSH does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Uses tool calls as sparse rewards in a simulation environment to extract ideal dialogue turns&lt;/li&gt; &lt;li&gt;Trains models on their own outputs through beam search exploration (reminiscent of test time scaling methods that are currently used)&lt;/li&gt; &lt;li&gt;Significantly improves tool-based interactions across model sizes (from smaller Llama models to frontier models like GPT-4o)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;74% improvement in success rate for Llama3-8B on our ToolWOZ benchmark&lt;/li&gt; &lt;li&gt;State-of-the-art performance on τ-bench when applied to GPT-4o&lt;/li&gt; &lt;li&gt;Maintains general model capabilities on MT-Bench and LMSYS while specializing in tool use&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;p&gt;With today's Anthropic announcement showing improvements on τ-bench, it's worth noting how our approach can already be applied to improve its capabilities! JOSH offers a general approach that works across model sizes and doesn't require human feedback - potentially making it more scalable as models continue to improve.&lt;/p&gt; &lt;p&gt;We've made our code and the ToolWOZ dataset publicly available: &lt;a href="https://github.com/asappresearch/josh-llm-simulation-training"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2409.04617"&gt;Sparse Rewards Can Self-Train Dialogue Agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear the community's thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bmlattimer"&gt; /u/bmlattimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixe6yo/great_announcement_today_heres_how_we_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixfbzd</id>
    <title>Sonnet-3.7 is best non-thinking model in the Misguided Attention eval.</title>
    <updated>2025-02-24T22:41:46+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt; &lt;img alt="Sonnet-3.7 is best non-thinking model in the Misguided Attention eval." src="https://external-preview.redd.it/3Xlrhru-DocPv1ONkF-Le04N8KrkOyM1Ydkeb2ft68s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afcf14938726714cc9d549d6ef3ea05fd4f2b3c" title="Sonnet-3.7 is best non-thinking model in the Misguided Attention eval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information. It consists of slightly modified well known logical problems and riddles. Many model are overfit to these problems and will therefore report a response to the unmodified problem. &lt;/p&gt; &lt;p&gt;Claude-3.7-Sonnet was evaluated in the non-thinking mode in the long eval with 52 prompt. It almost beats o3-mini despite not using the thinking mode. This is a very impressive result. &lt;/p&gt; &lt;p&gt;I will benchmark the thinking mode once I have figured out how to activate it in the openrouter API...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sui6i1l4z5le1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d11d6a08386a45914660a0f576b2c4c58ae88d4"&gt;https://preview.redd.it/sui6i1l4z5le1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d11d6a08386a45914660a0f576b2c4c58ae88d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e1p7r416z5le1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2cd9467c077b18212e70943d731009ff62430a6"&gt;https://preview.redd.it/e1p7r416z5le1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2cd9467c077b18212e70943d731009ff62430a6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixfbzd/sonnet37_is_best_nonthinking_model_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixckba</id>
    <title>Making older LLMs (Llama 2 and Gemma 1) reason</title>
    <updated>2025-02-24T20:48:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"&gt; &lt;img alt="Making older LLMs (Llama 2 and Gemma 1) reason" src="https://external-preview.redd.it/Y21xb3pldThnNWxlMe68FKKrQSi1VIWXGB4I0FX2lDdJRybemxt5jwSyAisL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fc17b1b05cbeed10c64cb4b0ff38aed9587d31e" title="Making older LLMs (Llama 2 and Gemma 1) reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/frk5teu8g5le1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixckba/making_older_llms_llama_2_and_gemma_1_reason/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T20:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixamd9</id>
    <title>QwQ-Max-Preview soon</title>
    <updated>2025-02-24T19:30:38+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that they have been updating their website on another branch:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734"&gt;https://github.com/QwenLM/qwenlm.github.io/commit/5d009b319931d473211cb4225d726b322afbb734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Apache 2.0 licensed QwQ-Max, Qwen2.5-Max, QwQ-32B and probably other smaller QwQ variants, and an app for qwen chat.&lt;/p&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;We’re happy to unveil QwQ-Max-Preview , the latest advancement in the Qwen series, designed to push the boundaries of deep reasoning and versatile problem-solving. Built on the robust foundation of Qwen2.5-Max , this preview model excels in mathematics, coding, and general-domain tasks, while delivering outstanding performance in Agent-related workflows. As a sneak peek into our upcoming QwQ-Max release, this version offers a glimpse of its enhanced capabilities, with ongoing refinements and an official Apache 2.0-licensed open-source launch of QwQ-Max and Qwen2.5-Max planned soon. Stay tuned for a new era of intelligent reasoning.&lt;/p&gt; &lt;p&gt;As we prepare for the official open-source release of QwQ-Max under the Apache 2.0 License, our roadmap extends beyond sharing cutting-edge research. We are committed to democratizing access to advanced reasoning capabilities and fostering innovation across diverse applications. Here’s what’s next:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;APP Release&lt;/strong&gt; To bridge the gap between powerful AI and everyday users, we will launch a dedicated APP for Qwen Chat. This intuitive interface will enable seamless interaction with the model for tasks like problem-solving, code generation, and logical reasoning—no technical expertise required. The app will prioritize real-time responsiveness and integration with popular productivity tools, making advanced AI accessible to a global audience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Open-Sourcing Smaller Reasoning Models&lt;/strong&gt; Recognizing the need for lightweight, resource-efficient solutions, we will release a series of smaller QwQ variants , such as QwQ-32B, for local device deployment. These models will retain robust reasoning capabilities while minimizing computational demands, allowing developers to integrate them into devices. Perfect for privacy-sensitive applications or low-latency workflows, they will empower creators to build custom AI solutions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Community-Driven Innovation&lt;/strong&gt; By open-sourcing QwQ-Max, Qwen2.5-Max, and its smaller counterparts, we aim to spark collaboration among developers, researchers, and hobbyists. We invite the community to experiment, fine-tune, and extend these models for specialized use cases—from education tools to autonomous agents. Our goal is to cultivate an ecosystem where innovation thrives through shared knowledge and collective problem-solving.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Stay tuned as we roll out these initiatives, designed to empower users at every level and redefine the boundaries of what AI can achieve. Together, we’re building a future where intelligence is not just powerful, but universally accessible.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixamd9/qwqmaxpreview_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T19:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix98kq</id>
    <title>Claude 3.7 Sonnet and Claude Code</title>
    <updated>2025-02-24T18:34:27+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"&gt; &lt;img alt="Claude 3.7 Sonnet and Claude Code" src="https://external-preview.redd.it/V8JG-mmrlkT02vKigktdXzK2PH-CSO-CrYueRmf_OX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8beabe2cc993eb0a304f0f44ee00c9b8eb681095" title="Claude 3.7 Sonnet and Claude Code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/claude-3-7-sonnet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix98kq/claude_37_sonnet_and_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T18:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixefsf</id>
    <title>I created a new structured output method and it works really well</title>
    <updated>2025-02-24T22:04:49+00:00</updated>
    <author>
      <name>/u/jckwind11</name>
      <uri>https://old.reddit.com/user/jckwind11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"&gt; &lt;img alt="I created a new structured output method and it works really well" src="https://preview.redd.it/i55e55gkt5le1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fc258672a6dae100b76e5c3df682bffb3f9b2a" title="I created a new structured output method and it works really well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jckwind11"&gt; /u/jckwind11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i55e55gkt5le1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixefsf/i_created_a_new_structured_output_method_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T22:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixczae</id>
    <title>QwQ-Max Preview is here...</title>
    <updated>2025-02-24T21:05:36+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"&gt; &lt;img alt="QwQ-Max Preview is here..." src="https://external-preview.redd.it/bQFl5DBj7QNi2--7cYMNDWqUV0PSTT-usX89HeDXsMM.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1084b413b6f8c22df7000f62d2cf3888172ab3eb" title="QwQ-Max Preview is here..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://twitter.com/Alibaba_Qwen/status/1894130603513319842"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixczae/qwqmax_preview_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T21:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix96pq</id>
    <title>Claude 3.7 is real</title>
    <updated>2025-02-24T18:32:00+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"&gt; &lt;img alt="Claude 3.7 is real" src="https://preview.redd.it/2qkaymexr4le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bf2edcabf3c5b2063f0fb29bc1b4f7da023acfe" title="Claude 3.7 is real" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2qkaymexr4le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ix96pq/claude_37_is_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-24T18:32:00+00:00</published>
  </entry>
</feed>
