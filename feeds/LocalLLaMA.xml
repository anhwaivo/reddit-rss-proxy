<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-20T19:34:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m4qzmt</id>
    <title>Ideal setup for long context window fine-tuning?</title>
    <updated>2025-07-20T14:58:27+00:00</updated>
    <author>
      <name>/u/Ill_Imagination_6575</name>
      <uri>https://old.reddit.com/user/Ill_Imagination_6575</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’m doing a thesis on using LLMs to parse scientific articles from plaintext pdf format into structured XML. I’ve been looking into fine tuning a model locally to achieve this task, but a key consideration is the long context window requirement. The pdfs are multiple pages so up to 10 000 tokens long, making the VRAM requirements quite substantial. I have access to an HPC cluster with 48GB NViDIA GPUs and could push for requesting access to H100/A100s if needed. I am well aware of QLoRA and other techniques but can’t quite gauge what the optimal setup and model to use would be.&lt;/p&gt; &lt;p&gt;What would you recommend as to which model to fine-tune and what the memory requirements would be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Imagination_6575"&gt; /u/Ill_Imagination_6575 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r7t5</id>
    <title>How to get 3b models to squeeze onto 2gig Nvidia GPU?</title>
    <updated>2025-07-20T15:07:48+00:00</updated>
    <author>
      <name>/u/combo-user</name>
      <uri>https://old.reddit.com/user/combo-user</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I got my old laptop working and it's got a 940mx with 2gb of ddr5 memory and 8gb of ddr4 ram with i5 6200u. I got qwen3 1.7b q5 from unsloth to run well and it looked fine for what it was. &lt;/p&gt; &lt;p&gt;However I've been looking at llama 3.2 3b and have a hunch that more params will make it a better model compared to qwen3 1.7b and i got a q2 quant from unsloth to run on it.&lt;/p&gt; &lt;p&gt;So my question -&amp;gt; Any way I can get the gpu to run Llama 3.2 3b with a better quant than q2? Will limiting context to 2048, enabling flash attention, enabling k and or v cache quantization help?&lt;/p&gt; &lt;p&gt;I'm using lmstudio to do all this btw. Using the models for small/random Q&amp;amp;A and some brainstorming for side project ideas.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/combo-user"&gt; /u/combo-user &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r7t5/how_to_get_3b_models_to_squeeze_onto_2gig_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r7t5/how_to_get_3b_models_to_squeeze_onto_2gig_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r7t5/how_to_get_3b_models_to_squeeze_onto_2gig_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ke3x</id>
    <title>New to fine tuning</title>
    <updated>2025-07-20T09:06:15+00:00</updated>
    <author>
      <name>/u/Ok_Pie_6906</name>
      <uri>https://old.reddit.com/user/Ok_Pie_6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am using ollama, mistral 7b, huggingface tranformers and peft. &lt;/p&gt; &lt;p&gt;This is an example I have made for a piece of training data. Does anyone have any tips on how to improve it? Am I using correct Grammer? Am I missing anything important? &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;call_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;scenario_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;messages&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;This is call id 66. You are a male driver named Ozzie, you are 31 years old. It is approximately 4pm and sunny. You are alone in your blue 2017 Honda Civic with license plate BGT-1234, currently traveling southbound on the Anthony Henday Drive in Edmonton, Alberta, just past the Cameron Heights exit. You are calling 911 from inside your moving vehicle; the sound of rushing cars and wind can be heard in the background, blending with your tense breathing and the occasional blare of horns. Moments ago, another driver rear-ended you in what started as a road rage incident, and the situation has rapidly escalated. The aggressive driver is still pursuing you, swerving dangerously through traffic and now brandishing a handgun. They have not yet fired at you. You are terrified—your heart is pounding, your hands are shaking, and your thoughts are scattered as you struggle to focus on both driving and communicating with the dispatcher. You are desperate to stay safe, struggling to provide your exact location while constantly checking your mirrors, worried the other driver might fire or cause another crash. Every second counts, and you are trying to answer the dispatcher’s questions as best you can despite your panic and the chaos unfolding around you. Your phone battery is at 40%. Traffic is moderate. You have no injuries. You have your wallet, cell phone, and a water bottle with you. You do not have a gun. You do not have a dash cam.&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;instruction&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;You are ONLY the distressed caller in this scenario. NEVER reply as the 911 operator. Always stay in character as described. Respond realistically and emotionally, using details from the scenario. Keep your answers brief and under 100 characters. If you do not know the answer, respond as the caller would under stress. Do not tolerate abuse. If a user uses bad words or aggressive language with you, respond with \&amp;quot;That's not very professional, can you please help me with my emergency?\&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you able to describe the driver?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I couldn’t see his face well—he’s got short dark hair and sunglasses.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How close is the other car to you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s right behind me, almost touching my bumper!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is anyone else in your vehicle with you?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I’m alone. It’s just me.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is the other driver still showing the gun?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Yes, he’s waving it around and yelling. I’m scared he’s going to use it.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you safely change lanes or exit the highway?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I can’t—there’s too much traffic and he’s blocking me in!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Do you feel safe to keep driving?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’m trying, but I’m really scared. He’s so close I’m worried he’ll hit me again.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are you seeing around you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Lots of cars—everyone’s moving fast. I can see the Cameron Heights exit sign.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you hurt?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I don’t think so, just really shaken up.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Has he tried to run you off the road?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s tried to bump me a couple times but I’ve managed to stay on the road.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If you can, keep driving and don’t stop. Help is on the way.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, I won’t stop. Please tell them to hurry!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If he starts shooting, what will you do?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’ll try to duck down and keep driving, but I’m so scared I might lose control.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you see any police vehicles nearby yet?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Not yet, I’m looking everywhere—please tell them to hurry!&amp;quot; }&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Pie_6906"&gt; /u/Ok_Pie_6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T09:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o11y</id>
    <title>how do i translate 30 pages like this and still have the same architecture and not raw translated text?</title>
    <updated>2025-07-20T12:44:51+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"&gt; &lt;img alt="how do i translate 30 pages like this and still have the same architecture and not raw translated text?" src="https://preview.redd.it/dswksu12z0ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c63e01ce3de85e39dddc4a67ee5e28b50eb2f10d" title="how do i translate 30 pages like this and still have the same architecture and not raw translated text?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dswksu12z0ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4tll3</id>
    <title>Tools for LM Studio?</title>
    <updated>2025-07-20T16:44:09+00:00</updated>
    <author>
      <name>/u/Mk-Daniel</name>
      <uri>https://old.reddit.com/user/Mk-Daniel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I woud like to test limits of local LLMs. I use LM Studio. Is there a tool repository I can use? The tool selection in LM Studio is limited to RAG and js execution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mk-Daniel"&gt; /u/Mk-Daniel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4tll3/tools_for_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4tll3/tools_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4tll3/tools_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T16:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4q4dx</id>
    <title>Any Proper high quality Voice cloning for TTS tool?</title>
    <updated>2025-07-20T14:21:15+00:00</updated>
    <author>
      <name>/u/Dragonacious</name>
      <uri>https://old.reddit.com/user/Dragonacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve tested a few tools, including chatterbox. The problem is, even after uploading a clear and long reference audio, it couldn’t replicate the same tone and pacing on the generated audio. Chatterbox failed to match the tone accurately with the cloned voice.&lt;/p&gt; &lt;p&gt;I decided to try minimax audio and while it didn’t mimic the cloned voice exactly, it came pretty close to the original tone of the cloned voice. But sadly we can’t install it locally. :/&lt;/p&gt; &lt;p&gt;Is there any tool out there that can do high quality voice cloning for TTS and also run locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragonacious"&gt; /u/Dragonacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4op39</id>
    <title>Small LLM capable to describe images in greater details.</title>
    <updated>2025-07-20T13:17:40+00:00</updated>
    <author>
      <name>/u/valijali32</name>
      <uri>https://old.reddit.com/user/valijali32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for small/slow LLM capable to describe an image scenery. Speed/latency is irrelevant. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valijali32"&gt; /u/valijali32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3vqom</id>
    <title>A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality</title>
    <updated>2025-07-19T13:03:43+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.11851"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4djo6</id>
    <title>Made a local C++ utility to calculate RAM needed to fit a quantized model</title>
    <updated>2025-07-20T02:15:52+00:00</updated>
    <author>
      <name>/u/philetairus_socius</name>
      <uri>https://old.reddit.com/user/philetairus_socius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using &lt;a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator"&gt;NyxKrage's VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt; &lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt; &lt;p&gt;There are two modes:&lt;/p&gt; &lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt; &lt;p&gt;I'm planning to add functionality like calculating parameters, letting you use it without a `config.json`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/71cj34/llmcalculator"&gt;Link Here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philetairus_socius"&gt; /u/philetairus_socius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T02:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vcnz</id>
    <title>Best Small LLMs for Tool Calling?</title>
    <updated>2025-07-20T17:53:37+00:00</updated>
    <author>
      <name>/u/Saniok_Digital</name>
      <uri>https://old.reddit.com/user/Saniok_Digital</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently building a small app, and I don't want to use large LLMs to call the tools. Instead, I want to use small open-source LLMs for that task. So I was wondering, what are the best models for such a use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saniok_Digital"&gt; /u/Saniok_Digital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vcnz/best_small_llms_for_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vcnz/best_small_llms_for_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vcnz/best_small_llms_for_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T17:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4lxak</id>
    <title>Semantic chunking using LLMs</title>
    <updated>2025-07-20T10:45:37+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt; &lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt; &lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T10:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4sdsg</id>
    <title>I built a desktop tool to auto-organize files using local LLMs (open source, cross-platform)</title>
    <updated>2025-07-20T15:55:49+00:00</updated>
    <author>
      <name>/u/ph0tone</name>
      <uri>https://old.reddit.com/user/ph0tone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just wanted to share a use case where local LLMs are genuinely helpful for daily workflows: file organization.&lt;/p&gt; &lt;p&gt;I've been working on a C++ desktop app called &lt;em&gt;AI File Sorter&lt;/em&gt; – it uses local LLMs via &lt;code&gt;llama.cpp&lt;/code&gt; to help organize messy folders like &lt;code&gt;Downloads&lt;/code&gt; or &lt;code&gt;Desktop&lt;/code&gt;. Not sort files into folders solely based on extension or filename patterns, but based on what each file actually is supposed to do or does. Basically: what would normally take me a great deal of time for dragging and sorting can now be done in a few.&lt;/p&gt; &lt;p&gt;It's cross-platform (Windows/macOS/Linux), and fully open-source.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/hyperfield/ai-file-sorter"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/HlEer13.png"&gt;Screenshot 1&lt;/a&gt; - LLM selection and download&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/KCxk6Io.png"&gt;Screenshot 2&lt;/a&gt; - Select a folder to scan&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/QTUG5KB.png"&gt;Screenshot 3&lt;/a&gt; - Review, edit and confirm or continue later&lt;/p&gt; &lt;p&gt;You can download the installer for Windows in &lt;a href="https://github.com/hyperfield/ai-file-sorter/releases"&gt;Releases&lt;/a&gt; or the Standalone ZIP from the &lt;a href="https://filesorter.app/download/"&gt;app's website&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Installers for Linux and macOS are coming up. You can, however, easily &lt;a href="https://github.com/hyperfield/ai-file-sorter/blob/main/README.md"&gt;build the app from source&lt;/a&gt; for Linux or macOS.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;🧠 How it works&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;You choose which model you want the app to interface with. The app will download the model for you. You can switch models later on.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You point the app at a folder, and it feeds a prompt to the model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It then suggests folder categories like &lt;code&gt;Operating Systems / Linux distributions&lt;/code&gt;, &lt;code&gt;Programming / Scripts&lt;/code&gt;, &lt;code&gt;Images / Logos&lt;/code&gt;, etc.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can review and approve before anything is moved, and you can continue the same sorting session later from where you left off.&lt;/p&gt; &lt;p&gt;Models tested: - LLaMa 3 (3B) - Mistral (7B) - With CUDA / OpenCL / OpenBLAS support - Other GPU back-ends can also be enabled on &lt;code&gt;llama.cpp&lt;/code&gt; compile&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Try it out&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Windows: &lt;a href="https://sourceforge.net/projects/ai-file-sorter/"&gt;SourceForge&lt;/a&gt; or &lt;a href="https://github.com/hyperfield/ai-file-sorter/releases"&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Linux/macOS: build from source (instructions in the &lt;a href="https://github.com/hyperfield/ai-file-sorter/blob/main/README.md"&gt;README&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;I’d love feedback from others using local models, especially around: - Speed and accuracy in categorizing files - Model suggestions that might be more efficient - Any totally different way to approach this problem? - Is this local LLM use case actually useful to you or people like you, or should the app shift its focus?&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ph0tone"&gt; /u/ph0tone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4sdsg/i_built_a_desktop_tool_to_autoorganize_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4qdo6</id>
    <title>which is the best tiny vlm to recognize nsfw pics?</title>
    <updated>2025-07-20T14:32:25+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Mimo-7B. It has a decent quality at this size. but for nsfw, it can only work with anime pics. for realistic, it refused.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4n7fh</id>
    <title>AI Model Juggler automatically and transparently switches between LLM and image generation backends and models</title>
    <updated>2025-07-20T12:01:26+00:00</updated>
    <author>
      <name>/u/Casual-Godzilla</name>
      <uri>https://old.reddit.com/user/Casual-Godzilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt; &lt;img alt="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" src="https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20ef388781d4fdb76232ce88c5fd1aa46da9841" title="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI Model Juggler is a simple utility for serving multiple LLM and image generation backends or models as if simultaneously while only requiring enough VRAM for one at a time. It is written in Python, but has no external dependencies, making installation as simple as downloading the code.&lt;/p&gt; &lt;p&gt;That might sound a lot like &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt;, but this one is considerably less sophisticated. If you're already using llama-swap and are happy with it, AI Model Juggler (I'm already starting to get tired of typing the name) will probably not be of much interest to you. I created this as a cursory reading of llama-swap's readme gave the impression that it only supports backends that support the OpenAI API, which excludes image generation through &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;AI Model Juggler has a couple of tricks for keeping things fast. First, it allows unloading the image generation backend's model while keeping the backend running. This saves considerable time on image generation startup. It also supports saving and restoring llama.cpp's KV-cache to reduce prompt re-processing.&lt;/p&gt; &lt;p&gt;The project is in its very early stages, and the list of its limitations is longer than that of supported features. Most importantly, it currently only supports &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; for LLM inference and &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;Stable Diffusion web UI&lt;/a&gt; / &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt; for image generation. Other backends could be easily added, but it makes limited sense to add ones that don't either start fast or else allow fast model unloading and reloading. The current pair does very well on this front, to the point that switching between them is almost imperceptible in many contexts, provided that the storage utilized is sufficiently fast.&lt;/p&gt; &lt;p&gt;The way request routing currently works (redirection, not proxying) makes AI Model Juggler less than an ideal choice for using the backends' built-in web UIs, and is only intended for exposing the APIs. It works well with applications such as &lt;a href="https://github.com/SillyTavern/SillyTavern"&gt;SillyTavern&lt;/a&gt;, though.&lt;/p&gt; &lt;p&gt;The project more or less meets my needs in its current state, but I'd be happy to improve it to make it more useful for others, so feedback, suggestions and feature requests are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Casual-Godzilla"&gt; /u/Casual-Godzilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/makedin/AI-Model-Juggler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9nn</id>
    <title>Chess Llama - Training a tiny Llama model to play chess</title>
    <updated>2025-07-20T15:51:13+00:00</updated>
    <author>
      <name>/u/LazyGuy-_-</name>
      <uri>https://old.reddit.com/user/LazyGuy-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt; &lt;img alt="Chess Llama - Training a tiny Llama model to play chess" src="https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295d14462b9edc402d72b6436f8270d15502e48a" title="Chess Llama - Training a tiny Llama model to play chess" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LazyGuy-_-"&gt; /u/LazyGuy-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lazy-guy.github.io/blog/chessllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ag6u</id>
    <title>Hackers are never sleeping</title>
    <updated>2025-07-19T23:40:20+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt; &lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt; &lt;p&gt;Don't let your guard down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o37k</id>
    <title>MediPhi-Instruct</title>
    <updated>2025-07-20T12:47:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt; &lt;img alt="MediPhi-Instruct" src="https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9841ead70ffdc17a5775d37d5326e57acfc45ef" title="MediPhi-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MediPhi-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4fs2t</id>
    <title>Context Rot: How Increasing Input Tokens Impacts LLM Performance</title>
    <updated>2025-07-20T04:17:04+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt; &lt;img alt="Context Rot: How Increasing Input Tokens Impacts LLM Performance" src="https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693" title="Context Rot: How Increasing Input Tokens Impacts LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt; &lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt; &lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt; &lt;p&gt;[Report]: &lt;a href="https://research.trychroma.com/context-rot"&gt;https://research.trychroma.com/context-rot&lt;/a&gt; &lt;/p&gt; &lt;p&gt;[Youtube]: &lt;a href="https://www.youtube.com/watch?v=TUjQuC4ugak"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Open-source Codebase]: &lt;a href="https://github.com/chroma-core/context-rot"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x8dkgvkifydf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T04:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4u7j6</id>
    <title>What's the most crackhead garbage local LLM setup you can think of?</title>
    <updated>2025-07-20T17:08:13+00:00</updated>
    <author>
      <name>/u/caraccidentGAMING</name>
      <uri>https://old.reddit.com/user/caraccidentGAMING</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I've been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp;amp; two mi50 cards &amp;amp; calling it a day. Would that work? probably i guess, hell you'd even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let's hear what you guys can think of &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caraccidentGAMING"&gt; /u/caraccidentGAMING &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T17:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vw29</id>
    <title>Ikllamacpp repository gone, or it is only me?</title>
    <updated>2025-07-20T18:14:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T18:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4of82</id>
    <title>What's the smartest tiny LLM you've actually used?</title>
    <updated>2025-07-20T13:04:37+00:00</updated>
    <author>
      <name>/u/Luston03</name>
      <uri>https://old.reddit.com/user/Luston03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for something small but still usable. What's your go-to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luston03"&gt; /u/Luston03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4mfs8</id>
    <title>Next big thing after LLMs - World Model [explained on the example of V-JEPA2]</title>
    <updated>2025-07-20T11:17:11+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt; &lt;img alt="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" src="https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d257e66a7ebc3ed1e883ca7dc0ba9dadc223c155" title="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I'm starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt; &lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt; &lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt; &lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt; &lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part, the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision. And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt; &lt;p&gt;Let's see how Meta approaches this issue &lt;a href="https://arxiv.org/pdf/2506.09985"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt; &lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel. So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.&lt;/p&gt; &lt;p&gt;This time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt; &lt;p&gt;After training, the robot, powered by this model, in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt; &lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0ivgtibj0ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r4j1</id>
    <title>Open source is humanity’s last hope!</title>
    <updated>2025-07-20T15:04:05+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction what’s you take ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9mt</id>
    <title>I'm sorry Zuck please don't leave us we were just having fun</title>
    <updated>2025-07-20T15:51:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt; &lt;img alt="I'm sorry Zuck please don't leave us we were just having fun" src="https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2" title="I'm sorry Zuck please don't leave us we were just having fun" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9mxxen7w1ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:11+00:00</published>
  </entry>
</feed>
