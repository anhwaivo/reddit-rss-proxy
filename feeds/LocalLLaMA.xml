<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-10T13:49:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mm2ae5</id>
    <title>Reasoning Models + Tool Use outperform most vision models for complex object detection</title>
    <updated>2025-08-09T22:34:15+00:00</updated>
    <author>
      <name>/u/bci-hacker</name>
      <uri>https://old.reddit.com/user/bci-hacker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm2ae5/reasoning_models_tool_use_outperform_most_vision/"&gt; &lt;img alt="Reasoning Models + Tool Use outperform most vision models for complex object detection" src="https://external-preview.redd.it/bjJwOG1yb2FtMmlmMSubCZJHqCfZ9hv7QWq0qC3p6f_M7abBmrHK3aFoqfso.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e659c4dfe48656372efcddf0abfe7e1042cac4b6" title="Reasoning Models + Tool Use outperform most vision models for complex object detection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Task: detect the street sign in this image.&lt;/p&gt; &lt;p&gt;This is a hard problem for most SOTA object detectors. The sign is barely visible, even for humans. So we gave a reasoning system (o3) access to tools: zoom, crop, and call an external detector. No training, no fine-tuning—just a single prompt. And it worked. See it in action: &lt;a href="https://www.spatial-reasoning.com/share/d7bab348-3389-41c7-9406-5600adb92f3e"&gt;https://www.spatial-reasoning.com/share/d7bab348-3389-41c7-9406-5600adb92f3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think this is quite cool in that you can take a difficult problem and make it more tractable by letting the model reason through pixels. It's not perfect, it's slow and brittle, but the capability unlock over vanilla reasoning model (i.e. just ask ChatGPT to generate bounding box coordinates) is quite strong.&lt;/p&gt; &lt;p&gt;Opportunities for future research:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tokenization - all these models operate in compressed latent space. If your object was 20x20 crop, then in the latent space (assume 8x compression), it now represents 2x2 crop which makes it extremely hard to &amp;quot;see&amp;quot;. Unlocking tokenization is also tricky since if you shrink the encoding factor the model gets larger which just makes everything more expensive and slow&lt;/li&gt; &lt;li&gt;Decoder. Gemini 2.5 is awesome since i believe (my hunch) is that their MoE has an object detection specific decoder that lets them generate bounding boxes accurately.&lt;/li&gt; &lt;li&gt;Tool use. I think it's quite clear from some of these examples that tool use applied to vision can help with some of these challenges. This means that we'd need to build RL recipes (similar to &lt;a href="https://arxiv.org/html/2507.05791v1"&gt;https://arxiv.org/html/2507.05791v1&lt;/a&gt;) paper that showcased that CUA (computer use agents) benefit from RL for object detection related tasks to further&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I think this is a powerful capability unlock that previously wasn't possible. For example VLMs such as 4o and CLIP can't get anywhere close to this. Reasoning seems to be that paradigm shift.&lt;/p&gt; &lt;p&gt;NOTE: there's still lots of room to innovate. not making any claims that vision is dead lol&lt;/p&gt; &lt;p&gt;Try the demo: &lt;a href="http://spatial-reasoning.com/"&gt;spatial-reasoning.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/QasimWani/spatial-reasoning"&gt;https://github.com/QasimWani/spatial-reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bci-hacker"&gt; /u/bci-hacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jna4croam2if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm2ae5/reasoning_models_tool_use_outperform_most_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm2ae5/reasoning_models_tool_use_outperform_most_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlomlb</id>
    <title>My thoughts on gpt-oss-120b</title>
    <updated>2025-08-09T12:48:57+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the model dropped, it's become notoriously hated on for its censorship. (Idk what people were expecting from OpenAI of all companies)&lt;/p&gt; &lt;p&gt;All the chat template issues and performance fluctuations with varying cloud providers made it even worse for all the people who were optimistic to try it out. &lt;/p&gt; &lt;p&gt;On the first day, I remember the model rejecting my request to generate some code with: I'm sorry, I cannot provide long lines of code as per my policy (or something ridiculous like this)&lt;/p&gt; &lt;p&gt;A lot of the decisions were new: Harmony chat template, MXFP4 format for full precision and confusing quantization options. The model's inference speed was also unusually slow for 5.1B active params (I got 10-15tps initially). And naturally, I decided to wait a little for things to settle down a bit before getting to test the model. I just downloaded the original HF repo from openai and waited a few days.&lt;/p&gt; &lt;p&gt;Yesterday, I pulled the latest chat template changes from HF repo, pulled the latest llama.cpp code, changed the model template file to ensure I set the model's default reasoning to &amp;quot;high&amp;quot;(since my day-to-day tasks mostly deal with reasoning, coding and instruction following), made a local MXFP4 quant myself, set the right inference settings from Unsloth docs and took it for a spin.&lt;/p&gt; &lt;p&gt;Right off the bat, I noticed a lot of improvements! My inference speed went up to 35tps(!), the weird rejections were gone and the model felt actually usable.&lt;/p&gt; &lt;p&gt;Ran a few of my smoke tests to see what the vibe is like and I feel it's quite good for its size. Sharing a few below:&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Generate a website for a SaaS startup titled Steve's PC Repair. The startup is a cloud rental service offering refurbished multi-gpu systems for users to rent compute on. The site needs to fit the modern tech aesthetic and be a step above current competitors offerings. Return the code in a single self-contained code file.&lt;/p&gt; &lt;p&gt;Result: &lt;a href="https://codepen.io/Loki-the-animator/pen/RNWZQbv"&gt;https://codepen.io/Loki-the-animator/pen/RNWZQbv&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Create me a visualization using javascript. Make it as interesting as you possibly can...&lt;/p&gt; &lt;p&gt;Result: &lt;a href="https://codepen.io/Loki-the-animator/pen/empEVmW"&gt;https://codepen.io/Loki-the-animator/pen/empEVmW&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Which word comes next: Stone, Often, Canine, _: A Helpful B Freight C Glow D Grape&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer: B – Freight&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;br /&gt; Each word hides the name of a number inside it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stone&lt;/strong&gt; → “ONE” (st &lt;strong&gt;ONE&lt;/strong&gt;) = 1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Often&lt;/strong&gt; → “TEN” (of &lt;strong&gt;TEN&lt;/strong&gt;) = 10&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Canine&lt;/strong&gt; → “NINE” (ca &lt;strong&gt;NINE&lt;/strong&gt;) = 9&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next word should therefore contain the next number in the descending sequence … 8.&lt;br /&gt; &lt;strong&gt;Freight&lt;/strong&gt; contains “EIGHT” (fr &lt;strong&gt;EIGHT&lt;/strong&gt;) and fits the pattern, while the other choices do not hide a number.&lt;/p&gt; &lt;p&gt;So the word that comes next is &lt;strong&gt;Freight&lt;/strong&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;One recurring theme with the model is that it simply does only what it's asked to but it does it right. However, when you decide to invest time in your prompts, it has incredible attention to detail breaking down and adhering to the intricacies of a complex set of instructions.&lt;/p&gt; &lt;p&gt;For example, it nailed the following prompt first try:&lt;/p&gt; &lt;p&gt;Using the Pygame library in Python, create a simple turn-based tactical game on an 8x8 grid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Game Board:&lt;/strong&gt; Create an 8x8 grid. Display it graphically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Units:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Create a Unit class. Each unit has attributes: hp (health points), attack_power, move_range (e.g., 3 tiles), and team ('blue' or 'red').&lt;/li&gt; &lt;li&gt;Place two &amp;quot;blue&amp;quot; units and two &amp;quot;red&amp;quot; units on the board at starting positions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Game Flow (Turn-Based):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;The game should alternate turns between the 'blue' team and the 'red' team.&lt;/li&gt; &lt;li&gt;During a team's turn, the player can select one of their units by clicking on it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Player Actions:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Selection:&lt;/strong&gt; When a player clicks on one of their units during their turn, that unit becomes the &amp;quot;selected unit.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Movement:&lt;/strong&gt; After selecting a unit, the game should highlight all valid tiles the unit can move to (any tile within its move_range, not occupied by another unit). Clicking a highlighted tile moves the unit there and ends its action for the turn.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attack:&lt;/strong&gt; If an enemy unit is adjacent to the selected unit, clicking on the enemy unit should perform an attack. The enemy's hp is reduced by the attacker's attack_power. This ends the unit's action. A unit can either move OR attack in a turn, not both.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;End Condition:&lt;/strong&gt; The game ends when all units of one team have been defeated (HP &amp;lt;= 0). Display a &amp;quot;Blue Team Wins!&amp;quot; or &amp;quot;Red Team Wins!&amp;quot; message.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Provide the full, single-script, runnable Pygame code. The code should be well-structured. Include comments explaining the main parts of the game loop, the event handling, and the logic for movement and combat.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Additionally, to test its instruction following capabilities, I used prompt templates from: &lt;a href="https://www.jointakeoff.com/prompts"&gt;https://www.jointakeoff.com/prompts&lt;/a&gt; and asked it to build an e-commerce website for AI gear and this is honestly where I was blown away.&lt;/p&gt; &lt;p&gt;It came up with a pretty comprehensive 40-step plan to build the website iteratively while fully adhering to my instructions (I could share it here but it's too long)&lt;/p&gt; &lt;p&gt;To spice things up a little, I gave the same planner prompt to Gemini 2.5 Pro and GLM 4.5 Air Q4_0 and had a new context window pulled up with Gemini 2.5 Pro to judge all 3 results and provide a score on a scale of 1-100 based on the provided plan's feasibility and adherence to instructions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss-120b (high): 95&lt;/li&gt; &lt;li&gt;Gemini 2.5 Pro: 99&lt;/li&gt; &lt;li&gt;GLM 4.5 Air: 45&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;I ran tons and tons of such tests that I can share but they would honestly clutter the intended takeaway of this post at this point.&lt;/p&gt; &lt;p&gt;To summarize, here are my honest impressions about the model so far: 1) The model is so far the best I've gotten to run locally in terms of instruction following. 2) Reasoning abilities are top-notch. It's minimal yet thorough and effective. I refrained from using the Qwen thinking models since they think quite extensively (though they provide good results) and I couldn't fit them into my workflow. GLM 4.5 Air thinks less but the results are not as effective as the Qwen ones. gpt-oss-120b seems like the right sweet spot for me. 3) Good coder but nothing to be blown away from. Writes error-free code and does what you ask it to. If you write comprehensive prompts, you can expect good results. 4) Have tested basic agentic capabilities and have had no issues on that front so far. Yet to do extensive tests 5) The best size-to-speed model so far. The fact that I can actually run a full-precision 120b at 30-35TPS with my setup is impressive!&lt;/p&gt; &lt;p&gt;It's the best &amp;lt;120B model in my books for my use cases and it's gonna be my new daily driver from here on out. &lt;/p&gt; &lt;p&gt;I honestly feel like its censorship and initial setup-related hiccups has led to preconceived bad opinions but you have to try it out to really understand what I'm talking about.&lt;/p&gt; &lt;p&gt;I'm probably gonna get down-voted for this amidst all the hate but I don't really care. I'm just keepin' it real and it's a solid model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T12:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmdft7</id>
    <title>Uncensored rp models</title>
    <updated>2025-08-10T08:41:26+00:00</updated>
    <author>
      <name>/u/Imaginary_Bread9711</name>
      <uri>https://old.reddit.com/user/Imaginary_Bread9711</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any good newer models that i can use for uncensored rp? Will gemma-3n-4b-abliterated work? Is it better than qwen3-4b-abliterated? Is there any newer models that were learning with nsfw material and made for uncensored rp? Preferably models with 8 billion parameters or lower. My pc: gtx 1660 super (6gb vram), xeon e5-2650v2 (2.6hz, 8c16t) 16gb ddr3 ram, sata ssd.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imaginary_Bread9711"&gt; /u/Imaginary_Bread9711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdft7/uncensored_rp_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdft7/uncensored_rp_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdft7/uncensored_rp_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T08:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmcxug</id>
    <title>Gemma-3n 2b Rough Benchmarks on Mid-Range Android Device (8gb RAM)</title>
    <updated>2025-08-10T08:08:10+00:00</updated>
    <author>
      <name>/u/ditpoo94</name>
      <uri>https://old.reddit.com/user/ditpoo94</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcxug/gemma3n_2b_rough_benchmarks_on_midrange_android/"&gt; &lt;img alt="Gemma-3n 2b Rough Benchmarks on Mid-Range Android Device (8gb RAM)" src="https://preview.redd.it/mcxrhxe8g5if1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50e26cc8264ced9ca1b736802780e864f6a5d911" title="Gemma-3n 2b Rough Benchmarks on Mid-Range Android Device (8gb RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma-3n 2b Rough Benchmarks, for one's looking to run/build local/offline AI/Llm apps.&lt;/p&gt; &lt;p&gt;These are not rigorous, but to give an rough idea about how well they perform, they have scope to offer better throughput. (via engg, optimizations).&lt;/p&gt; &lt;p&gt;IOS devices have few times better throughput, so larger 4b model variant are also viable for IOS devices.&lt;/p&gt; &lt;p&gt;Shared by &amp;quot;@dpawnlabs&amp;quot; on x/twitter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ditpoo94"&gt; /u/ditpoo94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mcxrhxe8g5if1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcxug/gemma3n_2b_rough_benchmarks_on_midrange_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcxug/gemma3n_2b_rough_benchmarks_on_midrange_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T08:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm58wv</id>
    <title>MiMo-VL-7B-RL-2508 from XiaoMi is out</title>
    <updated>2025-08-10T00:56:40+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm58wv/mimovl7brl2508_from_xiaomi_is_out/"&gt; &lt;img alt="MiMo-VL-7B-RL-2508 from XiaoMi is out" src="https://external-preview.redd.it/z7jaNHadE97J-68rBpePIzvUnN2-TEcrofNYwD6SFpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=707ecc4f614891edd3d17f689c8f41dfdc3e0b0d" title="MiMo-VL-7B-RL-2508 from XiaoMi is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;📈 Performance Improvements&lt;/h1&gt; &lt;p&gt;MiMo-VL-7B-RL-2508 demonstrates consistent improvements across both image and video benchmarks, achieving notable milestones of &lt;strong&gt;70.6 on MMMU&lt;/strong&gt; and &lt;strong&gt;70.8 on VideoMME&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL-2508"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm58wv/mimovl7brl2508_from_xiaomi_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm58wv/mimovl7brl2508_from_xiaomi_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T00:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmce2h</id>
    <title>Anyone here with an AMD AI Max+ 395 + 128GB setup running coding agents?</title>
    <updated>2025-08-10T07:32:31+00:00</updated>
    <author>
      <name>/u/Admirable_Reality281</name>
      <uri>https://old.reddit.com/user/Admirable_Reality281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who happen to own an AMD AI Max+ 395 machine with 128GB of RAM, have you tried running models with coding agents like Cline, Aider, or similar tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Reality281"&gt; /u/Admirable_Reality281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmce2h/anyone_here_with_an_amd_ai_max_395_128gb_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmce2h/anyone_here_with_an_amd_ai_max_395_128gb_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmce2h/anyone_here_with_an_amd_ai_max_395_128gb_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T07:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmgm19</id>
    <title>Anyone experienced with self hosting at enterprise level: how do you handle KV caching?</title>
    <updated>2025-08-10T11:54:02+00:00</updated>
    <author>
      <name>/u/Budget_Map_3333</name>
      <uri>https://old.reddit.com/user/Budget_Map_3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm setting up a platform where I intend to self host models. Starting off with serverless runpod GPUs for now (what I can afford).&lt;/p&gt; &lt;p&gt;So I came to the realisation that one of the core variables for keeping costs down will be KV caching. My platform will be 100% around multi turn conversations with long contexts. In principle, from what I understand the KV cache is stored on the actual GPU in a LRU way which is fine for a few concurrent users. &lt;/p&gt; &lt;p&gt;But what happens when we start to scale up? Many users. Many serverless endpoints. Many multi turn conversations with long contexts. To not &amp;quot;waste&amp;quot; KV caching I guess one way would be to configure vLLM or SGLang to offload the KV cache to CPU, then to local NVMe and then finally to a network volume based on the interval. I guess. But it seems like this is gonna be a very difficult task working with serverless, permament pods are probably a different story. &lt;/p&gt; &lt;p&gt;Just looking for some tips here from any engineers who have experience self-hosting at a large scale and serving concurrent sessions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Budget_Map_3333"&gt; /u/Budget_Map_3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1mcj</id>
    <title>A personal take: LLMs are stuck, but local might win?</title>
    <updated>2025-08-09T22:04:08+00:00</updated>
    <author>
      <name>/u/Truncleme</name>
      <uri>https://old.reddit.com/user/Truncleme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some personal rambling thoughts on LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretraining decides what the model knows. By chewing through a massive slice of the internet, it picks up world knowledge, language patterns, common sense, and the core generalization that underwrites its “understanding.” That’s the source—and the cap—of its intelligence.&lt;/li&gt; &lt;li&gt;Post‑training (RLHF/SFT/etc.) teaches the model what to do. With curated examples, it learns to follow instructions, take on styles, and execute tasks. Think of it as behavior shaping and skill training: making the model useful and controllable.&lt;/li&gt; &lt;li&gt;Inference is where it actually does the work, conditioned on your prompt/context. It pulls from pretrained knowledge and post‑trained skills to generate an answer. The context determines what gets activated, but the model isn’t learning new knowledge or skills at this step.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Where things feel stuck: the real limiter is broad generalization/learning—the thing we hand‑wave as “emergence.” So far we mostly “unlocked” it via Scaling Laws (bigger models during pretraining). But we all know the data produced by human is not infinite, and those ultra-large model releases since 2024 haven’t delivered the jump people expected. And because we don’t actually know what causes emergence, we can’t even say for sure it’s just a data ceiling; maybe the next tier of emergence simply doesn’t happen.&lt;/p&gt; &lt;p&gt;So then, every AI compony turn their eyes to the post‑training and inference‑time tricks. Without fresh emergence, though, what we get is “skill generalization,” not “cognitive generalization.” That pushes us toward stronger narrow AI rather than stronger AGI. The most broadly useful “narrow” skill right now is coding. I’m still not overly bullish on this path: it can go far on those well‑specified tasks, but fails on comparing 9.9 and 9.11, which some called jagged intelligence.&lt;/p&gt; &lt;p&gt;On “reasoning”: to me, this is a post‑training‑shaped output pattern that helps the model structure and steer the context to better elicit knowledge it already has from pretraining. It’s a hybrid of post‑training and inference, but it can’t break the pretraining ceiling. It also locks the model into rigid scaffolds (step‑by‑step templates), which reduces the human’s control over the prompt. That’s why on some tasks—like creative writing—reasoning mode can backfire.&lt;/p&gt; &lt;p&gt;Agents are the same idea turned up: use post‑training to learn some strategies that manipulates context more flexibly (tools, memory, multi‑turn planning), not just the slice between user input and model output. But the fundamental limitation remains: no real learning at run‑time, no genuinely new generalization.&lt;/p&gt; &lt;p&gt;Why this might be good news for local LLMs: if this framing is right, local setups actually benefit. Our poor local hardwares haven’t hit the scaling wall yet, so there’s still headroom to gain from Scaling Laws at the sizes we can run. If the agent approach keeps working, even better: since the “agent era” kicked off, model sizes seem to be trending down. Opus 4 doesn’t look like as big as Opus 3; GPT‑5 looks much smaller than GPT‑4.5; even Qwen‑30B‑A3 and GPT‑OSS‑20B‑A3 are putting up strong agent results. That suggests agents don’t need a lot parameters to run, especially for active params. There’s even a strange 120B‑A5 here. If the thought is true, long‑running local agents should be feasible soon without falling far behind—because the parameter gap isn’t huge anymore. FWIW, despite the flak OpenAI gets, that 120B-A5 model’s quite decent for its size, with 64 GB RAM + disk swap + a 5070 Ti you can get close to 20 tok/s.&lt;/p&gt; &lt;h1&gt;Curious what the crowd thinks: does this line up with your experience, or am I way off?&lt;/h1&gt; &lt;p&gt;PS: This was loosely translated from another language —sorry for any awkward phrasing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Truncleme"&gt; /u/Truncleme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:04:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh7nq</id>
    <title>Qwen and DeepSeek is great for coding but</title>
    <updated>2025-08-10T12:24:23+00:00</updated>
    <author>
      <name>/u/XiRw</name>
      <uri>https://old.reddit.com/user/XiRw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ever noticed how it takes it upon itself (sometimes) to change shit around on the frontend to make it the way it wants without your permission?? &lt;/p&gt; &lt;p&gt;It’s not even little insignificant things it’s major changes.&lt;/p&gt; &lt;p&gt;Not only that but with Qwen3 coder especially I tell it instructions with how to format its response back to me and it ignores it unless I call it out for not listening and become dramatic about it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiRw"&gt; /u/XiRw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmioub</id>
    <title>GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?</title>
    <updated>2025-08-10T13:32:55+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt; &lt;img alt="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" src="https://external-preview.redd.it/SN7M9mchkv9CXffJ79rhHnoX7FuJFy8UkokLfATf5x4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48da4438afbefc792c2d3d5523373a4ba43a7e48" title="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/10jLvkV.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1c2w</id>
    <title>I attempted to clone Grok's Ani, while its not perfect it's a start</title>
    <updated>2025-08-09T21:51:26+00:00</updated>
    <author>
      <name>/u/ELPascalito</name>
      <uri>https://old.reddit.com/user/ELPascalito</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"&gt; &lt;img alt="I attempted to clone Grok's Ani, while its not perfect it's a start" src="https://preview.redd.it/6tb6td9re2if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fabc59b6422d1568506def7099315610271ddf4" title="I attempted to clone Grok's Ani, while its not perfect it's a start" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a good developer by any means, but I made this for the player2 jam in only 7 days! It's a humble start, it's still very rough but emotions work well, it called me yogurt boy for no reason 😭&lt;/p&gt; &lt;p&gt;&lt;a href="https://player2.game/discover/games/019884e5-3dd9-7872-97b3-88b8c81237a2"&gt;https://player2.game/discover/games/019884e5-3dd9-7872-97b3-88b8c81237a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model made in vroid by me, It uses the player2 app, to utilise the free LLM and TTS for both the text and sound, it's not perfect but it's free you just install the app, then play the game and it'll autodetect the player2 AI, the Emotions system works and face and TTS syncs to the lips as bet as it cann, again this is my humble creation, it's open source do check out the GitHub I believe we need to all unite in trying to creating a better version of this 3D tech for free!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ELPascalito"&gt; /u/ELPascalito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6tb6td9re2if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmg9hx</id>
    <title>Go home ChatGPT, I’m drunk</title>
    <updated>2025-08-10T11:34:54+00:00</updated>
    <author>
      <name>/u/ExplorerWhole5697</name>
      <uri>https://old.reddit.com/user/ExplorerWhole5697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg9hx/go_home_chatgpt_im_drunk/"&gt; &lt;img alt="Go home ChatGPT, I’m drunk" src="https://preview.redd.it/zjya22rqh6if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b196868a8aabdbdf987918a2202b0f36273eb70f" title="Go home ChatGPT, I’m drunk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This proves that ChatGPT 5 is ve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplorerWhole5697"&gt; /u/ExplorerWhole5697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zjya22rqh6if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg9hx/go_home_chatgpt_im_drunk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg9hx/go_home_chatgpt_im_drunk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlymxq</id>
    <title>When exactly "Qwen3-235B-A22B-2507" started generating flow charts?</title>
    <updated>2025-08-09T19:56:09+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"&gt; &lt;img alt="When exactly &amp;quot;Qwen3-235B-A22B-2507&amp;quot; started generating flow charts?" src="https://preview.redd.it/hw6j18x8u1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aea18c868a634606799ef5b85c9eff6ae5777684" title="When exactly &amp;quot;Qwen3-235B-A22B-2507&amp;quot; started generating flow charts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hw6j18x8u1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T19:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm7azs</id>
    <title>OpenAI gpt-oss-20b &amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M</title>
    <updated>2025-08-10T02:40:18+00:00</updated>
    <author>
      <name>/u/traderjay_toronto</name>
      <uri>https://old.reddit.com/user/traderjay_toronto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"&gt; &lt;img alt="OpenAI gpt-oss-20b &amp;amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M" src="https://preview.redd.it/g22r9c3au3if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c1aebfd70d7867330b7be6dcf6a550cceebb0e3" title="OpenAI gpt-oss-20b &amp;amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preface - I am not a programmer just an AI enthusiast and user. The GPU I got is mainly used for video editing and creative work but I know its very well suited to run large AI models so I decided to test it out. If you want me to test the performance of other models let me know as long it works in LM studio. &lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/Beta87"&gt;u/Beta87&lt;/a&gt; I got LM studio up and running and loaded the two latest model from OpenAI to test it out. Here is what I got performance wise on two wildly different systems:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;20b model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 Blackwell -&lt;/strong&gt; 205 tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 5090M -&lt;/strong&gt; 145tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;120b model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 Blackwell -&lt;/strong&gt; 145 tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 5090M -&lt;/strong&gt; 11 tokens/sec&lt;/p&gt; &lt;p&gt;Had to turn off all guardrail on the laptop to make the 120b model run and it's using system ram as it ran out of GPU memory but it didn't crash.&lt;/p&gt; &lt;p&gt;What a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traderjay_toronto"&gt; /u/traderjay_toronto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g22r9c3au3if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T02:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1zcg</id>
    <title>i'm making dating simulator game with ai npc using open source llm</title>
    <updated>2025-08-09T22:20:10+00:00</updated>
    <author>
      <name>/u/aziib</name>
      <uri>https://old.reddit.com/user/aziib</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt; &lt;img alt="i'm making dating simulator game with ai npc using open source llm" src="https://external-preview.redd.it/ZjR3N2g2MmhjMmlmMW57LOTD_dYSJqtRCJKUw8LsjdMQCiQ_6aIAc3dh1JZp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18b063cf71d62103bbeb0fff25811ee3533f54d1" title="i'm making dating simulator game with ai npc using open source llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you can play on your browser: &lt;a href="https://romram.itch.io/break-time"&gt;https://romram.itch.io/break-time&lt;/a&gt;&lt;br /&gt; you need LM Studio as a local server: &lt;a href="https://lmstudio.ai/"&gt;https://lmstudio.ai/&lt;/a&gt;&lt;br /&gt; use uncensored llama 8b model or more and 8k context window or more for better experience.&lt;br /&gt; i use blacksheep gguf models:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the game engine is using rpg maker mz with some of my modified custom plugins&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aziib"&gt; /u/aziib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hchl142hc2if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmg8uj</id>
    <title>GLM 4.5 355b (IQ3_XXS) is amazing at creative writing.</title>
    <updated>2025-08-10T11:33:52+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 128gb RAM and 16gb VRAM (144gb total RAM) this quant runs pretty well with low context and a little bit of hard drive offloading with &lt;code&gt;mmap&lt;/code&gt;, only resulting in occasional &lt;em&gt;brief&lt;/em&gt; hiccups. Getting ~3 t/s with 4k context, and ~2.4 t/s with 8k context and &lt;code&gt;Flash Attention&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Even at this relatively low quant, the model is extremely coherent, knowledgeable and smart. It's the best one for writing I've used, even better than Qwen3-235b-A22b at Q4_K_XL. Its brilliance has made me genuinely laugh on several occasions and left me in awe of its excellent logic and profound grasp of hypothetical scenarios, and its great ability with character interactions.&lt;/p&gt; &lt;p&gt;However, there are two quirks that I think are (mostly?) low-quant related:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It seems to be actually worse at coding than GLM 4.5 Air at Q5_K_XL. My guess is that while the model has a lot of parameters, the IQ3_XSS quant reduces its precision, which is important in programming.&lt;/li&gt; &lt;li&gt;It sometimes makes minor word-choice errors. For example, it once wrote &lt;em&gt;&amp;quot;He was a bright blue jacket&amp;quot;&lt;/em&gt;, when the correct phrasing should have been &lt;em&gt;&amp;quot;He was&lt;/em&gt; &lt;strong&gt;&lt;em&gt;wearing&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;a bright blue jacket&amp;quot;&lt;/em&gt;. Again, I suspect the lower precision of IQ3_XSS causes these oversights.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because I can only run this model with a relatively limited context window, and while the speed is acceptable (imo), it's still not exactly lightning fast - there may not be many practical uses. Nevertheless, it's great for shorter conversations, and it's fun to experiment and play around with. I'm amazed that a powerful model like this is even runnable at all on consumer hardware and RAM, something that was unthinkable just 1-2 years ago.&lt;/p&gt; &lt;p&gt;Just thought I would share my experience with this quant and model. Maybe someone finds this interesting, or have their own insights/opinions with the model/quants to share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mllt5x</id>
    <title>Imagine an open source code model that in the same level of claude code</title>
    <updated>2025-08-09T10:04:00+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt; &lt;img alt="Imagine an open source code model that in the same level of claude code" src="https://preview.redd.it/diwwcslbwyhf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fa0ed92a8f036fa157e0bb1b95df2206b17f91a" title="Imagine an open source code model that in the same level of claude code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diwwcslbwyhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T10:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm6hsg</id>
    <title>The model router system of GPT-5 is flawed by design.</title>
    <updated>2025-08-10T01:59:20+00:00</updated>
    <author>
      <name>/u/True_Requirement_891</name>
      <uri>https://old.reddit.com/user/True_Requirement_891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model router system or GPT-5 is flawed by design.&lt;/p&gt; &lt;p&gt;The model router has to be fast and cheap, which means using a small model lightweight (low-param). But small models lack deep comprehension and intelligence of larger models. &lt;/p&gt; &lt;p&gt;There are 100s of posts I've seen people claiming GPT-5 can't do basic math or the reasoning is quite lacking which is usually being solved by promoting the model to &amp;quot;think&amp;quot; which usually routes it to the thinking variant or makes the chat model reason more which leads to better output. &lt;/p&gt; &lt;p&gt;Basically, the router sees: A simple arithmetic question or a single line query -&amp;gt; Hmm, looks like simple math, don't need the reasoning model &amp;gt; Routes to non-reasoning chat model.&lt;/p&gt; &lt;p&gt;You need reasoning and intelligence to tell what’s complex and what’s simple.&lt;/p&gt; &lt;p&gt;A simple fix might be to route all number-related queries or logic puzzles to the think model. But do you really need reasoning only for numbers and obvious puzzles...? There are tons of tasks that require reasoning for increased intelligence. &lt;/p&gt; &lt;p&gt;This system is inherently flawed, IMO.&lt;/p&gt; &lt;p&gt;I tried implementing a similar router-like system a year ago. I used another small but very fast LLM to analyze the query and choose between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;A reasoning model (smart but slow and expensive) for complex queries&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A non-reasoning model (not very smart but cheap and fast) for simple queries&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Since the router model had to be low-latency, I used a smaller model, and it always got confused because it lacked understanding of what makes something &amp;quot;complex.&amp;quot; Fine-tuning might’ve helped, but I hardly think so. You need an extremely large amount of training data and give the model time to reason.&lt;/p&gt; &lt;p&gt;The router model has to be lightweight and fast, meaning it’s a cheap, small model. But the biggest issue with small models is their lack of deep comprehension, world knowledge, or nuanced understanding to gauge &amp;quot;complexity&amp;quot; reliably.&lt;/p&gt; &lt;p&gt;You need a larger and intelligent model with deep comprehension fine-tuned to route. You might even need to give it reasoning to make it reliably distinguish between simple and complex.&lt;/p&gt; &lt;p&gt;But this will make it slow and expensive making the whole system pointless...&lt;/p&gt; &lt;p&gt;What am I missing here???? Is it simply built for the audience that used gpt-4o for every task and then this system improves upon it by invoking the reasoning model for &amp;quot;very obviously complex&amp;quot; queries?&lt;/p&gt; &lt;p&gt;Edit: I'd like to clarify I'm not trying to hate on open ai here but trying to discuss the model router system and if it's even worth replicating locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/True_Requirement_891"&gt; /u/True_Requirement_891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm6hsg/the_model_router_system_of_gpt5_is_flawed_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm6hsg/the_model_router_system_of_gpt5_is_flawed_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm6hsg/the_model_router_system_of_gpt5_is_flawed_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T01:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh4tv</id>
    <title>now we have the best open source model that we can use at human level , and all this possible bcz of the chinese model , we have best image generation model ( qwen , seeddream) , video generation ( wan ) , coding model ( qwen 3 ) , coding terminal model ( qwen 3) , overall best model ( deepseek v3)</title>
    <updated>2025-08-10T12:20:21+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open source in coding has like 2 month gap and in image generation model they have like the 1 year gap but now that gap doesnt matter , video generation model is good .&lt;/p&gt; &lt;p&gt;so from all side chinese people did a great job &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmcg87</id>
    <title>New Nemo finetune: Impish_Nemo</title>
    <updated>2025-08-10T07:36:26+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;New creative model with some sass, very large dataset used, super fun for adventure &amp;amp; creative writing, while also being a strong assistant.&lt;br /&gt; Here's the TL;DR, for details check the model card: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;My &lt;strong&gt;best model yet!&lt;/strong&gt; Lots of &lt;strong&gt;sovl!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart, sassy, creative, and unhinged&lt;/strong&gt; — without the brain damage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bulletproof temperature&lt;/strong&gt;, can take in a much &lt;strong&gt;higher temperatures&lt;/strong&gt; than vanilla Nemo.&lt;/li&gt; &lt;li&gt;Feels close to &lt;strong&gt;old CAI&lt;/strong&gt;, as the characters are very present and responsive.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incredibly powerful&lt;/strong&gt; roleplay &amp;amp; adventure model for the size.&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;adventure&lt;/strong&gt; insanely well for its size!&lt;/li&gt; &lt;li&gt;Characters have a &lt;strong&gt;massively upgraded agency!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Over &lt;strong&gt;1B tokens trained&lt;/strong&gt;, carefully preserving intelligence — even upgrading it in some aspects.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt; and &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;Impish_LLAMA_4B&lt;/a&gt; + some &lt;strong&gt;upgrades&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent assistant&lt;/strong&gt; — so many new assistant capabilities I won’t even bother listing them here, just try it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less positivity bias&lt;/strong&gt; , all lessons from the successful &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data learned &amp;amp; integrated, with serious upgrades added — &lt;strong&gt;and it shows!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Trained on an &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic length&lt;/strong&gt; response (1–3 paragraphs, usually 1–2). Length is adjustable via 1–3 examples in the dialogue. &lt;strong&gt;No more rigid short-bias!&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T07:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlsm8e</id>
    <title>Qwen 3 0.6B beats GPT-5 in simple math</title>
    <updated>2025-08-09T15:46:37+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt; &lt;img alt="Qwen 3 0.6B beats GPT-5 in simple math" src="https://preview.redd.it/vtstf4nql0if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1592043c0537a1ea1b41a75985a4e87e590870fd" title="Qwen 3 0.6B beats GPT-5 in simple math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this comparison between Grok and GPT-5 on X for solving the equation 5.9 = x + 5.11. In the comparison, Grok solved it but GPT-5 without thinking failed.&lt;/p&gt; &lt;p&gt;It could have been handpicked after multiples runs, so out of curiosity and for fun I decided to test it myself. Not with Grok but with local models running on iPhone since I develop an app around that, &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt; for those interested but you can reproduce the result below with &lt;a href="https://lmstudio.ai"&gt;LMStudio&lt;/a&gt;, &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt; or any other local chat app of course.&lt;/p&gt; &lt;p&gt;And I was honestly surprised.In my very first run, GPT-5 failed (screenshot) while Qwen 3 0.6B without thinking succeeded. After multiple runs, I would say GPT-5 fails around 30-40% of the time, while Qwen 3 0.6B, which is a tiny 0.6 billion parameters local model around 500 MB in size, solves it every time.Yes it’s one example, GPT-5 was without thinking and it’s not really optimized for math in this mode but Qwen 3 too. And honestly, it’s a simple equation I did not think GPT-5 would fail to solve, thinking or not. Of course, GPT-5 is better than Qwen 3 0.6B, but it’s still interesting to see cases like this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vtstf4nql0if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T15:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmdtox</id>
    <title>Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts.</title>
    <updated>2025-08-10T09:06:37+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt; &lt;img alt="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." src="https://preview.redd.it/5uhq9gouq5if1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c85ba16ed7db24ddf4888d7d267c25350285ed3" title="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've just released a big update for &lt;strong&gt;Speakr&lt;/strong&gt;, my open-source tool for transcribing audio and using your local LLMs to create intelligent summaries. This version is all about giving you more control over how your models process your audio data.&lt;/p&gt; &lt;p&gt;You can use speakr to record notes on your phone or computer directly (including system audio to record online meetings), as well as for drag and drop processing for files recorded elsewhere.&lt;/p&gt; &lt;p&gt;The biggest new feature is an &lt;strong&gt;Advanced Tagging System&lt;/strong&gt; designed for custom, automated workflows. You can now create different tags, and each tag can have its own &lt;strong&gt;unique summary prompt&lt;/strong&gt; that gets sent to your configured local model.&lt;/p&gt; &lt;p&gt;For example, you can set up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;meeting&lt;/code&gt; tag with a prompt to extract key decisions and action items.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;brainstorm&lt;/code&gt; tag with a prompt to group ideas by theme.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;lecture&lt;/code&gt; tag with a prompt to create flashcard-style Q&amp;amp;A pairs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can even &lt;strong&gt;combine tags&lt;/strong&gt; on a single recording to stack their prompts, allowing for really complex and tailored summaries from your LLM.&lt;/p&gt; &lt;p&gt;Once your model generates the summary, you can now &lt;strong&gt;export it as a formatted .docx Word file&lt;/strong&gt; to use in your reports or notes. Other updates include automatic speaker detection from your transcription model and a more polished UI.&lt;/p&gt; &lt;p&gt;The goal is to provide a practical, private tool to leverage the power of your local models on your own audio data. I'd love to hear your feedback, especially from those of you running custom setups!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;You can find the project on GitHub.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5uhq9gouq5if1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0jb6</id>
    <title>I'm sure it's a small win, but I have a local model now!</title>
    <updated>2025-08-09T21:16:39+00:00</updated>
    <author>
      <name>/u/LAKnerd</name>
      <uri>https://old.reddit.com/user/LAKnerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt; &lt;img alt="I'm sure it's a small win, but I have a local model now!" src="https://b.thumbs.redditmedia.com/HaQf-dSQq6o5l-RbrCuiK821KRDb54xdQNlpGMr1Kvc.jpg" title="I'm sure it's a small win, but I have a local model now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took some troubleshooting but apparently I just had the wrong kind of SD card for my Jetson Orin nano. No more random ChatAI changes now though!&lt;/p&gt; &lt;p&gt;I'm using openwebui in a container and Ollama as a service. For now it's running from an SD card but I'll move it to the m.2 sata soon-ish. Performance on a 3b model is fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LAKnerd"&gt; /u/LAKnerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mm0jb6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmgoxe</id>
    <title>Surprised by GPT-5 with reasoning level "minimal" for UI generation</title>
    <updated>2025-08-10T11:58:29+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgoxe/surprised_by_gpt5_with_reasoning_level_minimal/"&gt; &lt;img alt="Surprised by GPT-5 with reasoning level &amp;quot;minimal&amp;quot; for UI generation" src="https://preview.redd.it/k8gflix4l6if1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=563586de3e983fdee4fae66f7856b60da219101a" title="Surprised by GPT-5 with reasoning level &amp;quot;minimal&amp;quot; for UI generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been in the top 5 since showing up on &lt;a href="http://DesignArena.ai"&gt;DesignArena.ai&lt;/a&gt;, despite the reasoning level set to &amp;quot;minimal&amp;quot; in the system prompt. I wonder how it would perform at the highest reasoning level, better than Opus 4.1 (maybe /u/&lt;a href="https://www.reddit.com/user/Accomplished-Copy332/"&gt;Accomplished-Copy332&lt;/a&gt; knows)? Asking because GPT-5 with minimal reasoning is quite cheap and presents a good distillation and fine-tuning opportunity.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k8gflix4l6if1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgoxe/surprised_by_gpt5_with_reasoning_level_minimal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgoxe/surprised_by_gpt5_with_reasoning_level_minimal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmen16</id>
    <title>Why are Diffusion-Encoder LLMs not more popular?</title>
    <updated>2025-08-10T09:59:20+00:00</updated>
    <author>
      <name>/u/AcanthocephalaNo8273</name>
      <uri>https://old.reddit.com/user/AcanthocephalaNo8273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Autoregressive inference will &lt;em&gt;always&lt;/em&gt; have a non-zero chance of hallucination. It’s baked into the probabilistic framework, and we probably waste a decent chunk of parameter space just trying to minimise it.&lt;/p&gt; &lt;p&gt;Decoder-style LLMs have an inherent trade-off across early/middle/late tokens:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Early tokens = not enough context → low quality&lt;/li&gt; &lt;li&gt;Middle tokens = “goldilocks” zone&lt;/li&gt; &lt;li&gt;Late tokens = high noise-to-signal ratio (only a few relevant tokens, lots of irrelevant ones)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this, autoregressive decoders dominate because they’re computationally efficient in a very specific way:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training is causal, which gives you lots of “training samples” per sequence (though they’re not independent, so I question how useful that really is for &lt;em&gt;quality&lt;/em&gt;).&lt;/li&gt; &lt;li&gt;Inference matches training (also causal), so the regimes line up.&lt;/li&gt; &lt;li&gt;They’re memory-efficient in &lt;em&gt;some&lt;/em&gt; ways… but not necessarily when you factor in KV-cache storage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I don’t get is why Diffusion-Encoder type models aren’t more common.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All tokens see all other tokens → no “goldilocks” problem.&lt;/li&gt; &lt;li&gt;Can decode a whole sequence at once → efficient in computation (though maybe heavier in memory, but no KV-cache).&lt;/li&gt; &lt;li&gt;Diffusion models focus on finding the high-probability manifold → hallucinations should be less common if they’re outside that manifold.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Biggest challenge vs. diffusion image models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text = discrete tokens, images = continuous colours.&lt;/li&gt; &lt;li&gt;But… we already use embeddings to make tokens continuous. So why &lt;em&gt;couldn’t&lt;/em&gt; we do diffusion in embedding space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am aware that Google have a diffusion LLM now, but for open source I'm not really aware of any. I'm also aware that you can do diffusion directly on the discrete tokens but personally I think this wastes a lot of the power of the diffusion process and I don't think that guarantees convergence onto a high-probability manifold.&lt;/p&gt; &lt;p&gt;And as a side note: Softmax attention is brilliant engineering, but we’ve been stuck with SM attention + FFN forever, even though it’s O(N²). You &lt;em&gt;can&lt;/em&gt; operate over the full sequence in O(N log N) using convolutions of any size (including the sequence length) via the Fast Fourier Transform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcanthocephalaNo8273"&gt; /u/AcanthocephalaNo8273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
