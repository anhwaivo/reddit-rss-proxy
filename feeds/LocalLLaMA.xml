<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-17T21:21:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i3n37b</id>
    <title>RAM usage of context versus parameters</title>
    <updated>2025-01-17T18:27:20+00:00</updated>
    <author>
      <name>/u/Mysterious-Rent7233</name>
      <uri>https://old.reddit.com/user/Mysterious-Rent7233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How should I think about the relationship between the amount of VRAM needed for large-context windows versus large model parameter counts. If context window sizes started to shift towards the billions, would we just start to load up our models with knowledge in the context instead of in pre-training? How should we think about the tradeoff between knowledge gained in training and knowledge gained in context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious-Rent7233"&gt; /u/Mysterious-Rent7233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3n37b/ram_usage_of_context_versus_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3n37b/ram_usage_of_context_versus_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3n37b/ram_usage_of_context_versus_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i35hs3</id>
    <title>My Tesla P40 just caught on fire and exploded… help?</title>
    <updated>2025-01-17T01:52:52+00:00</updated>
    <author>
      <name>/u/Cressio</name>
      <uri>https://old.reddit.com/user/Cressio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://imgur.com/a/1ViaFVL"&gt;https://imgur.com/a/1ViaFVL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Um… so, this GPU has an insanely long lore. To summarize, I ended up trying to sell it, UPS ravaged the box and the buyer claimed the GPU didn’t work anymore (wouldn’t power on), I received it back, tried to power it up, and it immediately caught on fire in catastrophic fashion and shot flames into my motherboard. &lt;/p&gt; &lt;p&gt;I’m powering them with a good quality PCIe to EPS adapter, which I just used again to try and check if it was indeed dead. Well, it sure as hell is now.&lt;/p&gt; &lt;p&gt;Uh, what the hell happened? What is the component that exploded? It looks to be power related &lt;del&gt;and it had a thermal pad on the backplate that is now scorched.&lt;/del&gt; I don’t actually know if it had a thermal pad, I think it may not have&lt;/p&gt; &lt;p&gt;I actually have ANOTHER P40 from this shipment that I’m wanting to test and I’m absolutely mortified to plug it in now. I don’t think I’ll ever trust a PC build again.&lt;/p&gt; &lt;p&gt;Edit: just to super clarify, these P40s worked before with this exact same setup and adapters. It just… happened to explode this time for some reason. System still powers on just fine without the GPU, thank god.&lt;/p&gt; &lt;p&gt;Edit 2: I was right, the other one works and he claimed both didn’t. I bet the one that just exploded worked too. Fuck. My life. The one that’s powered on right now is using the other (I had 2) adapter and a different PCIe cable. I am so absolutely terrified to try the other cables. This is almost a worst outcome than them both being DOA. Now I don’t even know what to do or trust lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cressio"&gt; /u/Cressio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T01:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3khqj</id>
    <title>Best Approach to Create MCQs from Large PDFs with Correct Answers as Ground Truth?</title>
    <updated>2025-01-17T16:38:45+00:00</updated>
    <author>
      <name>/u/suns9</name>
      <uri>https://old.reddit.com/user/suns9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on generating multiple-choice questions (MCQs) from large PDFs (400-500 pages). The goal is to create a training dataset with correct answers as ground truth. My main concerns are: Efficiently extracting and summarizing content from such large PDFs to generate relevant MCQs, and add varying level of relevancy to test retrieval. &lt;/p&gt; &lt;p&gt;I’m considering using LLM for summarization and question generation, but I’m unsure about the best tools or frameworks to handle this effectively. Additionally, I’d appreciate any recommendations on where to start learning about this process (e.g., tutorials, courses, or resources).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suns9"&gt; /u/suns9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3khqj/best_approach_to_create_mcqs_from_large_pdfs_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3khqj/best_approach_to_create_mcqs_from_large_pdfs_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3khqj/best_approach_to_create_mcqs_from_large_pdfs_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T16:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3kohb</id>
    <title>Is there a difference between chat and repeated calling from scratch?</title>
    <updated>2025-01-17T16:46:27+00:00</updated>
    <author>
      <name>/u/yelling-at-clouds-40</name>
      <uri>https://old.reddit.com/user/yelling-at-clouds-40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I do chat with a bot, it is like:&lt;/p&gt; &lt;p&gt;- #0: &amp;lt;me writing&amp;gt;&lt;/p&gt; &lt;p&gt;- #1 &amp;lt;LLM writing&amp;gt;&lt;/p&gt; &lt;p&gt;- #2: &amp;lt;me writing&amp;gt;&lt;/p&gt; &lt;p&gt;- #3 &amp;lt;LLM writing&amp;gt;&lt;/p&gt; &lt;p&gt;- #4: &amp;lt;me writing&amp;gt;&lt;/p&gt; &lt;p&gt;- #5 &amp;lt;LLM writing&amp;gt;&lt;/p&gt; &lt;p&gt;Is there any fundamental difference between that and calling the LLM with #0, then with the concatenation of #0 and #2 (or is it #0, #1, #2?), and then #0, #2, and #4 (or is it #0..#4?)&lt;/p&gt; &lt;p&gt;Is there any difference between the models, whether they respond significantly different ways?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yelling-at-clouds-40"&gt; /u/yelling-at-clouds-40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kohb/is_there_a_difference_between_chat_and_repeated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kohb/is_there_a_difference_between_chat_and_repeated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kohb/is_there_a_difference_between_chat_and_repeated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T16:46:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3cdws</id>
    <title>"Can't live without tool" for LLM datasets?</title>
    <updated>2025-01-17T09:07:39+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought it would be interesting to know what tool people absolutely love using when it comes to LLM training - more specifically creating and preparing datasets?&lt;/p&gt; &lt;p&gt;Also, feel free to just share any knowledge you feel is a &amp;quot;cheatsheet&amp;quot; or too good to be true?&lt;/p&gt; &lt;p&gt;Have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T09:07:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3d6t0</id>
    <title>Hugging Face Spaces make the perfect agent tools!</title>
    <updated>2025-01-17T10:09:40+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt; &lt;img alt="Hugging Face Spaces make the perfect agent tools!" src="https://a.thumbs.redditmedia.com/2x8xoBKRakcFYgt6BTinie0kUaqgzqu8c3brufszWM0.jpg" title="Hugging Face Spaces make the perfect agent tools!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured out that you could use Gradio based spaces on the hub as tools for agents. I don't get why everyone isn't doing this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/etq92tij3jde1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baf38c94e6240885d8d4d02953e16f9414a12a02"&gt;https://preview.redd.it/etq92tij3jde1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baf38c94e6240885d8d4d02953e16f9414a12a02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a guide here: &lt;a href="https://huggingface.co/blog/burtenshaw/gradio-spaces-agent-tools"&gt;https://huggingface.co/blog/burtenshaw/gradio-spaces-agent-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T10:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3ipgs</id>
    <title>NVIDIA RTX 5090: Limited Availability and Restrictions on AI and Multi-GPU</title>
    <updated>2025-01-17T15:21:45+00:00</updated>
    <author>
      <name>/u/Spiritual_Tie_5574</name>
      <uri>https://old.reddit.com/user/Spiritual_Tie_5574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ipgs/nvidia_rtx_5090_limited_availability_and/"&gt; &lt;img alt="NVIDIA RTX 5090: Limited Availability and Restrictions on AI and Multi-GPU " src="https://external-preview.redd.it/bteDT1wTCKTt11oDOxCHMBb98egjkfRqBELv99v2-pQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=289505e7d1bdf4bb60380e17eb9f3257cce959d5" title="NVIDIA RTX 5090: Limited Availability and Restrictions on AI and Multi-GPU " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to a recent article from El Chapuzas Informático, NVIDIA’s upcoming RTX 50 series GPUs will not only be released in limited quantities but will also include built-in restrictions on certain functionalities. These include reduced performance for AI workloads, cryptocurrency mining, and the use of multiple GPUs in the same setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Tie_5574"&gt; /u/Spiritual_Tie_5574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://elchapuzasinformatico.com/2025/01/nvidia-rtx-50-limitadas-tiendas-capadas-ia-criptomineria-multi-gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ipgs/nvidia_rtx_5090_limited_availability_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ipgs/nvidia_rtx_5090_limited_availability_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pr5v</id>
    <title>Forwarding Email to LLM</title>
    <updated>2025-01-17T20:21:46+00:00</updated>
    <author>
      <name>/u/AlphaTechBro</name>
      <uri>https://old.reddit.com/user/AlphaTechBro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pr5v/forwarding_email_to_llm/"&gt; &lt;img alt="Forwarding Email to LLM" src="https://b.thumbs.redditmedia.com/iwIUVdpa3hnCDji_E9dGo6Hh6NeVYm19TFYScWwFR2k.jpg" title="Forwarding Email to LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday I stumbled upon the following post on X:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fif7s4sv0mde1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2a40aaeab14216c714993ff7f15b6acaf644ccf"&gt;https://preview.redd.it/fif7s4sv0mde1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2a40aaeab14216c714993ff7f15b6acaf644ccf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would very much like to learn how to build this. So I dug a little deeper:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o5gcnca41mde1.png?width=986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=709e90238bb9253ea90c0ddd831c743a9b15e90e"&gt;https://preview.redd.it/o5gcnca41mde1.png?width=986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=709e90238bb9253ea90c0ddd831c743a9b15e90e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Got it. Also:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0o1tvr081mde1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a93b205b92bf181415e147ae16c1061dd35e6f9"&gt;https://preview.redd.it/0o1tvr081mde1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a93b205b92bf181415e147ae16c1061dd35e6f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I should have basically everything I need to do this. At least a general path forward. Problem is, I'm stumped. I've signed up for Upstash (and within it, QStash is free to use) and have been poking around in the documentation (&lt;a href="https://upstash.com/docs/workflow/getstarted"&gt;https://upstash.com/docs/workflow/getstarted&lt;/a&gt;), but I'm not making much progress.&lt;/p&gt; &lt;p&gt;What is the HTTP endpoint in this context? What URL would you use?&lt;/p&gt; &lt;p&gt;If any of the superior talent here could expand on this or point me in the right direction, it will be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlphaTechBro"&gt; /u/AlphaTechBro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pr5v/forwarding_email_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pr5v/forwarding_email_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pr5v/forwarding_email_to_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2t82i</id>
    <title>Introducing Wayfarer: a brutally challenging roleplay model trained to let you fail and die.</title>
    <updated>2025-01-16T16:46:20+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One frustration we’ve heard from many AI Dungeon players is that AI models are too nice, never letting them fail or die. So we decided to fix that. We trained a model we call Wayfarer where adventures are much more challenging with failure and death happening frequently.&lt;/p&gt; &lt;p&gt;We released it on AI Dungeon several weeks ago and players loved it, so we’ve decided to open source the model for anyone to experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-12B"&gt;https://huggingface.co/LatitudeGames/Wayfarer-12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T16:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3px18</id>
    <title>Current SoTA for local speech to text + diarization?</title>
    <updated>2025-01-17T20:29:12+00:00</updated>
    <author>
      <name>/u/dat09</name>
      <uri>https://old.reddit.com/user/dat09</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the current sota for local speech to text + diarization? Is it still whisper + pyannote? feel like it’s been 1yr+ without any significant jumps in performance/ efficiency.&lt;/p&gt; &lt;p&gt;Wondering if anyone else has found a step change since?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat09"&gt; /u/dat09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3b1jb</id>
    <title>New framework aims to mimic human thinking for writing long-form content (OmniThink)</title>
    <updated>2025-01-17T07:22:39+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt; &lt;img alt="New framework aims to mimic human thinking for writing long-form content (OmniThink)" src="https://external-preview.redd.it/P3wPulsj-vbHIfL8pdoJemWboTREaTu--SoaotPYjzU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=098b6934967a73dbc796419d5bd3b3397ed04814" title="New framework aims to mimic human thinking for writing long-form content (OmniThink)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a paper about OmniThink - an approach that tries to replicate how humans write long-form content. The framework focuses on continuous reflection and exploration, similar to how we gather information and refine our understanding when writing detailed articles.&lt;/p&gt; &lt;p&gt;(Not affiliated with the authors)&lt;/p&gt; &lt;p&gt;The paper's style reminds me of Google Deep Research's functionality. I couldn't get their online demo to work, but the ideas in the paper are worth checking out, IMO. I will spend some time on their repo to see if that will work out of the box.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2501.09751"&gt;https://huggingface.co/papers/2501.09751&lt;/a&gt;&lt;br /&gt; Project page: &lt;a href="https://zjunlp.github.io/project/OmniThink/"&gt;https://zjunlp.github.io/project/OmniThink/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/zjunlp/OmniThink"&gt;https://github.com/zjunlp/OmniThink&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb"&gt;https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qfgy</id>
    <title>AI Research</title>
    <updated>2025-01-17T20:52:03+00:00</updated>
    <author>
      <name>/u/ASI-Enjoyer</name>
      <uri>https://old.reddit.com/user/ASI-Enjoyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do we still need AI research, or is ASI just a matter of scaling? I'm 17 years old and I want to become an AI researcher. I want to know your opinion/get advice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASI-Enjoyer"&gt; /u/ASI-Enjoyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3itva</id>
    <title>"I/We/They Couldn't Help But..." Repeating LLM Phrasing?</title>
    <updated>2025-01-17T15:27:12+00:00</updated>
    <author>
      <name>/u/Jattoe</name>
      <uri>https://old.reddit.com/user/Jattoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The spacecraft's sensors detected a safe landing spot near a lush forest, and the pilot navigated the ship towards the area. As they approached, they couldn't help but notice the array of exotic flora that thrived in the region.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;To those that use LLMs often, I imagine you too have noticed the same phrases being used, and in very odd ways (why stress helplessness to notice an array of exotic flora in the region?)&lt;br /&gt; I've actually added &amp;quot;Don't use the words '&lt;em&gt;I couldn't help but&lt;/em&gt;' in your output&amp;quot; and have still had the LLM put the phrase in there, almost like it worked like the &amp;quot;don't think of an elephant,&amp;quot; concept for humans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jattoe"&gt; /u/Jattoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31ji5</id>
    <title>What is ElevenLabs doing? How is it so good?</title>
    <updated>2025-01-16T22:42:26+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. What's their trick? On everything but voice, local models are pretty good for what they are, but ElevenLabs just blows everyone out of the water. &lt;/p&gt; &lt;p&gt;Is it full Transformer? Some sort of Diffuser? Do they model the human anatomy to add accuracy to the model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3ilu3</id>
    <title>[REPOST]Linux 6.14 will have amdxdna! The Ryzen AI NPU driver</title>
    <updated>2025-01-17T15:17:23+00:00</updated>
    <author>
      <name>/u/KillerX629</name>
      <uri>https://old.reddit.com/user/KillerX629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What will this mean for amd cards and AI inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillerX629"&gt; /u/KillerX629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3h7hs</id>
    <title>Attend - Proof of Concept</title>
    <updated>2025-01-17T14:12:36+00:00</updated>
    <author>
      <name>/u/Pedalnomica</name>
      <uri>https://old.reddit.com/user/Pedalnomica</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've gotten fed up with hoping on the computer to do one thing, and doing other stuff instead.&lt;/p&gt; &lt;p&gt;I'm building Attend so that our devices can help us dedicate our time and attention on what matters to us, instead of what some algorithm was optimized for.&lt;/p&gt; &lt;p&gt;Right now, it is a voice assistant that uses a vision LLM to &amp;quot;watch&amp;quot; your screen and help you get back on track if what you're doing isn't aligned with what you said you wanted to do.&lt;/p&gt; &lt;p&gt;I've got some work to do on the workflows and prompts to reduce false positives, but it &amp;quot;works&amp;quot; and I'm very excited about it!&lt;/p&gt; &lt;p&gt;I'd like to get this down to a single 3090, but two seems pretty feasible. Part of the problem most open weight vision language models are garbage with 4K images/screenshots. Qwen2-VL seems to be an exception, but it (especially the 7B) is garbage when it comes to driving the workflows behind Attend. So, I've just been using Qwen2-VL-7B-Instruct and Llama-3.3 at 8-bit as I get it working. I'd love to hear suggestions for minimizing VRAM (Intern2_5-VL also seems to handle 4K alright, but I haven't tested it enough on the workflows).&lt;/p&gt; &lt;p&gt;Attend interfaces with all models using OpenAI compatable API calls. So, you should be able to use the cloud, if you're into that kinda thing... You could also take a hybrid approach. I think you could get the STT and vision LLM into 16GB VRAM and run that locally. Piper TTS runs well on CPU. You could then use a cloud model just for the text LLM and keep the most sensitive stuff (screenshots!) local.&lt;/p&gt; &lt;p&gt;Check out the open-source code &lt;a href="https://github.com/hyperfocAIs/Attend/"&gt;https://github.com/hyperfocAIs/Attend/&lt;/a&gt; and a proof of concept video &lt;a href="https://youtu.be/PETrY540zMM"&gt;https://youtu.be/PETrY540zMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Typos, clarified that this project is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedalnomica"&gt; /u/Pedalnomica &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T14:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3fli7</id>
    <title>Laptop LLM performance - beware of the power settings!</title>
    <updated>2025-01-17T12:48:10+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's pity that I did such a lame negligence, but want to share with you, in case someone struggles with the same issue.&lt;/p&gt; &lt;p&gt;Both me and the wife have Lenovo gaming laptops:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Rizen 5, 16GB DDR5 RAM, 3050ti 4GB&lt;/li&gt; &lt;li&gt;i5, 16GB DDR5 RAM, 4060 8GB&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Logically, if a model fits entirely in the VRAM, the machine 2 runs it noticeble faster. BUT, everything beyond 7B which is partially offloaded in VRAM, (like Qwen 2.5 14B, 26/49 layers offloaded to GPU) practically goes with less than 0.2T/s and takes 2-3 minutes to output the first token on the machine 2! While machine 1 runs the same Qwen 2.5 (14B, 9/49 layers offloaded to GPU) quite acceptable with around 2T/s.&lt;/p&gt; &lt;p&gt;I was changing nVidia/CUDA drivers, settings of llama.cpp - nothing helped. Till I checked the &amp;quot;power settings&amp;quot; of Windows and changed the presets from &amp;quot;balanced&amp;quot; to &amp;quot;performance&amp;quot;. It was the CPU/RAM of the machine which killed all the fun. Now I get 5-10 T/s with 14B model and 26/49 layers to GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T12:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nbb7</id>
    <title>Any "mainstream" apps with genuinely useful local AI features?</title>
    <updated>2025-01-17T18:36:57+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if any of you actually regularly use features in apps with local AI processing?&lt;/p&gt; &lt;p&gt;When I say &amp;quot;mainstream app&amp;quot;, I mean more like PyCharm from JetBrains (i.e. making lots of money, large teams behind them, etc.) than an open-source/indie dev app.&lt;/p&gt; &lt;p&gt;And I'm more talking about a feature in an app (which does a bunch of things other than that AI feature), as opposed to an app that's entirely about using AI locally, like Ollama, LMStudio, etc.&lt;/p&gt; &lt;p&gt;I'm also not talking about OS features, e.g. auto-complete on iPhones. More interested in apps that you've downloaded.&lt;/p&gt; &lt;p&gt;Currently, the only thing I can think of in my day-to-day is &lt;a href="https://blog.jetbrains.com/ai/2024/11/jetbrains-ai-assistant-2024-3/"&gt;code completion in PyCharm&lt;/a&gt;, but even that is now some kind of hybrid local/cloud thing.&lt;/p&gt; &lt;p&gt;EDIT: Not necessarily just talking about LLM stuff. Realized that I also use some photo editing apps every now and then with local ML models (but that's all pretty old tech, e.g. interactive background removal/segmentation)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3as1m</id>
    <title>OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)</title>
    <updated>2025-01-17T07:02:43+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt; &lt;img alt="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" src="https://a.thumbs.redditmedia.com/11a6AQbm8PHTqUzymosrsOz6WrQ1h5fnohaqQF7icF0.jpg" title="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ytezb1q05ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93364222443da5f695a745265842c91ee604d9e5"&gt;C# and XML View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ttzjm4s5ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd00eb16ef20e090d9f5ebee0d69f48c4f3b8bf0"&gt;Design View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tj92xav5ide1.png?width=1749&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81d8f9dec9bd3575fb4fc4ea8d399627b2eacd4a"&gt;Code View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I'm implementing Canvas (beefing up Artifacts) on OpenWebUI.&lt;/p&gt; &lt;p&gt;This was my only issue ever with OpenWebUI, just the very limited canvas feature (only restricted to HTML, CSS, JavaScript and SVG).&lt;/p&gt; &lt;p&gt;I've expanded support for the following languages:&lt;/p&gt; &lt;p&gt;C#, Python, Java, PHP, Ruby, Bash, Shell, AppleScript, SQL, JSON, XML, YAML, Markdown, HTML&lt;/p&gt; &lt;p&gt;If I'm missing one feel free to comment it! It's super easy to add at this point.&lt;/p&gt; &lt;p&gt;Another notable feature I'm adding is to switch between Design view and Code view for web design.&lt;/p&gt; &lt;p&gt;I'm super close to finishing! I just need to clean it up and visualize/track changes between revisions. Expect my pull request it in the next couple of weeks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3kv1n</id>
    <title>[Magnum/SE] LLama 3.3 70b</title>
    <updated>2025-01-17T16:54:07+00:00</updated>
    <author>
      <name>/u/lucyknada</name>
      <uri>https://old.reddit.com/user/lucyknada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, folks!&lt;/p&gt; &lt;p&gt;We've got something a little different to share this time. It's not a full release or a new series as of yet, but more like an epilogue to the v4 series we released a few months back. DoctorShotgun wasn't entirely satisfied with how the large models in the series turned out, so he spent some more time in the lab - this time on the newer llama 3.3 model for a change:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE"&gt;https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This time, the model was trained as an rslora with recommendations from Gryphe of Mythomax fame, and it comes with the full set of adapter checkpoints for mergers and other experimenters to play around with (&lt;a href="https://huggingface.co/Doctor-Shotgun/Magnum-v4-SE-70B-LoRA"&gt;available here&lt;/a&gt;). Preliminary testing suggests that rslora adequately style-transfers the classic Claude-y flavor of magnum to the llama 3.3 model.&lt;/p&gt; &lt;p&gt;In terms of changes to the data, the model doesn't deviate too far from the v4 series. The dataset includes some further cleaning of the RP log dataset used in v4, as well as the re-introduction of a subset of the data used in the v2 and earlier models. As per usual, the training config is linked from the model card in the spirit of open source.&lt;/p&gt; &lt;p&gt;No first-party quants are available at this time, but links to those created by well-known quanters are linked in the model description.&lt;/p&gt; &lt;p&gt;Hope you enjoy this belated New Years present, and stay tuned for what's to come!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucyknada"&gt; /u/lucyknada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pup0</id>
    <title>Beating cuBLAS in SGEMM from Scratch</title>
    <updated>2025-01-17T20:26:10+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt; &lt;img alt="Beating cuBLAS in SGEMM from Scratch" src="https://a.thumbs.redditmedia.com/5VtAEp46vu6qAMyyshFSAQ0PS4VyO1ibLsIEkWU_HY0.jpg" title="Beating cuBLAS in SGEMM from Scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs, achieving performance that outpaced NumPy on modern desktop AMD and Intel CPUs - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from you, and today I'm excited to share my second blog post. This one focuses on an SGEMM (Single-precision GEneral Matrix Multiply) that outperforms NVIDIA's implementation from cuBLAS library with its (modified?) CUTLASS kernel across a wide range of matrix sizes. This project primarily targets &lt;strong&gt;CUDA-learners&lt;/strong&gt; and aims to bridge the gap between the SGEMM implementations explained in books/blogs and those used in NVIDIA’s BLAS libraries. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage through shared memory.&lt;/p&gt; &lt;p&gt;The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. Below, I've included performance comparisons against cuBLAS and Simon Boehm’s highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;P.S. The next blog post will cover implementing HGEMM (FP16 GEMM) and HGEMV (FP16 Matrix-Vector Multiplication) on Tensor Cores achieving performance comparable to cuBLAS (or maybe even faster? let's see). If you enjoy educational content like this and would like to see more, please share the article. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have!&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f"&gt;https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3mybo</id>
    <title>LCLV: Real-time video analysis with Moondream 2B &amp; OLLama (open source, local). Anyone want a set up guide?</title>
    <updated>2025-01-17T18:21:33+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt; &lt;img alt="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" src="https://external-preview.redd.it/MXZ5aHh4bWZpbGRlMSTqk2DOPEdgmnDyQ8guvDBrE8AyiWMeqDE4BRKGe_SG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecd38999e371e083e545019f1eaf8d324a146b50" title="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3kcfymfilde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pexj</id>
    <title>DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench</title>
    <updated>2025-01-17T20:06:47+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt; &lt;img alt="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" src="https://external-preview.redd.it/RiXxcULN7VvmAA8zRKm9Hg6sMZIuDEZ9SdZM3h7z4e0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c43191d847a8866681673c575cc88d8e702dd05" title="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/WdpIkiy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nsbx</id>
    <title>Realtime speaker diarization</title>
    <updated>2025-01-17T18:57:16+00:00</updated>
    <author>
      <name>/u/Lonligrin</name>
      <uri>https://old.reddit.com/user/Lonligrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt; &lt;img alt="Realtime speaker diarization" src="https://external-preview.redd.it/JO_yTxc06ktYf5LFR-Rn-h9sKgRJ8XcsPo1m_3iqmLE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a91a5195fd0960c0d708c2f400bd55c115bba5a" title="Realtime speaker diarization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonligrin"&gt; /u/Lonligrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=-zpyi1KHOUk&amp;amp;si=qzksOIhsLjo9J8Zp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3o7a8</id>
    <title>I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)</title>
    <updated>2025-01-17T19:14:56+00:00</updated>
    <author>
      <name>/u/yyjhao</name>
      <uri>https://old.reddit.com/user/yyjhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt; &lt;img alt="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" src="https://external-preview.redd.it/MGt0ZzN4Y3NzbGRlMeSgvI1GdDqWZSs569grdhgwadhN-F5M6UL9TiNWoaqW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c56ce3e01a5f41dcffc115930e49f7b1fee821" title="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yyjhao"&gt; /u/yyjhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n3fmqwcsslde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T19:14:56+00:00</published>
  </entry>
</feed>
