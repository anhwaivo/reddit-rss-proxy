<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-20T17:06:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k313fv</id>
    <title>Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)</title>
    <updated>2025-04-19T17:08:27+00:00</updated>
    <author>
      <name>/u/nn0951123</name>
      <uri>https://old.reddit.com/user/nn0951123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt; &lt;img alt="Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)" src="https://b.thumbs.redditmedia.com/MzEimIan87friFnfN_Uz_A5BJeJXIMQ0RA6lgC4MGSs.jpg" title="Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got around to finishing my weird-but-effective AMD homelab/server build. The idea was simpleâ€”max performance without totally destroying my wallet (spoiler: &lt;span class="md-spoiler-text"&gt;my wallet is still crying&lt;/span&gt;).&lt;/p&gt; &lt;p&gt;Decided on Ryzen because of price/performance, and got this oddball ASUS boardâ€”&lt;strong&gt;Pro WS X570-ACE&lt;/strong&gt;. It's the only consumer Ryzen board I've seen that can run 3 PCIe Gen4 slots at x8 each, perfect for multi-GPU setups. Plus it has a sneaky PCIe x1 slot ideal for my AQC113 10GbE NIC.&lt;/p&gt; &lt;h1&gt;Current hardware:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5950X (yep, still going strong after owning it for 4 years)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASUS Pro WS X570-ACE (even provides built in remote management but i opt for using pikvm)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB Corsair 3600MHz (maybe upgrade later to ECC 128GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Slot 3 (bottom): RTX 4090 48GB, 2-slot blower style (~$3050, sourced from Chinese market)&lt;/li&gt; &lt;li&gt;Slots 1 &amp;amp; 2 (top): RTX 3080 20GB, 2-slot blower style (~$490 each, same as above, but the rebar on this variant did not work properly)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Networking:&lt;/strong&gt; AQC113 10GbE NIC in the x1 slot (fits perfectly!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is my messy build shot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8fgi0n98mtve1.jpg?width=5334&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1623684b11f6cd9770a8f8408e50e5692bb45a94"&gt;https://preview.redd.it/8fgi0n98mtve1.jpg?width=5334&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1623684b11f6cd9770a8f8408e50e5692bb45a94&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Those gpu works out of the box, no weirdo gpu driver required at all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6o5wy6jzmtve1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8332b9b63e43f1aac8de9a10cedf633d11c2aa6"&gt;https://preview.redd.it/6o5wy6jzmtve1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8332b9b63e43f1aac8de9a10cedf633d11c2aa6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So, why two 3080s vs one 4090?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Initially got curious after seeing these bizarre Chinese-market 3080 cards with 20GB VRAM for under $500 each. I wondered if two of these budget cards could match the performance of a single $3000+ RTX 4090. For the price difference, it felt worth the gamble.&lt;/p&gt; &lt;h1&gt;Benchmarks (because of course):&lt;/h1&gt; &lt;p&gt;I ran a bunch of benchmarks using various LLM models. Graph attached for your convenience.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vupybaceotve1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b356f0748171143ee4b58c478349e9a38ccb377"&gt;https://preview.redd.it/vupybaceotve1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b356f0748171143ee4b58c478349e9a38ccb377&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Fine-tuning:&lt;/h1&gt; &lt;p&gt;Fine-tuned Qwen2.5-7B (QLoRA 4bit, DPO, Deepspeed) because, duh.&lt;/p&gt; &lt;p&gt;RTX 4090 (no ZeRO): 7 min 5 sec per epoch (3.4 s/it), ~420W.&lt;/p&gt; &lt;p&gt;2Ã—3080 with ZeRO-3: utterly painful, about 11.4 s/it across both GPUs (440W).&lt;/p&gt; &lt;p&gt;2Ã—3080 with ZeRO-2: actually decent, 3.5 s/it, ~600W total. Just ~14% slower than the 4090. 8 min 4 sec per epoch.&lt;/p&gt; &lt;p&gt;So, it turns out that if your model fits nicely in each GPU's VRAM (ZeRO-2), two 3080s come surprisingly close to one 4090. ZeRO-3 murders performance, though. (waiting on an 3-slot NVLink bridge to test if that works and helps).&lt;/p&gt; &lt;p&gt;Roast my choices, or tell me how much power Iâ€™m wasting running dual 3080s. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nn0951123"&gt; /u/nn0951123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T17:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ycef</id>
    <title>I've built a lightweight hallucination detector for RAG pipelines â€“ open source, fast, runs up to 4K tokens</title>
    <updated>2025-04-19T15:06:53+00:00</updated>
    <author>
      <name>/u/henzy123</name>
      <uri>https://old.reddit.com/user/henzy123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hallucinations are still one of the biggest headaches in RAG pipelines, especially in tricky domains (medical, legal, etc). Most detection methods either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Has context window limitations&lt;/strong&gt;, particularly in encoder-only models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Has high inference costs&lt;/strong&gt; from LLM-based hallucination detectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we've put together &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;&lt;strong&gt;LettuceDetect&lt;/strong&gt;&lt;/a&gt; â€” an open-source, encoder-based framework that flags hallucinated spans in LLM-generated answers. No LLM required, runs faster, and integrates easily into any RAG setup.&lt;/p&gt; &lt;h1&gt;ðŸ¥¬ Quick highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token-level detection&lt;/strong&gt; â†’ tells you exactly which parts of the answer aren't backed by your retrieved context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-context ready&lt;/strong&gt; â†’ built on ModernBERT, handles up to 4K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accurate &amp;amp; efficient&lt;/strong&gt; â†’ hits 79.22% F1 on the RAGTruth benchmark, competitive with fine-tuned LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MIT licensed&lt;/strong&gt; â†’ comes with Python packages, pretrained models, Hugging Face demo&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;https://github.com/KRLabsOrg/LettuceDetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog: &lt;a href="https://huggingface.co/blog/adaamko/lettucedetect"&gt;https://huggingface.co/blog/adaamko/lettucedetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Preprint: &lt;a href="https://arxiv.org/abs/2502.17125"&gt;https://arxiv.org/abs/2502.17125&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo + models: &lt;a href="https://huggingface.co/KRLabsOrg"&gt;https://huggingface.co/KRLabsOrg&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what you think here â€” especially if you're doing local RAG, hallucination eval, or trying to keep things lightweight. Also working on real-time detection (not just post-gen), so open to ideas/collabs there too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henzy123"&gt; /u/henzy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T15:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zqdq</id>
    <title>ubergarm/gemma-3-27b-it-qat-GGUF</title>
    <updated>2025-04-19T16:07:45+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt; &lt;img alt="ubergarm/gemma-3-27b-it-qat-GGUF" src="https://external-preview.redd.it/zP1F1IPzW39T2A62RLwjUYufUjCjW6qaeKGMiNx3iNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fca46c2f6538aed2b4f6365368f26987fd027b43" title="ubergarm/gemma-3-27b-it-qat-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just quantized two GGUFs that beat google's 4bit GGUF in perplexity comparisons!&lt;/p&gt; &lt;p&gt;They only run on &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork which provides new SotA quantizationsof google's recently updated Quantization Aware Training (QAT) 4bit full model.&lt;/p&gt; &lt;p&gt;32k context in 24GB VRAM or as little as 12GB VRAM offloading just KV Cache and attention layers with repacked CPU optimized tensors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/gemma-3-27b-it-qat-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3imyi</id>
    <title>Audio transcription?</title>
    <updated>2025-04-20T09:14:12+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any good models that are light enough to run on a phone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3imyi/audio_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3imyi/audio_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3imyi/audio_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T09:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k35kh5</id>
    <title>Fine-tuning LLMs to 1.58bit: extreme quantization experiment</title>
    <updated>2025-04-19T20:29:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/blog/blob/main/1_58_llm_extreme_quantization.md"&gt;https://github.com/huggingface/blog/blob/main/1_58_llm_extreme_quantization.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/1_58_llm_extreme_quantization"&gt;https://huggingface.co/blog/1_58_llm_extreme_quantization&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T20:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3a3kl</id>
    <title>I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients.</title>
    <updated>2025-04-20T00:12:05+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"&gt; &lt;img alt="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." src="https://external-preview.redd.it/M3V1aDB5bW5sdnZlMZppp7K11at-IaSBIx6ekIrLv_SO-25kJ8guE9xrX4Mu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e87601b43c5ad86c1fc1c8ed213e74dec5bc5" title="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Example using Claude Desktop and Tableau&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/brx0wxmnlvve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T00:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3i5hr</id>
    <title>How would this breakthrough impact running LLMs locally?</title>
    <updated>2025-04-20T08:37:20+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device"&gt;https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PoX is a non-volatile flash memory that programs a single bit in 400 picoseconds (0.0000000004 seconds), equating to roughly 25 billion operations per second. This speed is a significant leap over traditional flash memory, which typically requires microseconds to milliseconds per write, and even surpasses the performance of volatile memories like SRAM and DRAM (1â€“10 nanoseconds). The Fudan team, led by Professor Zhou Peng, achieved this by replacing silicon channels with two-dimensional Dirac graphene, leveraging its ballistic charge transport and a technique called &amp;quot;2D-enhanced hot-carrier injection&amp;quot; to bypass classical injection bottlenecks. AI-driven process optimization further refined the design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3i5hr/how_would_this_breakthrough_impact_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3i5hr/how_would_this_breakthrough_impact_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3i5hr/how_would_this_breakthrough_impact_running_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T08:37:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3r3eo</id>
    <title>FULL LEAKED Windsurf Agent System Prompts and Internal Tools</title>
    <updated>2025-04-20T16:59:28+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 20/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get the full official Windsurf Agent system prompts, including its internal tools (JSON). Over 200 lines. Definitely worth to take a look.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3r3eo/full_leaked_windsurf_agent_system_prompts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3r3eo/full_leaked_windsurf_agent_system_prompts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3r3eo/full_leaked_windsurf_agent_system_prompts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T16:59:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3or5z</id>
    <title>Is there anything like an AI assistant for a Linux operating system?</title>
    <updated>2025-04-20T15:14:04+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not just for programming related tasks, but also able to recommend packages/software to install/use, troubleshooting tips etc. Basically a model with good technical knowledge (not just programming) or am I asking for too much?&lt;/p&gt; &lt;p&gt;Some examples of questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should I install this package from apt or snap?&lt;/li&gt; &lt;li&gt;There is this cool software/package that could do etc etc on Windows. What are some similar options on Linux?&lt;/li&gt; &lt;li&gt;Recommend some UI toolkits I can use with Next/Astro&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3or5z/is_there_anything_like_an_ai_assistant_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3or5z/is_there_anything_like_an_ai_assistant_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3or5z/is_there_anything_like_an_ai_assistant_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T15:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3p96f</id>
    <title>LightRAG Chunking Strategies</title>
    <updated>2025-04-20T15:36:44+00:00</updated>
    <author>
      <name>/u/umen</name>
      <uri>https://old.reddit.com/user/umen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; Iâ€™m using LightRAG and Iâ€™m trying to figure out the best way to chunk my data before indexing. My sources include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;XML data (~300 MB)&lt;/li&gt; &lt;li&gt;Source code (200+ files)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What chunking strategies do you recommend for these types of data? Should I use fixed-size chunks, split by structure (like tags or functions), or something else?&lt;/p&gt; &lt;p&gt;Any tips or examples would be really helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umen"&gt; /u/umen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3p96f/lightrag_chunking_strategies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3p96f/lightrag_chunking_strategies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3p96f/lightrag_chunking_strategies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T15:36:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3hq3o</id>
    <title>Gemma 3 speculative decoding</title>
    <updated>2025-04-20T08:05:14+00:00</updated>
    <author>
      <name>/u/qqYn7PIE57zkf6kn</name>
      <uri>https://old.reddit.com/user/qqYn7PIE57zkf6kn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any way to use speculative decoding with Gemma3 models? It doesnt show up in Lm studio. Are there other tools that support it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qqYn7PIE57zkf6kn"&gt; /u/qqYn7PIE57zkf6kn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3hq3o/gemma_3_speculative_decoding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3hq3o/gemma_3_speculative_decoding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3hq3o/gemma_3_speculative_decoding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T08:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3plzq</id>
    <title>Llama 4 - Slow Prompt Processing on Llama.cpp with partial offload</title>
    <updated>2025-04-20T15:52:51+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing with Maverick with the following command:&lt;br /&gt; ./llama-server -m maverick.gguf -c 16384 -ngl 99 -ot &amp;quot;.*ffn_.*_exps.*=CPU&amp;quot;&lt;/p&gt; &lt;p&gt;In theory this loads the ~14B worth of shared tensors onto the gpu,&lt;br /&gt; And leaves the ~384B worth of MoE experts on the CPU.&lt;/p&gt; &lt;p&gt;At inference time all 14B on the GPU is active + 3B worth of experts from the CPU.&lt;/p&gt; &lt;p&gt;Generation speed is great at 25T/s&lt;br /&gt; However prompt processing speed is 18T/s,&lt;/p&gt; &lt;p&gt;I've never seen Prefill slower than generation, so feels like I'm doing something wrong...&lt;/p&gt; &lt;p&gt;Doing a little messing around I realized I could double my Prefill speed by switching from pcie gen3 to gen4, also cpu apear mostly idle while doing prefill. &lt;/p&gt; &lt;p&gt;Is there a command that will tell Llama.cpp to do the prefill for the CPU layers on CPU?&lt;br /&gt; Any other tweaks to get faster prefill?&lt;/p&gt; &lt;p&gt;This is Llama.cpp, 1 RTX3090, and a 16 core 7F52 Epyc (DDR4)&lt;/p&gt; &lt;p&gt;Ktransformers already does something like this and gets over 100T/s prefill on this model and hardware,&lt;br /&gt; But I'm running into a bug where it loses it's mind at longer context lengths. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3plzq/llama_4_slow_prompt_processing_on_llamacpp_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3plzq/llama_4_slow_prompt_processing_on_llamacpp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3plzq/llama_4_slow_prompt_processing_on_llamacpp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T15:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k35orj</id>
    <title>FramePack is a next-frame (next-frame-section) prediction neural network structure that generates videos progressively. (Local video gen model)</title>
    <updated>2025-04-19T20:35:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lllyasviel.github.io/frame_pack_gitpage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35orj/framepack_is_a_nextframe_nextframesection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k35orj/framepack_is_a_nextframe_nextframesection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T20:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3o50u</id>
    <title>Google's Agent2Agent Protocol Explained</title>
    <updated>2025-04-20T14:46:14+00:00</updated>
    <author>
      <name>/u/aravindputrevu</name>
      <uri>https://old.reddit.com/user/aravindputrevu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3o50u/googles_agent2agent_protocol_explained/"&gt; &lt;img alt="Google's Agent2Agent Protocol Explained" src="https://external-preview.redd.it/Tf5VQdhFACHFr9kuFzXJMOWjLO-fGpy5FF5hRUHxBPg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e4d43c53fba25be4cbc0534abb32f5802530b07" title="Google's Agent2Agent Protocol Explained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wrote a&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aravindputrevu"&gt; /u/aravindputrevu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.substack.com/pub/devshorts/p/agent2agent-a2a-protocol-explained?r=1cg0b&amp;amp;utm_medium=ios"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3o50u/googles_agent2agent_protocol_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3o50u/googles_agent2agent_protocol_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T14:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3j2ke</id>
    <title>Please forgive me if this isn't allowed, but I often see others looking for a way to connect LM Studio to their Android devices and I wanted to share.</title>
    <updated>2025-04-20T09:47:35+00:00</updated>
    <author>
      <name>/u/CowMan30</name>
      <uri>https://old.reddit.com/user/CowMan30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3j2ke/please_forgive_me_if_this_isnt_allowed_but_i/"&gt; &lt;img alt="Please forgive me if this isn't allowed, but I often see others looking for a way to connect LM Studio to their Android devices and I wanted to share." src="https://external-preview.redd.it/D1Y7m9Um70_VXnvIYtH0AwTVKF3MOI-Q-hwQ4BkuPSc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5ea2fb1745c766a43bbe565ad87a8865ee9f2a1" title="Please forgive me if this isn't allowed, but I often see others looking for a way to connect LM Studio to their Android devices and I wanted to share." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CowMan30"&gt; /u/CowMan30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmsa.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3j2ke/please_forgive_me_if_this_isnt_allowed_but_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3j2ke/please_forgive_me_if_this_isnt_allowed_but_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T09:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3dq8n</id>
    <title>Easter Egg: FULL Windsurf leak - SYSTEM, FUNCTIONS, CASCADE</title>
    <updated>2025-04-20T03:40:07+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extracted today with o4-mini-high: &lt;a href="https://github.com/dontriskit/awesome-ai-system-prompts/blob/main/windsurf/system-2025-04-20.md"&gt;https://github.com/dontriskit/awesome-ai-system-prompts/blob/main/windsurf/system-2025-04-20.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;inside windsurf prompt clever way to enforce larger responses:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Yap score is a measure of how verbose your answer to the user should be. Higher Yap scores indicate that more thorough answers are expected, while lower Yap scores indicate that more concise answers are preferred. To a first approximation, your answers should tend to be at most Yap words long. Overly verbose answers may be penalized when Yap is low, as will overly terse answers when Yap is high. Today's Yap score is: 8192. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;---&lt;br /&gt; in the reporeverse engineered Claude Code, Same new, v0 and few other unicorn ai projects.&lt;br /&gt; ---&lt;br /&gt; HINT: use prompts from that repo inside R1, QWQ, o3 pro, 2.5 pro requests to build agents faster.&lt;/p&gt; &lt;p&gt;Who's going to be first to the egg?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T03:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k30y9g</id>
    <title>China scientists develop flash memory 10,000Ã— faster than current tech</title>
    <updated>2025-04-19T17:02:13+00:00</updated>
    <author>
      <name>/u/jailbot11</name>
      <uri>https://old.reddit.com/user/jailbot11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"&gt; &lt;img alt="China scientists develop flash memory 10,000Ã— faster than current tech" src="https://external-preview.redd.it/X_N3xnxwcR-funbS9qKd_xVJ5wZAuZj5iSs8sRFg_KU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d17abe200d8f41354590639a014690faad979d2e" title="China scientists develop flash memory 10,000Ã— faster than current tech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jailbot11"&gt; /u/jailbot11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device?group=test_a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T17:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3n4u1</id>
    <title>I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne</title>
    <updated>2025-04-20T13:58:12+00:00</updated>
    <author>
      <name>/u/Jattoe</name>
      <uri>https://old.reddit.com/user/Jattoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt; &lt;img alt="I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne" src="https://a.thumbs.redditmedia.com/MK3_Ar5JxQPHqjerMJ0s5Kt_ypxo56WxUgndoco5GA8.jpg" title="I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use it for rewrites of my own writing, not for original content, but moreso stylistic ideas and such, and it's the best so far.&lt;/p&gt; &lt;p&gt;But it has some weird information in there, I'm guessing perhaps as a thumbprint? It's such a shame because if it wasn't for this dastardly Dr. Aris Thorne and whatever crop of nonsenses that are shoved into the pot in order to make such a thing repetitive despite different prompts... Well, it'd be just about the best Google has ever produced, perhaps even better than the refined Llamas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jattoe"&gt; /u/Jattoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T13:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3pw8n</id>
    <title>Intel releases AI Playground software for generative AI as open source</title>
    <updated>2025-04-20T16:05:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt; &lt;img alt="Intel releases AI Playground software for generative AI as open source" src="https://external-preview.redd.it/UgxZ6n5LChFWd0HyYltaWavgJZl1YXoXz0YJ03N3rv0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb487f61c95f7de558ca1ddf5ec1b62010b36b5e" title="Intel releases AI Playground software for generative AI as open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Announcement video: &lt;a href="https://www.youtube.com/watch?v=dlNvZu-vzxU"&gt;https://www.youtube.com/watch?v=dlNvZu-vzxU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt; AI Playground open source project and AI PC starter app for doing AI image creation, image stylizing, and chatbot on a PC powered by an IntelÂ® Arcâ„¢ GPU. AI Playground leverages libraries from GitHub and Huggingface which may not be available in all countries world-wide. AI Playground supports many Gen AI libraries and models including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image Diffusion: Stable Diffusion 1.5, SDXL, Flux.1-Schnell, LTX-Video&lt;/li&gt; &lt;li&gt;LLM: Safetensor PyTorch LLMs - DeepSeek R1 models, Phi3, Qwen2, Mistral, GGUF LLMs - Llama 3.1, Llama 3.2: OpenVINO - TinyLlama, Mistral 7B, Phi3 mini, Phi3.5 mini&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/intel/AI-Playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T16:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3jal4</id>
    <title>Gemma 3 QAT versus other q4 quants</title>
    <updated>2025-04-20T10:03:50+00:00</updated>
    <author>
      <name>/u/Timely_Second_6414</name>
      <uri>https://old.reddit.com/user/Timely_Second_6414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I benchmarked googles QAT gemma against the Q4_K_M (bartowski/lmstudio) and UD-Q4_K_XL (unsloth) quants on GPQA diamond to assess performance drops.&lt;/p&gt; &lt;p&gt;Results: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 27B QAT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 27B Q4_K_XL&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 27B Q4_K_M&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM to fit model&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16.43 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.88 GB&lt;/td&gt; &lt;td align="left"&gt;17.40 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPQA diamond score&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;36.4%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;34.8%&lt;/td&gt; &lt;td align="left"&gt;33.3%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All of these are benchmarked locally with temp=0 for reproducibility across quants. It seems the QAT really does work well. I also tried with the recommended temperature of 1, which gives a score of 38-40% (closer to the original BF16 score of 42.4 on google model card).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Second_6414"&gt; /u/Timely_Second_6414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jal4/gemma_3_qat_versus_other_q4_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jal4/gemma_3_qat_versus_other_q4_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jal4/gemma_3_qat_versus_other_q4_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T10:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3n7od</id>
    <title>PocketPal</title>
    <updated>2025-04-20T14:01:55+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"&gt; &lt;img alt="PocketPal" src="https://preview.redd.it/rfaxzunvxzve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aff8aa5797ead7a7b17086a66b51c1f74f7bbff" title="PocketPal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying my Donald system prompt with Gemma&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rfaxzunvxzve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T14:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3jpal</id>
    <title>Trying to create a Sesame-like experience Using Only Local AI</title>
    <updated>2025-04-20T10:34:52+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"&gt; &lt;img alt="Trying to create a Sesame-like experience Using Only Local AI" src="https://external-preview.redd.it/d3VlZmk5dm93eXZlMdxX11AfgZTEMF7oSAzFAlLSpvlezRf_S3o9RpaxpyHo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec75d44b379dfe02094858707c3beff78197228a" title="Trying to create a Sesame-like experience Using Only Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a personal project I've been working on in my freetime. I'm trying to build an interactive, voice-driven avatar. Think sesame but the full experience running locally.&lt;/p&gt; &lt;p&gt;The basic idea is: my voice goes in -&amp;gt; gets transcribed locally with Whisper -&amp;gt; that text gets sent to the Ollama api (along with history and a personality prompt) -&amp;gt; the response comes back -&amp;gt; gets turned into speech with a local TTS -&amp;gt; and finally animates the Live2D character (lipsync + emotions).&lt;/p&gt; &lt;p&gt;My main goal was to see if I could get this whole thing running smoothly locally on my somewhat old GTX 1080 Ti. Since I also like being able to use latest and greatest models + ability to run bigger models on mac or whatever, I decided to make this work with ollama api so I can just plug and play that.&lt;/p&gt; &lt;p&gt;I shared the initial release around a month back, but since then I have been working on V2 which just makes the whole experience a tad bit nicer. A big added benefit is also that the whole latency has gone down.&lt;br /&gt; I think with time, it might be possible to get the latency down enough that you could havea full blown conversation that feels instantanious. The biggest hurdle at the moment as you can see is the latency causes by the TTS.&lt;/p&gt; &lt;p&gt;The whole thing's built in C#, which was a fun departure from the usual Python AI world for me, and the performance has been pretty decent.&lt;/p&gt; &lt;p&gt;Anyway, the code's here if you want to peek or try it: &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x8koh8vowyve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T10:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3fdqa</id>
    <title>I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!</title>
    <updated>2025-04-20T05:23:56+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"&gt; &lt;img alt="I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!" src="https://external-preview.redd.it/ZTh6bHV5dXpjeHZlMYnlOxWogVD-LlbQBY7Zy9u919rxevDxwZbMMM1phMmn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01477954af0e59cfd452236b1012f46967be20f6" title="I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community! I recently open-sourced &lt;strong&gt;Hyprnote&lt;/strong&gt; â€” a smart notepad built for people with back-to-back meetings.&lt;/p&gt; &lt;p&gt;In a nutshell, Hyprnote is a note-taking app that listens to your meetings and &lt;strong&gt;creates an enhanced version by combining the raw notes with context from the audio&lt;/strong&gt;. It runs on local AI models, so you donâ€™t have to worry about your data going anywhere.&lt;/p&gt; &lt;p&gt;Hope you enjoy the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2njzhyuzcxve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T05:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3kuqb</id>
    <title>Hopes for cheap 24GB+ cards in 2025</title>
    <updated>2025-04-20T11:52:23+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before AMD launched their 9000 series GPUs I had hope they would understand the need for a high VRAM GPU but hell no. They are either stupid or not interested in offering AI capable GPUs: Their 9000 series GPUs both have 16 GB VRAM, down from 20 and 24GB from the previous(!) generation of 7900 XT and XTX.&lt;/p&gt; &lt;p&gt;Since it takes 2-3 years for a new GPU generation does this mean no hope for a new challenger to enter the arena this year or is there something that has been announced and about to be released in Q3 or Q4?&lt;/p&gt; &lt;p&gt;I know there is this AMD AI Max and Nvidia Digits, but both seem to have low memory bandwidth (even too low for MoE?)&lt;/p&gt; &lt;p&gt;Is there no chinese competitor who can flood the market with cheap GPUs that have low compute but high VRAM?&lt;/p&gt; &lt;p&gt;EDIT: There is Intel, they produce their own chips, they could offer something. Are they blind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T11:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3l728</id>
    <title>AMD preparing RDNA4 Radeon PRO series with 32GB memory on board</title>
    <updated>2025-04-20T12:13:00+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"&gt; &lt;img alt="AMD preparing RDNA4 Radeon PRO series with 32GB memory on board" src="https://external-preview.redd.it/j-nxBdzwljPlN5seurBLaB3DFUJJUSe9zBrJeVFzQtc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a798518e71babd3749ea83f0d2ad7aae5967f3f" title="AMD preparing RDNA4 Radeon PRO series with 32GB memory on board" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-preparing-radeon-pro-series-with-navi-48-xtw-gpu-and-32gb-memory-on-board"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T12:13:00+00:00</published>
  </entry>
</feed>
