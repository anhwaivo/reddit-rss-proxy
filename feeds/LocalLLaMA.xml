<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-27T08:52:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kw8a4h</id>
    <title>Is there a high-throughput engine that is actually stable?</title>
    <updated>2025-05-26T23:05:32+00:00</updated>
    <author>
      <name>/u/No-Break-7922</name>
      <uri>https://old.reddit.com/user/No-Break-7922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using vLLM (vllm serve). It's nice when it runs but it keeps hanging and crashing while attempting the simplest of tasks. A prompt or request that works perfectly fine one time will hang or crash sending back two minutes later. Is there an inferencing engine that can handle high throughput while not crashing or hanging every two minutes?&lt;/p&gt; &lt;p&gt;Edit: Info on my setup since people are asking:&lt;/p&gt; &lt;p&gt;vllm 0.8.5.post1, CUDA 12.4 via nvidia's docker image (cudnn-devel), A100 80GB&lt;/p&gt; &lt;p&gt;Installed vllm via Python per instructions at &lt;a href="https://docs.vllm.ai/en/v0.8.5.post1/getting_started/installation/gpu.html#set-up-using-python"&gt;https://docs.vllm.ai/en/v0.8.5.post1/getting_started/installation/gpu.html#set-up-using-python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How I start the V0 engine: &lt;code&gt;vllm serve Qwen/Qwen2.5-VL-32B-Instruct-AWQ --gpu-memory-utilization 0.75 --max-model-len 8192 --limit-mm-per-prompt image=1,video=0 --mm-processor-kwargs '{&amp;quot;min_pixels&amp;quot;: 3136, &amp;quot;max_pixels&amp;quot;: 519400}'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This above gives me max concurrency much higher than 1.00. I've also tried with gpu-memory-utilization 0.3, 0.9, they all hang.&lt;/p&gt; &lt;p&gt;I've been log hunting with all the debug env vars on, but it doesn't even log any errors. It'll just randomly decide to hang or crash. Most of the time it hangs but only once it gave it away that I saw logs indicating it had restarted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Break-7922"&gt; /u/No-Break-7922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw8a4h/is_there_a_highthroughput_engine_that_is_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw8a4h/is_there_a_highthroughput_engine_that_is_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw8a4h/is_there_a_highthroughput_engine_that_is_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T23:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw9ecd</id>
    <title>PC for local AI</title>
    <updated>2025-05-26T23:59:44+00:00</updated>
    <author>
      <name>/u/amunocis</name>
      <uri>https://old.reddit.com/user/amunocis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there! I use AI a lot. For the last 2 months I'm being experimenting with Roo Code and MCP servers, but always using Gemini, Claude and Deepseek. I would like to try local models but not sure what I need to get a good model running, like Devstral or Qwen 3. My actual PC is not that big: i5 13600kf, 32gb ram, rtx4070 super.&lt;/p&gt; &lt;p&gt;Should I sell this gpu and buy a 4090 or 5090? Can I add a second gpu to add bulk gpu ram? &lt;/p&gt; &lt;p&gt;Thanks for your answers!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amunocis"&gt; /u/amunocis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw9ecd/pc_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw9ecd/pc_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw9ecd/pc_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T23:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvqgpv</id>
    <title>Deepseek R2 might be coming soon, unsloth released an article about deepseek v3 -05-26</title>
    <updated>2025-05-26T09:48:05+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It should be coming soon! &lt;a href="https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally"&gt;https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally&lt;/a&gt;&lt;br /&gt; opus 4 level? I think v3 0526 should be out this week, actually i think it is probable that it will be like qwen, reasoning and nonthinking will be together…Maybe it will be called v4 or 3.5?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqgpv/deepseek_r2_might_be_coming_soon_unsloth_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T09:48:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwelai</id>
    <title>Prompting for agentic workflows</title>
    <updated>2025-05-27T04:36:09+00:00</updated>
    <author>
      <name>/u/ansmo</name>
      <uri>https://old.reddit.com/user/ansmo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Under the hood I have a project memory that's fed into each new conversation. I tell this to one of my agents at the start of a session and I pretty much have my next day (or sometimes week) planned out:&lt;/p&gt; &lt;p&gt;Break down this (plan.md) into steps that can each be completed within one hour. Publish each of these step plans into serialized markdown files with clear context and deliverables. If it's logical for a task to be completed in one step but would take more than an hour keep it together, just make note that it will take more than an hour in the markdown file. &lt;/p&gt; &lt;p&gt;I'm still iterating on the &amp;quot;completed within x&amp;quot; part. I've tried tokens, context, and complexity. The hour is pretty ambitious for a single agent to complete without any intervention but I don't think it will be that way much longer. I could probably cut out a few words to save tokens but I don't want there to be any chance of confusion. &lt;/p&gt; &lt;p&gt;What kind of prompts are you using to create plans that are suitable for llm agents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ansmo"&gt; /u/ansmo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwelai/prompting_for_agentic_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwelai/prompting_for_agentic_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwelai/prompting_for_agentic_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T04:36:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwfcw1</id>
    <title>PFN Launches PLaMo Translate,a LLM model made for translation task</title>
    <updated>2025-05-27T05:22:58+00:00</updated>
    <author>
      <name>/u/rikimtasu</name>
      <uri>https://old.reddit.com/user/rikimtasu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Archive Link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.preferred.jp/en/news/pr20250527/"&gt;https://www.preferred.jp/en/news/pr20250527/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web Translation Demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://translate-demo.plamo.preferredai.jp/"&gt;https://translate-demo.plamo.preferredai.jp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model on Huggingface:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/pfnet/plamo-2-translate"&gt;https://huggingface.co/pfnet/plamo-2-translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rikimtasu"&gt; /u/rikimtasu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfcw1/pfn_launches_plamo_translatea_llm_model_made_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfcw1/pfn_launches_plamo_translatea_llm_model_made_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfcw1/pfn_launches_plamo_translatea_llm_model_made_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvskpq</id>
    <title>Leveling Up: From RAG to an AI Agent</title>
    <updated>2025-05-26T12:00:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"&gt; &lt;img alt="Leveling Up: From RAG to an AI Agent" src="https://preview.redd.it/qourugv0943f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dd5856da1c363f33ad9545c0f33914cbc5403a" title="Leveling Up: From RAG to an AI Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I've been exploring more advanced ways to use AI, and recently I made a big jump - moving from the usual RAG (Retrieval-Augmented Generation) approach to something more powerful: an &lt;strong&gt;AI Agent that uses a real web browser to search the internet and get stuff done on its own&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In my last guide (&lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;), I showed how we could manually gather info online and feed it into a RAG pipeline. It worked well, but it still needed a human in the loop.&lt;/p&gt; &lt;p&gt;This time, the AI Agent does &lt;em&gt;everything&lt;/em&gt; by itself.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;I asked it the same question - &lt;em&gt;“How much tax was collected in the US in 2024?”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The Agent opened a browser, went to Google, searched the query, clicked through results, read the content, and gave me a clean, accurate answer.&lt;/p&gt; &lt;p&gt;I didn’t touch the keyboard after asking the question.&lt;/p&gt; &lt;p&gt;I put together a guide so you can run this setup on your own bare metal server with an Nvidia GPU. It takes just a few minutes:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-AI-AGENT.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-AI-AGENT.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🛠️ What you'll spin up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A server running &lt;strong&gt;Sbnb Linux&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;A VM with &lt;strong&gt;Ubuntu 24.04&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ollama with default model &lt;code&gt;qwen2.5:7b&lt;/code&gt; for local GPU-accelerated inference (no cloud, no API calls)&lt;/li&gt; &lt;li&gt;The open-source &lt;strong&gt;Browser Use AI Agent&lt;/strong&gt; &lt;a href="https://github.com/browser-use/web-ui"&gt;https://github.com/browser-use/web-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Give it a shot and let me know how it goes! Curious to hear what use cases you come up with (for more ideas and examples of AI Agents, be sure to follow the amazing Browser Use project!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qourugv0943f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvskpq/leveling_up_from_rag_to_an_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T12:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvqrzl</id>
    <title>AI Baby Monitor – fully local Video-LLM nanny (beeps when safety rules are violated)</title>
    <updated>2025-05-26T10:09:06+00:00</updated>
    <author>
      <name>/u/CheeringCheshireCat</name>
      <uri>https://old.reddit.com/user/CheeringCheshireCat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"&gt; &lt;img alt="AI Baby Monitor – fully local Video-LLM nanny (beeps when safety rules are violated)" src="https://external-preview.redd.it/dXQydzR0cjNwMzNmMVMRslQYMYRN8ZJ1qBgR4-LlFEA6jckhHIJ4it6HP21k.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07fe467bd7f592919e65184006ed23558a3fe52e" title="AI Baby Monitor – fully local Video-LLM nanny (beeps when safety rules are violated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I’ve hacked together a VLM video nanny, that watches a video stream(s) and predefined set of safety instructions, and makes a beep sound if the instructions are violated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/zeenolife/ai-baby-monitor"&gt;https://github.com/zeenolife/ai-baby-monitor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;br /&gt; First day we assembled the crib, my daughter tried to climb over the rail. I got a bit paranoid about constantly watching her. So I thought of an additional eye that would actively watch her, while parent is semi-actively alert.&lt;br /&gt; It's not meant to be a replacement for an adult supervision, more of a supplement, thus just a &amp;quot;beep&amp;quot; sound, so that you could quickly turn back attention to the baby when you got a bit distracted.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works?&lt;/strong&gt;&lt;br /&gt; I'm using Qwen 2.5VL(empirically it works better) and vLLM. Redis is used to orchestrate video and llm log streams. Streamlit for UI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Funny bit&lt;/strong&gt;&lt;br /&gt; I've also used it to monitor my smartphone usage. When you subconsciously check on your phone, it beeps :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Further plans&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add support for other backends apart from vLLM&lt;/li&gt; &lt;li&gt;Gemma 3n looks rather promising&lt;/li&gt; &lt;li&gt;Add support for image based &amp;quot;no-go-zones&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback is welcome :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheeringCheshireCat"&gt; /u/CheeringCheshireCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gzn6itr3p33f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvqrzl/ai_baby_monitor_fully_local_videollm_nanny_beeps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T10:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnti4</id>
    <title>Open-source project that use LLM as deception system</title>
    <updated>2025-05-26T06:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone 👋&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on that I think you'll find really interesting. It's called Beelzebub, an open-source honeypot framework that uses LLMs to create incredibly realistic and dynamic deception environments.&lt;/p&gt; &lt;p&gt;By integrating LLMs, it can mimic entire operating systems and interact with attackers in a super convincing way. Imagine an SSH honeypot where the LLM provides plausible responses to commands, even though nothing is actually executed on a real system.&lt;/p&gt; &lt;p&gt;The goal is to keep attackers engaged for as long as possible, diverting them from your real systems and collecting valuable, real-world data on their tactics, techniques, and procedures. We've even had success capturing real threat actors with it!&lt;/p&gt; &lt;p&gt;I'd love for you to try it out, give it a star on GitHub, and maybe even contribute! Your feedback,&lt;br /&gt; especially from an LLM-centric perspective, would be incredibly valuable as we continue to develop it.&lt;/p&gt; &lt;p&gt;You can find the project here:&lt;/p&gt; &lt;p&gt;👉 GitHub:&lt;a href="https://github.com/mariocandela/beelzebub"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think in the comments! Do you have ideas for new LLM-powered honeypot features?&lt;/p&gt; &lt;p&gt;Thanks for your time! 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwfftk</id>
    <title>AgentKit - Drop-in plugin system for AI agents and MCP servers</title>
    <updated>2025-05-27T05:28:10+00:00</updated>
    <author>
      <name>/u/atrfx</name>
      <uri>https://old.reddit.com/user/atrfx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"&gt; &lt;img alt="AgentKit - Drop-in plugin system for AI agents and MCP servers" src="https://external-preview.redd.it/zd-ZiUz7OOd-ZVTfTYwqHZdD3FQ-Zlrik2BdtQdhqMY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d2e9c474ad2b76e2777114e47d96183c9dfe273" title="AgentKit - Drop-in plugin system for AI agents and MCP servers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of rebuilding the same tools every time I started a new project, or ripping out server/agent implementation to switch solutions, so I built a lightweight plugin system that lets you drop Python files into a folder and generate requirements.txt for them, create a .env with all the relevant items, and dynamically load them into an MCP/Agent solution. It also has a CLI to check compatibility and conflicts.&lt;/p&gt; &lt;p&gt;Hope it's useful to someone else - feedback would be greatly appreciated.&lt;/p&gt; &lt;p&gt;I also converted some of my older tools into this format like a glossary lookup engine and a tool I use to send myself MacOS notifications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/batteryshark/agentkit_plugins"&gt;https://github.com/batteryshark/agentkit_plugins&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atrfx"&gt; /u/atrfx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/batteryshark/agentkit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwi08a</id>
    <title>Open Source iOS OLLAMA Client</title>
    <updated>2025-05-27T08:22:08+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwi08a/open_source_ios_ollama_client/"&gt; &lt;img alt="Open Source iOS OLLAMA Client" src="https://b.thumbs.redditmedia.com/iROMG31yrS6faizWTgItkT1_USO8Q9txCK5xVNyPtyk.jpg" title="Open Source iOS OLLAMA Client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you all know, ollama is a program that allows you to install and use various latest LLMs on your computer. Once you install it on your computer, you don't have to pay a usage fee, and you can install and use various types of LLMs according to your performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wb9qvk3vaa3f1.png?width=1984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0dbebcfe065996625fa68a698b78a24ebf0eaac6"&gt;https://preview.redd.it/wb9qvk3vaa3f1.png?width=1984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0dbebcfe065996625fa68a698b78a24ebf0eaac6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, the company that makes ollama does not make the UI. So there are several ollama-specific programs on the market. Last year, I made an ollama iOS client with Flutter and opened the code, but I didn't like the performance and UI, so I made it again. I will release the source code with the link. You can download the entire Swift source.&lt;/p&gt; &lt;p&gt;You can build it from the source, or you can download the app by going to the link.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bipark/swift_ios_ollama_client_v3"&gt;https://github.com/bipark/swift_ios_ollama_client_v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwi08a/open_source_ios_ollama_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwi08a/open_source_ios_ollama_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwi08a/open_source_ios_ollama_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhg3t</id>
    <title>What are the best vision models at the moment ?</title>
    <updated>2025-05-27T07:42:01+00:00</updated>
    <author>
      <name>/u/Wintlink-</name>
      <uri>https://old.reddit.com/user/Wintlink-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to create an app that extract data from scanned documents and photos, and I was using InterVL2.5-4b running with ollama, but I was wondering if there are better models out there ?&lt;br /&gt; What are your recommendation ?&lt;br /&gt; I wanted to try the 8b version of intervl but there is no GGUF available at the moment.&lt;br /&gt; Thank you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wintlink-"&gt; /u/Wintlink- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T07:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw1hfd</id>
    <title>POC: Running up to 123B as a Letterfriend on &lt;300€ for all hardware.</title>
    <updated>2025-05-26T18:19:12+00:00</updated>
    <author>
      <name>/u/Ploepxo</name>
      <uri>https://old.reddit.com/user/Ploepxo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's swap. This is about my experience running large models on affordable hardware. Who needs NVIDIA when you have some time?&lt;/p&gt; &lt;p&gt;My intention was to have a local, private LLM of the best quality for responding to letters with a large context (8K).&lt;/p&gt; &lt;p&gt;Letters? Yep, it's all about slow response time. Slow. Really slow, so letters seemed to be the best equivalent. You write a long text and receive a long response. But you have to wait for the response. To me, writing a letter instead of sending a quick message isn't that stupid — it takes some classic human intelligence and reflection first.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;, 123B is possible, but we're sending letters overseas. The response took about 32 hours :-) Would you prefer email instead of a letter? 32B gets you an answer in about one and a half to two hours.&lt;/p&gt; &lt;p&gt;Of course, there are several points to fine-tune for performance, but I wanted to focus on the best answers. That's why there is an 8K context window. It's filled with complete letters and summaries of previous conversations. Also n_predict is at 2048&lt;/p&gt; &lt;p&gt;I use llama-server on Linux and a few Python scripts with an SQLite database.&lt;/p&gt; &lt;p&gt;My setup for this is:&lt;/p&gt; &lt;p&gt;ThinkCentre M710q - 100€ &lt;/p&gt; &lt;p&gt;64GB DDR4 SO-Dimms - 130€ &lt;/p&gt; &lt;p&gt;500GB M2.SSD WD Black SN770 - 60€ &lt;/p&gt; &lt;p&gt;SATA SSD - &amp;gt; build in...&lt;/p&gt; &lt;p&gt;So, it's a cheap ThinkCentre that I upgraded with 64 GB of RAM for €130 and an M.2 SSD for swapping. SSD for swap? Yep. I know there will be comments. Don't try this at home ;-)&lt;/p&gt; &lt;p&gt;&lt;code&gt;Available Spare: 100%&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;Available Spare Threshold: 10%&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;Percentage Used: 0%&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;Data Units Read: 108.885.834 [55,7 TB]&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;Data Units Written: 1.475.250 [755 GB]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is after general use and two 123B runs (*lol*). The SSD has a TBW of 300. I only partitioned 250 for swap, so there is significant overprovisioning to prevent too many writes to the cells. This should give me around 600 TBW before the SSD fails — that's over 750 letters or 1,000 days of 24/7 computing! A new SSD for €50 every three years? Not a showstopper at least. The temperature was at a maximum of 60°C, so all is well.&lt;/p&gt; &lt;p&gt;The model used was Bartowski_Mistral-Large-Instruct-2407-GGUF_Mistral-Large-Instruct-2407-Q4_K_S. It used 67 GB of swap...hm. &lt;/p&gt; &lt;p&gt;And then there are the smaller alternatives now. For example, unsloth_Qwen3-32B-GGUF_Qwen3-32B-Q8_0.gguf.&lt;/p&gt; &lt;p&gt;This model fits completely into RAM and does not use swap. It only takes 1/10 of the processing time and still provides very good answers. I'm really impressed! &lt;/p&gt; &lt;p&gt;My conclusion is that running Qwen3-32B-Q8 on RAM is really an option at the moment. &lt;/p&gt; &lt;p&gt;The 123B model is really more a proof of concept, but at least it works. There may be edge use cases for this...if you have some time, you CAN run such a model at low end hardware. These ThinkCentres are really cool - cheap to buy and really stable systems, I had not one crash while testing around....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ploepxo"&gt; /u/Ploepxo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw1hfd/poc_running_up_to_123b_as_a_letterfriend_on_300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw1hfd/poc_running_up_to_123b_as_a_letterfriend_on_300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw1hfd/poc_running_up_to_123b_as_a_letterfriend_on_300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T18:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvytjg</id>
    <title>Just Enhanced my Local Chat Interface</title>
    <updated>2025-05-26T16:33:29+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvytjg/just_enhanced_my_local_chat_interface/"&gt; &lt;img alt="Just Enhanced my Local Chat Interface" src="https://external-preview.redd.it/MXoxbmx4cmdsNTNmMTAE8zj230R8PkW0x6hVMYM0mH-xWYPpjgmp27xhD-Lj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa2d3672fb15e60e66904fa654f1547e038cf53f" title="Just Enhanced my Local Chat Interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve just added significant upgrades to my self-hosted LLM chat application:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Switching&lt;/strong&gt;: Seamlessly toggle between reasoning and non-reasoning models via a dropdown menu—no manual configuration required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Canvas&lt;/strong&gt;: A new document workspace with real-time editing, version history, undo/redo, and PDF export functionality.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live System Prompt Updates&lt;/strong&gt;: Modify and deploy prompts instantly with a single click, ideal for rapid experimentation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Implementation in Database:&lt;/strong&gt; Control the memory or let the model figure it out. Memory is added to the system prompt.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Motivation&lt;/strong&gt;: &lt;/p&gt; &lt;p&gt;As an AI researcher, I wanted a unified tool for coding, brainstorming, and documentation - without relying on cloud services. This update brings everything into one private, offline-first interface. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features to Implement Next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep research&lt;/li&gt; &lt;li&gt;Native MCP servers support&lt;/li&gt; &lt;li&gt;Image native models and image generation support&lt;/li&gt; &lt;li&gt;Chat in both voice and text mode support, live chat and TTS&lt;/li&gt; &lt;li&gt;Accessibility features for Screen Reader and keyboard support&lt;/li&gt; &lt;li&gt;Calling prompts and tools using @ in chat for ease of use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What is crappy here and could be improved? What other things should be implemented? Please provide feedback. I am putting in quite some time and I am loving the UI design and the subtle animations that I put in which lead to a high quality product. Please message me directly in case you do have some direct input, I would love to hear it from you personally!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh1joyrgl53f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvytjg/just_enhanced_my_local_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvytjg/just_enhanced_my_local_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T16:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvzkb5</id>
    <title>350k samples to match distilled R1 on *all* benchmark</title>
    <updated>2025-05-26T17:02:43+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"&gt; &lt;img alt="350k samples to match distilled R1 on *all* benchmark" src="https://preview.redd.it/fblf9e21q53f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c79dbfdfebcffa0c87fa3cb2dbcdee441fc3ade" title="350k samples to match distilled R1 on *all* benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;dataset: &lt;a href="https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts"&gt;https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts&lt;/a&gt;&lt;br /&gt; Cool project from our post training team at Hugging Face, hope you will like it! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fblf9e21q53f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T17:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwdpey</id>
    <title>Best settings for running Qwen3-30B-A3B with llama.cpp (16GB VRAM and 64GB RAM)</title>
    <updated>2025-05-27T03:44:47+00:00</updated>
    <author>
      <name>/u/gamesntech</name>
      <uri>https://old.reddit.com/user/gamesntech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past I used to mostly configure gpu layers to fit as closely as possible on the 16GB RAM. But lately there seem to be much better options to optimize for VRAM/RAM split. Especially with MoE models? I'm currently running Q4_K_M version (about 18.1 GB in size) with 38 layers and 8k context size because I was focusing on fitting as much of the model as possible on VRAM. That runs fairly well but I want to know if there is a much better way to optimize for my configuration.&lt;/p&gt; &lt;p&gt;I would really like to see if I can run the Q8_0 (32 GB obviously) version in a way to utilize my VRAM and RAM as effectively possible and still be usable? I would also love to at least use the full 40K context if possible in this setting.&lt;/p&gt; &lt;p&gt;Lastly, for anyone experimenting with the A22B version as well, I assume it's usable with 128GB RAM? In this scenario, I'm not sure how much the 16GB VRAM can actually help.&lt;/p&gt; &lt;p&gt;Thanks for any advice in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamesntech"&gt; /u/gamesntech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T03:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvpwq3</id>
    <title>Deepseek v3 0526?</title>
    <updated>2025-05-26T09:09:20+00:00</updated>
    <author>
      <name>/u/Stock_Swimming_6015</name>
      <uri>https://old.reddit.com/user/Stock_Swimming_6015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt; &lt;img alt="Deepseek v3 0526?" src="https://external-preview.redd.it/fxYCW6fqdbJ5RWjh_x1fsIyj0ZtZFx8MOAvXVxIw2PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e74df95b54af72feafa558281ef5e11bc4e8a7c" title="Deepseek v3 0526?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock_Swimming_6015"&gt; /u/Stock_Swimming_6015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T09:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvxn13</id>
    <title>🎙️ Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2</title>
    <updated>2025-05-26T15:45:47+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt; &lt;img alt="🎙️ Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" src="https://external-preview.redd.it/YRkD_4f9GG3JjS7U-VyOMhD6UqAgTs9g61YUbxvrlqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b248daf592d1e451e027b35573c081cecc63696" title="🎙️ Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt; &lt;p&gt;I recently built a fully local speech-to-text system using &lt;strong&gt;NVIDIA’s Parakeet-TDT 0.6B v2&lt;/strong&gt; — a 600M parameter ASR model capable of transcribing real-world audio &lt;strong&gt;entirely offline with GPU acceleration&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; Most ASR tools rely on cloud APIs and miss crucial formatting like punctuation or timestamps. This setup works offline, includes segment-level timestamps, and handles a range of real-world audio inputs — like news, lyrics, and conversations.&lt;/p&gt; &lt;p&gt;📽️ &lt;strong&gt;Demo Video:&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Shows transcription of 3 samples — financial news, a song, and a conversation between Jensen Huang &amp;amp; Satya Nadella.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kvxn13/video/1ho0mrnrc53f1/player"&gt;A full walkthrough of the local ASR system built with Parakeet-TDT 0.6B. Includes architecture overview and transcription demos for financial news, song lyrics, and a tech dialogue.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧪 &lt;strong&gt;Tested On:&lt;/strong&gt;&lt;br /&gt; ✅ Stock market commentary with spoken numbers&lt;br /&gt; ✅ Song lyrics with punctuation and rhyme&lt;br /&gt; ✅ Multi-speaker tech conversation on AI and silicon innovation&lt;/p&gt; &lt;p&gt;🛠️ &lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA Parakeet-TDT 0.6B v2 (ASR model)&lt;/li&gt; &lt;li&gt;NVIDIA NeMo Toolkit&lt;/li&gt; &lt;li&gt;PyTorch + CUDA 11.8&lt;/li&gt; &lt;li&gt;Streamlit (for local UI)&lt;/li&gt; &lt;li&gt;FFmpeg + Pydub (preprocessing)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/82jw99tvc53f1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f142584ca7752c796c8efcefa006dd7692500d9b"&gt;Flow diagram showing Local ASR using NVIDIA Parakeet-TDT with Streamlit UI, audio preprocessing, and model inference pipeline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs 100% offline (no cloud APIs required)&lt;/li&gt; &lt;li&gt;Accurate punctuation + capitalization&lt;/li&gt; &lt;li&gt;Word + segment-level timestamp support&lt;/li&gt; &lt;li&gt;Works on my local RTX 3050 Laptop GPU with CUDA 11.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📌 &lt;strong&gt;Full blog + code + architecture + demo screenshots:&lt;/strong&gt;&lt;br /&gt; 🔗 &lt;a href="https://medium.com/towards-artificial-intelligence/%EF%B8%8F-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c"&gt;https://medium.com/towards-artificial-intelligence/️-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SridharSampath/parakeet-asr-demo"&gt;https://github.com/SridharSampath/parakeet-asr-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🖥️ &lt;strong&gt;Tested locally on:&lt;/strong&gt;&lt;br /&gt; NVIDIA RTX 3050 Laptop GPU + CUDA 11.8 + PyTorch&lt;/p&gt; &lt;p&gt;Would love to hear your feedback! 🙌&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T15:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw310h</id>
    <title>I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories</title>
    <updated>2025-05-26T19:21:25+00:00</updated>
    <author>
      <name>/u/DanielAPO</name>
      <uri>https://old.reddit.com/user/DanielAPO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"&gt; &lt;img alt="I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories" src="https://external-preview.redd.it/ZXJzMW1rY2RmNjNmMdaMStUEb5oAuu0jCl0Xw3e5m5dlVJowjoJYmTy8vqCj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae8d75bb795c12292f2f9e29616605f17169afc" title="I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielAPO"&gt; /u/DanielAPO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0yb58acdf63f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T19:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwd7tg</id>
    <title>I forked llama-swap to add an ollama compatible api, so it can be a drop in replacement</title>
    <updated>2025-05-27T03:17:12+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone else who has been annoyed with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ollama&lt;/li&gt; &lt;li&gt;client programs that only support ollama for local models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I present you with &lt;a href="https://github.com/kooshi/llama-swappo"&gt;llama-swappo&lt;/a&gt;, a bastardization of the simplicity of llama-swap which adds an ollama compatible api to it.&lt;/p&gt; &lt;p&gt;This was mostly a quick hack I added for my own interests, so I don't intend to support it long term. All credit and support should go towards the original, but I'll probably set up a github action at some point to try to auto-rebase this code on top of his.&lt;/p&gt; &lt;p&gt;I offered to merge it, but he, correctly, declined based on concerns of complexity and maintenance. So, if anyone's interested, it's available, and if not, well at least it scratched my itch for the day. (Turns out Qwen3 isn't all that competent at driving the Github Copilot Agent, it gave it a good shot though)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T03:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhw20</id>
    <title>Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.</title>
    <updated>2025-05-27T08:13:51+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easiest Setup: No python, no docker, no endless dev packages.&lt;/strong&gt; Just download it from &lt;a href="https://chromewebstore.google.com/detail/pphjdjdoclkedgiaahmiahladgcpohca?utm_source=item-share-cb"&gt;Chrome&lt;/a&gt; or my &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick"&gt;Github&lt;/a&gt; (Same with the store, just the latest release). You don't need an exe.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No privacy issue: you can check the code yourself.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless AI Integration:&lt;/strong&gt; Connect to a wide array of powerful AI models: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; Ollama, LM Studio, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud Services: several&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Connections:&lt;/strong&gt; all OpenAI compatible endpoints.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Content Interaction:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instant Summaries:&lt;/strong&gt; Get the gist of any webpage in seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextual Q&amp;amp;A:&lt;/strong&gt; Ask questions about the current page, PDFs, selected text in the notes or you can simply send the urls directly to the bot, the scrapper will give the bot context to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Web Search with scrapper:&lt;/strong&gt; Conduct context-aware searches using Google, DuckDuckGo, and Wikipedia, with the ability to fetch and analyze content from search results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Personas (system prompts):&lt;/strong&gt; Choose from 7 pre-built AI personalities (Researcher, Strategist, etc.) or create your own.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Speech (TTS):&lt;/strong&gt; Hear AI responses read aloud (supports browser TTS and integration with external services like Piper).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; You can search it (also planed to be used in RAG).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't know how to post image here, tried links, markdown links or directly upload, all failed to display. Screenshots gifs links below: &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw6akn</id>
    <title>CRAZY voice quality for uncensored roleplay, I wish it's local.</title>
    <updated>2025-05-26T21:37:18+00:00</updated>
    <author>
      <name>/u/ExplanationEqual2539</name>
      <uri>https://old.reddit.com/user/ExplanationEqual2539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Fcq85N0grk4"&gt;https://www.youtube.com/watch?v=Fcq85N0grk4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplanationEqual2539"&gt; /u/ExplanationEqual2539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T21:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw7n6w</id>
    <title>DIA 1B Podcast Generator - With Consistent Voices and Script Generation</title>
    <updated>2025-05-26T22:36:14+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt; &lt;img alt="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" src="https://external-preview.redd.it/NG1pdDduNDFlNzNmMUcfJmyGLBoX3HGWzWW7GBEQ5TlU9sPw-Gkkjhi-K8NK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07c6ebc8b3055e1fcfc4b4a856d4bdb99beffb3f" title="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pleased to share 🐐 GOATBookLM 🐐... &lt;/p&gt; &lt;p&gt;A dual voice Open Source podcast generator powered by &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23narilabs&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#NariLabs&lt;/a&gt; &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23dia&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#Dia&lt;/a&gt; 1B audio model (with a little sprinkling of &lt;a href="https://www.linkedin.com/company/googledeepmind/"&gt;Google DeepMind&lt;/a&gt;'s Gemini Flash 2.5 and &lt;a href="https://www.linkedin.com/company/anthropicresearch/"&gt;Anthropic&lt;/a&gt; Sonnet 4) &lt;/p&gt; &lt;p&gt;What started as an evening playing around with a new open source audio model on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt; ended up as a week building an open source podcast generator.&lt;/p&gt; &lt;p&gt;Out of the box Dia 1B, the model powering the audio, is a rather unpredictable model, with random voices spinning up for every audio generation.&lt;/p&gt; &lt;p&gt;With a little exploration and testing I was able to fix this, and optimize the speaker dialogue format for pretty strong results.&lt;/p&gt; &lt;p&gt;Running entirely in Google colab 🐐 GOATBookLM 🐐 includes:&lt;/p&gt; &lt;p&gt;🔊 Dual voice/ speaker podcast script creation from any text input file&lt;/p&gt; &lt;p&gt;🔊 Full consistency in Dia 1B voices using a selection of demo cloned voices&lt;/p&gt; &lt;p&gt;🔊 Full preview and regeneration of audio files (for quick corrections)&lt;/p&gt; &lt;p&gt;🔊 Full final output in .wav or .mp3&lt;/p&gt; &lt;p&gt;Link to the Notebook: &lt;a href="https://github.com/smartaces/dia_podcast_generator"&gt;https://github.com/smartaces/dia_podcast_generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4ym9al41e73f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T22:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwer9z</id>
    <title>Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</title>
    <updated>2025-05-27T04:46:15+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt; &lt;img alt="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" src="https://external-preview.redd.it/Mslr5FmgDa5Wl6TVAGHIe-yyfpC8KB7GpupP6mmM8Ko.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aefa740a7c49432c821d22fe05c260150bb95bc" title="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.&lt;br /&gt; Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Haoz0206/Omni-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T04:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvz322</id>
    <title>Qwen 3 30B A3B is a beast for MCP/ tool use &amp; Tiny Agents + MCP @ Hugging Face! 🔥</title>
    <updated>2025-05-26T16:44:22+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heya everyone, I'm VB from Hugging Face, we've been experimenting with MCP (Model Context Protocol) quite a bit recently. In our (vibe) tests, Qwen 3 30B A3B gives the best performance overall wrt size and tool calls! Seriously underrated.&lt;/p&gt; &lt;p&gt;The most recent &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12379"&gt;streamable tool calling support&lt;/a&gt; in llama.cpp makes it even more easier to use it locally for MCP. Here's how you can try it out too:&lt;/p&gt; &lt;p&gt;Step 1: Start the llama.cpp server `llama-server --jinja -fa -hf unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M -c 16384`&lt;/p&gt; &lt;p&gt;Step 2: Define an `agent.json` file w/ MCP server/s&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M&amp;quot;, &amp;quot;endpointUrl&amp;quot;: &amp;quot;http://localhost:8080/v1&amp;quot;, &amp;quot;servers&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;sse&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse&amp;quot; } } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Step 3: Run it &lt;/p&gt; &lt;pre&gt;&lt;code&gt;npx @huggingface/tiny-agents run ./local-image-gen &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details here: &lt;a href="https://github.com/Vaibhavs10/experiments-with-mcp"&gt;https://github.com/Vaibhavs10/experiments-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To make it easier for tinkerers like you, we've been experimenting around tooling for MCP and registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MCP Registry - you can now host spaces as MCP server on Hugging Face (with just one line of code): &lt;a href="https://huggingface.co/spaces?filter=mcp-server"&gt;https://huggingface.co/spaces?filter=mcp-server&lt;/a&gt; (all the spaces that are MCP compatible)&lt;/li&gt; &lt;li&gt;MCP Clients - we've created &lt;a href="https://github.com/huggingface/huggingface.js/tree/main/packages/tiny-agents"&gt;TypeScript&lt;/a&gt; and &lt;a href="https://huggingface.co/blog/python-tiny-agents"&gt;Python interfaces&lt;/a&gt; for you to experiment local and deployed models directly w/ MCP&lt;/li&gt; &lt;li&gt;MCP Course - learn more about MCP in an applied manner directly here: &lt;a href="https://huggingface.co/learn/mcp-course/en/unit0/introduction"&gt;https://huggingface.co/learn/mcp-course/en/unit0/introduction&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We're experimenting a lot more with open models, local + remote workflows for MCP, do let us know what you'd like to see. Moore so keen to hear your feedback on all!&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;VB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T16:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwfp8v</id>
    <title>Used A100 80 GB Prices Don't Make Sense</title>
    <updated>2025-05-27T05:44:37+00:00</updated>
    <author>
      <name>/u/fakebizholdings</name>
      <uri>https://old.reddit.com/user/fakebizholdings</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone explain what I'm missing? The median price of the A100 80GB PCIe on eBay is $18,502 RTX 6000 Pro Blackwell cards can be purchased new for $8500. &lt;/p&gt; &lt;p&gt;What am I missing here? Is there something about the A100s that justifies the price difference? The only thing I can think of is 200w less power consumption and NVlink.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakebizholdings"&gt; /u/fakebizholdings &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:44:37+00:00</published>
  </entry>
</feed>
