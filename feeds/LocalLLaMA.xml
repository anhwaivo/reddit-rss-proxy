<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-16T06:25:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1knfggw</id>
    <title>Are there any models that are even half funny?</title>
    <updated>2025-05-15T18:24:27+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any models that can write funny text including jokes? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T18:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2aay</id>
    <title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title>
    <updated>2025-05-15T07:28:36+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt; &lt;img alt="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" src="https://preview.redd.it/ww4aygc1ew0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18baa07396402b906dd387ccabc4f5bab873fba3" title="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.09343"&gt;https://arxiv.org/abs/2505.09343&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww4aygc1ew0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1knjt9r</id>
    <title>Running VLM on-device (iPhone or Android)</title>
    <updated>2025-05-15T21:22:49+00:00</updated>
    <author>
      <name>/u/Ill-Still-6859</name>
      <uri>https://old.reddit.com/user/Ill-Still-6859</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjt9r/running_vlm_ondevice_iphone_or_android/"&gt; &lt;img alt="Running VLM on-device (iPhone or Android)" src="https://external-preview.redd.it/VarQD-feovIEBvsJewLTMSKZlEmb4mPmFvJ5wH85xBY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5eadbdcf56e8dbe5c1a5c2f89c614cd8d91b65f9" title="Running VLM on-device (iPhone or Android)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is not a release yet, just a poc. Still, it's exciting to see a VLM running on-device with such low latency..&lt;br /&gt; Demo device: iPhone 13 Pro&lt;br /&gt; Repo: &lt;a href="https://github.com/a-ghorbani/pocketpal-ai"&gt;https://github.com/a-ghorbani/pocketpal-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Major ingredients:&lt;br /&gt; - SmolVLM (500m)&lt;br /&gt; - llama.cpp&lt;br /&gt; - llama.rn&lt;br /&gt; - &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/mtmd"&gt;mtmd tool from llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1knjt9r/video/n728h3fai01f1/player"&gt;https://reddit.com/link/1knjt9r/video/n728h3fai01f1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Still-6859"&gt; /u/Ill-Still-6859 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjt9r/running_vlm_ondevice_iphone_or_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjt9r/running_vlm_ondevice_iphone_or_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knjt9r/running_vlm_ondevice_iphone_or_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T21:22:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1knnb6u</id>
    <title>Context parsing utility</title>
    <updated>2025-05-16T00:04:30+00:00</updated>
    <author>
      <name>/u/MichalRoth</name>
      <uri>https://old.reddit.com/user/MichalRoth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’ve been running local models and kept needing a way to manage structured context without hacking together prompts every time. So I wrote a small thing - prompt-shell&lt;/p&gt; &lt;p&gt;It lets you define pieces of context (&lt;code&gt;rules.md&lt;/code&gt;, &lt;code&gt;identity.md&lt;/code&gt;, &lt;code&gt;input.md&lt;/code&gt;, etc.), assembles them into a final prompt, and counts tokens with tiktoken.&lt;/p&gt; &lt;p&gt;No UI, no framework, just files + a build script. Not meant to be a product — just something that made my workflow cleaner.&lt;/p&gt; &lt;p&gt;Sharing in case it’s useful to anyone else: &lt;a href="https://gitlab.com/michalrothcz/prompt-shell"&gt;https://gitlab.com/michalrothcz/prompt-shell&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MichalRoth"&gt; /u/MichalRoth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnb6u/context_parsing_utility/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnb6u/context_parsing_utility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knnb6u/context_parsing_utility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T00:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6mic</id>
    <title>Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?</title>
    <updated>2025-05-15T12:12:37+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt; &lt;img alt="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" src="https://preview.redd.it/kq34jkwvsx0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b2276e1df77e5648f6b562bf60fe6c8a922ea4" title="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been digging into the latest base models and wanted to get some practical opinions beyond just benchmark numbers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;For those who have actually used both Qwen 2.5 and Qwen 3 base models&lt;/strong&gt;: Did you notice a truly big jump in general usage (reasoning, instruction following, robustness), or is the improvement mostly confined to coding and math tasks? I’m not talking about fine-tuned chat versions, just the raw base models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 3 vs Qwen&lt;/strong&gt;: Is Gemma 3 genuinely that far behind, or is there some possible benchmark leakage or overfitting with Qwen? A few benchmark charts make me suspicious. Would love to hear hands-on perspectives if anyone has experimented with both.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I’m asking:&lt;/strong&gt;&lt;br /&gt; I want to build a highly &lt;em&gt;steerable&lt;/em&gt; model for my research and product work. I only have budget for one serious base model to work from, so I want to select the absolute best starting point. I’m focusing on openness, quality, and steerability, not just raw benchmark wins.&lt;/p&gt; &lt;p&gt;Any honest feedback, experiments, or even failures you’ve had with these models would help me massively. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kq34jkwvsx0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn9882</id>
    <title>LLaDA-8B-Tools: A diffusion language model fine-tuned for tool use</title>
    <updated>2025-05-15T14:12:37+00:00</updated>
    <author>
      <name>/u/ProximileLLC</name>
      <uri>https://old.reddit.com/user/ProximileLLC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instead of generating token-by-token, this architecture refines the whole output by replacing mask tokens across the sequence.&lt;/p&gt; &lt;p&gt;The bidirectional attention seems to help with structured outputs, though this is just a rough first attempt with some issues (e.g. extra text after a message, because of this architecture's preset generation length).&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Dataset: &lt;a href="https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Format mostly follows Llama 3.1: &lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a variant tuned for more general tool use using a range of i/o formats.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProximileLLC"&gt; /u/ProximileLLC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn8m8t</id>
    <title>Open-source general purpose agent with built-in MCPToolkit support</title>
    <updated>2025-05-15T13:46:21+00:00</updated>
    <author>
      <name>/u/Fluffy_Sheepherder76</name>
      <uri>https://old.reddit.com/user/Fluffy_Sheepherder76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt; &lt;img alt="Open-source general purpose agent with built-in MCPToolkit support" src="https://preview.redd.it/h6y4hb7s9y0f1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee3d023b25d2e2f99165aa457441e34896b8d16c" title="Open-source general purpose agent with built-in MCPToolkit support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The open-source OWL agent now comes with built-in MCPToolkit support, just drop in your MCP servers (Playwright, desktop-commander, custom Python tools, etc.) and OWL will automatically discover and call them in its multi-agent workflows.&lt;/p&gt; &lt;p&gt;OWL: &lt;a href="https://github.com/camel-ai/owl"&gt;https://github.com/camel-ai/owl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Sheepherder76"&gt; /u/Fluffy_Sheepherder76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h6y4hb7s9y0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1knca48</id>
    <title>Quick Qwen3-30B-A6B-16-Extreme vs Qwen3-30B A3B Benchmark</title>
    <updated>2025-05-15T16:17:15+00:00</updated>
    <author>
      <name>/u/terhechte</name>
      <uri>https://old.reddit.com/user/terhechte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I have a Benchmark suite of 110 tasks across multiple programming languages. The focus really is on more complex problems and not Javascript one-shot problems. I was interested in comparing the above two models. &lt;/p&gt; &lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;- Qwen3-30B-A6B-16-Extreme Q4_K_M running in LMStudio&lt;br /&gt; - Qwen3-30B A3B on OpenRouter&lt;/p&gt; &lt;p&gt;I understand that this is not a fair fight because the A6B is heavily quantized, but running this benchmark on my Macbook takes almost 12 hours with reasoning models, so a better comparison will take a bit longer. &lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;p&gt;| lmstudio/qwen3-30b-a6b-16-extreme | correct: 56 | wrong: 54 |&lt;/p&gt; &lt;p&gt;| openrouter/qwen/qwen3-30b-a3b | correct: 68 | wrong: 42 |&lt;/p&gt; &lt;p&gt;I will try to report back in a couple of days with more comparisons. &lt;/p&gt; &lt;p&gt;You can learn more about the benchmark here (&lt;a href="https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html"&gt;https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html&lt;/a&gt;) but I've since also added support for more models and languages. However I haven't really released the results in some time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terhechte"&gt; /u/terhechte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T16:17:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kna53n</id>
    <title>Qwen3-32B hallucinates more than QwQ-32B</title>
    <updated>2025-05-15T14:50:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt; &lt;img alt="Qwen3-32B hallucinates more than QwQ-32B" src="https://b.thumbs.redditmedia.com/BKpqaFBECcC520jrmC7_8NwZgTpcy4cdtN47rrRQrVU.jpg" title="Qwen3-32B hallucinates more than QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing some people complaining about Qwen3's hallucination issues. Personally, I have never run into such issue, but I recently came across some Chinese benchmarks of Qwen3 and QwQ, so I might as well share them here.&lt;/p&gt; &lt;p&gt;I translated these to English; the sources are in the images.&lt;/p&gt; &lt;p&gt;TLDR:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B has a lower SimpleQA score than QwQ (5.87% vs 8.07%)&lt;/li&gt; &lt;li&gt;Qwen3-32B has a higher hallucination rate than QwQ in reasoning mode (30.15% vs 22.7%)&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;SuperCLUE-Faith is designed to evaluate Chinese language performance, so it obviously gives Chinese models an advantage over American ones, but should be useful for comparing Qwen models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee"&gt;https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132"&gt;https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22"&gt;https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f"&gt;https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;I have no affiliation with either of the two evaluation agencies. I'm simply sharing the review results that I came across.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2mv9</id>
    <title>LLMs Get Lost In Multi-Turn Conversation</title>
    <updated>2025-05-15T07:53:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt; &lt;img alt="LLMs Get Lost In Multi-Turn Conversation" src="https://b.thumbs.redditmedia.com/MIMwMQ4O4HnoFjzXbBTjShTxVfai2B_u3_lcuHpfKVk.jpg" title="LLMs Get Lost In Multi-Turn Conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://arxiv.org/abs/2505.06120"&gt;paper&lt;/a&gt; found that the performance of open and closed LLMs drops significantly in multi-turn conversations. Most benchmarks focus on single-turn, fully-specified instruction settings. They found that LLMs often make (incorrect) assumptions in early turns, on which they rely going forward and never recover from.&lt;/p&gt; &lt;p&gt;They concluded that when a multi-turn conversation doesn't yield the desired results, it might help to restart with a fresh conversation, putting all the relevant information from the multi-turn conversation into the first turn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836"&gt;https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Sharded&amp;quot; means they split an original fully-specified single-turn instruction into multiple tidbits of information that they then fed the LLM turn by turn. &amp;quot;Concat&amp;quot; is a comparison as a baseline where they fed all the generated information pieces in the same turn. Here are examples on how they did the splitting:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2"&gt;https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kntnfn</id>
    <title>New Wayfarer</title>
    <updated>2025-05-16T05:57:44+00:00</updated>
    <author>
      <name>/u/ScavRU</name>
      <uri>https://old.reddit.com/user/ScavRU</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"&gt; &lt;img alt="New Wayfarer" src="https://external-preview.redd.it/LU2gTXNE2BU0Un_eM36qvVexiACBVgpQzKg0ygmj_bE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50b539fa8660c67ab3d2b2b6de1d79bf8ba7373b" title="New Wayfarer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScavRU"&gt; /u/ScavRU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Harbinger-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T05:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbdd3</id>
    <title>Hugging Face free and open source MCP course</title>
    <updated>2025-05-15T15:40:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're thrilled to announce the launch of our comprehensive Model Context Protocol (MCP) Course! This free program is designed to take learners from foundational understanding to practical application of MCP in AI.&lt;/p&gt; &lt;p&gt;Join the course on the hub:&lt;a href="https://huggingface.co/mcp-course"&gt;https://huggingface.co/mcp-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this course, you will: 📖 Study Model Context Protocol in theory, design, and practice. 🧑‍💻 Learn to use established MCP SDKs and frameworks. 💾 Share your projects and explore applications created by the community. 🏆 Participate in challenges and evaluate your MCP implementations. 🎓 Earn a certificate of completion.&lt;/p&gt; &lt;p&gt;At the end, you'll understand how MCP works and how to build your own AI applications that leverage external data and tools using the latest MCP standards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn75q8</id>
    <title>PDF input merged into llama.cpp</title>
    <updated>2025-05-15T12:39:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1knnyco</id>
    <title>Mistral Small/Medium vs Qwen 3 14/32B</title>
    <updated>2025-05-16T00:37:51+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since things have been a little slow over the past couple weeks, figured throw mistral's new releases against Qwen3. I chose 14/32B, because the scores seem in the same ballpark.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IgyP5EWW6qk"&gt;https://www.youtube.com/watch?v=IgyP5EWW6qk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key Findings:&lt;/p&gt; &lt;p&gt;Mistral medium is definitely an improvement over mistral small, but not by a whole lot, mistral small in itself is a very strong model. Qwen is a clear winner in coding, even the 14b beats both mistral models. The NER (structured json) test Qwen struggles but this is because of its weakness in non English questions. RAG I feel mistral medium is better than the rest. Overall, I feel Qwen 32b &amp;gt; mistral medium &amp;gt; mistral small &amp;gt; Qwen 14b. But again, as with anything llm, YMMV.&lt;/p&gt; &lt;p&gt;Here is a summary table&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Timestamp&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Harmful Question Detection&lt;/td&gt; &lt;td align="left"&gt;Mistral Medium&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mistral Small&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 14B&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Named Entity Recognition&lt;/td&gt; &lt;td align="left"&gt;Both Mistral&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=412"&gt;06:52&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Both Qwen&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=412"&gt;06:52&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SQL Query Generation&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 models&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=602"&gt;10:02&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Both Mistral&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=691"&gt;11:31&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Retrieval Augmented Generation&lt;/td&gt; &lt;td align="left"&gt;Mistral Medium&lt;/td&gt; &lt;td align="left"&gt;93%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;92.5%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mistral Small&lt;/td&gt; &lt;td align="left"&gt;90.75%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 14B&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=796"&gt;13:16&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T00:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1knh0dq</id>
    <title>Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes</title>
    <updated>2025-05-15T19:27:35+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"&gt; &lt;img alt="Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes" src="https://external-preview.redd.it/fOELlCefhPVcX_I27jAk8-oOBjhtXlke2ANY2PCUgkA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1294dd31165e64cd951bf768591cb3a24a57502f" title="Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ReisCook/Voice_Extractor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T19:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1knji91</id>
    <title>Soon if a model architecture is supported by "transformers", you can expect it to be supported in the rest of the ecosystem.</title>
    <updated>2025-05-15T21:10:11+00:00</updated>
    <author>
      <name>/u/behradkhodayar</name>
      <uri>https://old.reddit.com/user/behradkhodayar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"&gt; &lt;img alt="Soon if a model architecture is supported by &amp;quot;transformers&amp;quot;, you can expect it to be supported in the rest of the ecosystem." src="https://external-preview.redd.it/vXXJM0Qn_BM8I_YhlKgXpMf8jgjpmMwwyNtmu7BK1pM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1dfc69125f94a6206257f867746e11c5b8f79e49" title="Soon if a model architecture is supported by &amp;quot;transformers&amp;quot;, you can expect it to be supported in the rest of the ecosystem." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More model interoperability through HF's joint efforts w lots of model builders.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/behradkhodayar"&gt; /u/behradkhodayar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/transformers-model-definition"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T21:10:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1knl587</id>
    <title>Meta is delaying the rollout of its flagship AI model (WSJ)</title>
    <updated>2025-05-15T22:20:53+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"&gt; &lt;img alt="Meta is delaying the rollout of its flagship AI model (WSJ)" src="https://preview.redd.it/gdsyodsot01f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a311d3625dba19a91ac3b06067a91ee8aa3bc7a" title="Meta is delaying the rollout of its flagship AI model (WSJ)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the article: &lt;a href="https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7"&gt;https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gdsyodsot01f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T22:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1knqfw3</id>
    <title>Simple generation speed test with 2x Arc B580</title>
    <updated>2025-05-16T02:48:57+00:00</updated>
    <author>
      <name>/u/prompt_seeker</name>
      <uri>https://old.reddit.com/user/prompt_seeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt; &lt;img alt="Simple generation speed test with 2x Arc B580" src="https://b.thumbs.redditmedia.com/Z5P1XL9jS1O1ASJhe2Dv2-OiirlE7J5MmCa_bH9LdMQ.jpg" title="Simple generation speed test with 2x Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There have been recent rumors about the B580 24GB, so I ran some new tests using my B580s. I used llama.cpp with some backends to test text generation speed using google_gemma-3-27b-it-IQ4_XS.gguf.&lt;/p&gt; &lt;h1&gt;Tested backends&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;IPEX-LLM llama.cpp &lt;ul&gt; &lt;li&gt;build: 1 (3b94b45) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;official llama.cpp SYCL &lt;ul&gt; &lt;li&gt;build: 5400 (c6a2c9e7) with Intel(R) oneAPI DPC++/C++ Compiler 2025.1.1 (2025.1.1.20250418) for x86_64-unknown-linux-gnu&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;official llama.cpp VULKAN &lt;ul&gt; &lt;li&gt;build: 5395 (9c404ed5) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu &lt;em&gt;(from release)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Base command&lt;/h1&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m AI-12/google_gemma-3-27b-it-Q4_K_S.gguf -ngl 99 -c 8192 -b 512 -p &amp;quot;Why is sky blue?&amp;quot; -no-cnv&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Build&lt;/th&gt; &lt;th align="left"&gt;&lt;code&gt;-fa&lt;/code&gt; Option&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Eval Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Total Tokens Generated&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;3b94b45 (IPEX-LLM)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;52.22&lt;/td&gt; &lt;td align="left"&gt;8.18&lt;/td&gt; &lt;td align="left"&gt;393&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3b94b45 (IPEX-LLM)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;(corrupted text)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;c6a2c9e7 (SYCL)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;13.72&lt;/td&gt; &lt;td align="left"&gt;5.66&lt;/td&gt; &lt;td align="left"&gt;545&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;c6a2c9e7 (SYCL)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;10.73&lt;/td&gt; &lt;td align="left"&gt;5.04&lt;/td&gt; &lt;td align="left"&gt;362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9c404ed5 (vulkan)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;35.38&lt;/td&gt; &lt;td align="left"&gt;4.85&lt;/td&gt; &lt;td align="left"&gt;487&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9c404ed5 (vulkan)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;32.99&lt;/td&gt; &lt;td align="left"&gt;4.78&lt;/td&gt; &lt;td align="left"&gt;559&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Thoughts&lt;/h1&gt; &lt;p&gt;The results are disappointing. I previously tested google-gemma-2-27b-IQ4_XS.gguf with 2x 3060 GPUs, and achieved around 15 t/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xuijd9iz121f1.png?width=606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b280fe6c9e3ca8f752ae59208008fed818f1d8d1"&gt;https://preview.redd.it/xuijd9iz121f1.png?width=606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b280fe6c9e3ca8f752ae59208008fed818f1d8d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With image generation models, the B580 achieves generation speeds close to the RTX 4070, but its performance with LLMs seems to fall short of expectations.&lt;/p&gt; &lt;p&gt;I don’t know how much the PRO version (B580 with 24GB) will cost, but if you’re looking for a budget-friendly way to get more RAM, it might be better to consider the AI MAX+ 395 (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;I’ve heard it can reach 6.4 tokens per second with 32B Q8&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I tested this on Linux, but since Arc GPUs are said to perform better on Windows, you might get faster results there. If anyone has managed to get better performance with the B580, please let me know in the comments.&lt;/p&gt; &lt;p&gt;* Interestingly, generation is fast up to around 100–200 tokens, but then it gradually slows down. so using&lt;code&gt;llama-bench&lt;/code&gt; with tg512/pp128 is not a good way to test this GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prompt_seeker"&gt; /u/prompt_seeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T02:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1knfe13</id>
    <title>ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB</title>
    <updated>2025-05-15T18:21:39+00:00</updated>
    <author>
      <name>/u/nostriluu</name>
      <uri>https://old.reddit.com/user/nostriluu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"&gt; &lt;img alt="ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB" src="https://external-preview.redd.it/Bf1eGAFfYgmopj7bn8x57X5Vubn-mFaf7TrFzb01Rl4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd1c6de1776b9a8e40c014c77fcc5ce1cd52905" title="ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nostriluu"&gt; /u/nostriluu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.lenovo.com/all-new-lenovo-thinkstation-pgx-big-ai-innovation-in-a-small-form-factor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn542r</id>
    <title>Introducing A.I.T.E Ball</title>
    <updated>2025-05-15T10:45:28+00:00</updated>
    <author>
      <name>/u/tonywestonuk</name>
      <uri>https://old.reddit.com/user/tonywestonuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt; &lt;img alt="Introducing A.I.T.E Ball" src="https://external-preview.redd.it/NXllMTcxNDFkeDBmMcTQf63cMAAIN-71fn86oCbnKUR2tA_D5RmS947R5l7-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf3613bb545a67e6ba0ae442a8d9fddc761c89a7" title="Introducing A.I.T.E Ball" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a totally self contained (no internet) AI powered 8ball.&lt;/p&gt; &lt;p&gt;Its running on an Orange pi zero 2w, with whisper.cpp to do the text-2-speach, and llama.cpp to do the llm thing, Its running Gemma 3 1b. About as much as I can do on this hardware. But even so.... :-) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonywestonuk"&gt; /u/tonywestonuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/scyofz31dx0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1knjm0s</id>
    <title>Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24</title>
    <updated>2025-05-15T21:14:28+00:00</updated>
    <author>
      <name>/u/TokyoCapybara</name>
      <uri>https://old.reddit.com/user/TokyoCapybara</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"&gt; &lt;img alt="Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24" src="https://external-preview.redd.it/aTdnbWV3c25kMDFmMckumtgWbpWBlQZ_vRBN65fbuS7eF6LKJlM_WmjlxhmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81dfc6fb92a74b03033c4ffb473c5f4ea11f0bde" title="Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow-up on a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kckxgg/qwen3_06b_running_at_75_toks_on_iphone_15_pro/"&gt;previous post&lt;/a&gt;, but this time for Android and on a larger Qwen3 model for those who are interested. Here is 4-bit quantized Qwen3 4B with thinking mode running on a Samsung Galaxy 24 using ExecuTorch - runs at up to 20 tok/s.&lt;/p&gt; &lt;p&gt;Instructions on how to export and run the model on ExecuTorch &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokyoCapybara"&gt; /u/TokyoCapybara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/drks9osnd01f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T21:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1knh1yd</id>
    <title>Meta delaying the release of Behemoth</title>
    <updated>2025-05-15T19:29:28+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7"&gt;https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T19:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kno67v</id>
    <title>Ollama now supports multimodal models</title>
    <updated>2025-05-16T00:49:35+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"&gt; &lt;img alt="Ollama now supports multimodal models" src="https://external-preview.redd.it/pRVigNZNHcUydRnImgoAZkA_b3OfVw4eace1TFmQGPk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f3c5bd5d3b3d4eebb1d060ea3a5f9818c6d5026" title="Ollama now supports multimodal models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.7.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T00:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kndp9f</id>
    <title>TTS Fine-tuning now in Unsloth!</title>
    <updated>2025-05-15T17:14:19+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt; &lt;img alt="TTS Fine-tuning now in Unsloth!" src="https://external-preview.redd.it/bXI4dnBsa3phejBmMfDzohHQ2IN6C0pCi0KaT-g2AEXeep08I3DgQhQN5vF7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57583ee26a1b7da14346a3cc45ff72ecbf34831b" title="TTS Fine-tuning now in Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Not the usual LLMs talk but we’re excited to announce that you can now train Text-to-Speech (TTS) models in &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;! Training is ~1.5x faster with 50% less VRAM compared to all other setups with FA2. :D&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support includes &lt;code&gt;Sesame/csm-1b&lt;/code&gt;, &lt;code&gt;OpenAI/whisper-large-v3&lt;/code&gt;, &lt;code&gt;CanopyLabs/orpheus-3b-0.1-ft&lt;/code&gt;, and any Transformer-style model including LLasa, Outte, Spark, and more.&lt;/li&gt; &lt;li&gt;The goal of TTS fine-tuning to minic voices, adapt speaking styles and tones, support new languages, handle specific tasks etc.&lt;/li&gt; &lt;li&gt;We’ve made notebooks to train, run, and save these models for free on Google Colab. Some models aren’t supported by llama.cpp and will be saved only as safetensors, but others should work. See our TTS docs and notebooks: &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The training process is similar to SFT, but the dataset includes audio clips with transcripts. We use a dataset called ‘Elise’ that embeds emotion tags like &amp;lt;sigh&amp;gt; or &amp;lt;laughs&amp;gt; into transcripts, triggering expressive audio that matches the emotion.&lt;/li&gt; &lt;li&gt;Since TTS models are usually small, you can train them using 16-bit LoRA, or go with FFT. Loading a 16-bit LoRA model is simple.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've uploaded most of the TTS models (quantized and original) to &lt;a href="https://huggingface.co/collections/unsloth/text-to-speech-tts-models-68007ab12522e96be1e02155"&gt;Hugging Face here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And here are our TTS notebooks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B"&gt;Sesame-CSM (1B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B"&gt;Orpheus-TTS (3B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb"&gt;Whisper Large V3&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B"&gt;Spark-TTS (0.5B)&lt;/a&gt;.ipynb)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thank you for reading and please do ask any questions!!&lt;/p&gt; &lt;p&gt;P.S. We also now support Qwen3 GRPO. We use the base model + a new custom proximity-based reward function to favor near-correct answers and penalize outliers. Pre-finetuning mitigates formatting bias and boosts evaluation accuracy via regex matching: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/faqjz7kzaz0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T17:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1knqap9</id>
    <title>Are we finally hitting THE wall right now?</title>
    <updated>2025-05-16T02:41:06+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw in multiple articles today that Llama Behemoth is delayed: &lt;a href="https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html"&gt;https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html&lt;/a&gt; . I tried the open models from Llama 4 and felt not that great progress. I am also getting underwhelming vibes from the qwen 3, compared to qwen 2.5. Qwen team used 36 trillion tokens to train these models, which even had trillions of STEM tokens in mid-training and did all sorts of post training, the models are good, but not that great of a jump as we expected. &lt;/p&gt; &lt;p&gt;With RL we definitely got a new paradigm on making the models think before speaking and this has led to great models like Deepseek R1, OpenAI O1, O3 and possibly the next ones are even greater, but the jump from O1 to O3 seems to be not that much, me being only a plus user and have not even tried the Pro tier. Anthropic Claude Sonnet 3.7 is not better than Sonnet 3.5, where the latest version seems to be good but mainly for programming and web development. I feel the same for Google where Gemini 2.5 Pro 1 seemed to be a level above the rest of the models, I finally felt that I could rely on a model and company, then they also rug pulled the model totally with Gemini 2.5 Pro 2 where I do not know how to access the version 1 and they are field testing a lot in lmsys arena which makes me wonder that they are not seeing those crazy jumps as they were touting.&lt;/p&gt; &lt;p&gt;I think Deepseek R2 will show us the ultimate conclusion on this, whether scaling this RL paradigm even further will make models smarter.&lt;/p&gt; &lt;p&gt;Do we really need a new paradigm? Or do we need to go back to architectures like T5? Or totally novel like JEPA from Yann Lecunn, twitter has hated him for not agreeing that the autoregressors can actually lead to AGI, but sometimes I feel it too with even the latest and greatest models do make very apparent mistakes and makes me wonder what would it take to actually have really smart and reliable models.&lt;/p&gt; &lt;p&gt;I love training models using SFT and RL especially GRPO, my favorite, I have even published some work on it and making pipelines for clients, but seems like when used in production for longer, the customer sentiment seems to always go down and not even maintain as well.&lt;/p&gt; &lt;p&gt;What do you think? Is my thinking in this saturation of RL for Autoregressor LLMs somehow flawed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T02:41:06+00:00</published>
  </entry>
</feed>
