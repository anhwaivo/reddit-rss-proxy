<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-14T15:07:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mprb4d</id>
    <title>Devs: Devstral VS Qwen3-30b/GPT-OSS?</title>
    <updated>2025-08-14T05:04:57+00:00</updated>
    <author>
      <name>/u/mitchins-au</name>
      <uri>https://old.reddit.com/user/mitchins-au</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just reaching out for anyone with first hand experience in real world coding tasks between the dense devstral small and the light MOE.&lt;/p&gt; &lt;p&gt;I know there’s benchmarks but real world experience tends to be better. If you’ve played both both what’s your advice? Mainly python and some JS stuff. &lt;/p&gt; &lt;p&gt;Tooling support would be crucial.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mitchins-au"&gt; /u/mitchins-au &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprb4d/devs_devstral_vs_qwen330bgptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1j7e</id>
    <title>Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp</title>
    <updated>2025-08-13T11:12:30+00:00</updated>
    <author>
      <name>/u/csixtay</name>
      <uri>https://old.reddit.com/user/csixtay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt; &lt;img alt="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" src="https://preview.redd.it/j7hi9xgjrrif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e2e9fc9cd738d0907c1394e77c1ec12b827b3" title="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csixtay"&gt; /u/csixtay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7hi9xgjrrif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpvjww</id>
    <title>[MLX Knife] Ollama-like CLI for Apple Silicon - manage MLX models natively</title>
    <updated>2025-08-14T09:15:17+00:00</updated>
    <author>
      <name>/u/broke_team</name>
      <uri>https://old.reddit.com/user/broke_team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvjww/mlx_knife_ollamalike_cli_for_apple_silicon_manage/"&gt; &lt;img alt="[MLX Knife] Ollama-like CLI for Apple Silicon - manage MLX models natively" src="https://external-preview.redd.it/lnRbx9LE8sCQtA1gVudRcgYN1zBpcsRAjnMl5pVUkT8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ee7780da85a5c28d39f360db2a5a8242067d65d" title="[MLX Knife] Ollama-like CLI for Apple Silicon - manage MLX models natively" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;We're the BROKE team 🦫 and we've been building tools for the Apple Silicon ML community.&lt;/p&gt; &lt;p&gt;Our first release is **MLX Knife** - a CLI tool specifically for MLX models.&lt;/p&gt; &lt;p&gt;**What it does:**&lt;/p&gt; &lt;p&gt;- Manages your HuggingFace cache directly&lt;/p&gt; &lt;p&gt;- Native MLX execution with streaming&lt;/p&gt; &lt;p&gt;- OpenAI-compatible API server&lt;/p&gt; &lt;p&gt;- Works great on 8GB M1 Macs&lt;/p&gt; &lt;p&gt;**Why we built it:**&lt;/p&gt; &lt;p&gt;The MLX ecosystem needed something as simple as Ollama but purpose-built &lt;/p&gt; &lt;p&gt;for MLX models and Apple's unified memory architecture.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mzau/mlx-knife"&gt;https://github.com/mzau/mlx-knife&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/m8a5cvt1cyif1.gif"&gt;https://i.redd.it/m8a5cvt1cyif1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on BROKE Cluster for multi-node setups, but that's still in research phase.&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feedback!&lt;/p&gt; &lt;p&gt;- The BROKE Team 🦫&lt;/p&gt; &lt;p&gt;Edit: v1.0-rc2 out with improved memory handling!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/broke_team"&gt; /u/broke_team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvjww/mlx_knife_ollamalike_cli_for_apple_silicon_manage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvjww/mlx_knife_ollamalike_cli_for_apple_silicon_manage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvjww/mlx_knife_ollamalike_cli_for_apple_silicon_manage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpw8s7</id>
    <title>Updated my setup &lt;3</title>
    <updated>2025-08-14T09:56:28+00:00</updated>
    <author>
      <name>/u/Wooden_Yam1924</name>
      <uri>https://old.reddit.com/user/Wooden_Yam1924</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt; &lt;img alt="Updated my setup &amp;lt;3" src="https://b.thumbs.redditmedia.com/Z8qaY02yyXET_0RFo2p9TACB74TtdXHAWrwwKDQEqZY.jpg" title="Updated my setup &amp;lt;3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just got 2x rtx pro 6000, probably eventually I sell A6000 for the next one, I'm so excited that wanted to share it with someone :D what benchmarks/llms should I run?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pxp6orxnjyif1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e96a0b89446f43b029db8162f50cf2962d5ab051"&gt;https://preview.redd.it/pxp6orxnjyif1.jpg?width=4284&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e96a0b89446f43b029db8162f50cf2962d5ab051&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden_Yam1924"&gt; /u/Wooden_Yam1924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpw8s7/updated_my_setup_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mptv5t</id>
    <title>How does Mistral Medium 3.1 fare?</title>
    <updated>2025-08-14T07:31:30+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone have a chance to try? Curious how it compares to Qwen 3s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptv5t/how_does_mistral_medium_31_fare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptv5t/how_does_mistral_medium_31_fare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mptv5t/how_does_mistral_medium_31_fare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mppqtu</id>
    <title>YAMS: Yet Another Memory System for LLM's</title>
    <updated>2025-08-14T03:41:54+00:00</updated>
    <author>
      <name>/u/blkmanta</name>
      <uri>https://old.reddit.com/user/blkmanta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt; &lt;img alt="YAMS: Yet Another Memory System for LLM's" src="https://external-preview.redd.it/Wdl3okAHvOXaOkYdjPCaNhixUPRpyzUYZxAYiri9Ewg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=365c61553d764b17eeff651fffa6fb624ced2120" title="YAMS: Yet Another Memory System for LLM's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;|| || |Built this for my LLM workflows - needed searchable, persistent memory that wouldn't blow up storage costs. I also wanted to use it locally for my research. It's a content-addressed storage system with block-level deduplication (saves 30-40% on typical codebases). I have integrated the CLI tool into most of my workflows in Zed, Claude Code, and Cursor, and I provide the prompt I'm currently using in the repo. The project is in C++ and the build system is rough around the edges but is tested on macOS and Ubuntu 24.04.|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blkmanta"&gt; /u/blkmanta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trvon/yams"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T03:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpabh1</id>
    <title>Matrix-Game 2.0 — first open-source, real-time, long-sequence interactive world model. 25 FPS, minutes-long interaction</title>
    <updated>2025-08-13T17:10:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Skywork_ai/status/1955237399912648842?t=hsxnA2t2FyKxRsSRBCJ1kA&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmeba</id>
    <title>Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude</title>
    <updated>2025-08-14T01:03:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt; &lt;img alt="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" src="https://b.thumbs.redditmedia.com/3X645e074wAZwFAUEsvdDQTEHHMP4k46jGqsiJfXi6w.jpg" title="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;this post&lt;/a&gt; yesterday, here are the updated results using Q8 of the Jan V1 model with Serper search.&lt;/p&gt; &lt;p&gt;Summaries corresponding to each image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with brave search: Actually produces an answer. But it gives the result for 2023.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with serper: Same result as above. It seems to make the mistake in the first thinking step in initiating the search - &amp;quot;Let me phrase the query as &amp;quot;US GDP current value&amp;quot; or something similar. Let me check the parameters: I need to specify a query. Let's go with &amp;quot;US GDP 2023 latest&amp;quot; to get recent data.&amp;quot; It thinks its way to the wrong query... &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3 A3B:30B via OpenRouter (with Msty's inbuilt web search): It had the right answer but then included numbers from 1999 and was far too verbose. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GPT-OSS 20B via OpenRouter (with Msty's inbuilt web search): On the ball but a tad verbose&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Perplexity Pro: nailed it&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Claude Desktop w Sonnet 4: got it as well, but again more info than requested.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I didnt bother trying anything more.. Its harsh to jump to conclusions with just 1 question but its hard for me to see how Jan V1 is actually better than Perplexity or any other LLM+search tool &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpmeba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T01:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp2wq3</id>
    <title>There is a new text-to-image model named nano-banana</title>
    <updated>2025-08-13T12:20:32+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt; &lt;img alt="There is a new text-to-image model named nano-banana" src="https://preview.redd.it/jmw88evj4sif1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53c768eb9781ffe9119d98e2a2e9f3c88c8adab5" title="There is a new text-to-image model named nano-banana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmw88evj4sif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk834</id>
    <title>Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s.</title>
    <updated>2025-08-13T23:27:12+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt; &lt;img alt="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." src="https://external-preview.redd.it/dTd1ZDB2NnBldmlmMavIpB9AHSfqY-PSwwptUZpoVAt7ZMaVO0xQLghP-sG0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f0830ecafdacba42b232e8ce354f02aa9a68f2" title="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ycrl1u6pevif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpm8kr</id>
    <title>ERNIE 4.5 21BA3B appreciation post.</title>
    <updated>2025-08-14T00:55:45+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it's the best model of it's size, outshining gpt-oss 20 and qwen 3 30BA3B.&lt;/p&gt; &lt;p&gt;It's not as good at coding, but it runs without error even at decent context. I find the qwen a3b to be better for code gen, but prefer ernie for everythign else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T00:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpz2ef</id>
    <title>REINFORCE++-baseline is all you need in RLVR</title>
    <updated>2025-08-14T12:20:05+00:00</updated>
    <author>
      <name>/u/seventh_day123</name>
      <uri>https://old.reddit.com/user/seventh_day123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What is REINFORCE++-baseline?&lt;/h1&gt; &lt;p&gt;In essence, REINFORCE++-baseline (detailed in &lt;a href="https://arxiv.org/abs/2501.03262"&gt;arXiv:2501.03262&lt;/a&gt;) replaces the critic network used in PPO with the group mean reward and applies global batch advantage normalization. The KL loss is computed using the unbiased K2 KL estimator. Because the global batch standard deviation is significantly more stable than the local group standard deviation used in GRPO, this approach enhances training stability.&lt;/p&gt; &lt;p&gt;More details are in &lt;a href="https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85"&gt;https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seventh_day123"&gt; /u/seventh_day123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T12:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpez1p</id>
    <title>Added locally generated dialogue + voice acting to my game!</title>
    <updated>2025-08-13T20:02:24+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt; &lt;img alt="Added locally generated dialogue + voice acting to my game!" src="https://external-preview.redd.it/N2szZGNtMzRldWlmMZmQp7O5BpjYg7UqegAgE9IdgP7TYx8Szh9dJVqIheQu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eccefbf13830c4a641bc7633ab5e9b01c2c86540" title="Added locally generated dialogue + voice acting to my game!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1qgim34euif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqew3</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:16:29+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2k00</id>
    <title>[FEEDBACK] Better packaging for llama.cpp to support downstream consumers</title>
    <updated>2025-08-14T14:40:23+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's about time we build an easier UX for llama.cpp 🤗&lt;/p&gt; &lt;p&gt;I've used llama.cpp for better part of last 2 years for playing with LLMs and use it in production too&lt;/p&gt; &lt;p&gt;Whilst it takes a bit to setup llama.cpp, once done, it *just* works! &lt;/p&gt; &lt;p&gt;Come along with your ideas/ solutions on how we can package it better, and make it easier for people to use and install llama.cpp with ease ❤️&lt;/p&gt; &lt;p&gt;Drop your ideas here on the discussion: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15313"&gt;https://github.com/ggml-org/llama.cpp/discussions/15313&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1w1z</id>
    <title>Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed</title>
    <updated>2025-08-14T14:14:52+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt; &lt;img alt="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" src="https://preview.redd.it/fq1mo49ftzif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd94153bce02ae49a1589ce33689d2667aa1ecd" title="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.1.3"&gt;v8.1.3&lt;/a&gt; just released today as part of our ongoing sprint to implement the community's suggestions!&lt;/p&gt; &lt;p&gt;Lemonade lets you run local LLMs with high performance on your NPU or GPU, and the new release includes:&lt;/p&gt; &lt;p&gt;💻 Ryzen AI Strix Point self-hosted runners have been added to the CI system. - Allows contributors to test NPU-related features. &lt;/p&gt; &lt;p&gt;📃 A &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;detailed guide&lt;/a&gt; for how to use Lemonade with Continue.dev's IDE-based local coding assistant. - Very easy to get up and running thanks to the Continue Hub's one-click setup.&lt;/p&gt; &lt;p&gt;🧰 Overhauled the web ui (see post's image). - 100% replaced the model manager with filters, load/unload, and install/delete buttons for each model. - The currently-loaded model is always displayed at the top, with a convenient eject button. - Selecting a model in the chat tab now loads it immediately.&lt;/p&gt; &lt;p&gt;🌡️ temperature, top_k, top_p, and repeat_penalty are supported in HTTP requests and the web app. - I know, we put this off way too long. - The PR that added these is a good blueprint for adding any more parameters people want.&lt;/p&gt; &lt;p&gt;🐍 Added support for Python 3.11 and 3.13 (another community ask we put off too long).&lt;/p&gt; &lt;p&gt;🚀 Community contributions: - Customize .exe behavior using environment variables, and added GLM-4.5-Air, by @tylerstraub - Update server startup message to include version number by @henrylearn2rock - Add .gitignore by @bsoyka (we were such noobs for not having this... thank you!)&lt;/p&gt; &lt;p&gt;🤖 What’s Coming Next: we have a very fun gaming side project in the works for Strix Halo and Radeon devices, stay tuned :)&lt;/p&gt; &lt;p&gt;If Lemonade has been useful to you, take a moment to add a star/issue on &lt;a href="http://github.com/lemonade-sdk/lemonade"&gt;Github&lt;/a&gt; and/or tell us about it in the &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;Discord&lt;/a&gt;. Feedback help others discover it and help us improve the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq1mo49ftzif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mprwv9</id>
    <title>Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?</title>
    <updated>2025-08-14T05:38:43+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt; &lt;img alt="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" src="https://preview.redd.it/ydbnycjn8xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8bcb62748563efa5cb1f78789aa2cd8f3b2860a" title="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's rumors that R2 is coming up sometime in the next month. It does feel that the release of the recent proprietary models have been a bit disappointing, given the marginal gains (e.g. on my &lt;a href="https://www.designarena.ai/"&gt;frontend benchmark&lt;/a&gt;, GPT-5, Opus 4, and 4.1 are basically equivalent though there's a small sample size for the new versions. &lt;/p&gt; &lt;p&gt;In terms of recent releases, open source and open weight models have been amazing. DeepSeek R1-0528 and Qwen3 Coder are #5 and #6 respectively, while GLM 4.5 is #9. &lt;/p&gt; &lt;p&gt;I'm am interested to see what happens with R2. My prediction is that it will basically match GPT-5 and Opus 4 (perhaps might even be a bit better) and we might see a moment similar to when DeepSeek R1 came out. &lt;/p&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ydbnycjn8xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mptvsl</id>
    <title>tencent/Hunyuan-GameCraft-1.0 · Hugging Face</title>
    <updated>2025-08-14T07:32:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt; &lt;img alt="tencent/Hunyuan-GameCraft-1.0 · Hugging Face" src="https://external-preview.redd.it/aPfnDoE4lStgbUiMQComf1wdLlqoQrdsgG6jn-2D3d8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47b9265890a5ca1272fc550cc59c1e4e8a3a0326" title="tencent/Hunyuan-GameCraft-1.0 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition&lt;/p&gt; &lt;p&gt;📜 Requirements An NVIDIA GPU with CUDA support is required. The model is tested on a machine with 8GPUs. Minimum: The minimum GPU memory required is 24GB but very slow. Recommended: We recommend using a GPU with 80GB of memory for better generation quality. Tested operating system: Linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-GameCraft-1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpuvok</id>
    <title>Qwen Coder 30bA3B harder... better... faster... stronger...</title>
    <updated>2025-08-14T08:33:51+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt; &lt;img alt="Qwen Coder 30bA3B harder... better... faster... stronger..." src="https://external-preview.redd.it/ZzJlajBibXkzeWlmMSKN9Y-F1uPgmObNpOLYQwn_bi3ofDf3vCkP-ziGE8lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f02def6a5e8cd415b1d862b2482b085e1338926" title="Qwen Coder 30bA3B harder... better... faster... stronger..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing around with 30b a3b to get tool calling up and running and I was bored in the CLI so I asked it to punch things up and make things more exciting... and this is what it spit out. I thought it was hilarious, so I thought I'd share :). Sorry about the lower quality video, I might upload a cleaner copy in 4k later.&lt;/p&gt; &lt;p&gt;This is all running off a single 24gb vram 4090. Each agent has its own 15,000 token context window independent of the others and can operate and handle tool calling at near 100% effectiveness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mnpg8bmy3yif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T08:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpvije</id>
    <title>Swiss Canton Basel open sourced multiple tools for on-premise hosting of LLM services</title>
    <updated>2025-08-14T09:12:54+00:00</updated>
    <author>
      <name>/u/fabkosta</name>
      <uri>https://old.reddit.com/user/fabkosta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought this is worth sharing: The Swiss Canton of Basel has made available multiple tools they built for on-premise hosting of LLM-based services (text transcription, RAG, document conversion etc.). None of this is totally breaking news, but they did a solid job building an API plus frontend on top of all their services. And it's there entirely for free, using an MIT license, so everyone may re-use or extend the tools as they wish.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DCC-BS"&gt;https://github.com/DCC-BS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most of the services are relying on a combination of vLLM, Qwen3 32b, LlamaIndex, Python (FastAPI), and Whisper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabkosta"&gt; /u/fabkosta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpr0nc</id>
    <title>Who are the 57 million people who downloaded bert last month?</title>
    <updated>2025-08-14T04:49:28+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt; &lt;img alt="Who are the 57 million people who downloaded bert last month?" src="https://preview.redd.it/vk2njmk01xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9a1c88826aae167a25ae0705a428dcb9f502529" title="Who are the 57 million people who downloaded bert last month?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vk2njmk01xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq19x6</id>
    <title>1 million context is the scam , the ai start hallucinating after the 90k . im using the qwen cli and its become trash after 10 percent context window used</title>
    <updated>2025-08-14T13:51:38+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is the major weakness ai have and they will never bring this on the benchmark , if u r working on the codebase the ai will work like a monster for the first 100k context aftert that its become the ass &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T13:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxumt</id>
    <title>MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200</title>
    <updated>2025-08-14T11:23:08+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt; &lt;img alt="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" src="https://external-preview.redd.it/3RGDYz9vGH8VQTfhA0sqrehkFc8q3f4WnHv1sjovwaY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51cffad7eff8e127873e566d22bc7c9880032b82" title="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-arc-pro-b60-dual-with-48gb-memory-reportedly-starts-shipping-next-week-priced-at-1200"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T11:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeek’s next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeek’s next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeek’s next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
