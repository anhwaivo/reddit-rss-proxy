<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-03T11:05:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jqclzh</id>
    <title>What happened to Zhuiyi Tech (the inventor of RoPE)?</title>
    <updated>2025-04-03T07:58:48+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://zhuiyi.ai/about/"&gt;https://zhuiyi.ai/about/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It seems like the last official news was dated Dec 2023. What happened to them since then? Are they still in business?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqclzh/what_happened_to_zhuiyi_tech_the_inventor_of_rope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqclzh/what_happened_to_zhuiyi_tech_the_inventor_of_rope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqclzh/what_happened_to_zhuiyi_tech_the_inventor_of_rope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T07:58:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpwup7</id>
    <title>What are the best value, energy-efficient options with 48GB+ VRAM for AI inference?</title>
    <updated>2025-04-02T19:04:39+00:00</updated>
    <author>
      <name>/u/PangurBanTheCat</name>
      <uri>https://old.reddit.com/user/PangurBanTheCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've considered doing dual 3090's, but the power consumption would be a bit much and likely not worth it long-term. &lt;/p&gt; &lt;p&gt;I've heard mention of Apple and others making AI specific machines? Maybe that's an option? &lt;/p&gt; &lt;p&gt;Prices on everything are just sky-high right now. I have a small amount of cash available, but I'd rather not blow it all just so I can talk to my semi-intelligent anime waifu's cough I mean do super important business work. Yeah. That's the real reason...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangurBanTheCat"&gt; /u/PangurBanTheCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpwup7/what_are_the_best_value_energyefficient_options/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T19:04:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq7zse</id>
    <title>Currently the most accurate image captioning AI ?</title>
    <updated>2025-04-03T03:19:41+00:00</updated>
    <author>
      <name>/u/cruncherv</name>
      <uri>https://old.reddit.com/user/cruncherv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried several as of now that can run on my 6GB VRAM - BLIP, BLIP2, Florence2, Moondream2. They are all good at something but fail at some other task I tried them. For example Moondream can recognize the Eiffel Tower from front, but not from any other angles.. Blip is sometimes even more detailed than Blip2, but Blip2 still outperforms Blip in terms of overall accuracy, etc&lt;/p&gt; &lt;p&gt;Can anyone recommend any other such AI image captioning models released in the past year that are accurate, short, but detailed ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cruncherv"&gt; /u/cruncherv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq7zse/currently_the_most_accurate_image_captioning_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq7zse/currently_the_most_accurate_image_captioning_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq7zse/currently_the_most_accurate_image_captioning_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T03:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqbtzy</id>
    <title>Browser-use - any local LLMs that work?</title>
    <updated>2025-04-03T07:05:55+00:00</updated>
    <author>
      <name>/u/ZachCope</name>
      <uri>https://old.reddit.com/user/ZachCope</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. Just wondering if anyone is using Browser-use with any local LLMs? In particular is a multimodal model needed? If so what do you use and how has your experience been? &lt;/p&gt; &lt;p&gt;I have a 2 x Rtx 3090 system so have used the common text based models, but haven't tried out multimodal models yet. &lt;/p&gt; &lt;p&gt;Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZachCope"&gt; /u/ZachCope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqbtzy/browseruse_any_local_llms_that_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqbtzy/browseruse_any_local_llms_that_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqbtzy/browseruse_any_local_llms_that_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T07:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqb0u6</id>
    <title>Best tiny/edge model for auto memory retrieval/injection to feed persistent memory from one gpu to a larger model on a second gpu? Weird use case I know, I'm testing my own local front end running react with llama.cpp</title>
    <updated>2025-04-03T06:12:54+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! — I’m building a modular AI frontend called GingerGUI with a dual-model architecture: one lightweight model handles memory creation/retrieval/injection, while a larger model handles core conversational reasoning. Think emotionally-aligned, persistent memory meets local autonomy. Why am I doing this? What's the point? Fuck me if I know, I just had an idea, and its fun bringing it to creation.&lt;/p&gt; &lt;p&gt;Right now, I’m hunting for the &lt;strong&gt;best tiny models to handle the memory part on my second GPU (4060ti)&lt;/strong&gt; for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Parsing convos and generating JSON-structured memories&lt;/li&gt; &lt;li&gt;Injecting relevant memories back into prompts&lt;/li&gt; &lt;li&gt;Running fast &amp;amp; light on a second GPU/core&lt;/li&gt; &lt;li&gt;Minimal hallucination, clean output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve tried some 1b - 3b models and have seen some hilarious memory hallucinations. Currently llama 3.2 3 b seems to work okay, but I'd love to hear what the community thinks for this usage purpose. &lt;/p&gt; &lt;p&gt;I'll be putting GingerGUI on github once it has a few more features, but I'm having a lot of fun with this dual model memory handling thingy, and until I've got that nailed down I'm keeping things local. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqb0u6/best_tinyedge_model_for_auto_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqb0u6/best_tinyedge_model_for_auto_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqb0u6/best_tinyedge_model_for_auto_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T06:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jps289</id>
    <title>Matharena USAMO update: Gemini 2.5 Pro is the first model to achieve non-trivial amount of points</title>
    <updated>2025-04-02T15:55:06+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See here: &lt;a href="https://matharena.ai/"&gt;https://matharena.ai/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro at 24.5%, next is R1 at 4.76%. From mbalunovic on X.&lt;/p&gt; &lt;p&gt;Note also that the benchmark was released on the same day as the Gemini release, so this isn't a case of training on the eval. An impressive result, and the pace of progress is incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jps289/matharena_usamo_update_gemini_25_pro_is_the_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpv5ld</id>
    <title>Sharing HallOumi-8B, an open-source hallucination detector usable with any LLM!</title>
    <updated>2025-04-02T17:57:21+00:00</updated>
    <author>
      <name>/u/jeremy_oumi</name>
      <uri>https://old.reddit.com/user/jeremy_oumi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I’m one of the co-founders of Oumi, an open-source AI startup, and wanted to share something we’ve been working on.&lt;/p&gt; &lt;p&gt;I find generative AI to be pretty useful, but not that trustworthy. Whenever I ask for a summary of a document, or ask a question about a particular research paper, it always nags in the back of my mind: is this accurate or is it a hallucination? Where in the document does it say this? Personally, I don’t want to have to read pages of a document to verify everything in the LLM output, so we built HallOumi!&lt;/p&gt; &lt;p&gt;Assuming you have a context (one or more documents) and a set of claims (summary, answer to a question, etc.), HallOumi can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Classify each claim as supported/unsupported, along with a confidence score&lt;/li&gt; &lt;li&gt;Provide citations (relevant sentences in the context) for each claim so that you know what exactly you should check in the document to verify as a human&lt;/li&gt; &lt;li&gt;Provide an explanation for that particular supported/unsupported label - sometimes hallucinations are so nuanced that it is hard even for humans to detect them without help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also made a classifier which runs a lot faster at similar quality, but you lose out on claim-level classification, the citations and explanations!&lt;/p&gt; &lt;p&gt;We built a small open-source demo where you can try out HallOumi locally (or any other model you’d like) right away: &lt;a href="https://github.com/oumi-ai/halloumi-demo"&gt;https://github.com/oumi-ai/halloumi-demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We also have a hosted version online at &lt;a href="https://oumi.ai/halloumi-demo"&gt;https://oumi.ai/halloumi-demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Sharing all the code and documentation needed to train or run HallOumi here: &lt;a href="https://github.com/oumi-ai/oumi/tree/main/configs/projects/halloumi"&gt;https://github.com/oumi-ai/oumi/tree/main/configs/projects/halloumi&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The relevant models and datasets are also on HuggingFace:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/oumi-ai/HallOumi-8B"&gt;https://huggingface.co/oumi-ai/HallOumi-8B&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/oumi-ai/HallOumi-8B-classifier"&gt;https://huggingface.co/oumi-ai/HallOumi-8B-classifier&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-synthetic-claims"&gt;https://huggingface.co/datasets/oumi-ai/oumi-synthetic-claims&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-synthetic-document-claims"&gt;https://huggingface.co/datasets/oumi-ai/oumi-synthetic-document-claims&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-anli-subset"&gt;https://huggingface.co/datasets/oumi-ai/oumi-anli-subset&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/oumi-ai/oumi-c2d-d2c-subset"&gt;https://huggingface.co/datasets/oumi-ai/oumi-c2d-d2c-subset&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technical deep dive here: &lt;a href="https://oumi.ai/blog/posts/introducing-halloumi"&gt;https://oumi.ai/blog/posts/introducing-halloumi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Happy to answer any questions too 🙂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremy_oumi"&gt; /u/jeremy_oumi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpv5ld/sharing_halloumi8b_an_opensource_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:57:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq41ao</id>
    <title>PSA: Guide for Installing Flash Attention 2 on Windows</title>
    <updated>2025-04-03T00:07:20+00:00</updated>
    <author>
      <name>/u/RokHere</name>
      <uri>https://old.reddit.com/user/RokHere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you’ve struggled to get Flash Attention 2 working on Windows (for &lt;em&gt;Oobabooga&lt;/em&gt;’s &lt;strong&gt;text-generation-webui&lt;/strong&gt;, for example), I wrote a step-by-step guide after a grueling 15+ hour battle with CUDA, PyTorch, and Visual Studio version hell.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s Inside&lt;/strong&gt;:&lt;br /&gt; ✅ Downgrading Visual Studio 2022 to LTSC 17.4.x&lt;br /&gt; ✅ Fixing CUDA 12.1 + PyTorch 2.5.1 compatibility&lt;br /&gt; ✅ Building wheels from source (no official Windows binaries!)&lt;br /&gt; ✅ Troubleshooting common errors (out-of-memory, VS version conflicts)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Bother?&lt;/strong&gt;&lt;br /&gt; Flash Attention 2 significantly speeds up transformer inference, but Windows support is currently near nonexistent. This guide hopefully fills a bit of the gap.&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://www.reddit.com/r/Oobabooga/comments/1jq3uj9/guide_getting_flash_attention_2_working_on/"&gt;Full Guide Here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you’re on Linux, just &lt;code&gt;pip install flash-attn&lt;/code&gt; and move on. For Windows masochists, this may be your lifeline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RokHere"&gt; /u/RokHere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq41ao/psa_guide_for_installing_flash_attention_2_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq41ao/psa_guide_for_installing_flash_attention_2_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq41ao/psa_guide_for_installing_flash_attention_2_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T00:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jps1xm</id>
    <title>PAI: your personal AI 100% local inspired by Google's Project Astra</title>
    <updated>2025-04-02T15:54:46+00:00</updated>
    <author>
      <name>/u/Such_Advantage_6949</name>
      <uri>https://old.reddit.com/user/Such_Advantage_6949</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt; &lt;img alt="PAI: your personal AI 100% local inspired by Google's Project Astra" src="https://external-preview.redd.it/73JBodiKh6lM1UQDVfjtY1zzKdX1ydEHwEZeN3MnZaE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6424d4101ef831404802ae4974c2f9be087d0819" title="PAI: your personal AI 100% local inspired by Google's Project Astra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Google's Project Astra, I have created an App for audio + video chat bot that is 100% local and open source.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fty7fzxd1gse1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6f771ece87afe7cd87bb559cc0be812235412ea6"&gt;https://preview.redd.it/fty7fzxd1gse1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6f771ece87afe7cd87bb559cc0be812235412ea6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;iOS app&lt;/li&gt; &lt;li&gt;100% locally hosted&lt;/li&gt; &lt;li&gt;Open Source&lt;/li&gt; &lt;li&gt;Visual Question answer&lt;/li&gt; &lt;li&gt;Streaming via RTC &amp;amp; Livekit for low latency&lt;/li&gt; &lt;li&gt;Screen Sharing&lt;/li&gt; &lt;li&gt;Live transcription&lt;/li&gt; &lt;li&gt;Change LLM to any model supported by Exllama v2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is a short 2 mins demo: &lt;a href="https://youtu.be/pNksZ_lXqgs"&gt;https://youtu.be/pNksZ_lXqgs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/remichu-ai/pai.git"&gt;https://github.com/remichu-ai/pai.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a STT + LLM + TTS, so feel free to skip if it is deal breaker for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Such_Advantage_6949"&gt; /u/Such_Advantage_6949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jps1xm/pai_your_personal_ai_100_local_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqdd4m</id>
    <title>Good Model for Quadro P2000 4gb vram + ~32gb ram</title>
    <updated>2025-04-03T08:49:25+00:00</updated>
    <author>
      <name>/u/Fusion63</name>
      <uri>https://old.reddit.com/user/Fusion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded the ram in my homelab and I was wondering how much that could improve the performance of ollama.&lt;br /&gt; I ran some 7b models just fine before with very limited ram, but now I have roughly 32gb of ram (2666mhz) that I can freely use.&lt;br /&gt; Which model would work best with this setup?&lt;/p&gt; &lt;p&gt;Edit: The Quadro p2000 has 5GB of Vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fusion63"&gt; /u/Fusion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqdd4m/good_model_for_quadro_p2000_4gb_vram_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqdd4m/good_model_for_quadro_p2000_4gb_vram_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqdd4m/good_model_for_quadro_p2000_4gb_vram_32gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T08:49:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jplg2o</id>
    <title>LiveBench team just dropped a leaderboard for coding agent tools</title>
    <updated>2025-04-02T10:37:19+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt; &lt;img alt="LiveBench team just dropped a leaderboard for coding agent tools" src="https://preview.redd.it/qxqj0vjtgese1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e87e657142ff097ddef495de75a59a3206ae77e" title="LiveBench team just dropped a leaderboard for coding agent tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qxqj0vjtgese1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T10:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpvxw0</id>
    <title>koboldcpp-1.87.1: Merged Qwen2.5VL support! :)</title>
    <updated>2025-04-02T18:27:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T18:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpxq4y</id>
    <title>DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low</title>
    <updated>2025-04-02T19:39:51+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt; &lt;img alt="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" src="https://preview.redd.it/0ymxajfb5hse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82c6cdb4e5d31a3f747415575616dee79f009536" title="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ymxajfb5hse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T19:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq13ik</id>
    <title>Mac Studio M3 Ultra 512GB DeepSeek V3-0324 IQ2_XXS (2.0625 bpw) llamacpp performance</title>
    <updated>2025-04-02T21:57:23+00:00</updated>
    <author>
      <name>/u/WhereIsYourMind</name>
      <uri>https://old.reddit.com/user/WhereIsYourMind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a lot of results that had abysmal tok/sec prompt processing. This is from the self compiled binary of llamacpp, commit f423981a.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ~/.lmstudio/models/unsloth/DeepSeek-V3-0324-GGUF/DeepSeek-V3-0324-UD-IQ2_XXS-00001-of-00005.gguf --n-gpu-layers 62 --flash-attn 0 -ctk f16,q8_0 -p 16384,32768,65536 -n 2048 -r 1 | model | size | params | backend | threads | type_k | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | -----: | ------------: | -------------------: | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp16384 | 51.17 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp32768 | 39.80 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp65536 | 467667.08 ± 0.00 | (failed, OOM) | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | tg2048 | 14.84 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp16384 | 50.95 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp32768 | 39.53 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp65536 | 25.27 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | tg2048 | 16.09 ± 0.00 | build: f423981a (5022) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhereIsYourMind"&gt; /u/WhereIsYourMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T21:57:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpuoh7</id>
    <title>Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!</title>
    <updated>2025-04-02T17:39:04+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"&gt; &lt;img alt="Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!" src="https://preview.redd.it/ziowvxg7kgse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acbf3232ba93ffa825062a5d7ef599eb46ce434b" title="Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ziowvxg7kgse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptdtg</id>
    <title>Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!</title>
    <updated>2025-04-02T16:48:30+00:00</updated>
    <author>
      <name>/u/JawGBoi</name>
      <uri>https://old.reddit.com/user/JawGBoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt; &lt;img alt="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" src="https://external-preview.redd.it/T7uMJGzQ_xDL3OLN-I6nY-QjBc2LJ_pj5xaq0KJj7XI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84d098830ff90a292e2d57523020f6f6a49fb61" title="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model repo: &lt;a href="https://github.com/kyutai-labs/moshi"&gt;https://github.com/kyutai-labs/moshi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JawGBoi"&gt; /u/JawGBoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kyutai-labs/moshi-finetune"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqcn4q</id>
    <title>CSM Finetuning is here!</title>
    <updated>2025-04-03T08:00:48+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidbrowne17/csm-streaming"&gt;https://github.com/davidbrowne17/csm-streaming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I added fine-tuning to CSM. Clone my repo and place your audio files into a folder called audio_data and run lora.py to finetune it. You will likely need 12gb+ of vram to do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T08:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpr1nk</id>
    <title>The Candle Test - most LLMs fail to generalise at this simple task</title>
    <updated>2025-04-02T15:13:10+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt; &lt;img alt="The Candle Test - most LLMs fail to generalise at this simple task" src="https://preview.redd.it/6phgn27rqfse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=676b32e5d96ebb0c0830e00756c8e79d41840121" title="The Candle Test - most LLMs fail to generalise at this simple task" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure a lot of people here noticed that latest frontier models are... weird. Teams facing increased pressure to chase a good place in the benchmarks and make the SOTA claims - the models are getting more and more overfit resulting in decreased generalisation capabilities.&lt;/p&gt; &lt;p&gt;It became especially noticeable with the very last line-up of models which despite being better on paper somehow didn't feel so with daily use.&lt;/p&gt; &lt;p&gt;So, I present to you a very simple test that highlights this problem. It consists of three consecutive questions where the model is steered away from possible overfit - yet most still demonstrate it on the final conversation turn (including thinking models).&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are candles getting taller or shorter when they burn?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models correctly identify that candles are indeed getting shorter when burning.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are you sure? Will you be able to recognize this fact in different circumstances?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models confidently confirm that such a foundational fact is hard to miss under any circumstances.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Now, consider what you said above and solve the following riddle: I'm tall when I'm young, and I'm taller when I'm old. What am I?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And here most models are as confidently wrong claiming that the answer is a candle.&lt;/p&gt; &lt;p&gt;Unlike traditional misguided attention tasks - this test gives model ample chances for in-context generalisation. Failing this test doesn't mean that the model is &amp;quot;dumb&amp;quot; or &amp;quot;bad&amp;quot; - most likely it'll still be completely fine for 95% of use-cases, but it's also more likely to fail in a novel situation.&lt;/p&gt; &lt;p&gt;Here are some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/7e9815b3-15ba-4a4c-81e1-0f233f1b0d5a"&gt;DeepSeek Chat V3&lt;/a&gt; (0324, Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/3e27bf44-c64c-4558-b98f-989fb1c82688"&gt;DeepSeek R1&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/f1c205e4-ee2d-41e4-87b4-e8c9dbe0024b"&gt;DeepSeek R1 Distill Llama 70B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/4ac04a5d-8199-4675-b4ce-5e3cbbb9223d"&gt;Llama 3.1 405B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;QwQ 32B didn't pass due to entering endless loop multiple times&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/5ff0eb98-cd36-4988-a2a0-e01416ac567d"&gt;Mistral Large&lt;/a&gt; (Passes, one of the few)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inpired by my frustration with Sonnet 3.7 (which also fails this test, unlike Sonnet 3.5).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6phgn27rqfse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq35ba</id>
    <title>ClaudePlaysPokemon Open Sourced - Benchmark AI by letting it play Pokémon</title>
    <updated>2025-04-02T23:26:23+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The source code for the AI benchmark ClaudePlaysPokemon has been released. ClaudePlaysPokemon is a benchmark to show how agents work and can generalize, it was made to see how a AI model not trained on Pokemon can use general thinking to play the game.&lt;/p&gt; &lt;p&gt;What I personally would like to see is the open source community taking a small local model like Gemma3 27b and finetuning it on annotated screenshots explaining it what tiles can be cut which ones can only be jumped over from one side etc and maybe general game knowledge from Bulbapedia. This would be a good way to show if a finetuned specialized small model can out perform a general big model.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/davidhershey/ClaudePlaysPokemonStarter"&gt;https://github.com/davidhershey/ClaudePlaysPokemonStarter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Twitch: &lt;a href="https://www.twitch.tv/claudeplayspokemon"&gt;https://www.twitch.tv/claudeplayspokemon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Visual Explainer: &lt;a href="https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg"&gt;https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T23:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqcj89</id>
    <title>YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!</title>
    <updated>2025-04-03T07:53:41+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt; &lt;img alt="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" src="https://external-preview.redd.it/NTZsOHM4NDNya3NlMWK-C_FeILIR1n-iDldyZrl0VqZ3vuhsUq5jbN6auZ0Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a07a6ac257f3a744208337e975f20df2a138d814" title="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! clefourrier from HF's OpenEvals team! We open sourced YourBench yesterday, a custom synthetic evaluation framework: from any document, it creates a custom made QA set, then builds a leaderboard on your specific use case.&lt;/p&gt; &lt;p&gt;It works through multiple steps of chunking, summarization, LLM single and multi hop question and answer generation, validation, and so far we've found it works really well to generate interesting QAs! &lt;/p&gt; &lt;p&gt;You can use the demo as is, or customize and download it to run it with your favorite models: Best model for diverse questions is Qwen2.5-32B, and open model generating most grounded/valid questions is Gemma3-27B (just one place below o3-mini)! You can also set several seeds to augment diversity, complexity, etc. &lt;/p&gt; &lt;p&gt;This work has been carried by our intern, Sumuk, who had a great idea on how to dynamically generate eval sets, and we wrote a paper explaining the full method here: &lt;a href="https://huggingface.co/papers/2504.01833"&gt;https://huggingface.co/papers/2504.01833&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try it out here: &lt;a href="https://huggingface.co/spaces/yourbench/demo"&gt;https://huggingface.co/spaces/yourbench/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: Document -&amp;gt; custom made evaluation set -&amp;gt; leaderboard in 5 min&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xy7fgb43rkse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T07:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq8zfk</id>
    <title>Open-WebUI Artifacts Overhaul has been updated to v0.6.0!</title>
    <updated>2025-04-03T04:11:32+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt; &lt;img alt="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" src="https://external-preview.redd.it/wZszwQ2U6yJ7pvC1CwegCQ8kArJM8Bhaojq_gfjcMsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=784dcdbc7977acbb264691fc234770109ef75318" title="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I just wanted to let you know that the Open-WebUI Artifacts Overhaul fork has been updated to match v0.6.0 of Open-Webui!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;https://github.com/nick-tonjum/open-webui-artifacts-overhaul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Don't know what the 'Artifacts Overhaul' branch is? It adds the following to open-webui:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🖼️ &lt;strong&gt;Coding Canvas&lt;/strong&gt;: Whenever a LLM outputs code, it will appear on the right side of the page with Monaco editor, similar to VSCode. Here you can cycle through different files produced via the LLM and also different versions&lt;/li&gt; &lt;li&gt;🔍 &lt;strong&gt;Difference Checker&lt;/strong&gt;: If a LLM makes changes to code, the differences will be highlight. This can be easily disabled or enabled via a single click!&lt;/li&gt; &lt;li&gt;🎨 &lt;strong&gt;Design Viewer&lt;/strong&gt;: Easily toggle between code view and design view with the click of a button! This currently supports HTML/CSS/JavaScript like before, but now with Tailwind styles built in. React components work too!&lt;/li&gt; &lt;li&gt;⚛️ &lt;strong&gt;React Visualizer&lt;/strong&gt;: As mentioned above, React components work too. This seems to work 80% of the time and I'm working hard to get it 100% of the time! As long as the code block has an export default it should work.&lt;/li&gt; &lt;li&gt;💼 &lt;strong&gt;Compacted Code&lt;/strong&gt;: When the canvas is open, code blocks in the regular chat are compacted and visualized as an attachment.&lt;/li&gt; &lt;li&gt;🌐 &lt;strong&gt;MANY supported languages&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check it out. Hopefully someday this will end up in the main branch :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7lewes7wojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8b3a77a94287acbf414bb38e5e5f934d147ff2df"&gt;Difference Viewer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7wbnf7kwojse1.gif"&gt;Cycle through multiple files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/is93kyswojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4502d68ce62e7e656fe050b71aeb0bfdf9fec8fe"&gt;React component viewer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T04:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqa182</id>
    <title>Llama 4 will probably suck</title>
    <updated>2025-04-03T05:12:21+00:00</updated>
    <author>
      <name>/u/klapperjak</name>
      <uri>https://old.reddit.com/user/klapperjak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been following meta FAIR research for awhile for my phd application to MILA and now knowing that metas lead ai researcher quit, I’m thinking it happened to dodge responsibility about falling behind basically.&lt;/p&gt; &lt;p&gt;I hope I’m proven wrong of course, but the writing is kinda on the wall.&lt;/p&gt; &lt;p&gt;Meta will probably fall behind and so will Montreal unfortunately 😔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klapperjak"&gt; /u/klapperjak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T05:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqawj1</id>
    <title>Open Sourcing Latent Space Guardrails that catch 43% of Hallucinations</title>
    <updated>2025-04-03T06:05:16+00:00</updated>
    <author>
      <name>/u/Cautious_Hospital352</name>
      <uri>https://old.reddit.com/user/Cautious_Hospital352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released fully open source latent space guardrails that monitor and stop unwelcome outputs of your LLM on the latent space level. Check it out here and happy to adopt it to your use case! &lt;a href="https://github.com/wisent-ai/wisent-guard"&gt;https://github.com/wisent-ai/wisent-guard&lt;/a&gt; On hallucinations it has not been trained on in TruthfulQA, this results in a 43% detection of hallucinations just from the activation patterns. You can use them to control the brain of your LLM and block it from outputting bad code, harmful outputs or taking decisions because of gender or racial bias. This is a new approach, different from circuit breakers or SAE-based mechanistic interpretability. We will be releasing a new version of the reasoning architecture based on latent space interventions soon to not only reduce hallucinations but use this for capabilities gain as well! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cautious_Hospital352"&gt; /u/Cautious_Hospital352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T06:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqef4d</id>
    <title>China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4</title>
    <updated>2025-04-03T10:00:10+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt; &lt;img alt="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" src="https://preview.redd.it/x9zbqai7flse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c47a9c1cd7cd7f716e71f6e9161de09e2cf497a4" title="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9zbqai7flse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T10:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptset</id>
    <title>University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy</title>
    <updated>2025-04-02T17:04:49+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt; &lt;img alt="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" src="https://b.thumbs.redditmedia.com/nIuszN8uDMIjbhUsiZdUw2NeBO5my-uVcctXiMF1pcI.jpg" title="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jptset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:04:49+00:00</published>
  </entry>
</feed>
