<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-09T20:50:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvirqs</id>
    <title>Hunyuan A13B tensor override</title>
    <updated>2025-07-09T13:26:49+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\.[1-9]\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\.1[6-9]\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvhxe7</id>
    <title>üöÄ Built another 124m parameter transformer based model from scratch.This time with multi GPU training using DDP.Inspired from nanoGPT.But redesigned to suit my own training pipeline.Model and training code is on huggingface‚¨áÔ∏è</title>
    <updated>2025-07-09T12:48:51+00:00</updated>
    <author>
      <name>/u/Remarkable-Ad3290</name>
      <uri>https://old.reddit.com/user/Remarkable-Ad3290</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/abhinavv3/MEMGPT"&gt;https://huggingface.co/abhinavv3/MEMGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.&lt;/p&gt; &lt;p&gt;Bt these changes haven‚Äôt been implemented yet.Hopefully,finish them this weekend&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Ad3290"&gt; /u/Remarkable-Ad3290 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvtp4h</id>
    <title>New Nvidia Jetson AGX Thor developer kit specs</title>
    <updated>2025-07-09T20:40:26+00:00</updated>
    <author>
      <name>/u/martincerven</name>
      <uri>https://old.reddit.com/user/martincerven</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"&gt; &lt;img alt="New Nvidia Jetson AGX Thor developer kit specs" src="https://b.thumbs.redditmedia.com/LcS0MUSWf5hhmUX4e22WKX2znHiU8l47X_F0lXz5ctQ.jpg" title="New Nvidia Jetson AGX Thor developer kit specs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From &lt;a href="https://www.siliconhighway.com/wp-content/robotics-and-edge-ai-datasheet-jetson-thor-devkit-nvidia-us-web.pdf"&gt;siliconhighway&lt;/a&gt;&lt;br /&gt; Look &lt;strong&gt;BIG,&lt;/strong&gt; but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AGX Orin: 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores @ 1.3 GHz&lt;/li&gt; &lt;li&gt;AGX Thor: 2560-core NVIDIA Blackwell architecture GPU with 96 fifth-gen Tensor Cores @ 1.575 GHz&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How is &lt;strong&gt;275 -&amp;gt;1000 TOPS&lt;/strong&gt; (FP8/INT8) computed? (with NVDEC,NVENC, +??)&lt;br /&gt; Additional info to &lt;a href="https://developer.nvidia.com/embedded/downloads"&gt;look through&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martincerven"&gt; /u/martincerven &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvtp4h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T20:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvglk7</id>
    <title>vLLM vs SGLang vs MAX ‚Äî Who's the fastest?</title>
    <updated>2025-07-09T11:42:12+00:00</updated>
    <author>
      <name>/u/rkstgr</name>
      <uri>https://old.reddit.com/user/rkstgr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt; &lt;img alt="vLLM vs SGLang vs MAX ‚Äî Who's the fastest?" src="https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805" title="vLLM vs SGLang vs MAX ‚Äî Who's the fastest?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkstgr"&gt; /u/rkstgr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ersteiger.com/posts/vllm-vs-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T11:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvpp0e</id>
    <title>Help settle a debate on the Lemonade team: how much web UI is too much for a local server?</title>
    <updated>2025-07-09T18:03:05+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"&gt; &lt;img alt="Help settle a debate on the Lemonade team: how much web UI is too much for a local server?" src="https://external-preview.redd.it/a3NlMzVteGUwd2JmMbld0vN-YDqXepEZ7jk7fsVH50PMdq02YFgXtbKrMRDk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92189cf6cbbfff48a59734a348e39abad21cc159" title="Help settle a debate on the Lemonade team: how much web UI is too much for a local server?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jeremy from the AMD Lemonade team here. We just released Lemonade v8.0.4, which adds some often-requested formatting to the LLM Chat part of our web ui (see video).&lt;/p&gt; &lt;p&gt;A discussion we keep having on the team is: how far does it make sense to develop our own web ui, if the primary purpose of Lemonade is to be a local server that connects to other apps?&lt;/p&gt; &lt;p&gt;My take is that people should just use the web ui to try things out for the first time, then connect to a more capable end-user app like Open WebUI or Continue.dev. There's another take that we should just make the web ui as nice as possible, since it is the first thing our users see after they install.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some things we should almost certainly add: image input, buttons to load and unload models.&lt;/li&gt; &lt;li&gt;Something we're on the fence about is a sidebar with a chat history.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious to get the community's feedback to help settle the debate!&lt;/p&gt; &lt;p&gt;PS. details of the video:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Quick Start: &lt;a href="https://lemonade-server.ai/"&gt;Lemonade Server&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: Qwen3 MOE (30B total / 3B active)&lt;/li&gt; &lt;li&gt;Hardware: Strix Halo (Ryzen AI Max 395+ with 128 GB RAM)&lt;/li&gt; &lt;li&gt;Inference engine: llama.cpp with Vulkan&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lqvyapxe0wbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvsw5d</id>
    <title>Bulk captioning/VLM query tool, standalone app</title>
    <updated>2025-07-09T20:08:28+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvsw5d/bulk_captioningvlm_query_tool_standalone_app/"&gt; &lt;img alt="Bulk captioning/VLM query tool, standalone app" src="https://preview.redd.it/iznsrnd5iwbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f593b9ad38e31ba6846b8ef9d3682f4bbbfa1cf" title="Bulk captioning/VLM query tool, standalone app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an app that is intended for bulk captioning directories full of images. Mostly useful for people who have a lot of images and want to train diffusion model LORAs or similar and 1) don't want to caption by hand and 2) don't get acceptable results from plain 1-shotting with other VLM/captioning scripts.&lt;/p&gt; &lt;p&gt;The reason for the app is often fine tuners just try to 1-shot with their favorite VLM but adding a bit of process and some features can help immensely. This app is setup to N-shot through a series of prompts, then capture the final output and save as a .txt file alongside each image. You can paste in large documents describing the general &amp;quot;universe&amp;quot; of images, such as physical descriptions of every character in a fiction, and use the multi-step prompt to ask the VLM to identify the characters, ask it to describe the overall scene, then finally summarize the overall image to get a final caption. I get remarkable results with this with modern VLMs like Gemma 3 27B.&lt;/p&gt; &lt;p&gt;The secondary reason for the app is to disconnect this type of automated captioning workflow with actual VLM hosting. This app will require you host with something like LM Studio or ollama, but unlocks every gguf model out there without this app having to manage compatibility and dependencies or be updated when new models come out or HF transformers is updated. The app itself doesn't host anything but a Python/Flask/React/Electron app and is relatively small. I've previously made some caption scripts that require python, transformers, diffusers, etc. and often shit just breaks over time and requiring pytorch makes delivering a small portable app virtually impossible.&lt;/p&gt; &lt;p&gt;The app also has some ability to read from extra metadata files, though not currently exposed in the electron GUI app. See hint sources documentation, but tldr: it can optionally add more context like file path or read from metadata in the folder or alongside images (i.e. stuff you might have collected from webscraping scripts).&lt;/p&gt; &lt;p&gt;Prerequisite:&lt;/p&gt; &lt;p&gt;Install LM Studio or whatever VLM/LLM host you want. In LM Studio, enable the service from Developer tab. You'll also need to Enable CORS as well if you want to use the Electron app/GUI. Ollama or others, read docs, this is &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; I'm sure you know wtf you're doing here.&lt;/p&gt; &lt;p&gt;Repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/victorchall/vlm-caption"&gt;https://github.com/victorchall/vlm-caption&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Latest release for standalone/installer:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36"&gt;https://github.com/victorchall/vlm-caption/releases/tag/v1.0.36&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are a few options to run this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Python command line (git clone, setup venv, install requirements, edit `caption.yaml` to configure, run `python caption_openai.py`)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Same as above but then run `cd ui &amp;amp;&amp;amp; npm run electron-dev` to run the entire GUI/app from source. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Windows portable CLI EXE - download &lt;a href="https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/vlm-caption-cli.zip"&gt;vlm-caption-cli.zip&lt;/a&gt;, unzip, edit caption.yaml and run the exe. This is standalone so you don't need to even install python. If you're ok with editing a yaml file and reading some documentation and don't care about a pretty GUI, this will work.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Windows standalone/installer electron GUI app. Uuse the &lt;a href="https://github.com/victorchall/vlm-caption/releases/download/v1.0.36/VLM.Caption.Setup.0.1.0.exe"&gt;LM.Caption.Setup.0.1.0.exe&lt;/a&gt; installer.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full code and build process is in the repo and it builds on a hosted Github Action runner if you're nervous about running an unknown exe or are wary of the &amp;quot;unknown publisher&amp;quot; warning. Or run it from source, idgaf, it's a FOSS hobby project.&lt;/p&gt; &lt;p&gt;Docs in the repo are relatively up to date if you want to look them over. The GUI could use a bit of work as it is missing a minor feature or two, will likely update later this week or weekend. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iznsrnd5iwbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvsw5d/bulk_captioningvlm_query_tool_standalone_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvsw5d/bulk_captioningvlm_query_tool_standalone_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T20:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvh87a</id>
    <title>What modes can expect I run on an AMD Ryzen AI Max+ 395?</title>
    <updated>2025-07-09T12:14:37+00:00</updated>
    <author>
      <name>/u/electrickangaroo31</name>
      <uri>https://old.reddit.com/user/electrickangaroo31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thinking about buying a GMKTEK Evo-2. Which models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt; &lt;p&gt;EDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/electrickangaroo31"&gt; /u/electrickangaroo31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv53nn</id>
    <title>What's local about this?</title>
    <updated>2025-07-09T00:32:32+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt; &lt;img alt="What's local about this?" src="https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1" title="What's local about this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rqrg67unoobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T00:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvbzpx</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-09T06:47:06+00:00</updated>
    <author>
      <name>/u/PotatoFormal8751</name>
      <uri>https://old.reddit.com/user/PotatoFormal8751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotatoFormal8751"&gt; /u/PotatoFormal8751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T06:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvt4a9</id>
    <title>Open-source SLM for games, Unity package, demo game The Tell-Tale Heart</title>
    <updated>2025-07-09T20:17:25+00:00</updated>
    <author>
      <name>/u/formicidfighter</name>
      <uri>https://old.reddit.com/user/formicidfighter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"&gt; &lt;img alt="Open-source SLM for games, Unity package, demo game The Tell-Tale Heart" src="https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd26c3ec9734e5ad3c175e9139c86cfd3b35fbea" title="Open-source SLM for games, Unity package, demo game The Tell-Tale Heart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we‚Äôve been experimenting with small language models (SLMs) as a new type of game asset. We think they‚Äôre a promising way to make game mechanics more dynamic. Especially when finetuned to your game world and for focused, constrained mechanics designed to allow for more reactive output.&lt;/p&gt; &lt;p&gt;You can try our demo game, inspired by Edgar Allan Poe‚Äôs short story The Tell-Tale Heart, &lt;a href="https://aviadai.itch.io/the-tell-tale-heart"&gt;on itch&lt;/a&gt;. We spent two weeks pulling it together, so it‚Äôs not the most polished game. But we hope it captures a bit of the delight that emergent mechanics can provide.&lt;/p&gt; &lt;p&gt;Design-wise, we chose to constrain the model to picking one of 3 pre-written choices for each scenario and generating an in-character explanation for its choice. This way, the model is in a controlled environment crafted by the dev, but also adds some flavor and surprise. You can play around with editing the character background to explore the boundaries and limits of the model. We finetuned it to be quite general, but you can imagine finetuning the SLM much more closely to your game world and characters.&lt;/p&gt; &lt;p&gt;In the spirit of seeing more experimentation with SLMs, we‚Äôve open-sourced everything:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/aviad-ai"&gt;This SLM&lt;/a&gt; (it‚Äôs a finetuned llama model, so under llama3 license). Performance-wise, it‚Äôs quite small at 770 MB and runs comfortably on CPU.&lt;/li&gt; &lt;li&gt;A &lt;a href="https://github.com/aviad-ai/unity"&gt;Unity package&lt;/a&gt; for loading and integrating models into Unity (built on top of llama.cpp, under MIT license. Supports MacOS, Windows, WebGL). We‚Äôve done quite a lot of work to optimize it. We‚Äôre working on an Unreal integration coming soon!&lt;/li&gt; &lt;li&gt;The &lt;a href="https://github.com/aviad-ai/UnitySamples/tree/main/TheTellTaleHeart"&gt;sample game&lt;/a&gt; (under MIT license, except for the paid EndlessBook asset from the Unity store).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre excited about a potential future in which games are shipped with multiple, specialized SLMs running in tandem to make games more immersive. &lt;/p&gt; &lt;p&gt;If you‚Äôre also interested in the promise of SLMs in games, join us on &lt;a href="https://discord.gg/Jk4jUYghnA"&gt;Discord&lt;/a&gt;! We‚Äôre planning to open-source a lot more models, sample games, integration features, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/formicidfighter"&gt; /u/formicidfighter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kamkdq2xmwbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T20:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvcb72</id>
    <title>Here is how we beat ChatGPT at classification with 1 dollar in cloud compute</title>
    <updated>2025-07-09T07:07:53+00:00</updated>
    <author>
      <name>/u/iamMess</name>
      <uri>https://old.reddit.com/user/iamMess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.&lt;/p&gt; &lt;p&gt;This tutorial comes in 3 different formats: 1. This LocalLLaMA post - summary and discussion 2. Our blog post - &lt;a href="https://syv.ai/viden/beating-chatgpt-dollar-dream"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt; 3. Our research paper - &lt;a href="https://arxiv.org/abs/2507.00214"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt; &lt;p&gt;What we did:&lt;/p&gt; &lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt; &lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt; &lt;p&gt;Key results: - 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001) - Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%) - Built-in interpretability - model explains its reasoning for every prediction - Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt; &lt;p&gt;The interesting bits:&lt;/p&gt; &lt;p&gt;What worked: - The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification - Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations - Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt; &lt;p&gt;What didn't: - Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes - More computationally expensive than standard fine-tuning - Quality heavily depends on the initial reasoning generator&lt;/p&gt; &lt;p&gt;Technical details: - Base model: Llama-3.2-1B-Instruct (both stages) - Reasoning dataset: &lt;a href="https://huggingface.co/datasets/syvai/reasoning-gen"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts) - Target task: dair-ai/emotion (6 basic emotions) - Training: Axolotl framework on A40 GPU - Reasoning generator model: &lt;a href="https://huggingface.co/syvai/reasoning-gen-1b"&gt;syvai/reasoning-gen-1b&lt;/a&gt; - Datasets: &lt;a href="https://huggingface.co/datasets/syvai/emotion-reasoning"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/syvai/no-emotion-reasoning"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamMess"&gt; /u/iamMess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T07:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvd7z4</id>
    <title>support for Falcon-H1 model family has been merged into llama.cpp</title>
    <updated>2025-07-09T08:10:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt; &lt;img alt="support for Falcon-H1 model family has been merged into llama.cpp" src="https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c" title="support for Falcon-H1 model family has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt; &lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14534"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T08:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvj98v</id>
    <title>I built a Deep Researcher agent and exposed it as an MCP server!</title>
    <updated>2025-07-09T13:48:17+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br /&gt; So, the agent has 3 main stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There‚Äôs also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt; &lt;p&gt;Here‚Äôs what I used to build it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scrapegraph for web scraping&lt;/li&gt; &lt;li&gt;Nebius AI for open-source models&lt;/li&gt; &lt;li&gt;Agno for agent orchestration&lt;/li&gt; &lt;li&gt;Streamlit for the UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.&lt;/p&gt; &lt;p&gt;If you‚Äôre curious, I put a full video tutorial here: &lt;a href="https://www.youtube.com/watch?v=pdsk6yldZGI"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href="https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent"&gt;Full Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk1ms</id>
    <title>What impressive (borderline creepy) local AI tools can I run now that everything is local?</title>
    <updated>2025-07-09T14:21:39+00:00</updated>
    <author>
      <name>/u/PeithonKing</name>
      <uri>https://old.reddit.com/user/PeithonKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff ‚Äî always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt; &lt;p&gt;Now I‚Äôm on Linux, running my local models (Ollama, etc.), and I‚Äôm wondering ‚Äî what‚Äôs out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt; &lt;p&gt;Not just screen-watching ‚Äî anything that improves workflow or feels magically helpful... but because it‚Äôs all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt; &lt;p&gt;Looking for tools, recos or project links if anyone‚Äôs already doing this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeithonKing"&gt; /u/PeithonKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqtxa</id>
    <title>multimodal medgemma 27b</title>
    <updated>2025-07-09T18:47:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"&gt; &lt;img alt="multimodal medgemma 27b" src="https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fffb9bacdc1fe8701c5f0c1be15c595a886c9819" title="multimodal medgemma 27b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MedGemma is a collection of &lt;a href="https://ai.google.dev/gemma/docs/core"&gt;Gemma 3&lt;/a&gt; variants that are trained for performance on medical text and image comprehension. Developers can use MedGemma to accelerate building healthcare-based AI applications. MedGemma currently comes in three variants: a 4B multimodal version and 27B text-only and multimodal versions.&lt;/p&gt; &lt;p&gt;Both MedGemma multimodal versions utilize a &lt;a href="https://arxiv.org/abs/2303.15343"&gt;SigLIP&lt;/a&gt; image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Their LLM components are trained on a diverse set of medical data, including medical text, medical question-answer pairs, FHIR-based electronic health record data (27B multimodal only), radiology images, histopathology patches, ophthalmology images, and dermatology images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/medgemma-27b-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:47:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqnjh</id>
    <title>T5Gemma - A Google Collection</title>
    <updated>2025-07-09T18:40:25+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"&gt; &lt;img alt="T5Gemma - A Google Collection" src="https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61c73857aa577d73d06a45e7f0d9e3d669d8ffd9" title="T5Gemma - A Google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr711</id>
    <title>support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp</title>
    <updated>2025-07-09T19:01:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"&gt; &lt;img alt="support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp" src="https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef" title="support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI21 Jamba family of models are hybrid SSM-Transformer foundation models, blending speed, efficient long context processing, and accuracy.&lt;/p&gt; &lt;p&gt;from the website:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Model Size&lt;/th&gt; &lt;th align="left"&gt;Max Tokens&lt;/th&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;Snapshot&lt;/th&gt; &lt;th align="left"&gt;API Endpoint&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba Large&lt;/td&gt; &lt;td align="left"&gt;398B parameters (94B active)&lt;/td&gt; &lt;td align="left"&gt;256K&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2025-07&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;jamba-large&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba Mini&lt;/td&gt; &lt;td align="left"&gt;52B parameters (12B active)&lt;/td&gt; &lt;td align="left"&gt;256K&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2025-07&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;jamba-mini&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Engineers and data scientists at AI21 labs created the model to help developers and businesses leverage AI to build real-world products with tangible value. &lt;strong&gt;Jamba Mini&lt;/strong&gt; and &lt;strong&gt;Jamba Large&lt;/strong&gt; support zero-shot instruction-following and multi-language support. The Jamba models also provide developers with industry-leading APIs that perform a wide range of productivity tasks designed for commercial use.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Organization developing model:&lt;/strong&gt; AI21 Labs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model date:&lt;/strong&gt; July 3rd, 2025&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model type:&lt;/strong&gt; Joint Attention and Mamba (Jamba)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge cutoff date&lt;/strong&gt; August 22nd, 2024&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input Modality:&lt;/strong&gt; Text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Modality:&lt;/strong&gt; Text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;a href="https://www.ai21.com/licenses/jamba-open-model-license"&gt;Jamba open model license&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/7531"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T19:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqv8e</id>
    <title>new tiny 1.7B open-source reranker beats Cohere rerank3.5</title>
    <updated>2025-07-09T18:48:35+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt; &lt;img alt="new tiny 1.7B open-source reranker beats Cohere rerank3.5" src="https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a250c04d607c0b8a5f43196eba12971c7744065" title="new tiny 1.7B open-source reranker beats Cohere rerank3.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're looking for a cheap, fast but accurate reranker without having to fine-tune a SLM yourself&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zeroentropy/zerank-1-small"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvnkuk</id>
    <title>Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!</title>
    <updated>2025-07-09T16:41:42+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt; &lt;img alt="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" src="https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f27c3dcd38f51203dffa703e77dc78a0e131c7" title="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;12B version: &lt;a href="https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3"&gt;https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvf7ww</id>
    <title>First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community</title>
    <updated>2025-07-09T10:22:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt; &lt;img alt="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" src="https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg" title="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: &lt;a href="https://huggingface.co/blog/reachy-mini"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br /&gt; Thomas Wolf on ùïè: &lt;a href="https://x.com/Thom_Wolf/status/1942887160983466096"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvf7ww"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T10:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvp3qv</id>
    <title>GEMINI 3 PRO !</title>
    <updated>2025-07-09T17:40:17+00:00</updated>
    <author>
      <name>/u/omar07ibrahim1</name>
      <uri>https://old.reddit.com/user/omar07ibrahim1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt; &lt;img alt="GEMINI 3 PRO !" src="https://b.thumbs.redditmedia.com/d2eBFiZSJLkxDAA0QMlKs_RVctoYEIuFlOGx8XPUNQQ.jpg" title="GEMINI 3 PRO !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7"&gt;https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omar07ibrahim1"&gt; /u/omar07ibrahim1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T17:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvn1sd</id>
    <title>OpenAI's open-weight model will debut as soon as next week</title>
    <updated>2025-07-09T16:20:46+00:00</updated>
    <author>
      <name>/u/phantasm_ai</name>
      <uri>https://old.reddit.com/user/phantasm_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt; &lt;img alt="OpenAI's open-weight model will debut as soon as next week" src="https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd" title="OpenAI's open-weight model will debut as soon as next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as ‚Äúsimilar to o3 mini,‚Äù complete with the reasoning capabilities that have made OpenAI‚Äôs latest models so powerful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantasm_ai"&gt; /u/phantasm_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr3ym</id>
    <title>OpenAI's open source LLM is a reasoning model, coming Next Thursday!</title>
    <updated>2025-07-09T18:58:30+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt; &lt;img alt="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" src="https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7" title="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q01afp6lbwbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:58:30+00:00</published>
  </entry>
</feed>
