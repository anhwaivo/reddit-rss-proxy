<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-11T10:06:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1imsoyk</id>
    <title>Really Fast TTS for Low-Performance Devices?</title>
    <updated>2025-02-11T06:50:42+00:00</updated>
    <author>
      <name>/u/i_am_vsj</name>
      <uri>https://old.reddit.com/user/i_am_vsj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any TTS that can generate speech in seconds on low-end devices (CPU-based)? I can compromise on quality—just needs to be better than gTTS.&lt;/p&gt; &lt;p&gt;I tried Edge TTS, but the response time is around 5-10 seconds, which isn't real-time enough. I need something much faster.&lt;/p&gt; &lt;p&gt;I know my requirements are a bit high, but if you know any solution, please share. Also, I heard OpenVoice can reduce latency—does that actually work like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_am_vsj"&gt; /u/i_am_vsj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imsoyk/really_fast_tts_for_lowperformance_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imsoyk/really_fast_tts_for_lowperformance_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imsoyk/really_fast_tts_for_lowperformance_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T06:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1impw1y</id>
    <title>Wouldn't be possible to train the reasoning step to use tools?</title>
    <updated>2025-02-11T04:02:07+00:00</updated>
    <author>
      <name>/u/xandykati98</name>
      <uri>https://old.reddit.com/user/xandykati98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The way we use web search is really not ideal, the model needs to search before it even reasons about the problem. Could we reward the format for tool use? Using predefined tool results during RL training for predefined possible tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xandykati98"&gt; /u/xandykati98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1impw1y/wouldnt_be_possible_to_train_the_reasoning_step/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1impw1y/wouldnt_be_possible_to_train_the_reasoning_step/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1impw1y/wouldnt_be_possible_to_train_the_reasoning_step/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T04:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1imgyw9</id>
    <title>How to create a knowledge graph from 1000s of unstructured documents?</title>
    <updated>2025-02-10T21:01:21+00:00</updated>
    <author>
      <name>/u/MonkeyMaster64</name>
      <uri>https://old.reddit.com/user/MonkeyMaster64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a dataset that contains a few 1000 PDFs related to a series of interviews and case studies performed. All of it is related to a specific event. I want to create a knowledge graph that can identify, explain, and synthesize how all the documents tie together. I'd also like an LLM to be able to use the knowledge graph to answer open-ended questions. But, primarily I'm interested in the synthesizing of new connections between the documents. Any recommendations on how best to go about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MonkeyMaster64"&gt; /u/MonkeyMaster64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imgyw9/how_to_create_a_knowledge_graph_from_1000s_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T21:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1im561m</id>
    <title>Gylphstral-24B: v1 Released! (MLX)</title>
    <updated>2025-02-10T12:38:59+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, everyone, the time is here - Glyphstral v1 is officially RELEASED!&lt;/p&gt; &lt;p&gt;Following up on my preview post from last week (&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1ikn5fg%2Fglyphstral24b_symbolic_deductive_reasoning_model%2F"&gt;link to original Reddit post here&lt;/a&gt;), I've finally got the repo all setup and the first version of Glyphstral-24b is now live on Hugging Face: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FSeverian%2FGlyphstral-24b-v1"&gt;https://huggingface.co/Severian/Glyphstral-24b-v1&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As you know, I've been diving deep into symbolic AI and really trying to see if we can push LLMs to be better at actual reasoning and multi-dimensional thought. Glyphstral is the result of that deep dive, trained to work with my &amp;quot;Glyph Code Logic Flow&amp;quot; framework. It's all about getting models to use structured, deductive symbolic logic, which you can read all about over here: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fseverian42%2FComputational-Model-for-Symbolic-Representations%2Ftree%2Fmain"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I have been very low on time so I haven't been able to make the GGUF's, as I know most of you will need those instead of the MLX version, so apologies for the delay.&lt;/p&gt; &lt;p&gt;A benchmark is also in the works! I honestly just didn't feel like holding off on the release so that some people could start testing it right away. More updates coming this week, just think of this as a soft launch.&lt;/p&gt; &lt;p&gt;This is very much a first step, and there's definitely tons more to do, but I'm genuinely excited about where this is heading. Check out the Hugging Face repo, give it a spin, and let me know what you think! Docs and more info are up there too.&lt;/p&gt; &lt;p&gt;Huge thanks for all the initial interest and encouragement on the first post. Let's see what Glyphstral can do.&lt;/p&gt; &lt;p&gt;Tell me if it works well, tell me if it sucks. All feedback is welcome!&lt;/p&gt; &lt;p&gt;EDIT: hahaha so I accidentally mistyped the title as 'Gylphstral' when it should really be 'Glyphstral'. Can't undo it, so it'll just have to live it out&lt;/p&gt; &lt;p&gt;GGUFs Thanks to the incredible Bartowski!!! &lt;a href="https://huggingface.co/bartowski/Severian_Glyphstral-24b-v1-GGUF"&gt;https://huggingface.co/bartowski/Severian_Glyphstral-24b-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note on the GGUFs: I am getting weird outputs as well. I noticed that GGUF Is labeled as a Llama arch and 13B. Might be a weird conversion that is causing the bad outputs. I'll keep looking into it, sorry for any wasted downloads. If you can, try the MLX&lt;/p&gt; &lt;p&gt;HuggingChat Assistant Version Available too for those who want to try this concept out right away (NOT THE FINE_TUNED VERSION: Uses pure in-context learning through a very detailed and long prompt). Base model is Qwen coder 32B (has the best execution of the symbolic AI over the reasoning models):&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/chat/assistant/678cfe9655026c306f0a4dab"&gt;https://hf.co/chat/assistant/678cfe9655026c306f0a4dab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im561m/gylphstral24b_v1_released_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1im7393</id>
    <title>Audiblez v4.0 is out: Generate Audiobooks from Ebooks</title>
    <updated>2025-02-10T14:16:46+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7393/audiblez_v40_is_out_generate_audiobooks_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im7393/audiblez_v40_is_out_generate_audiobooks_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T14:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1imlxji</id>
    <title>Here is a little Python script that detects clipboard text and plays it using Kokoro TTS. I use it to play on fables.gg and get voice lines.</title>
    <updated>2025-02-11T00:37:20+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;import os os.environ[&amp;quot;PHONEMIZER_ESPEAK_LIBRARY&amp;quot;] = &amp;quot;C:\\Program Files\\eSpeak NG\\libespeak-ng.dll&amp;quot; os.environ[&amp;quot;PHONEMIZER_ESPEAK_PATH&amp;quot;] = &amp;quot;C:\\Program Files\\eSpeak NG\\espeak-ng.exe&amp;quot; import sounddevice as sd import pyperclip import time import torch, re from models import build_model from kokoro import generate device = 'cuda' if torch.cuda.is_available() else 'cpu' MODEL = build_model('pth/kokoro-v0_19.pth', device) VOICE_NAME = 'af' VOICEPACK = torch.load(f'voices/{VOICE_NAME}.pt', weights_only=True).to(device) print(f'Loaded voice: {VOICE_NAME}') def play_audio_from_text(text): sentences = re.split(r'[.\n]+', text) for sentence in sentences: if sentence.strip() == '': continue audio, out_ps = generate(MODEL, sentence, VOICEPACK, lang=VOICE_NAME[0]) sd.play(audio, samplerate=24000) sd.wait() def monitor_clipboard(): last_clipboard = pyperclip.paste() while True: time.sleep(0.5) # Reduce CPU usage current_clipboard = pyperclip.paste() if current_clipboard and current_clipboard != last_clipboard: play_audio_from_text(current_clipboard) last_clipboard = current_clipboard if __name__ == &amp;quot;__main__&amp;quot;: print(&amp;quot;Monitoring clipboard for text changes...&amp;quot;) monitor_clipboard() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I saved the file as monitor.py I used uv to install the dependencies:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;uv venv&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This creates a .venv folder in the project. No need to activate it manually.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; uv pip install torch --index-url https://download.pytorch.org/whl/cu124 &amp;gt; uv pip install requests &amp;gt; uv pip install numpy &amp;gt; uv pip install scipy &amp;gt; uv pip install phonemizer &amp;gt; uv pip install munch &amp;gt; uv pip install transformers &amp;gt; uv pip install soundfile &amp;gt; uv pip install pyperclip &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And then you can run the code like this:&lt;/p&gt; &lt;p&gt;uv run monitor.py&lt;/p&gt; &lt;p&gt;And now inside fables.gg copy the text from the story and enjoy the TTS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imlxji/here_is_a_little_python_script_that_detects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imlxji/here_is_a_little_python_script_that_detects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imlxji/here_is_a_little_python_script_that_detects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T00:37:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1im35yl</id>
    <title>How to scale RAG to 20 million documents ?</title>
    <updated>2025-02-10T10:33:39+00:00</updated>
    <author>
      <name>/u/Sarcinismo</name>
      <uri>https://old.reddit.com/user/Sarcinismo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Curious to hear if you worked on RAG use cases with 20+ million documents and how you handled such scale from latency, embedding and indexing perspectives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sarcinismo"&gt; /u/Sarcinismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im35yl/how_to_scale_rag_to_20_million_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T10:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1imu9nx</id>
    <title>Have you found issues on which LLMs does better without reasoning?</title>
    <updated>2025-02-11T08:46:59+00:00</updated>
    <author>
      <name>/u/Foxiya</name>
      <uri>https://old.reddit.com/user/Foxiya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foxiya"&gt; /u/Foxiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imu9nx/have_you_found_issues_on_which_llms_does_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imu9nx/have_you_found_issues_on_which_llms_does_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imu9nx/have_you_found_issues_on_which_llms_does_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T08:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1im141p</id>
    <title>Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth</title>
    <updated>2025-02-10T08:00:39+00:00</updated>
    <author>
      <name>/u/michaeljchou</name>
      <uri>https://old.reddit.com/user/michaeljchou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt; &lt;img alt="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" src="https://b.thumbs.redditmedia.com/4-1ai0qkO5Xa7dtRGbvrlqdk94A166Fd8-qidSmu1eY.jpg" title="Orange Pi AI Studio Pro mini PC with 408GB/s bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaeljchou"&gt; /u/michaeljchou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1im141p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T08:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzcwm</id>
    <title>671B DeepSeek-R1/V3-q4 on a Single Machine (2× Xeon + 24GB GPU) – Up to 286 tokens/s Prefill &amp; 14 tokens/s Decode</title>
    <updated>2025-02-10T05:58:47+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt; &lt;img alt="671B DeepSeek-R1/V3-q4 on a Single Machine (2× Xeon + 24GB GPU) – Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" src="https://external-preview.redd.it/HTxOk-pm59cMNSNVg9McK5hbABS7kz3K65hC8Z_V08I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f46113aaee323590b36c46395dcf62eb9140f297" title="671B DeepSeek-R1/V3-q4 on a Single Machine (2× Xeon + 24GB GPU) – Up to 286 tokens/s Prefill &amp;amp; 14 tokens/s Decode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1edbue3/local_deepseekv2_inference_120_ts_for_prefill_and/"&gt;our local CPU/GPU hybrid inference open source project with DeepSeek-V2&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We've heard your requests for DeepSeek-R1/V3 support—and we're excited to finally deliver!&lt;/p&gt; &lt;p&gt;Apologies for the wait, but we've been cooking up something truly amazing.&lt;/p&gt; &lt;p&gt;Today, we're proud to announce that we not only support DeepSeek-R1/V3, as showcased in the video at &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401"&gt;https://preview.redd.it/o0x777ie49ie1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23cd7dad22d211277f1787bd1f993c7c22200401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But we're also previewing our upcoming optimizations, including an Intel AMX-accelerated kernel and a selective expert activation method, which will significantly enhance performance.&lt;/p&gt; &lt;p&gt;With v0.3-preview, &lt;strong&gt;we achieve up to 286 tokens/s for prefill, making it up to 28× faster than llama.cpp&lt;/strong&gt; for local inference.&lt;/p&gt; &lt;p&gt;The binary distribution is available now and the source code will come ASAP! Check out the details here: &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some rationale behind this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why CPU/GPU Hybrid Inference?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;DeepSeek's MLA operators are highly computationally intensive. While running everything on CPU is possible, offloading the heavy computations to the GPU results in a massive performance boost.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Where Does the Speedup Come From?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Expert Offload: Unlike traditional layer-based or KVCache offloading (as seen in llama.cpp), we offload the expert computation to the CPU and MLA/KVCache to GPU, aligning perfectly with DeepSeek’s architecture for optimal efficiency.&lt;/p&gt; &lt;p&gt;- Intel AMX Optimization – Our AMX-accelerated kernel is meticulously tuned, running several times faster than existing llama.cpp implementations. We plan to open-source this kernel after cleansing and are considering upstream contributions to llama.cpp.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why Intel CPUs?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Intel is currently the only CPU vendor that supports AMX-like instructions, which delivers significantly better performance compared to AVX-only alternatives. BUT, we also support AMD CPUs and due to the Expert Offload it will also be faster than the current llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T05:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1imqgug</id>
    <title>LLM Reasoning via Inference Scaling - Open Source Research and Live Blog</title>
    <updated>2025-02-11T04:33:42+00:00</updated>
    <author>
      <name>/u/maxusmusti</name>
      <uri>https://old.reddit.com/user/maxusmusti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imqgug/llm_reasoning_via_inference_scaling_open_source/"&gt; &lt;img alt="LLM Reasoning via Inference Scaling - Open Source Research and Live Blog" src="https://a.thumbs.redditmedia.com/YbxuxSuU4JLNKesBFiOKoWqFa5rFj1YIejsVgzzIWu4.jpg" title="LLM Reasoning via Inference Scaling - Open Source Research and Live Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, as someone currently on the hunt for all LLM reasoning resources myself these past weeks, I figured some people out there might actually be interested in a cool resource from my team, for people looking to dive deeper into LLM reasoning research! The AI Innovation team at Red Hat has been sharing and updating a live public blog on their experiments to better understand reasoning with small language models. What's especially interesting in the latest update, though, is how we are achieving improved reasoning via inference-time scaling techniques, rather than the SFT+GRPO combo being heavily explored currently.&lt;/p&gt; &lt;p&gt;Using what we call &amp;quot;particle filtering-based inference-time scaling&amp;quot;, we are achieving improvements on Math 500 and AIME 2024 across Llama, Qwen, and Granite models. We are able to use all three models to beat 4o and Claude, and can get Qwen to outperform o1 as well! For people interested in learning more about the inference-scaling space, theres a write-up and video available &lt;a href="https://probabilistic-inference-scaling.github.io"&gt;here&lt;/a&gt;, and for those interested in more details on the other experiments we've tried and our future plans to train on custom reasoning trajectories, all &lt;em&gt;without&lt;/em&gt; distilling from R1 or its derivatives, feel free to check out the live blog &lt;a href="https://red-hat-ai-innovation-team.github.io/posts/r1-like-reasoning"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;And of course if anyone has any questions, thoughts, etc. I'd be more than happy to reply directly in the thread, as well as connect you all to the researchers working on all the avenues of reasoning we are exploring!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iufnqb08tfie1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2bfe318f5b17a30af0a9b27782fa838f94f4997"&gt;https://preview.redd.it/iufnqb08tfie1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2bfe318f5b17a30af0a9b27782fa838f94f4997&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxusmusti"&gt; /u/maxusmusti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imqgug/llm_reasoning_via_inference_scaling_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imqgug/llm_reasoning_via_inference_scaling_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imqgug/llm_reasoning_via_inference_scaling_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T04:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1imsx0k</id>
    <title>[2502.06772] ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</title>
    <updated>2025-02-11T07:06:04+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.06772"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imsx0k/250206772_reasonflux_hierarchical_llm_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imsx0k/250206772_reasonflux_hierarchical_llm_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T07:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1imv1jo</id>
    <title>DeepSeekV3 with web search and function calling available as API</title>
    <updated>2025-02-11T09:47:26+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added function calling on top of DeepSeekV3 and made it into an API (this API is not down). Open source code is here: &lt;a href="https://github.com/vadimen/llm-function-calling"&gt;https://github.com/vadimen/llm-function-calling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically, you send the list of your functions together with the prompt, and the LLM decides if there's a need to call it. It will return the names and parameters of functions to be called. Optionally web search results can be added to this prompt if parameter search=true.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, it creates a prompt with function names and asks the LLM if there's a need to use it&lt;/li&gt; &lt;li&gt;If yes, then another prompt is created for extracting parameters from the user prompt&lt;/li&gt; &lt;li&gt;All this is done while checking the returned JSON structure, and if it fails, there are 3 attempts to try&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here are some examples of usage:&lt;/p&gt; &lt;p&gt;Example 1:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I never was in Hawaii during summer, I wonder how it feels?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_weather&lt;/p&gt; &lt;p&gt;Arguments: {'location': 'Hawaii','season': 'summer'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 2:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I never bought Rivian stocks from Revolut, may I ask for more info about them?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_stock_price&lt;/p&gt; &lt;p&gt;Arguments: {'stock_name': 'RIVN','broker_name': 'Revolut'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 3:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I was once in Hawaii during summer and was buying Rivian stocks there using Revolut, I wonder how it all is now?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_weather&lt;/p&gt; &lt;p&gt;Arguments: {'location': 'Hawaii','season': 'summer'}&lt;/p&gt; &lt;p&gt;Function: get_stock_price&lt;/p&gt; &lt;p&gt;Arguments: {'stock_name': 'Rivian','broker_name': 'Revolut'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 4:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I would like to eat an apple pie&lt;/p&gt; &lt;p&gt;Response:None (no known function call needed)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T09:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdvpz</id>
    <title>DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark</title>
    <updated>2025-02-10T18:58:01+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"&gt; &lt;img alt="DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark" src="https://preview.redd.it/yz8n6c9nycie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dff4d66879feb128333d600069532abba98cfb81" title="DeepSeek R1 outperforms o3-mini (medium) on the Confabulations (Hallucinations) Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yz8n6c9nycie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imdvpz/deepseek_r1_outperforms_o3mini_medium_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T18:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1imf1s0</id>
    <title>First large scale open source math reasoning dataset with 800k R1 reasoning traces</title>
    <updated>2025-02-10T19:43:47+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"&gt; &lt;img alt="First large scale open source math reasoning dataset with 800k R1 reasoning traces" src="https://preview.redd.it/9hlvxhgp7die1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e7c1d035bde22a44686ad67c741cb943197747a" title="First large scale open source math reasoning dataset with 800k R1 reasoning traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9hlvxhgp7die1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imf1s0/first_large_scale_open_source_math_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:43:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1im4wxs</id>
    <title>They got the scent now..</title>
    <updated>2025-02-10T12:23:58+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt; &lt;img alt="They got the scent now.." src="https://preview.redd.it/euoub08i1bie1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4309ce55a1a79e872658fda4004c32f51846e045" title="They got the scent now.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/euoub08i1bie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im4wxs/they_got_the_scent_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T12:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1im7un9</id>
    <title>Hugging Face AI Agents course is LIVE!</title>
    <updated>2025-02-10T14:52:22+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"&gt; &lt;img alt="Hugging Face AI Agents course is LIVE!" src="https://preview.redd.it/da1ni91yrbie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c1b77fb25fdd40c81ba7533b5a8088b1d90ddaa" title="Hugging Face AI Agents course is LIVE!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/da1ni91yrbie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1im7un9/hugging_face_ai_agents_course_is_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T14:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdnap</id>
    <title>Zonos-v0.1 beta by Zyphra, featuring two expressive and real-time text-to-speech (TTS) models with high-fidelity voice cloning. 1.6B transformer and 1.6B hybrid under an Apache 2.0 license.</title>
    <updated>2025-02-10T18:48:40+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;Today, we're excited to announce a beta release of Zonos, a highly expressive TTS model with high fidelity voice cloning.&lt;/p&gt; &lt;p&gt;We release both transformer and SSM-hybrid models under an Apache 2.0 license.&lt;/p&gt; &lt;p&gt;Zonos performs well vs leading TTS providers in quality and expressiveness.&lt;/p&gt; &lt;p&gt;Zonos offers flexible control of vocal speed, emotion, tone, and audio quality as well as instant unlimited high quality voice cloning. Zonos natively generates speech at 44Khz. Our hybrid is the first open-source SSM hybrid audio model.&lt;/p&gt; &lt;p&gt;Tech report to be released soon.&lt;/p&gt; &lt;p&gt;Currently Zonos is a beta preview. While highly expressive, Zonos is sometimes unreliable in generations leading to interesting bloopers.&lt;/p&gt; &lt;p&gt;We are excited to continue pushing the frontiers of conversational agent performance, reliability, and efficiency over the coming months.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Details (+model comparisons with proprietary &amp;amp; OS SOTAs)&lt;/strong&gt;: &lt;a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1"&gt;https://www.zyphra.com/post/beta-release-of-zonos-v0-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the weights on Huggingface&lt;/strong&gt;: &lt;a href="http://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;http://huggingface.co/Zyphra/Zonos-v0.1-hybrid&lt;/a&gt; and &lt;a href="http://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;http://huggingface.co/Zyphra/Zonos-v0.1-transformer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download the inference code: &lt;a href="http://github.com/Zyphra/Zonos"&gt;http://github.com/Zyphra/Zonos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imdnap/zonosv01_beta_by_zyphra_featuring_two_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T18:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1imevcc</id>
    <title>Zonos: Incredible new TTS model from Zyphra</title>
    <updated>2025-02-10T19:36:34+00:00</updated>
    <author>
      <name>/u/DisjointedHuntsville</name>
      <uri>https://old.reddit.com/user/DisjointedHuntsville</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"&gt; &lt;img alt="Zonos: Incredible new TTS model from Zyphra" src="https://external-preview.redd.it/fT6Qh89XjjsFyuhQEgqnuEl6NQmutLjYQfjnFpXKnYk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b9925bc6f78bacf8995aece90f55807fa31091f" title="Zonos: Incredible new TTS model from Zyphra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DisjointedHuntsville"&gt; /u/DisjointedHuntsville &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/ZyphraAI/status/1888996367923888341"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imevcc/zonos_incredible_new_tts_model_from_zyphra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1imud7e</id>
    <title>Imo Sam Altman is using his board influence to privatize OpenAI’s nonprofit—owned by the American people—for a lowball $40B</title>
    <updated>2025-02-11T08:54:41+00:00</updated>
    <author>
      <name>/u/BananaKuma</name>
      <uri>https://old.reddit.com/user/BananaKuma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Under federal law, the IRS mandates that nonprofit organizations (501(c)(3)s) must use their assets for charitable purposes. If they dissolve or convert to a for-profit, their assets must be sold at fair market value, with proceeds usually going to another nonprofit. &lt;/p&gt; &lt;p&gt;That is to say the American people owns the assets of American nonprofits, and any conversion to a for-profit must first return these public assets before privatization.&lt;/p&gt; &lt;p&gt;Now what are OpenAI nonprofit’s main assets?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ultimate Governance Authority&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The nonprofit’s board legally controls all OpenAI entities (including model weights) through its ownership of OpenAI GP LLC. This gives it power to hire/fire leadership (like CEO Sam Altman) and veto major decisions of the for-profit arm.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AGI control rights&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The nonprofit board exclusively determines when OpenAI achieves Artificial General Intelligence. Once AGI is declared, all related IP becomes nonprofit-controlled and exempt from commercial licenses (including Microsoft’s $13B deal).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Mission Enforcement&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The for-profit subsidiary is legally required to pursue the nonprofit’s charter of developing “safe, broadly beneficial AGI.” Profit distributions to investors are capped, with excess funds flowing back to the nonprofit.&lt;/p&gt; &lt;p&gt;Are these assets fairly valued at 40B, out of 300B of latest SoftBank valuation? &lt;/p&gt; &lt;p&gt;(Edit: One could argue these assets belong to everyone on Earth, as many U.S. nonprofits, including OpenAI, operate globally.) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BananaKuma"&gt; /u/BananaKuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imud7e/imo_sam_altman_is_using_his_board_influence_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imud7e/imo_sam_altman_is_using_his_board_influence_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imud7e/imo_sam_altman_is_using_his_board_influence_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T08:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1imca0s</id>
    <title>New paper gives models a chance to think in latent space before outputting tokens, weights are already on HF - Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title>
    <updated>2025-02-10T17:54:21+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imca0s/new_paper_gives_models_a_chance_to_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imca0s/new_paper_gives_models_a_chance_to_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T17:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1imi9zz</id>
    <title>Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI</title>
    <updated>2025-02-10T21:54:45+00:00</updated>
    <author>
      <name>/u/Calcidiol</name>
      <uri>https://old.reddit.com/user/Calcidiol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt; &lt;img alt="Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI" src="https://external-preview.redd.it/3CzmElhUD3LvBmfuBJUXyad1ZSTywTDZ8sIcpcRW6Zg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8f2be0d3710f8029992a476ab3c9f0e606e4f6" title="Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calcidiol"&gt; /u/Calcidiol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/musk-led-group-bids-97-4-billion-for-openai-control-wsj-says"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T21:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1imm4wc</id>
    <title>DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL</title>
    <updated>2025-02-11T00:47:15+00:00</updated>
    <author>
      <name>/u/PC_Screen</name>
      <uri>https://old.reddit.com/user/PC_Screen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"&gt; &lt;img alt="DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL" src="https://preview.redd.it/ud7gdv14qeie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b7d6f21c5fbf0cb8924b813a909ff2ca372820" title="DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview"&gt;https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PC_Screen"&gt; /u/PC_Screen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ud7gdv14qeie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T00:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1imenfa</id>
    <title>fair use vs stealing data</title>
    <updated>2025-02-10T19:28:00+00:00</updated>
    <author>
      <name>/u/boxingdog</name>
      <uri>https://old.reddit.com/user/boxingdog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt; &lt;img alt="fair use vs stealing data" src="https://preview.redd.it/3bnanf625die1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491c94da04a0556ad84f59944bf9958350b5f675" title="fair use vs stealing data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxingdog"&gt; /u/boxingdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bnanf625die1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1imnaj2</id>
    <title>Elon's bid for OpenAI is about making the for-profit transition as painful as possible for Altman, not about actually purchasing it (explanation in comments).</title>
    <updated>2025-02-11T01:55:14+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From @ phill__1 on twitter:&lt;/p&gt; &lt;p&gt;OpenAI Inc. (the non-profit) wants to convert to a for-profit company. But you &lt;strong&gt;cannot&lt;/strong&gt; just turn a non-profit into a for-profit – that would be an incredible tax loophole. Instead, the new for-profit OpenAI company would need to &lt;strong&gt;pay out&lt;/strong&gt; OpenAI Inc.'s technology and IP (likely in equity in the new for-profit company). &lt;/p&gt; &lt;p&gt;The valuation is tricky since OpenAI Inc. is theoretically the sole controlling shareholder of the capped-profit subsidiary, OpenAI LP. But there have been some numbers floating around. Since the rumored SoftBank investment at a $260B valuation is dependent on the for-profit move, we're using the current ~$150B valuation. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Control premiums&lt;/em&gt; in market transactions typically range between 20-30% of enterprise value; experts have predicted something around $30B-$40B. &lt;strong&gt;The key is&lt;/strong&gt;, this valuation is ultimately signed off on by the California and Delaware Attorneys General. &lt;/p&gt; &lt;p&gt;Now, if you want to &lt;strong&gt;block&lt;/strong&gt; OpenAI from the for-profit transition, but have yet to be successful in court, what do you do? &lt;em&gt;Make it as painful as possible.&lt;/em&gt; Elon Musk just gave regulators a &lt;strong&gt;perfect&lt;/strong&gt; argument for why the non-profit should get $97B for selling their technology and IP. This would instantly make the non-profit the majority stakeholder at &lt;em&gt;62%&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's a clever move that throws a major wrench into the for-profit transition, potentially even stopping it dead in its tracks. Whether OpenAI accepts the offer or not (they won't), the mere existence of this valuation benchmark will be hard for regulators to ignore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T01:55:14+00:00</published>
  </entry>
</feed>
