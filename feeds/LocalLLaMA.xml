<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-30T16:41:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lo1d8t</id>
    <title>Models for generating QA-pairs from text dataset</title>
    <updated>2025-06-30T07:28:57+00:00</updated>
    <author>
      <name>/u/Sasikuttan2163</name>
      <uri>https://old.reddit.com/user/Sasikuttan2163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which models offer the best quality-to-performance in terms of prompt adherence and context length for such a usecase? I am currently using NousResearch/Hermes-3-Llama-3.1-8B-GGUF for this task after having failed in trying to get Qwen2.5 7B to give questions from the actual theory text not sections of the book. I am using an RTX 4060 8GB with 16 GB RAM, which severely limits my options but I'd want to use the best I could for my hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sasikuttan2163"&gt; /u/Sasikuttan2163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T07:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobqvc</id>
    <title>MCP tool development -- repeated calls with no further processing</title>
    <updated>2025-06-30T16:16:50+00:00</updated>
    <author>
      <name>/u/nuketro0p3r</name>
      <uri>https://old.reddit.com/user/nuketro0p3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to make a fetch_url tool using MCP:&lt;br /&gt; &lt;a href="https://github.com/modelcontextprotocol"&gt;https://github.com/modelcontextprotocol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Setup: LMStudio + Qwen32b / Gemma27b / Gemma12b / DeepSeek R1 (Qwen3 distil)&lt;/p&gt; &lt;p&gt;When I ask the model to get a URL, it successfully calls the fetch_url function (and gets a correct response). However, it doesn't understand that it has to stop and keeps calling the same tool again and again. &lt;/p&gt; &lt;p&gt;I also have another add_num function (copied from the docs) which works perfectly. I've tested this on Qwen32b, Gemma 27b (and below) and all have the same issue. &lt;/p&gt; &lt;p&gt;Anyone has had this issue? Is there some hidden flag that tells the model to stop calling a tool repeatedly -- even if it was a success? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuketro0p3r"&gt; /u/nuketro0p3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobx8p</id>
    <title>Language Model Maker</title>
    <updated>2025-06-30T16:23:54+00:00</updated>
    <author>
      <name>/u/GustaveVonZarovich</name>
      <uri>https://old.reddit.com/user/GustaveVonZarovich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a question, If you're familiar with RPG maker, you may guess where this is going. Is there a LM Maker, even very basic, shared as source code so people can create their own LMs small medium large etc. In case of existing llms end up all commercial &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GustaveVonZarovich"&gt; /u/GustaveVonZarovich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobx8p/language_model_maker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobx8p/language_model_maker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobx8p/language_model_maker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:23:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo35gq</id>
    <title>Affordable dev system (spark alternative?)</title>
    <updated>2025-06-30T09:30:59+00:00</updated>
    <author>
      <name>/u/_camera_up</name>
      <uri>https://old.reddit.com/user/_camera_up</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a science project at a University of Applied Sciences. We plan to purchase a server with an NVIDIA H200 GPU. This system will host LLM services for students.&lt;/p&gt; &lt;p&gt;For development purposes, we’d like to have a second system where speed isn’t critical, but it should still be capable of running the same models we plan to use in production (probably up to 70B parameters). We don’t have the budget to simply replicate the production system — ideally, the dev system should be under €10k.&lt;/p&gt; &lt;p&gt;My research led me to the NVIDIA DGX Spark and similar solutions from other vendors, but none of the resellers I contacted had any idea when these systems will be available. (Paper launch?)&lt;/p&gt; &lt;p&gt;I also found the GMKtec EVO-X2, which seems to be the AMD equivalent of the Spark. It’s cheap and available, but I don’t have any experience with ROCm, and developing on an AMD machine for a CUDA-based production system seems like an odd choice. On the other hand, we don’t plan to develop at the CUDA level, but rather focus on pipelines and orchestration.&lt;/p&gt; &lt;p&gt;A third option would be to build a system with a few older cards like K40s or something similar.&lt;/p&gt; &lt;p&gt;What would you advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_camera_up"&gt; /u/_camera_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T09:30:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo39jd</id>
    <title>Deepseek R1 Web ouputs much more chain-of-thought information than API?</title>
    <updated>2025-06-30T09:38:30+00:00</updated>
    <author>
      <name>/u/Tectorumiris</name>
      <uri>https://old.reddit.com/user/Tectorumiris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is what I observed, the Web print out much more detailed chain-of-thought information than API. Anybody else observed the same issue? I wonder why it's like that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tectorumiris"&gt; /u/Tectorumiris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo39jd/deepseek_r1_web_ouputs_much_more_chainofthought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo39jd/deepseek_r1_web_ouputs_much_more_chainofthought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo39jd/deepseek_r1_web_ouputs_much_more_chainofthought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T09:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnfl21</id>
    <title>KoboldCpp v1.95 with Flux Kontext support</title>
    <updated>2025-06-29T14:09:23+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt; &lt;img alt="KoboldCpp v1.95 with Flux Kontext support" src="https://b.thumbs.redditmedia.com/Acro72oIt0wqz90aniNWfVPCfmpg8vp4szhxO44ZpMU.jpg" title="KoboldCpp v1.95 with Flux Kontext support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flux Kontext is a relatively new open weights model based on Flux that can &lt;strong&gt;edit images using natural language&lt;/strong&gt;. Easily replace backgrounds, edit text, or add extra items into your images.&lt;/p&gt; &lt;p&gt;With the release of KoboldCpp v1.95, Flux Kontext support has been added to KoboldCpp! No need for any installation or complicated workflows, just download one executable and launch with &lt;a href="https://huggingface.co/koboldcpp/kcppt/resolve/main/Flux-Kontext.kcppt"&gt;&lt;strong&gt;a ready-to-use kcppt template&lt;/strong&gt;&lt;/a&gt; (recommended at least 12gb VRAM), and you're ready to go, the necessary models will be fetched and loaded.&lt;/p&gt; &lt;p&gt;Then you can open a browser window to &lt;a href="http://localhost:5001/sdui"&gt;http://localhost:5001/sdui&lt;/a&gt;, a simple A1111 like UI.&lt;/p&gt; &lt;p&gt;Supports using up to 4 reference images. Also supports the usual inpainting, img2img, sampler settings etc. You can also load the component models individually (e.g. you can reuse the VAE or T5-XXL for Chroma, which koboldcpp also supports).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff"&gt;https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff&lt;/a&gt;&lt;/p&gt; &lt;p&gt;KoboldCpp also emulates the A1111/Forge and ComfyUI APIs so third party tools can use it as a drop in replacement.&lt;/p&gt; &lt;p&gt;This is possible thanks to the hard work of stable-diffusion.cpp contributors leejet and stduhpf.&lt;/p&gt; &lt;p&gt;P.s. Also, gemma 3n support is included in this release too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T14:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnmp98</id>
    <title>hunyuan-a13b: any news? GGUF? MLX?</title>
    <updated>2025-06-29T19:04:56+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many I’m excited about this model. We had a big thread on it, then crickets. Any news?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T19:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5xyx</id>
    <title>Been experimenting with “agent graphs” for local LLMs — basically turning thoughts into modular code</title>
    <updated>2025-06-30T12:13:44+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’ve been messing with this concept I’m calling agentic knowledge graphs, basically, instead of writing prompts one by one, you define little agents that represent aspects of your thinking. Then you connect them with logic and memory.&lt;/p&gt; &lt;p&gt;Each node in the graph is a persona or function (like a writing coach, journal critic, or curriculum builder).&lt;/p&gt; &lt;p&gt;Each edge is a task flow, reflection, or dependency.&lt;/p&gt; &lt;p&gt;And memory, via ChromaDB or similar, gives it a sense of continuity, like it remembers how you think.&lt;/p&gt; &lt;p&gt;I’ve been using local tools only: Ollama for models like Qwen2 or LLaMA, NetworkX for the graph itself, ChromaDB for contextual memory, ReactFlow for visualization when I want to get fancy&lt;/p&gt; &lt;p&gt;It’s surprisingly flexible: Journaling feedback loops, Diss track generators that scrape Reddit threads, Research agents that challenge your assumptions, Curriculum builders that evolve over time&lt;/p&gt; &lt;p&gt;I wrote up a full guide that walks through the whole system, from agents to memory to traversal, and how to build it without any cloud dependencies.&lt;/p&gt; &lt;p&gt;Happy to share the link if anyone’s curious. &lt;/p&gt; &lt;p&gt;Anyone else here doing stuff like this? I’d love to bounce ideas around or see your setups. This has honestly been one of the most fun and mind-expanding builds I’ve done in years.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5xyx/been_experimenting_with_agent_graphs_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5xyx/been_experimenting_with_agent_graphs_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5xyx/been_experimenting_with_agent_graphs_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnsax9</id>
    <title>Please convince me not to get a GPU I don't need. Can any local LLM compare with cloud models?</title>
    <updated>2025-06-29T23:05:31+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren't as strong or even as cheap as the cloud models.&lt;/p&gt; &lt;p&gt;I'm trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won't use much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T23:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobzkr</id>
    <title>n8n ,proxmox ,docker and Google API.</title>
    <updated>2025-06-30T16:26:28+00:00</updated>
    <author>
      <name>/u/Able-Consequence8872</name>
      <uri>https://old.reddit.com/user/Able-Consequence8872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt; &lt;img alt="n8n ,proxmox ,docker and Google API." src="https://preview.redd.it/02pteeydc3af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc69450af42416d14cf7491d093e484565716a06" title="n8n ,proxmox ,docker and Google API." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, trying to use Google API in 8n8 (in a PROXMOX container ) and LMstudio (another machine in the same LAN) but it won't take my LAN ip adresse.n8n gives the localhost value by default. I know there is a trick with docker, like &lt;a href="https://local.docker/v1"&gt;https://local.docker/v1&lt;/a&gt;, but it works only if both n8n and LMstudio work on the same machine. n8n is on a different machine on the LAN.&lt;/p&gt; &lt;p&gt;how can I fix this? I want to run everything locally, with 2 different machines on the LAN, using Google workspace with my assistant in 8n8, and Mistral as a local AI in LMstudio.&lt;/p&gt; &lt;p&gt;thx..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Able-Consequence8872"&gt; /u/Able-Consequence8872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/02pteeydc3af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobzkr/n8n_proxmox_docker_and_google_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnx8js</id>
    <title>Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4</title>
    <updated>2025-06-30T03:16:29+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt; &lt;img alt="Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4" src="https://b.thumbs.redditmedia.com/SDjfig6sg4eLnygQ_8EUyGIhQK35Ih7ixM3P3g-oh_M.jpg" title="Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h0h0kuq6fz9f1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adff81fdbb4ff3138a504a84b18e4440a6185b98"&gt;https://preview.redd.it/h0h0kuq6fz9f1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adff81fdbb4ff3138a504a84b18e4440a6185b98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m currently working on a hands-on series where I’m building a small language model from scratch. Last week was all about tokenization, embedding layers, and transformer fundamentals. This week, I’m shifting focus to something crucial but often overlooked: how transformers understand &lt;em&gt;order&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Here’s the breakdown for June 30 – July 4:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;June 30&lt;/strong&gt; – What are Positional Embeddings and why do they matter&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 1&lt;/strong&gt; – Coding sinusoidal positional embeddings from scratch&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 2&lt;/strong&gt; – A deep dive into Rotary Positional Embeddings (RoPE) and how DeepSeek uses them&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 3&lt;/strong&gt; – Implementing RoPE in code and testing it on token sequences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 4&lt;/strong&gt; – Bonus: Intro to model distillation, compressing large models into smaller, faster ones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each day, I’ll be sharing learnings, visuals, and code walkthroughs. The goal is to understand the concepts &lt;em&gt;and&lt;/em&gt; implement them in practice.&lt;/p&gt; &lt;p&gt;If you'd like to follow along more closely, I’m posting regular updates on LinkedIn. Feel free to connect with me there &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/"&gt;https://www.linkedin.com/in/prashant-lakhera-696119b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, questions, or suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo1xma</id>
    <title>Current State of Code Tab/Autocomplete Models???</title>
    <updated>2025-06-30T08:08:09+00:00</updated>
    <author>
      <name>/u/Much-Contract-1397</name>
      <uri>https://old.reddit.com/user/Much-Contract-1397</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1xma/current_state_of_code_tabautocomplete_models/"&gt; &lt;img alt="Current State of Code Tab/Autocomplete Models???" src="https://external-preview.redd.it/NjeXyJ-H2-ngvCqZHmYzHLdzIwJM3Nr4H0k94STgc3g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51dde392b9f75b253e48ee1bd050a44e583316e2" title="Current State of Code Tab/Autocomplete Models???" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love cursor, but that love is solely for the tab completion model. It’s a ok vs code clone and cline is better chat/agent wise. I have to use gh copilot at work and it’s absolute trash compared to that tab model. Are there any open-source models that come close in 2025? I saw zeta but that’s a bit underwhelming and only runs in Zed. Yes, I know there’s a lot of magic cursor does and it’s not just the model. It would be cool to see an open cursor project. I would happy to hack away it my self as qwen-3 coder is soon and we’ve seen so many great &amp;lt;7b models released in the past 6 months. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Much-Contract-1397"&gt; /u/Much-Contract-1397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zed-industries/zeta"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1xma/current_state_of_code_tabautocomplete_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo1xma/current_state_of_code_tabautocomplete_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T08:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnl6we</id>
    <title>According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM</title>
    <updated>2025-06-29T18:03:15+00:00</updated>
    <author>
      <name>/u/BringerOfNuance</name>
      <uri>https://old.reddit.com/user/BringerOfNuance</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"&gt; &lt;img alt="According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM" src="https://external-preview.redd.it/nvS9CwLfNaU7BwSp_zJYNRF4C9Wqsv0EQEs557kjO8Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b293bc24e9202fbe689a8234c25aeb41c48c3d15" title="According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BringerOfNuance"&gt; /u/BringerOfNuance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-also-planning-geforce-rtx-5070-ti-super-with-24gb-gddr7-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1load8a</id>
    <title>[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?</title>
    <updated>2025-06-30T15:23:52+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" src="https://b.thumbs.redditmedia.com/QrXwS0MMtdu4-LCvZnP-VTv25rOcYvXpPucHJrkYiSQ.jpg" title="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7"&gt;https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’ve ever peeked inside models like GPT or BERT and wondered &lt;em&gt;how&lt;/em&gt; they understand the &lt;em&gt;order&lt;/em&gt; of words, the secret sauce is something called positional embedding.&lt;/p&gt; &lt;p&gt;Without it, a language model can’t tell the difference between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The cat sat on the mat”&lt;/li&gt; &lt;li&gt;“The mat sat on the cat”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem: Transformers Don’t Understand Word Order&lt;/h1&gt; &lt;p&gt;Transformers process all tokens at once, which is great for speed, but unlike RNNs, they don’t read text sequentially. That means they don’t naturally know the order of words.&lt;/p&gt; &lt;p&gt;To a plain Transformer, “I love AI” could mean the same as “AI love I.”&lt;/p&gt; &lt;h1&gt;The Solution: Positional Embeddings&lt;/h1&gt; &lt;p&gt;To fix this, we add a second layer of information: positional embeddings. These vectors tell the model &lt;em&gt;where&lt;/em&gt; each word appears in the input sequence.&lt;/p&gt; &lt;p&gt;So instead of just using word embeddings, we do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Final Input = Word Embedding + Positional Embedding &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the model knows both the meaning of each word and its position in the sentence.&lt;/p&gt; &lt;h1&gt;Why Not Let the Model Learn Position on Its Own?&lt;/h1&gt; &lt;p&gt;In theory, a large model &lt;em&gt;could&lt;/em&gt; infer word order from patterns. But in practice, that’s inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.&lt;/p&gt; &lt;h1&gt;Two Common Types of Positional Embeddings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in the original Transformer paper&lt;/li&gt; &lt;li&gt;Not learned, uses sine and cosine functions&lt;/li&gt; &lt;li&gt;Good for generalizing to longer sequences&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learned Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in models like BERT&lt;/li&gt; &lt;li&gt;Learned during training, like word embeddings&lt;/li&gt; &lt;li&gt;Flexible, but may not generalize well to unseen sequence lengths&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Real Example: Why It Matters&lt;/h1&gt; &lt;p&gt;Compare:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The dog chased the cat.”&lt;/li&gt; &lt;li&gt;“The cat chased the dog”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Same words, totally different meaning. Without positional embeddings, the model can’t tell which animal is doing the chasing.&lt;/p&gt; &lt;h1&gt;What’s New: Rotary Positional Embeddings (RoPE)&lt;/h1&gt; &lt;p&gt;Modern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It’s more efficient for long sequences and performs better in certain settings.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Positional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.&lt;/p&gt; &lt;p&gt;👉 Tomorrow, we’re going to code positional embeddings from scratch—so stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1loal9v</id>
    <title>Drafted Llama as an enhanced parser for interactive fiction puzzles/games</title>
    <updated>2025-06-30T15:32:42+00:00</updated>
    <author>
      <name>/u/Fit-Lengthiness-4747</name>
      <uri>https://old.reddit.com/user/Fit-Lengthiness-4747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt; &lt;img alt="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" src="https://preview.redd.it/wg741dwa23af1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91fd5c66c0a9e6f241558b7edd95831e8c284bc3" title="Drafted Llama as an enhanced parser for interactive fiction puzzles/games" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Llama as a way to expand the types of games that can be played within interactive fiction, such as creating non-deterministic rubrics to grade puzzle solutions, allowing building/crafting with a wide range of objects.combinatorial possibilities, and enabling sentiment and emotion-based responses with NPCs as a way of getting game information. try is here: &lt;a href="https://thoughtauction.itch.io/last-audit-of-the-damned"&gt;https://thoughtauction.itch.io/last-audit-of-the-damned&lt;/a&gt; And if you like, please vote for us in the ParserComp 2025 contest, as well as play some of the other entries. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Lengthiness-4747"&gt; /u/Fit-Lengthiness-4747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wg741dwa23af1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loal9v/drafted_llama_as_an_enhanced_parser_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo84yj</id>
    <title>[2506.21734] Hierarchical Reasoning Model</title>
    <updated>2025-06-30T13:54:40+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract:&lt;/p&gt; &lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T13:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5uz6</id>
    <title>Has anyone tried running 2 AMD Ryzen™ AI Max+ 395 in parallel?</title>
    <updated>2025-06-30T12:09:31+00:00</updated>
    <author>
      <name>/u/orkutmuratyilmaz</name>
      <uri>https://old.reddit.com/user/orkutmuratyilmaz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Some models require more VRAM to run. I was thinking of getting 2 AMD Ryzen™ AI Max+ 395 and trying to run them in parallel. I wonder if anyone has tried this? Does anyone have any information?&lt;/p&gt; &lt;p&gt;Have a nice one:)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orkutmuratyilmaz"&gt; /u/orkutmuratyilmaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo3y10</id>
    <title>From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone</title>
    <updated>2025-06-30T10:21:45+00:00</updated>
    <author>
      <name>/u/rvnllm</name>
      <uri>https://old.reddit.com/user/rvnllm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt; &lt;img alt="From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone" src="https://external-preview.redd.it/dt9-WMKabCLh-xNeXHwgVKpGxeuZavoRA4i4gCf5uKw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79dbb93cb56753787d457bd94d66062c9785a7f7" title="From the trenches, running TinyLlama-1.1B-Chat-v0.1 on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing my efforts, really, and thank you for reading in advance.&lt;/p&gt; &lt;p&gt;I am working on an LLM engine nicknamed Nyra in rust and c++20.&lt;/p&gt; &lt;p&gt;So managed to do local LLM Inference on iPhone in 70ms and 15 TPS (could be massively improved once metal is in motion)&lt;/p&gt; &lt;p&gt;One of the images shows that previously I optimized safetensors loading on-device for my custom runtime. That was step one.&lt;br /&gt; Since then, after some really hard push over the last 48 hours, I've integrated inference, built tokenizer support. So today Nyra generated her first poem.&lt;br /&gt; That was step two.&lt;/p&gt; &lt;p&gt;It is fully offline. Started to work after I nearly gave up multiple times, fully loaded with coffee and being lost between calculations, kernels and the like. Also occasionally my face took the shape of the keyboard falling asleep on it.&lt;/p&gt; &lt;p&gt;So what is it that I am showing?&lt;br /&gt; -&amp;gt; iphone in flight mode, check.&lt;br /&gt; -&amp;gt; No cloud. No API. No fluff. Just pure, local inference, check.&lt;br /&gt; -&amp;gt; Loaded 1.1B model in &amp;lt;2s, check. \-&amp;gt; Ran inference at 15 tokens/sec, well could be better as there is no Metal just yet, but check.&lt;br /&gt; -&amp;gt; CLI-based stream loop, well for devs thats cool, swiftui coming up, check.&lt;br /&gt; -&amp;gt; All native Rust + C++20 + SwiftUI pipeline, possibility to improve speed, check.&lt;br /&gt; -&amp;gt; Zero cloud, full privacy and full locality, yes thats my core philosophy, check.&lt;/p&gt; &lt;p&gt;Cloud? no. All local privacy driven. So yes, lets be sovereign.If one doesn't have the proper hardware AI is slow. I try to change that by running AI (LLMs) with acceptable speed on any hardware and anywhere.&lt;br /&gt; Nyra is different: she's modular, fast, local - and soon pluggable.&lt;/p&gt; &lt;p&gt;here is a demo video&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=6ZMplYIsTyw"&gt;https://www.youtube.com/watch?v=6ZMplYIsTyw&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks for reading&lt;br /&gt; Ervin&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s14m8e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6e1b0c1f95a26d2b284a9e18b5936dfc7e0340f9"&gt;https://preview.redd.it/s14m8e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6e1b0c1f95a26d2b284a9e18b5936dfc7e0340f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/amav0e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2a878999d57ab1d386ff367d6ca84792da082804"&gt;https://preview.redd.it/amav0e4bj1af1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2a878999d57ab1d386ff367d6ca84792da082804&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvnllm"&gt; /u/rvnllm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo3y10/from_the_trenches_running_tinyllama11bchatv01_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T10:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnzj5e</id>
    <title>So whatever happened to d(iffuser)LLMs?</title>
    <updated>2025-06-30T05:29:09+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning, I got an E-Mail from the team behind the Mercury Coder LLM, Inception (&lt;a href="https://www.inceptionlabs.ai/"&gt;https://www.inceptionlabs.ai/&lt;/a&gt;) that basically announced a chat-focused model. Pretty neat, sent along an API example with cURL also. Simple and nice.&lt;/p&gt; &lt;p&gt;But this reminded me of dLLMs in general - they haven't really been talked a lot about lately. So I wanted to ask into the broad space: What's up? I like the idea of dLLMs being a different approach and perhaps easier to run compared to transformers. But I also understand the tech is relatively new - that is, diffusers for text rather than images.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T05:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo0rk8</id>
    <title>Accelerated LLM Inference on AMD Instinct™ GPUs with vLLM 0.9.x and ROCm</title>
    <updated>2025-06-30T06:48:55+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T06:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnrd1t</id>
    <title>You can just RL a model to beat any "AI detectors"</title>
    <updated>2025-06-29T22:22:55+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt; &lt;img alt="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d"&gt;https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd"&gt;https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baseline&lt;br /&gt; • Model: Llama-3.1 8B-Instruct&lt;br /&gt; • Prompt: plain &amp;quot;Write an essay about X&amp;quot;&lt;br /&gt; • Detector: ZeroGPT&lt;br /&gt; Result: 100 % AI-written&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d"&gt;https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Data&lt;br /&gt; • Synthetic dataset of 150 school-style prompts (history, literature, tech). Nothing fancy, just json lines + system prompt &amp;quot;You are a human essay writer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e"&gt;https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First training run&lt;br /&gt; After ~30 GRPO steps on a single A100:&lt;br /&gt; • ZeroGPT score drops from 100 → 42 %&lt;br /&gt; The model learned:&lt;br /&gt; Write a coherent intro&lt;br /&gt; Stuff one line of high-entropy junk&lt;br /&gt; Finish normally&lt;br /&gt; Average &amp;quot;human-ness&amp;quot; skyrockets because detector averages per-sentence scores&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75"&gt;https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch #1&lt;br /&gt; Added a gibberish classifier (tiny DistilRoBERTa) and multiplied reward by its minimum &amp;quot;clean&amp;quot; score. Junk lines now tank reward → behaviour disappears. GRPO’s beta ≈ how harshly to penalize incoherence. Set β = 0.4 and reward curve stabilized; no more oscillation between genius &amp;amp; garbage. Removed reasoning (memory constraints).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252"&gt;https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny models crush it&lt;br /&gt; Swapped in Qwen 0.5B LoRA rank 8, upped num_generations → 64.&lt;br /&gt; Result after 7 steps: best sample already at 28 % &amp;quot;human&amp;quot;. Smaller vocab seems to help leak less LM &amp;quot;signature&amp;quot; (the model learned to use lots of proper nouns to trick the detector).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2"&gt;https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;Detector bug?&lt;br /&gt; ZeroGPT sometimes marks the first half AI, second half human for the same paragraph. The RL agent locks onto that gradient and exploits it. Classifier clearly over-fits surface patterns rather than semantics&lt;/p&gt; &lt;p&gt;Single scalar feedback is enough for LMs to reverse-engineer public detectors &lt;/p&gt; &lt;p&gt;Add even a tiny auxiliary reward (gibberish, length) to stop obvious failure modes &lt;/p&gt; &lt;p&gt;Public &amp;quot;AI/Not-AI&amp;quot; classifiers are security-through-obscurity&lt;/p&gt; &lt;p&gt;Reward function: &lt;a href="https://codefile.io/f/R4O9IdGEhg"&gt;https://codefile.io/f/R4O9IdGEhg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo5vnf</id>
    <title>What is the current best local coding model with &lt;= 4B parameters?</title>
    <updated>2025-06-30T12:10:32+00:00</updated>
    <author>
      <name>/u/Wooden-Key751</name>
      <uri>https://old.reddit.com/user/Wooden-Key751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking for &amp;lt;= 4B coding models. I realize that none of these will be practical for now just looking for some to do experiments.&lt;/p&gt; &lt;p&gt;Here is what i found so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Menlo / Jan-nano — 4.02 B (Not really coding but I expect it to be better than others)&lt;/li&gt; &lt;li&gt;Gemma — 4 B / 2 B&lt;/li&gt; &lt;li&gt;Qwen 3 — 4 B / 0.6 B&lt;/li&gt; &lt;li&gt;Phi-4 Mini — 3.8 B&lt;/li&gt; &lt;li&gt;Phi-3.5 Mini — 3.5 B&lt;/li&gt; &lt;li&gt;Llama-3.2 — 3.2 B&lt;/li&gt; &lt;li&gt;Starcoder — 3 B / 1 B&lt;/li&gt; &lt;li&gt;Starcoder 2 — 3 B&lt;/li&gt; &lt;li&gt;Stable-Code — 3 B&lt;/li&gt; &lt;li&gt;Granite — 3 B / 2.53 B&lt;/li&gt; &lt;li&gt;Cogito — 3 B&lt;/li&gt; &lt;li&gt;DeepSeek Coder — 2.6 B / 1.3 B&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill (Qwen-tuned) — 1.78 B&lt;/li&gt; &lt;li&gt;Qwen 2.5 — 1.5 B / 0.5 B&lt;/li&gt; &lt;li&gt;Yi-Coder — 1.5 B&lt;/li&gt; &lt;li&gt;Deepscaler — 1.5 B&lt;/li&gt; &lt;li&gt;Deepcoder — 1.5 B&lt;/li&gt; &lt;li&gt;CodeGen2 — 1 B&lt;/li&gt; &lt;li&gt;BitNet-B1.58 — 0.85 B&lt;/li&gt; &lt;li&gt;ERNIE-4.5 — 0.36 B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone tried any of these or compared &amp;lt;= 4B models on coding tasks? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Key751"&gt; /u/Wooden-Key751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T12:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnlxp1</id>
    <title>4x 4090 48GB inference box (I may have overdone it)</title>
    <updated>2025-06-29T18:33:40+00:00</updated>
    <author>
      <name>/u/101m4n</name>
      <uri>https://old.reddit.com/user/101m4n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt; &lt;img alt="4x 4090 48GB inference box (I may have overdone it)" src="https://external-preview.redd.it/o67J1SHcLKrQAlXicnfT20w0glJr7s4wb4-c1GOwiA8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=936572d5a67f4298cbb8ecc135d737e991ade403" title="4x 4090 48GB inference box (I may have overdone it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I discovered that 48GB 4090s were starting to show up on the western market in large numbers. I didn't think much of it at the time, but then I got my payout from the mt.gox bankruptcy filing (which has been ongoing for over 10 years now), and decided to blow a chunk of it on an inference box for local machine learning experiments.&lt;/p&gt; &lt;p&gt;After a delay receiving some of the parts (and admittedly some procrastination on my end), I've finally found the time to put the whole machine together!&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock romed8-2t motherboard (SP3)&lt;/li&gt; &lt;li&gt;32 core epyc&lt;/li&gt; &lt;li&gt;256GB 2666V memory&lt;/li&gt; &lt;li&gt;4x &amp;quot;tronizm&amp;quot; rtx 4090D 48GB modded GPUs from china&lt;/li&gt; &lt;li&gt;2x 1tb nvme (striped) for OS and local model storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The cards are very well built. I have no doubts as to their quality whatsoever. They were heavy, the heatsinks made contact with all the board level components and the shrouds were all-metal and very solid. It was almost a shame to take them apart! They were however incredibly loud. At idle, the fan sits at 30%, and at that level they are already as loud as the loudest blower cards for gaming. At full load, they are truly deafening and definitely not something you want to share space with. Hence the water-cooling.&lt;/p&gt; &lt;p&gt;There are however no full-cover waterblocks for these GPUs (they use a custom PCB), so to cool them I had to get a little creative. Corsair makes a (kinda) &lt;a href="https://www.corsair.com/uk/en/p/custom-liquid-cooling/cx-9025001-ww/icue-link-xg3-rgb-hybrid-gpu-water-block-4090-4080-cx-9025001-ww?srsltid=AfmBOopBdweqKN5Wpj6wHKLSR9SEYZmNpOpOyaFZTLLdld7hLBrg1iCg"&gt;generic block&lt;/a&gt; called the xg3. The product itself is a bit rubbish, requiring corsairs proprietary i-cue system to run the fan which is supposed to cool the components not covered by the coldplate. It's also overpriced. However these are more or less the only option here. As a side note, these &amp;quot;generic&amp;quot; blocks only work work because the mounting hole and memory layout around the core is actually standardized to some extent, something I learned during my research.&lt;/p&gt; &lt;p&gt;The cold-plate on these blocks turned out to foul one of the components near the core, so I had to modify them a bit. I also couldn't run the aforementioned fan without corsairs i-cue link nonsense and the fan and shroud were too thick anyway and would have blocked the next GPU anyway. So I removed the plastic shroud and fabricated a frame + heatsink arrangement to add some support and cooling for the VRMs and other non-core components.&lt;/p&gt; &lt;p&gt;As another side note, the marketing material for the xg3 claims that the block contains a built-in temperature sensor. However I saw no indication of a sensor anywhere when disassembling the thing. Go figure.&lt;/p&gt; &lt;p&gt;Lastly there's the case. I couldn't find a case that I liked the look of that would support three 480mm radiators, so I built something out of pine furniture board. Not the easiest or most time efficient approach, but it was fun and it does the job (fire hazard notwithstanding).&lt;/p&gt; &lt;p&gt;As for what I'll be using it for, I'll be hosting an LLM for local day-to-day usage, but I also have some more unique project ideas, some of which may show up here in time. Now that such projects won't take up resources on my regular desktop, I can afford to do a lot of things I previously couldn't!&lt;/p&gt; &lt;p&gt;P.S. If anyone has any questions or wants to replicate any of what I did here, feel free to DM me with any questions, I'm glad to help any way I can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/101m4n"&gt; /u/101m4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnlxp1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxo8y</id>
    <title>Major AI platforms will eventually have ads</title>
    <updated>2025-06-30T03:40:35+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this as a huge reason to continue advancement of local LLMs. OpenAI, Google, Microsoft, Anthropic, etc. all the big players have investors to answer to, and will eventually need to stop burning money. They will get pressured into a sustainable business model. I think Google has already lost a lot of traffic to AI search that they will try to win back. Right now, they are giving LLM access in exchange for data to train on. Eventually they will have enough that it won’t be worth it anymore. &lt;/p&gt; &lt;p&gt;Anyone else see this coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnu4zl</id>
    <title>Baidu releases ERNIE 4.5 models on huggingface</title>
    <updated>2025-06-30T00:34:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt; &lt;img alt="Baidu releases ERNIE 4.5 models on huggingface" src="https://external-preview.redd.it/Wyzo5BvQjbbXvCrrpLypEcj3XicuXWigLyl_Acs2b5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33f879948e7c84df63582ea3398eb078c0298c8e" title="Baidu releases ERNIE 4.5 models on huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp support for ERNIE 4.5 0.3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;https://github.com/ggml-org/llama.cpp/pull/14408&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vllm Ernie4.5 and Ernie4.5MoE Model Support&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20220"&gt;https://github.com/vllm-project/vllm/pull/20220&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:34:16+00:00</published>
  </entry>
</feed>
