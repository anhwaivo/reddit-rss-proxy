<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-17T06:08:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ir5gfk</id>
    <title>Local LLM for a noob</title>
    <updated>2025-02-16T23:11:29+00:00</updated>
    <author>
      <name>/u/Nice_Village_8610</name>
      <uri>https://old.reddit.com/user/Nice_Village_8610</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;G'day, I am new to the local llm crowd. Grabbed a cheap 3060 (12gb) to experiment. My primary purpose for now is coding, so am starting with qwen 2.5 coder.&lt;/p&gt; &lt;p&gt;Have so far: - ran model with ollama in cmd. - ran model through LM studio.&lt;/p&gt; &lt;p&gt;Specific need: My desktop with the graphics card is our primary media centre so my access to it is limited. I would like to start up a server on the desktop running the model and then connect to it using my laptop. I've seen I can do this in LM studio.&lt;/p&gt; &lt;p&gt;My lack of knowledge comes in with how to connect to this server. I am hopeful I can have a setup similar to most LLMs available where I can have my chats saved and have the model remember what is in my session so I can go back and forth. A sort of UI I suppose.&lt;/p&gt; &lt;p&gt;So end game would be. - qwen2.5coder running on desktop with server started. - laptop on same network able to access this with a UI that can save and has basic memory functions like CGPT etc....&lt;/p&gt; &lt;p&gt;Sorry for the essay!&lt;/p&gt; &lt;p&gt;Any help or advice on setups would be awesome &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice_Village_8610"&gt; /u/Nice_Village_8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir5gfk/local_llm_for_a_noob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir5gfk/local_llm_for_a_noob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir5gfk/local_llm_for_a_noob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T23:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir08pp</id>
    <title>The scoop on 4060 ti 16gb cards</title>
    <updated>2025-02-16T19:26:59+00:00</updated>
    <author>
      <name>/u/DramaLlamaDad</name>
      <uri>https://old.reddit.com/user/DramaLlamaDad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I searched and saw a number of old posts on 4060's for LLM but nothing recent. I'm wondering why I don't hear more talk about 4060 ti 16gb cards as a budget option. Sure, they won't beat out 3090's but honestly, I've had zero luck finding finding these 3080 or 3090 used deals locally that other people mention. 4060 ti's are in stock and $500, 4 of them gets you to 64GB and not crazy power usage.&lt;/p&gt; &lt;p&gt;Am I missing something? Seems like these guys would be the darling of the local llm world if there wasn't a catch, so what's the catch? Yes, I know they won't perform on par with the other options but again, $$ per GB and tokens per watt, they feel like a decent spot.&lt;/p&gt; &lt;p&gt;So, anyone know why there isn't more chatter about these guys or the upcoming 5060's with 16gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DramaLlamaDad"&gt; /u/DramaLlamaDad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T19:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbwt9</id>
    <title>Salamandra Technical Report</title>
    <updated>2025-02-17T04:48:23+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.08489"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbwt9/salamandra_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbwt9/salamandra_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:48:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1irce17</id>
    <title>Benchmarking for specific language</title>
    <updated>2025-02-17T05:16:30+00:00</updated>
    <author>
      <name>/u/Important-Novel1546</name>
      <uri>https://old.reddit.com/user/Important-Novel1546</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm trying to make a LLM chatbot for my company and want to potentially switch the LLM models if there are cheaper or better options. And i want to understand how to benchmark different LLMs on my language. I don't understand if i have to translate benchmark tools to run it on the LLM or benchmark the capabilities of the LLM on my language and just run the benchmark tools as is etc. Any direction is needed. I'm sorry if i come off as lazy, but i wanted to ask for professional opinions before diving into the research. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Novel1546"&gt; /u/Important-Novel1546 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irce17/benchmarking_for_specific_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irce17/benchmarking_for_specific_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irce17/benchmarking_for_specific_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T05:16:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4724</id>
    <title>Which draft model for Mistral Small 2501?</title>
    <updated>2025-02-16T22:15:00+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't seem to find one that's compatible according to LMStudio. I'd really like an MLX version, but a GGUF one would work too.&lt;/p&gt; &lt;p&gt;I want to try out Mistral for a bit and see how it performs compared to Qwen 14b, which I'm currently using. Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T22:15:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir43w5</id>
    <title>PLA shroud? Check. String supports? Check. 3x3090 on a budget.</title>
    <updated>2025-02-16T22:11:13+00:00</updated>
    <author>
      <name>/u/TyraVex</name>
      <uri>https://old.reddit.com/user/TyraVex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"&gt; &lt;img alt="PLA shroud? Check. String supports? Check. 3x3090 on a budget." src="https://a.thumbs.redditmedia.com/t5TlvfN7T0vbSmWFtVxNbBX-yzNFAYVWNo0jREYVLq4.jpg" title="PLA shroud? Check. String supports? Check. 3x3090 on a budget." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TyraVex"&gt; /u/TyraVex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ir43w5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T22:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvmba</id>
    <title>The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig</title>
    <updated>2025-02-16T16:14:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt; &lt;img alt="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" src="https://b.thumbs.redditmedia.com/gFgeC1rJT-bzG1Ahait3F528S102s2yDDa6EevYG4iI.jpg" title="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve wanted to build a quad 3090 server for llama.cpp/Open WebUI for a while now, but massive shrouds really hampered those efforts. There are very few blower style RTX 3090 out there. They typically cost more than RTX 4090. Experimentation with DeepSeek makes the thought of loading all those weights via x1 risers a nightmare. Already suffering with native x1 on CMP 100-210 trying to offload DeepSeek weights to 6 GPUs.&lt;/p&gt; &lt;p&gt;Also thinking with some systems with 7-8 x16 lane support, upto 32gpu on x4 is entirely possible. DeepSeek fp8 fully GPU powered on a ~$30k retail mostly build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iqvmba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T16:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1gh</id>
    <title>Why we don't use RXs 7600 XT?</title>
    <updated>2025-02-16T09:57:52+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This GPU has probably cheapest VRAM out there. $330 for 16gb is crazy value, but most people use RTXs 3090 which cost ~$700 on a used market and draw significantly more power. I know that RTXs are better for other tasks, but as far as I know, only important thing in running LLMs is VRAM, especially capacity. Or there's something I don't know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ircn4e</id>
    <title>Translation-Only models?</title>
    <updated>2025-02-17T05:31:43+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on a meme project where I need to translate strings from a game into several languages. The tool I initially planned to use, LibreTranslate, has given me countless headaches and alike... let alone that it's underlying dependency, Argos Translate, insisted on skipping my GPU.&lt;/p&gt; &lt;p&gt;So I wonder, are there LLMs trained specifically for translation? I have a 4090 and am just looking for options/alternatives to Argos/Libretranslate.&lt;/p&gt; &lt;p&gt;I also looked into just consuming Bing, Google and DeepL APIs, but ... the amount of text would blow right through most of their free plan in but a single go. x) Hence, local is my best bet, and using staged pipelining with persistence to strech it over days... it'll probably take weeks. But, I do need some translation thing for that...maybe an LLM can help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ircn4e/translationonly_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ircn4e/translationonly_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ircn4e/translationonly_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T05:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqysmn</id>
    <title>Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more</title>
    <updated>2025-02-16T18:27:29+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"&gt; &lt;img alt="Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more" src="https://preview.redd.it/lbzg9mvtnjje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62d7841f479d8e3b864893f1c84ad8bff30ec3ca" title="Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The attached image isn't from my setup specifically, but is the exact same issue. I have 3x 3090s in this machine.&lt;/p&gt; &lt;p&gt;Whenever I add the third card, one has this error and completely stops working for inference. It also usually triggers when I load a model.&lt;/p&gt; &lt;p&gt;After shuffling the cards around, I originally thought I had a bad PCIE slot (there are 8 on the Asrock Romed 8 2T) but no matter what I did, no matter what slot I picked or what GPU I had in it, there would always be one that failed. &lt;/p&gt; &lt;p&gt;I tested each GPU independently and it had no issues with any of them, reinstalled Ubuntu server, changed several driver versions etc. Nothing has worked yet. My power supply is 1600W and should have no issues delivering power to each card.&lt;/p&gt; &lt;p&gt;Has anyone had this issue? I'm out of ideas and so is the Internet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbzg9mvtnjje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp2dd</id>
    <title>I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B</title>
    <updated>2025-02-16T09:59:45+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt; &lt;img alt="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" src="https://external-preview.redd.it/dXNvamJ5ZjQ0aGplMYWu7AlJhgav8Lym1d_sC2ZIykYlrs6Ptnxf7yQvwsov.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8bdc25d398763f3bc1abe33281d773d4310de4" title="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ro798yf44hje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqugti</id>
    <title>Kernel refinement via LLM</title>
    <updated>2025-02-16T15:22:39+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt; &lt;img alt="Kernel refinement via LLM" src="https://external-preview.redd.it/Ud40tCnkvgrTgsbDjMTILGOg7G9SqNJ-hrdg_SDxvWo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b1994afccbdb6a19230f6779589edce8ca823a5" title="Kernel refinement via LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly haven't seen this mentioned as of yet. Has left me quite speechless tbh. &lt;/p&gt; &lt;p&gt;&amp;quot;This closed-loop approach makes the code generation process better by guiding it in a different way each time. The team found that by letting this process continue for 15 minutes resulted in an improved attention kernel. &lt;/p&gt; &lt;p&gt;A bar chart showing averaged attention kernel speedup on Hopper GPU, compares the speedup of different attention kernel types between two approaches: 'PyTorch API (Flex Attention)' in orange and 'NVIDIA Workflow with DeepSeek-R1' in green. The PyTorch API maintains a baseline of 1x for all kernels, while the NVIDIA Workflow with DeepSeek-R1 achieves speedups of 1.1x for Causal Mask and Document Mask, 1.5x for Relative Positional, 1.6x for Alibi Bias and Full Mask, and 2.1x for Softcap. Figure 3. Performance of automatically generated optimized attention kernels with flex attention This workflow produced numerically correct kernels for 100% of Level-1 problems and 96% of Level-2 problems, as tested by Stanford’s KernelBench benchmark. ‌&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1g6k</id>
    <title>Real AGI is an AI that learns on the fly , has long term memory and other features</title>
    <updated>2025-02-16T20:17:20+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People have been defining AGI as an AI that does all tasks a human can do, but even humans can't do every human task at the same level as an average human worker in that field. Instead humans can learn continously and on the fly to do a task and it takes time to be an expert. We shouldn't expect an AGI to do all tasks in the beginning, but rather it has long term and short term memory to learn to do different tasks and remember it for a later time. Unimportant tasks can be forgotten after the task is completed like short term memory. AGI might need to rest like humans to train and consolidate recently learned tasks into long term memory and to improve its abilities. It should be able to learn by itself constantly and discover how to improve itself. Learning to do many tasks is a corollary of being AGI IE having continuous learning abilities and long term memory, not a prerequisite for AGI. In addition, a human has crystal and fluid intelligence, the abilties to solve problems through memorized patterns and adaptive learning from minimal examples. Right now, AI has some crystalized intelligence via trained datasets and applying reasoning through learned examples. But AI's fluid intelligence is weak, their ability to learn and solve problems with minimal examples is weak. Kinesthetic intelligence is an intelligence too, this requires simulation(like Cosmos) and training in the real world to develop...Humans have social intelligence too.. What about AGI having the ability communicating and guiding other ais? Humans have various desires and drives and an instinct to preserve itself and to reproduce... Likely a human, an AGI should have desires. And perhaps the desire for self-preservation and the ability to self-reproduce or produce a newer better copy.(maybe optional) Also, a human has 150 trillion synapses in the size of a grapefruit and portable, whereas AI requires multiple large stationary GPUS. An ideal AGI should be human brain sized or at least no bigger than an Elephant's brain. This AGI size would require neuromorphic memristors or biocomputing or a compact quantum computer or something else. Humans have a brain stem and a secondary nervous system for automated bodily functions and instincts... Perhaps AGI should also have that...&lt;/p&gt; &lt;p&gt;Being an expert at something is not necessary for AGI as majority of humans aren't experts at anything except for a few... Even those who are experts have specialized domain knowledge. AGi should be average in a few fields at the start then learn to become an expert in various fields.&lt;/p&gt; &lt;p&gt;In conclusion, a true AGI should have these features above.( The most important is the ability and the desire to learn on the fly and continuously and have long term memory!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T20:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqynut</id>
    <title>Audiobook Creator - Releasing Version 2</title>
    <updated>2025-02-16T18:21:51+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Followup to my original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm releasing a version 2 of my open source project with cool new features !&lt;/p&gt; &lt;p&gt;Checkout sample multi voice audio for a short story : &lt;a href="https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook"&gt;https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Added Key Features&lt;/strong&gt;:&lt;br /&gt; ✅ &lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;br /&gt; ✅ &lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text. Uses calibre for better formatted text and wider compatibility.&lt;br /&gt; ✅ &lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B. Uses ffmpeg for wider format support. &lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Better narration&lt;/strong&gt;: Reads out only the dialogue in a different voice instead of the entire line in that voice. Also, improves single voice narration with a different dialogue voice from the narrator's voice. &lt;/p&gt; &lt;p&gt;✅ Automatically identifies chapters and adds some silence on audio end to mark its ending.&lt;/p&gt; &lt;p&gt;✅ Improved instructions and prompting while running the scripts for better clarity.&lt;/p&gt; &lt;p&gt;Github Repo Link: &lt;a href="https://github.com/prakharsr/audiobook-creator/"&gt;https://github.com/prakharsr/audiobook-creator/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try out the sample M4B audiobook with cover, chapter timestamps and metadata: &lt;a href="https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b"&gt;https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More new features coming soon !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbke1</id>
    <title>[New Benchmark] OptiLLMBench: Test how optimization tricks can boost your models at inference time!</title>
    <updated>2025-02-17T04:28:18+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt; &lt;p&gt;I'm excited to share OptiLLMBench, a new benchmark specifically designed to test how different inference optimization techniques (like ReRead, Chain-of-Thought, etc.) can improve LLM performance without any fine-tuning.&lt;/p&gt; &lt;p&gt;First results with Gemini 2.0 Flash show promising improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ReRead (RE2): +5% accuracy while being ~14% faster&lt;/li&gt; &lt;li&gt;Chain-of-Thought Reflection: +5% boost&lt;/li&gt; &lt;li&gt;Base performance: 51%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The benchmark tests models across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GSM8K math word problems&lt;/li&gt; &lt;li&gt;MMLU Math&lt;/li&gt; &lt;li&gt;AQUA-RAT logical reasoning&lt;/li&gt; &lt;li&gt;BoolQ yes/no questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;These optimization techniques work with ANY model&lt;/li&gt; &lt;li&gt;They can help squeeze better performance out of models without training&lt;/li&gt; &lt;li&gt;Some techniques (like RE2) actually run faster than base inference&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you're interested in trying it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dataset: &lt;a href="https://huggingface.co/datasets/codelion/optillmbench"&gt;https://huggingface.co/datasets/codelion/optillmbench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/codelion/optillm"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to see results from different models and how they compare. Share your findings! 🔬&lt;/p&gt; &lt;p&gt;Edit: The benchmark and the approach is completely open source. Feel free to try it with any model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:28:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1irad4h</id>
    <title>Distributed DeepSeek R1 inference?</title>
    <updated>2025-02-17T03:20:47+00:00</updated>
    <author>
      <name>/u/CalangoVelho</name>
      <uri>https://old.reddit.com/user/CalangoVelho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have about 12 GPUs (RTXs 2080 to 3090) located in different machines (each with 1-2 GPUs), mostly Windows hosts. I was wondering if it would be possible to run DeepSeek 671b (not the dist. or quant versions) by having it distributed across those hosts. Did anyone had luck doing something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CalangoVelho"&gt; /u/CalangoVelho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T03:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ircs2n</id>
    <title>Consistency in output is sometimes underrated</title>
    <updated>2025-02-17T05:40:30+00:00</updated>
    <author>
      <name>/u/FiacR</name>
      <uri>https://old.reddit.com/user/FiacR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ircs2n/consistency_in_output_is_sometimes_underrated/"&gt; &lt;img alt="Consistency in output is sometimes underrated" src="https://preview.redd.it/0jy4bifwzmje1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b67c4d5c3bf42b138c2b19671245897b0e97f74" title="Consistency in output is sometimes underrated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FiacR"&gt; /u/FiacR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0jy4bifwzmje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ircs2n/consistency_in_output_is_sometimes_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ircs2n/consistency_in_output_is_sometimes_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T05:40:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir362v</id>
    <title>Jankiest &lt;$1000 70B IQ3_M 8192ctx setup ever</title>
    <updated>2025-02-16T21:30:31+00:00</updated>
    <author>
      <name>/u/spokale</name>
      <uri>https://old.reddit.com/user/spokale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt; &lt;img alt="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" src="https://b.thumbs.redditmedia.com/44X6DS_-957GXg-I7vPFOA1akYTRMttqhRfJ-mtdpgk.jpg" title="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokale"&gt; /u/spokale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ir362v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtfy9</id>
    <title>Just a bunch of H100s required</title>
    <updated>2025-02-16T14:33:57+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt; &lt;img alt="Just a bunch of H100s required" src="https://b.thumbs.redditmedia.com/CMhWZRD3a6zl90Wagyddf6mqUWPT3h6kxFjzeLVrOCc.jpg" title="Just a bunch of H100s required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6"&gt;https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv5s0</id>
    <title>Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC.</title>
    <updated>2025-02-16T15:54:42+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt; &lt;img alt="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." src="https://preview.redd.it/asmx7nh0wije1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05a245ed92468ec7ad3869e7776b9c2d8b8e5f63" title="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asmx7nh0wije1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir6ha6</id>
    <title>DeepSeek-R1 CPU-only performances (671B , Unsloth 2.51bit, UD-Q2_K_XL)</title>
    <updated>2025-02-17T00:00:53+00:00</updated>
    <author>
      <name>/u/smflx</name>
      <uri>https://old.reddit.com/user/smflx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of us here like to run locally DeepSeek R1 (671B, not distill). Thanks to MoE nature of DeepSeek, CPU inference looks promising.&lt;/p&gt; &lt;p&gt;I'm testing on CPUs I have. Not completed yet, but would like to share &amp;amp; hear about other CPUs too.&lt;/p&gt; &lt;p&gt;Xeon w5-3435X has 195GB/s memory bandwidth (measured by stream)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Function Best Rate MB/s Avg time Copy: 195455.5 0.082330 Scale: 161245.0 0.100906 Add: 183597.3 0.131566 Triad: 181895.4 0.132163 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The active parameter of R1/V2 is 37B. So if Q4 used, theoretically 195 / 37 * 2 = 10.5 tok/s is possible.&lt;/p&gt; &lt;p&gt;Unsloth provided great quantizations from 1.58 ~ 2.51 bit. The generation speed could be more or less. (Actually less yet)&lt;/p&gt; &lt;p&gt;&lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tested both of 1.58 bit &amp;amp; 2.51 bit on few CPUs, now I stick to 2.51 bit. 2.51bit is better quality, surprisingly faster too.&lt;/p&gt; &lt;p&gt;I got 4.86 tok/s with 2.51bit, while 3.27 tok/s with 1.58bit, on Xeon w5-3435X (1570 total tokens). Also, 3.53 tok/s with 2.51bit, while 2.28 tok/s with 1.58bit, on TR pro 5955wx.&lt;/p&gt; &lt;p&gt;It means compute performance of CPU matters too, and slower with 1.58bit. So, use 2.51bit unless you don't have enough RAM. 256G RAM was enough to run 2.51 bit.&lt;/p&gt; &lt;p&gt;I have tested generation speed with llama.cpp using (1) prompt &amp;quot;hi&amp;quot;, and (2) &amp;quot;Write a python program to print the prime numbers under 100&amp;quot;. Number of tokens generated were (1) about 100, (2) 1500~5000.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama.cpp/build/bin/llama-cli --model DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf --cache-type-k q4_0 --threads 16 --prio 2 --temp 0.6 --ctx-size 8192 --seed 3407&lt;/code&gt;&lt;/p&gt; &lt;p&gt;OK, here is Table.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Cores (CCD)&lt;/th&gt; &lt;th align="left"&gt;COPY (GB/s)&lt;/th&gt; &lt;th align="left"&gt;TRIAD (GB/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;hi&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp&amp;quot;coding&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;kTransformer (tok/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;w5-3435X&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;195&lt;/td&gt; &lt;td align="left"&gt;181&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;4.86&lt;/td&gt; &lt;td align="left"&gt;8.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5955wx&lt;/td&gt; &lt;td align="left"&gt;16 (2)&lt;/td&gt; &lt;td align="left"&gt;96&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;4.29&lt;/td&gt; &lt;td align="left"&gt;3.53&lt;/td&gt; &lt;td align="left"&gt;7.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7F32&lt;/td&gt; &lt;td align="left"&gt;8 (8)&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;86&lt;/td&gt; &lt;td align="left"&gt;3.39&lt;/td&gt; &lt;td align="left"&gt;3.24&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9184X&lt;/td&gt; &lt;td align="left"&gt;16 (8)&lt;/td&gt; &lt;td align="left"&gt;298&lt;/td&gt; &lt;td align="left"&gt;261&lt;/td&gt; &lt;td align="left"&gt;7.52&lt;/td&gt; &lt;td align="left"&gt;4.82&lt;/td&gt; &lt;td align="left"&gt;11.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9534&lt;/td&gt; &lt;td align="left"&gt;64 (8)&lt;/td&gt; &lt;td align="left"&gt;351&lt;/td&gt; &lt;td align="left"&gt;276&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;7.26&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6426Y (2P)&lt;/td&gt; &lt;td align="left"&gt;16+16&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;coming...&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I expected a poor performance of 5955wx, because it has only two CCDs. We can see low memory bandwidth in the table. But, not much difference of performance compared to w5-3435X. Perhaps, compute matters too &amp;amp; memory bandwidth is not saturated in Xeon w5-3435X.&lt;/p&gt; &lt;p&gt;I have checked performance of kTransformer too. It's CPU inference with 1 GPU for compute bound process. While is not pure CPU inference, the performance gain is almost 2x. I didn't tested for all CPU yet, you can assume 2x performance to CPU-only llama.cpp.&lt;/p&gt; &lt;p&gt;With kTransformer, GPU was not saturated but CPU was all busy. I guess one 3090 or 4090 will be enough. One downside of kTransformer is that the context length is limited by VRAM.&lt;/p&gt; &lt;p&gt;The blanks are &amp;quot;not tested yet&amp;quot;. It takes time... Well, I'm testing two Genoa CPUs with only one mainboard.&lt;/p&gt; &lt;p&gt;I would like to hear about other CPUs. Maybe, I will update the table.&lt;/p&gt; &lt;p&gt;Note: I will update &amp;quot;how I checked memory bandwidth using stream&amp;quot;, if you want to check with the same setup. I couldn't get the memory bandwidth numbers I have seen here. My test numbers are lower.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 1) STREAM memory bandwidth benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jeffhammond/STREAM/blob/master/stream.c"&gt;https://github.com/jeffhammond/STREAM/blob/master/stream.c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gcc -Ofast -fopenmp -DSTREAM_ARRAY_SIZE=1000000000 -DSTREAM_TYPE=double -mcmodel=large stream.c -o stream&lt;/p&gt; &lt;p&gt;gcc -march=znver4 -march=native -Ofast -fopenmp -DSTREAM_ARRAY_SIZE=1000000000 -DSTREAM_TYPE=double -mcmodel=large stream.c -o stream (for Genoa, but it seems not different)&lt;/p&gt; &lt;p&gt;I have compiled stream.c with a big array size. Total memory required = 22888.2 MiB (= 22.4 GiB).&lt;/p&gt; &lt;p&gt;If somebody know about how to get STREAM benchmark score about 400GB TRIAD, please let me know. I couldn't get such number.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smflx"&gt; /u/smflx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T00:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbtc4</id>
    <title>ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models</title>
    <updated>2025-02-17T04:42:47+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.09696"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpzpk</id>
    <title>8x RTX 3090 open rig</title>
    <updated>2025-02-16T11:04:58+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt; &lt;img alt="8x RTX 3090 open rig" src="https://preview.redd.it/sx3t2omvghje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8156846c180a3c1bdf1f4c1dceba69bdbf7a6a6" title="8x RTX 3090 open rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The whole length is about 65 cm. Two PSUs 1600W and 2000W 8x RTX 3090, all repasted with copper pads Amd epyc 7th gen 512 gb ram Supermicro mobo&lt;/p&gt; &lt;p&gt;Had to design and 3D print a few things. To raise the GPUs so they wouldn't touch the heatsink of the cpu or PSU. It's not a bug, it's a feature, the airflow is better! Temperatures are maximum at 80C when full load and the fans don't even run full speed.&lt;/p&gt; &lt;p&gt;4 cards connected with risers and 4 with oculink. So far the oculink connection is better, but I am not sure if it's optimal. Only pcie 4x connection to each. &lt;/p&gt; &lt;p&gt;Maybe SlimSAS for all of them would be better? &lt;/p&gt; &lt;p&gt;It runs 70B models very fast. Training is very slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sx3t2omvghje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T11:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir3rsl</id>
    <title>Inference speed of a 5090.</title>
    <updated>2025-02-16T21:56:45+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've rented the 5090 on vast and ran my benchmarks (I'll probably have to make a new bech test with more current models but I don't want to rerun all benchs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 5090 is &amp;quot;only&amp;quot; 50% faster in inference than the 4090 (a much better gain than it got in gaming)&lt;/p&gt; &lt;p&gt;I've noticed that the inference gains are almost proportional to the ram speed till the speed is &amp;lt;1000 GB/s then the gain is reduced. Probably at 2TB/s the inference become GPU limited while when speed is &amp;lt;1TB it is vram limited.&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:56:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir9mcw</id>
    <title>Today I am launching OpenArc, a python serving API for faster inference on Intel CPUs, GPUs and NPUs. Low level, minimal dependencies and comes with the first GUI tools for model conversion.</title>
    <updated>2025-02-17T02:40:18+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, a lightweight inference engine built using Optimum-Intel from Transformers to leverage hardware acceleration on Intel devices. &lt;/p&gt; &lt;p&gt;Here are some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strongly typed API with four endpoints&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;/model/load: loads model and accepts ov_config&lt;/li&gt; &lt;li&gt;/model/unload: use gc to purge a loaded model from device memory&lt;/li&gt; &lt;li&gt;/generate/text: synchronous execution, select sampling parameters, token limits : also returns a performance report&lt;/li&gt; &lt;li&gt;/status: see the loaded model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Each endpoint has a pydantic model keeping exposed parameters easy to maintain or extend.&lt;/li&gt; &lt;li&gt;Native chat templates&lt;/li&gt; &lt;li&gt;Conda environment.yaml for portability with a proper .toml coming soon&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Audience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Owners of Intel accelerators&lt;/li&gt; &lt;li&gt;Those with access to high or low end CPU only servers&lt;/li&gt; &lt;li&gt;Edge devices with Intel chips&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OpenArc is my first open source project representing months of work with OpenVINO and Intel devices for AI/ML. Developers and engineers who work with OpenVINO/Transformers/IPEX-LLM will find it's syntax, tooling and documentation complete; new users should find it more approachable than the documentation available from Intel, including the mighty [openvino_notebooks](&lt;a href="https://github.com/openvinotoolkit/openvino%5C_notebooks"&gt;https://github.com/openvinotoolkit/openvino\_notebooks&lt;/a&gt;) which I cannot recommend enough.&lt;/p&gt; &lt;p&gt;My philosophy with OpenArc has been to make the project as low level as possible to promote access to the heart and soul of OpenArc, the conversation object. This is where the chat history lives 'traditionally'; in practice this enables all sorts of different strategies for context management that make more sense for agentic usecases, though OpenArc is low level enough to support many different usecases.&lt;/p&gt; &lt;p&gt;For example, a model you intend to use for a search task might not need a context window larger than 4k tokens; thus, you can store facts from the smaller agents results somewhere else, catalog findings, purge the conversation from conversation and an unbiased small agent tackling a fresh directive from a manager model can be performant with low context. &lt;/p&gt; &lt;p&gt;If we zoom out and think about how the code required for iterative search, database access, reading dataframes, doing NLP or generating synthetic data should be built- at least to me- inference code has no place in such a pipeline. OpenArc promotes API call design patterns for interfacing with LLMs locally that OpenVINO has lacked until now. Other serving platforms/projects have OpenVINO as a plugin or extension but none are dedicated to it's finer details, and fewer have quality documentation regarding the design of solutions that require deep optimization available from OpenVINO.&lt;/p&gt; &lt;p&gt;Coming soon;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Openai proxy&lt;/li&gt; &lt;li&gt;More OV_config documentation. It's quite complex!&lt;/li&gt; &lt;li&gt;docker compose examples&lt;/li&gt; &lt;li&gt;Multi GPU execution- I havent been able to get this working due to driver issues maybe, but as of now OpenArc fully supports it and models at my hf repo linked on git with the &amp;quot;-ns&amp;quot; suffix should work. It's a hard topic and requires more testing before I can document.&lt;/li&gt; &lt;li&gt;Benchmarks and benchmarking scripts&lt;/li&gt; &lt;li&gt;Load multiple models into memory and onto different devices&lt;/li&gt; &lt;li&gt;a Panel dashboard for managing OpenArc&lt;/li&gt; &lt;li&gt;Autogen and smolagents examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking out my project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T02:40:18+00:00</published>
  </entry>
</feed>
