<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-14T17:37:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mq4tnu</id>
    <title>No more guessing the best hyperparameters for fine-tuning</title>
    <updated>2025-08-14T16:03:52+00:00</updated>
    <author>
      <name>/u/OriginalSpread3100</name>
      <uri>https://old.reddit.com/user/OriginalSpread3100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq4tnu/no_more_guessing_the_best_hyperparameters_for/"&gt; &lt;img alt="No more guessing the best hyperparameters for fine-tuning" src="https://external-preview.redd.it/UcoYz3kN5H8FNJluKnJGIPKd6bmeNh87Yuw2bnUR0ZM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b8453ff0c2e29c7579ae4ca8c9a0496b349a52d" title="No more guessing the best hyperparameters for fine-tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/k8tebeiz90jf1.gif"&gt;https://i.redd.it/k8tebeiz90jf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We added a sweeps feature to Transformer Lab that helps with hyperparameter optimization for local model training. Feel free to try it and let us know if it‚Äôs helpful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of manually adjusting learning rates, batch sizes, etc. one at a time, you give Transformer Lab a set of values and let it explore systematically. The visualization makes it easy to see which configs actually improved performance. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Works with:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Most local setups (CUDA, ROCm, MLX) &lt;br /&gt; - Popular model architectures for fine-tuning &lt;br /&gt; - Any size model your hardware can handle&lt;br /&gt; - Open source (AGPL-3.0)&lt;/p&gt; &lt;p&gt;We built this because hyperparameter tuning was taking up too much manual effort. It's easy to miss good configurations when testing one by one. Best of all, it‚Äôs open source (AGPL-3.0).&lt;/p&gt; &lt;p&gt;Is this helpful? We‚Äôd love your feedback on how we can improve.&lt;br /&gt; üîó Try it here ‚Üí&lt;a href="https://transformerlab.ai/docs/intro"&gt; &lt;/a&gt;&lt;a href="https://transformerlab.ai/"&gt;transformerlab.ai&lt;/a&gt;&lt;br /&gt; üîó Useful? Give us a star on GitHub ‚Üí &lt;a href="https://github.com/transformerlab"&gt;github.com/transformerlab/transformerlab-app&lt;/a&gt;&lt;br /&gt; üîó Ask for help from our Discord Community ‚Üí &lt;a href="https://discord.gg/transformerlab"&gt;discord.gg/transformerlab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalSpread3100"&gt; /u/OriginalSpread3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq4tnu/no_more_guessing_the_best_hyperparameters_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq4tnu/no_more_guessing_the_best_hyperparameters_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq4tnu/no_more_guessing_the_best_hyperparameters_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5cwq</id>
    <title>So what is Neuro-sama (AI VTuber) built with?</title>
    <updated>2025-08-14T16:23:07+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep running into shorts of her and the fact that she replies so fast &lt;em&gt;and&lt;/em&gt; has a TTS &lt;em&gt;and&lt;/em&gt; has a model just got me wondering how she can do that. Like, how is this so obscenely fast o.o&lt;/p&gt; &lt;p&gt;Anyone happen to know how she's made?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5cwq/so_what_is_neurosama_ai_vtuber_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2tc5</id>
    <title>First Look: Our work on ‚ÄúOne-Shot CFT‚Äù ‚Äî 24√ó Faster LLM Reasoning Training with Single-Example Fine-Tuning</title>
    <updated>2025-08-14T14:50:09+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2tc5/first_look_our_work_on_oneshot_cft_24_faster_llm/"&gt; &lt;img alt="First Look: Our work on ‚ÄúOne-Shot CFT‚Äù ‚Äî 24√ó Faster LLM Reasoning Training with Single-Example Fine-Tuning" src="https://b.thumbs.redditmedia.com/8Xx0avgASrXS-xM-LdBk2CrBH6CEhvnjTqiivWTk-Jo.jpg" title="First Look: Our work on ‚ÄúOne-Shot CFT‚Äù ‚Äî 24√ó Faster LLM Reasoning Training with Single-Example Fine-Tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;First look at our latest collaboration with the&lt;/em&gt; &lt;a href="https://wenhuchen.github.io/lab"&gt;&lt;strong&gt;&lt;em&gt;University of Waterloo‚Äôs TIGER Lab&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;on a new approach to boost LLM reasoning post-training:&lt;/em&gt; &lt;strong&gt;One-Shot CFT (Critique Fine-Tuning)&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it worksÔºö&lt;/strong&gt;This approach uses &lt;strong&gt;20√ó less compute and just one piece of feedback&lt;/strong&gt;, yet still reaches SOTA accuracy ‚Äî unlike typical methods such as Supervised Fine-Tuning (SFT) that rely on thousands of examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs a game-changer:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;+15% math reasoning gain and +16% logic reasoning gain vs base models&lt;/li&gt; &lt;li&gt;Achieves peak accuracy in &lt;strong&gt;5 GPU hours&lt;/strong&gt; vs 120 GPU hours for RLVR, makes LLM reasoning training 24√ó Faster&lt;/li&gt; &lt;li&gt;Scales across 1.5B to 14B parameter models with consistent gains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results for Math and Logic Reasoning Gains:&lt;/strong&gt;&lt;br /&gt; Mathematical Reasoning and Logic Reasoning show large improvements over SFT and RL baselines &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results for Training efficiency:&lt;/strong&gt;&lt;br /&gt; One-Shot CFT hits peak accuracy in 5 GPU hours ‚Äî RLVR takes 120 GPU hoursWe‚Äôve summarized the core insights and experiment results. For full technical details, read: &lt;a href="https://blog.netmind.ai/article/QbitAI_Spotlights_TIGER_Lab%E2%80%99s_One-Shot_CFT_%E2%80%94_24%C3%97_Faster_AI_Training_to_Top_Accuracy%2C_Backed_by_NetMind_%26_other_collaborators?utm_source=Reddit&amp;amp;utm_medium=Organic+Social&amp;amp;utm_campaign=AI+Insight+Blog"&gt;QbitAI Spotlights TIGER Lab‚Äôs One-Shot CFT ‚Äî 24√ó Faster AI Training to Top Accuracy, Backed by NetMind &amp;amp; other collaborators&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We are also immensely grateful to the brilliant authors ‚Äî including Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, and Wenhu Chen ‚Äî whose expertise and dedication made this achievement possible. &lt;/p&gt; &lt;p&gt;What do you think ‚Äî could critique-based fine-tuning become the new default for cost-efficient LLM reasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mq2tc5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2tc5/first_look_our_work_on_oneshot_cft_24_faster_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2tc5/first_look_our_work_on_oneshot_cft_24_faster_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpm8kr</id>
    <title>ERNIE 4.5 21BA3B appreciation post.</title>
    <updated>2025-08-14T00:55:45+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it's the best model of it's size, outshining gpt-oss 20 and qwen 3 30BA3B.&lt;/p&gt; &lt;p&gt;It's not as good at coding, but it runs without error even at decent context. I find the qwen a3b to be better for code gen, but prefer ernie for everythign else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T00:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpez1p</id>
    <title>Added locally generated dialogue + voice acting to my game!</title>
    <updated>2025-08-13T20:02:24+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt; &lt;img alt="Added locally generated dialogue + voice acting to my game!" src="https://external-preview.redd.it/N2szZGNtMzRldWlmMZmQp7O5BpjYg7UqegAgE9IdgP7TYx8Szh9dJVqIheQu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eccefbf13830c4a641bc7633ab5e9b01c2c86540" title="Added locally generated dialogue + voice acting to my game!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1qgim34euif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpz2ef</id>
    <title>REINFORCE++-baseline is all you need in RLVR</title>
    <updated>2025-08-14T12:20:05+00:00</updated>
    <author>
      <name>/u/seventh_day123</name>
      <uri>https://old.reddit.com/user/seventh_day123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What is REINFORCE++-baseline?&lt;/h1&gt; &lt;p&gt;In essence, REINFORCE++-baseline (detailed in &lt;a href="https://arxiv.org/abs/2501.03262"&gt;arXiv:2501.03262&lt;/a&gt;) replaces the critic network used in PPO with the group mean reward and applies global batch advantage normalization. The KL loss is computed using the unbiased K2 KL estimator. Because the global batch standard deviation is significantly more stable than the local group standard deviation used in GRPO, this approach enhances training stability.&lt;/p&gt; &lt;p&gt;More details are in &lt;a href="https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85"&gt;https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seventh_day123"&gt; /u/seventh_day123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpz2ef/reinforcebaseline_is_all_you_need_in_rlvr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T12:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5zjv</id>
    <title>GLM-4.1V-Thinking and GLM-4.5V</title>
    <updated>2025-08-14T16:45:58+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.01006"&gt;https://arxiv.org/pdf/2507.01006&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/zai_org/status/1956030993569341556?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqew3</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:16:29+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpqew3/pruned_gptoss_60b_kinda_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq540c</id>
    <title>promptcat: A zero-dependency prompt manager in a single HTML file</title>
    <updated>2025-08-14T16:14:08+00:00</updated>
    <author>
      <name>/u/seven_reasons</name>
      <uri>https://old.reddit.com/user/seven_reasons</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"&gt; &lt;img alt="promptcat: A zero-dependency prompt manager in a single HTML file" src="https://b.thumbs.redditmedia.com/OBDg7zqijdse5-ktq6w0c4oXeYE0oGFX0MTePB4-3Ac.jpg" title="promptcat: A zero-dependency prompt manager in a single HTML file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A private, offline-first prompt manager in a single, dependency-free HTML file. It stores all data locally in your browser's IndexedDB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Offline:&lt;/strong&gt; All data is stored in your browser's IndexedDB.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Dependencies:&lt;/strong&gt; Just pure, vanilla JavaScript, HTML, and CSS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong Encryption:&lt;/strong&gt; Optional AES-GCM encryption (via Web Crypto API) for individual prompts or entire folders. Your password is never stored.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powerful Organization:&lt;/strong&gt; Use folders, favorites, and tags to structure your library.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Global Tag Management:&lt;/strong&gt; Rename or delete tags across all prompts from a single interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Search:&lt;/strong&gt; Instantly find prompts with keyword highlighting and a context snippet.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Control:&lt;/strong&gt; Full import/export of your entire database, or just specific parts, to JSON.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fsevenreasons.github.io%2Fpromptcat%2F"&gt;https://sevenreasons.github.io/promptcat/&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fsevenreasons%2Fpromptcat"&gt;https://github.com/sevenreasons/promptcat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seven_reasons"&gt; /u/seven_reasons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mq540c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq540c/promptcat_a_zerodependency_prompt_manager_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5qw5</id>
    <title>üìå Learn how to build an LLM from scratch step by step(without the hype)üìå</title>
    <updated>2025-08-14T16:37:16+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb"&gt;https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the biggest challenges I faced when trying to build an LLM or even a smaller language model from scratch was that I jumped straight into building. Very quickly, I was overwhelmed by a flood of unfamiliar terms, including Mixture of Experts, dropout, and others. I‚Äôd lose interest, jump back and forth between resources, only for a new buzzword to pop up, and the same cycle would repeat.&lt;/p&gt; &lt;p&gt;So here‚Äôs what I followed: a longer path, but one that builds confidence step-by-step. If I told you I‚Äôve learned everything here, I‚Äôd be lying. I‚Äôm still learning every day,but I‚Äôm doing it with a lot more clarity and confidence than before.&lt;/p&gt; &lt;p&gt;Details are in the first and second comments.‚¨áÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mprwv9</id>
    <title>Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?</title>
    <updated>2025-08-14T05:38:43+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt; &lt;img alt="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" src="https://preview.redd.it/ydbnycjn8xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8bcb62748563efa5cb1f78789aa2cd8f3b2860a" title="Since Design Arena was released 6 weeks ago, DeepSeek R1-0528 has had a foothold in the top 5 despite being open source. How good do you think R2 will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's rumors that R2 is coming up sometime in the next month. It does feel that the release of the recent proprietary models have been a bit disappointing, given the marginal gains (e.g. on my &lt;a href="https://www.designarena.ai/"&gt;frontend benchmark&lt;/a&gt;, GPT-5, Opus 4, and 4.1 are basically equivalent though there's a small sample size for the new versions. &lt;/p&gt; &lt;p&gt;In terms of recent releases, open source and open weight models have been amazing. DeepSeek R1-0528 and Qwen3 Coder are #5 and #6 respectively, while GLM 4.5 is #9. &lt;/p&gt; &lt;p&gt;I'm am interested to see what happens with R2. My prediction is that it will basically match GPT-5 and Opus 4 (perhaps might even be a bit better) and we might see a moment similar to when DeepSeek R1 came out. &lt;/p&gt; &lt;p&gt;What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ydbnycjn8xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mprwv9/since_design_arena_was_released_6_weeks_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T05:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3j12</id>
    <title>[2508.09874] Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models</title>
    <updated>2025-08-14T15:16:08+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.09874"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3j12/250809874_memory_decoder_a_pretrained_plugandplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3j12/250809874_memory_decoder_a_pretrained_plugandplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:16:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq450p</id>
    <title>Goedel-Prover-V2: The Strongest Open-Source Theorem Prover to Date</title>
    <updated>2025-08-14T15:38:55+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.goedel-prover.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq450p/goedelproverv2_the_strongest_opensource_theorem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq450p/goedelproverv2_the_strongest_opensource_theorem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq47we</id>
    <title>GLM 4.5v - Anyone try the quants?</title>
    <updated>2025-08-14T15:41:56+00:00</updated>
    <author>
      <name>/u/Bohdanowicz</name>
      <uri>https://old.reddit.com/user/Bohdanowicz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/QuantTrio/GLM-4.5V-AWQ"&gt;https://huggingface.co/QuantTrio/GLM-4.5V-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or...&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit"&gt;https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Only 17-30B from a 100+B model?&lt;/p&gt; &lt;p&gt;Praying these aren't garbage. Potentially fit in 48GB vram with full context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bohdanowicz"&gt; /u/Bohdanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq47we/glm_45v_anyone_try_the_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mptvsl</id>
    <title>tencent/Hunyuan-GameCraft-1.0 ¬∑ Hugging Face</title>
    <updated>2025-08-14T07:32:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt; &lt;img alt="tencent/Hunyuan-GameCraft-1.0 ¬∑ Hugging Face" src="https://external-preview.redd.it/aPfnDoE4lStgbUiMQComf1wdLlqoQrdsgG6jn-2D3d8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47b9265890a5ca1272fc550cc59c1e4e8a3a0326" title="tencent/Hunyuan-GameCraft-1.0 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition&lt;/p&gt; &lt;p&gt;üìú Requirements An NVIDIA GPU with CUDA support is required. The model is tested on a machine with 8GPUs. Minimum: The minimum GPU memory required is 24GB but very slow. Recommended: We recommend using a GPU with 80GB of memory for better generation quality. Tested operating system: Linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-GameCraft-1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mptvsl/tencenthunyuangamecraft10_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1w1z</id>
    <title>Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed</title>
    <updated>2025-08-14T14:14:52+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt; &lt;img alt="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" src="https://preview.redd.it/fq1mo49ftzif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd94153bce02ae49a1589ce33689d2667aa1ecd" title="Lemonade v8.1.3: Redesigned web ui, Ryzen AI Strix in CI, and lots of community suggestions addressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.1.3"&gt;v8.1.3&lt;/a&gt; just released today as part of our ongoing sprint to implement the community's suggestions!&lt;/p&gt; &lt;p&gt;Lemonade lets you run local LLMs with high performance on your NPU or GPU, and the new release includes:&lt;/p&gt; &lt;p&gt;üíª Ryzen AI Strix Point self-hosted runners have been added to the CI system. - Allows contributors to test NPU-related features. &lt;/p&gt; &lt;p&gt;üìÉ A &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;detailed guide&lt;/a&gt; for how to use Lemonade with Continue.dev's IDE-based local coding assistant. - Very easy to get up and running thanks to the Continue Hub's one-click setup.&lt;/p&gt; &lt;p&gt;üß∞ Overhauled the web ui (see post's image). - 100% replaced the model manager with filters, load/unload, and install/delete buttons for each model. - The currently-loaded model is always displayed at the top, with a convenient eject button. - Selecting a model in the chat tab now loads it immediately.&lt;/p&gt; &lt;p&gt;üå°Ô∏è temperature, top_k, top_p, and repeat_penalty are supported in HTTP requests and the web app. - I know, we put this off way too long. - The PR that added these is a good blueprint for adding any more parameters people want.&lt;/p&gt; &lt;p&gt;üêç Added support for Python 3.11 and 3.13 (another community ask we put off too long).&lt;/p&gt; &lt;p&gt;üöÄ Community contributions: - Customize .exe behavior using environment variables, and added GLM-4.5-Air, by @tylerstraub - Update server startup message to include version number by @henrylearn2rock - Add .gitignore by @bsoyka (we were such noobs for not having this... thank you!)&lt;/p&gt; &lt;p&gt;ü§ñ What‚Äôs Coming Next: we have a very fun gaming side project in the works for Strix Halo and Radeon devices, stay tuned :)&lt;/p&gt; &lt;p&gt;If Lemonade has been useful to you, take a moment to add a star/issue on &lt;a href="http://github.com/lemonade-sdk/lemonade"&gt;Github&lt;/a&gt; and/or tell us about it in the &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;Discord&lt;/a&gt;. Feedback help others discover it and help us improve the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq1mo49ftzif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq1w1z/lemonade_v813_redesigned_web_ui_ryzen_ai_strix_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2k00</id>
    <title>[FEEDBACK] Better packaging for llama.cpp to support downstream consumers</title>
    <updated>2025-08-14T14:40:23+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's about time we build an easier UX for llama.cpp ü§ó&lt;/p&gt; &lt;p&gt;I've used llama.cpp for better part of last 2 years for playing with LLMs and use it in production too&lt;/p&gt; &lt;p&gt;Whilst it takes a bit to setup llama.cpp, once done, it *just* works! &lt;/p&gt; &lt;p&gt;Come along with your ideas/ solutions on how we can package it better, and make it easier for people to use and install llama.cpp with ease ‚ù§Ô∏è&lt;/p&gt; &lt;p&gt;Drop your ideas here on the discussion: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15313"&gt;https://github.com/ggml-org/llama.cpp/discussions/15313&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq2k00/feedback_better_packaging_for_llamacpp_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T14:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpuvok</id>
    <title>Qwen Coder 30bA3B harder... better... faster... stronger...</title>
    <updated>2025-08-14T08:33:51+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt; &lt;img alt="Qwen Coder 30bA3B harder... better... faster... stronger..." src="https://external-preview.redd.it/ZzJlajBibXkzeWlmMSKN9Y-F1uPgmObNpOLYQwn_bi3ofDf3vCkP-ziGE8lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f02def6a5e8cd415b1d862b2482b085e1338926" title="Qwen Coder 30bA3B harder... better... faster... stronger..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing around with 30b a3b to get tool calling up and running and I was bored in the CLI so I asked it to punch things up and make things more exciting... and this is what it spit out. I thought it was hilarious, so I thought I'd share :). Sorry about the lower quality video, I might upload a cleaner copy in 4k later.&lt;/p&gt; &lt;p&gt;This is all running off a single 24gb vram 4090. Each agent has its own 15,000 token context window independent of the others and can operate and handle tool calling at near 100% effectiveness.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mnpg8bmy3yif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpuvok/qwen_coder_30ba3b_harder_better_faster_stronger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T08:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpvije</id>
    <title>Swiss Canton Basel open sourced multiple tools for on-premise hosting of LLM services</title>
    <updated>2025-08-14T09:12:54+00:00</updated>
    <author>
      <name>/u/fabkosta</name>
      <uri>https://old.reddit.com/user/fabkosta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought this is worth sharing: The Swiss Canton of Basel has made available multiple tools they built for on-premise hosting of LLM-based services (text transcription, RAG, document conversion etc.). None of this is totally breaking news, but they did a solid job building an API plus frontend on top of all their services. And it's there entirely for free, using an MIT license, so everyone may re-use or extend the tools as they wish.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DCC-BS"&gt;https://github.com/DCC-BS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most of the services are relying on a combination of vLLM, Qwen3 32b, LlamaIndex, Python (FastAPI), and Whisper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabkosta"&gt; /u/fabkosta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpvije/swiss_canton_basel_open_sourced_multiple_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T09:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpr0nc</id>
    <title>Who are the 57 million people who downloaded bert last month?</title>
    <updated>2025-08-14T04:49:28+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt; &lt;img alt="Who are the 57 million people who downloaded bert last month?" src="https://preview.redd.it/vk2njmk01xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9a1c88826aae167a25ae0705a428dcb9f502529" title="Who are the 57 million people who downloaded bert last month?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vk2njmk01xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq19x6</id>
    <title>1 million context is the scam , the ai start hallucinating after the 90k . im using the qwen cli and its become trash after 10 percent context window used</title>
    <updated>2025-08-14T13:51:38+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is the major weakness ai have and they will never bring this on the benchmark , if u r working on the codebase the ai will work like a monster for the first 100k context aftert that its become the ass &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T13:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxumt</id>
    <title>MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200</title>
    <updated>2025-08-14T11:23:08+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt; &lt;img alt="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" src="https://external-preview.redd.it/3RGDYz9vGH8VQTfhA0sqrehkFc8q3f4WnHv1sjovwaY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51cffad7eff8e127873e566d22bc7c9880032b82" title="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-arc-pro-b60-dual-with-48gb-memory-reportedly-starts-shipping-next-week-priced-at-1200"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T11:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3v93</id>
    <title>google/gemma-3-270m ¬∑ Hugging Face</title>
    <updated>2025-08-14T15:28:38+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt; &lt;img alt="google/gemma-3-270m ¬∑ Hugging Face" src="https://external-preview.redd.it/ROrEGumvbqFvKi3ZHhPgoXOITTfGnht6t4Oyu75k6fA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3285cbdf5f0615c00193bd341ec39a493e68509d" title="google/gemma-3-270m ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/gemma-3-270m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
