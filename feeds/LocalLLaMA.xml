<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-06T14:06:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j4tkan</id>
    <title>What's the progress on models that can be used on phones ?</title>
    <updated>2025-03-06T11:39:21+00:00</updated>
    <author>
      <name>/u/Axelni98</name>
      <uri>https://old.reddit.com/user/Axelni98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's nice we can have powerful llms on PCs, but to get to the masses you need to have llm access on the phone. Therefore what's the current climate on those models? Are they still weak for retro fitting, and thus need a few years of new powerful phones ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Axelni98"&gt; /u/Axelni98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4tkan/whats_the_progress_on_models_that_can_be_used_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4tkan/whats_the_progress_on_models_that_can_be_used_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4tkan/whats_the_progress_on_models_that_can_be_used_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T11:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4lqe6</id>
    <title>Test if your api provider is quantizing your Qwen/QwQ-32B!</title>
    <updated>2025-03-06T02:59:41+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4lqe6/test_if_your_api_provider_is_quantizing_your/"&gt; &lt;img alt="Test if your api provider is quantizing your Qwen/QwQ-32B!" src="https://b.thumbs.redditmedia.com/oZAPVDU1tIkIs2FLJTksSwLbGBrV0g0VMVGu77KRBME.jpg" title="Test if your api provider is quantizing your Qwen/QwQ-32B!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone I'm the author of AlphaMaze&lt;/p&gt; &lt;p&gt;As you might have known, I have a deep obsession with LLM solving maze (previously &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iulq4o/we%5C_grpoed%5C_a%5C_15b%5C_model%5C_to%5C_test%5C_llm%5C_spatial/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iulq4o/we\_grpoed\_a\_15b\_model\_to\_test\_llm\_spatial/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Today after the release of &lt;strong&gt;QwQ-32B&lt;/strong&gt; I noticed that the model, is indeed, &lt;strong&gt;can solve maze&lt;/strong&gt; just like Deepseek-R1 (671B) but strangle it &lt;strong&gt;cannot solve maze on 4bit model (Q4 on llama.cpp).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is the test:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are a helpful assistant that solves mazes. You will be given a maze represented by a series of tokens.The tokens represent:- Coordinates: &amp;lt;|row-col|&amp;gt; (e.g., &amp;lt;|0-0|&amp;gt;, &amp;lt;|2-4|&amp;gt;)&lt;/p&gt; &lt;p&gt;- Walls: &amp;lt;|no_wall|&amp;gt;, &amp;lt;|up_wall|&amp;gt;, &amp;lt;|down_wall|&amp;gt;, &amp;lt;|left_wall|&amp;gt;, &amp;lt;|right_wall|&amp;gt;, &amp;lt;|up_down_wall|&amp;gt;, etc.&lt;/p&gt; &lt;p&gt;- Origin: &amp;lt;|origin|&amp;gt;&lt;/p&gt; &lt;p&gt;- Target: &amp;lt;|target|&amp;gt;&lt;/p&gt; &lt;p&gt;- Movement: &amp;lt;|up|&amp;gt;, &amp;lt;|down|&amp;gt;, &amp;lt;|left|&amp;gt;, &amp;lt;|right|&amp;gt;, &amp;lt;|blank|&amp;gt;&lt;/p&gt; &lt;p&gt;Your task is to output the sequence of movements (&amp;lt;|up|&amp;gt;, &amp;lt;|down|&amp;gt;, &amp;lt;|left|&amp;gt;, &amp;lt;|right|&amp;gt;) required to navigate from the origin to the target, based on the provided maze representation. Think step by step. At each step, predict only the next movement token. Output only the move tokens, separated by spaces.&lt;/p&gt; &lt;p&gt;MAZE:&lt;/p&gt; &lt;p&gt;&amp;lt;|0-0|&amp;gt;&amp;lt;|up_down_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|0-1|&amp;gt;&amp;lt;|up_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|0-2|&amp;gt;&amp;lt;|up_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|0-3|&amp;gt;&amp;lt;|up_down_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|0-4|&amp;gt;&amp;lt;|up_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|1-0|&amp;gt;&amp;lt;|up_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|1-1|&amp;gt;&amp;lt;|down_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|1-2|&amp;gt;&amp;lt;|left_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|1-3|&amp;gt;&amp;lt;|up_left_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|1-4|&amp;gt;&amp;lt;|left_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|2-0|&amp;gt;&amp;lt;|down_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|2-1|&amp;gt;&amp;lt;|up_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|2-2|&amp;gt;&amp;lt;|down_left_wall|&amp;gt;&amp;lt;|target|&amp;gt;&amp;lt;|2-3|&amp;gt;&amp;lt;|down_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|2-4|&amp;gt;&amp;lt;|left_right_wall|&amp;gt;&amp;lt;|origin|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|3-0|&amp;gt;&amp;lt;|up_left_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|3-1|&amp;gt;&amp;lt;|down_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|3-2|&amp;gt;&amp;lt;|up_down_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|3-3|&amp;gt;&amp;lt;|up_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|3-4|&amp;gt;&amp;lt;|left_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;|4-0|&amp;gt;&amp;lt;|down_left_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|4-1|&amp;gt;&amp;lt;|up_down_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|4-2|&amp;gt;&amp;lt;|up_down_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|4-3|&amp;gt;&amp;lt;|down_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&amp;lt;|4-4|&amp;gt;&amp;lt;|down_right_wall|&amp;gt;&amp;lt;|blank|&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here is the result:&lt;br /&gt; - Qwen Chat result&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3md9gsoyhzme1.png?width=1046&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfd00690b5ebfedf3b1ff0651c85f679dcb2b1ca"&gt;QWQ-32B full precision per qwen claimed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Open router chutes:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qu6vqcr4izme1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc715265ff652dc90e30f58746bec0fd3305a048"&gt;A little bit off, probably int8? but solution correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Llama.CPP Q4_0&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a28n84t9izme1.png?width=522&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=532c6ebff282327b6773b996db43314e3aaeee49"&gt;Hallucination forever on every try&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So if you are worried that your &lt;strong&gt;api provider is secretly quantizing your api endpoint&lt;/strong&gt; please try the above test to see if it in fact can solve the maze! For some reason the model is truly good, but with &lt;strong&gt;4bit quant&lt;/strong&gt;, it just can't solve the maze!&lt;/p&gt; &lt;p&gt;Can it solve the maze?&lt;/p&gt; &lt;p&gt;Get more maze at: &lt;a href="https://alphamaze.menlo.ai/"&gt;https://alphamaze.menlo.ai/&lt;/a&gt; by clicking on the randomize button&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4lqe6/test_if_your_api_provider_is_quantizing_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4lqe6/test_if_your_api_provider_is_quantizing_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4lqe6/test_if_your_api_provider_is_quantizing_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T02:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j417qh</id>
    <title>llama.cpp is all you need</title>
    <updated>2025-03-05T11:45:16+00:00</updated>
    <author>
      <name>/u/s-i-e-v-e</name>
      <uri>https://old.reddit.com/user/s-i-e-v-e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only started paying somewhat serious attention to locally-hosted LLMs earlier this year.&lt;/p&gt; &lt;p&gt;Went with ollama first. Used it for a while. Found out by accident that it is using llama.cpp. Decided to make life difficult by trying to compile the llama.cpp ROCm backend from source on Linux for a somewhat unsupported AMD card. Did not work. Gave up and went back to ollama.&lt;/p&gt; &lt;p&gt;Built a simple story writing helper cli tool for myself based on file includes to simplify lore management. Added ollama API support to it.&lt;/p&gt; &lt;p&gt;ollama randomly started to use CPU for inference while &lt;code&gt;ollama ps&lt;/code&gt; claimed that the GPU was being used. Decided to look for alternatives.&lt;/p&gt; &lt;p&gt;Found koboldcpp. Tried the same ROCm compilation thing. Did not work. Decided to run the regular version. To my surprise, it worked. Found that it was using vulkan. Did this for a couple of weeks.&lt;/p&gt; &lt;p&gt;Decided to try llama.cpp again, but the vulkan version. And it worked!!!&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; gives you a clean and extremely competent web-ui. Also provides an API endpoint (including an OpenAI compatible one). llama.cpp comes with a million other tools and is extremely tunable. You do not have to wait for other dependent applications to expose this functionality.&lt;/p&gt; &lt;p&gt;llama.cpp is all you need.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s-i-e-v-e"&gt; /u/s-i-e-v-e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T11:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4qxpx</id>
    <title>QwQ-32B is the only model that can "Help me study vocabulary"</title>
    <updated>2025-03-06T08:26:44+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qxpx/qwq32b_is_the_only_model_that_can_help_me_study/"&gt; &lt;img alt="QwQ-32B is the only model that can &amp;quot;Help me study vocabulary&amp;quot;" src="https://a.thumbs.redditmedia.com/nRMRtJqP8jeqnUT5Rb1pMhgwcEfHKsw5KKuSdOom314.jpg" title="QwQ-32B is the only model that can &amp;quot;Help me study vocabulary&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is one of the default prompt suggestions in Open webui. &lt;/p&gt; &lt;p&gt;The other 32B reasoning models just start hallucinating about a vocabulary test out of nowhere.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n5k7c51u41ne1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fce6ca32f74dc73102b83ec739e77717caf707f9"&gt;https://preview.redd.it/n5k7c51u41ne1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fce6ca32f74dc73102b83ec739e77717caf707f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Help me study vocabulary: write a sentence for me to fill in the blank, and I'll try to pick the correct option.&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qxpx/qwq32b_is_the_only_model_that_can_help_me_study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qxpx/qwq32b_is_the_only_model_that_can_help_me_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qxpx/qwq32b_is_the_only_model_that_can_help_me_study/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T08:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4hrgt</id>
    <title>Honest question - what is QwQ actually useful for?</title>
    <updated>2025-03-05T23:41:43+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recognizing wholeheartedly that the title may come off as a smidge provocative, I really am genuinely curious if anyone has a real world example of something that QwQ actually does better than its peers at. I got all excited by the updated benchmarks showing what appeared to be a significant gain over the QwQ preview, and after seeing encouraging scores in coding-adjacent tasks I thought a good test would be having it do something I often have R1 do, which is operate in architect mode and create a plan for a change in Aider or Roo. One of the top posts on &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; right now reads &amp;quot;QwQ-32B released, equivalent or surpassing full Deepseek-R1!&amp;quot; &lt;/p&gt; &lt;p&gt;If that's the case, then it should be at least moderately competent at coding given they purport to match full fat R1 on coding benchmarks. So, I asked it to implement python logging in a ~105 line file based on the existing implementation in another 110 line file.&lt;/p&gt; &lt;p&gt;In both cases, it literally couldn't do it. In Roo, it just kept talking in circles and proposing Mermaid diagrams showing how files relate to each other, despite specifically attaching only the two files in question. After it runs around going crazy for too long, Roo actually force stops the model and writes back &lt;em&gt;&amp;quot;Roo Code uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.7 Sonnet for its advanced agentic coding capabilities.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Now, there are always nuances to agentic tools like Roo, so I went straight to the chat interface and fed it an even simpler file and asked it to perform a code review on a 90 line python script that’s already in good shape. In return, I waited ten minutes while it generated &lt;strong&gt;25,000&lt;/strong&gt; tokens in total (combined thinking and actual response) to suggest I implement an exception handler on a single function. Feeding the identical prompt to Claude took roughly 3 seconds to generate 6 useful suggestions with accompanying code change snippets.&lt;/p&gt; &lt;p&gt;So this brings me back to exactly where I was when I deleted QwQ-Preview after a week. What the hell is this thing actually for? What is it good at? I feel like it’s way more useful as a proof of concept than as a practical model for anything but the least performance sensitive possible tasks. So my question is this - can anyone provide an example (prompt and response) where QwQ was able to answer your question or prompt better than qwen2.5:32b (coder or instruct)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3zxwn</id>
    <title>Are we ready!</title>
    <updated>2025-03-05T10:16:41+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt; &lt;img alt="Are we ready!" src="https://preview.redd.it/m0ktikjrjume1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dc8037f70763ba02e1ed164ff1654c69921dfd" title="Are we ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m0ktikjrjume1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T10:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jxyq</id>
    <title>The Reason why open source models should be in the lead.</title>
    <updated>2025-03-06T01:30:21+00:00</updated>
    <author>
      <name>/u/Feisty-Pineapple7879</name>
      <uri>https://old.reddit.com/user/Feisty-Pineapple7879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is doubling down on its application business. Execs have spoken with investors about three classes of future agent launches, ranging from $2K to $20K/month to do tasks like automating coding and PhD-level research:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty-Pineapple7879"&gt; /u/Feisty-Pineapple7879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4qph0</id>
    <title>QwQ-32B generate speet test on Apple M2 Ultra</title>
    <updated>2025-03-06T08:09:05+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qph0/qwq32b_generate_speet_test_on_apple_m2_ultra/"&gt; &lt;img alt="QwQ-32B generate speet test on Apple M2 Ultra" src="https://preview.redd.it/99m22rzu11ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7bcd016849a883000e16e302c5abd1e9be58cec" title="QwQ-32B generate speet test on Apple M2 Ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/99m22rzu11ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qph0/qwq32b_generate_speet_test_on_apple_m2_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4qph0/qwq32b_generate_speet_test_on_apple_m2_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T08:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43ziq</id>
    <title>The new king? M3 Ultra, 80 Core GPU, 512GB Memory</title>
    <updated>2025-03-05T14:13:32+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt; &lt;img alt="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" src="https://preview.redd.it/jkhal4p0qvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cb3ce2fdbe1423c5cf740e8f17c9c8df2f9e7b2" title="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. With 512GB of memory a world of possibilities opens up. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkhal4p0qvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43us5</id>
    <title>Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory</title>
    <updated>2025-03-05T14:07:13+00:00</updated>
    <author>
      <name>/u/iCruiser7</name>
      <uri>https://old.reddit.com/user/iCruiser7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt; &lt;img alt="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" src="https://external-preview.redd.it/IUc-sq0jBjlLBbxyREexc_Ijkq_kHcRXYNu-Mr7u5LI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cb29351c6e2bde66e4d208acbdf5c007acd170" title="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iCruiser7"&gt; /u/iCruiser7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4p1fb</id>
    <title>Recommended settings for QwQ 32B</title>
    <updated>2025-03-06T06:10:34+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p1fb/recommended_settings_for_qwq_32b/"&gt; &lt;img alt="Recommended settings for QwQ 32B" src="https://external-preview.redd.it/lBZs0Q7c65_lYRXO4ivgUAuYiqvkD7hvWkRogLEWXvw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d8af51e17945c43660cee6e1f3aae2b98b5f55" title="Recommended settings for QwQ 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even though the Qwen team clearly stated how to set up QWQ-32B on HF, I still saw some people confused about how to set it up properly. So, here are all the settings in one image:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/767vpzn7g0ne1.png?width=684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44bfe2e121e8ebd4bd2e17729d50e2611e4a50aa"&gt;https://preview.redd.it/767vpzn7g0ne1.png?width=684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44bfe2e121e8ebd4bd2e17729d50e2611e4a50aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;system prompt: &lt;a href="https://huggingface.co/spaces/Qwen/QwQ-32B-Demo/blob/main/app.py"&gt;https://huggingface.co/spaces/Qwen/QwQ-32B-Demo/blob/main/app.py&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def format_history(history): messages = [{ &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a helpful and harmless assistant.&amp;quot;, }] for item in history: if item[&amp;quot;role&amp;quot;] == &amp;quot;user&amp;quot;: messages.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: item[&amp;quot;content&amp;quot;]}) elif item[&amp;quot;role&amp;quot;] == &amp;quot;assistant&amp;quot;: messages.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: item[&amp;quot;content&amp;quot;]}) return messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;generation_config.json: &lt;a href="https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json"&gt;https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;repetition_penalty&amp;quot;: 1.0, &amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_k&amp;quot;: 40, &amp;quot;top_p&amp;quot;: 0.95, &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p1fb/recommended_settings_for_qwq_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p1fb/recommended_settings_for_qwq_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p1fb/recommended_settings_for_qwq_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T06:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4bi0g</id>
    <title>brainless Ollama naming about to strike again</title>
    <updated>2025-03-05T19:26:23+00:00</updated>
    <author>
      <name>/u/gpupoor</name>
      <uri>https://old.reddit.com/user/gpupoor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt; &lt;img alt="brainless Ollama naming about to strike again" src="https://preview.redd.it/hnw7tvbo9xme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff02886ac4815c2bc162e157351ae6d100b824d0" title="brainless Ollama naming about to strike again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpupoor"&gt; /u/gpupoor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hnw7tvbo9xme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4d5fr</id>
    <title>Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎</title>
    <updated>2025-03-05T20:33:03+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt; &lt;img alt="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" src="https://preview.redd.it/ye3dq51qlxme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4849458d901ac4ba1ba6d60edc9a283317fab5bc" title="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thing is friggin sweet!! Can’t wait to fire it up and load up full DeepSeek 671b on this monster! It does look slightly different than the promotional photos I saw online which is a little concerning, but for $800 🤷‍♂️. They’ve got it mounted in some kind of acrylic case or something, it’s in there pretty good, can’t seem to remove it easily. As soon as I figure out how to plug it up to my monitor, I’ll give you guys a report. Seems to be missing DisplayPort and no HDMI either. Must be some new type of port that I might need an adapter for. That’s what I get for being on the bleeding edge I guess. 🤓&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ye3dq51qlxme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jrnl</id>
    <title>AMD new Fully Open Instella 3B model</title>
    <updated>2025-03-06T01:21:49+00:00</updated>
    <author>
      <name>/u/blazerx</name>
      <uri>https://old.reddit.com/user/blazerx</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blazerx"&gt; /u/blazerx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/introducing-instella-3B/README.html#additional-resources"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jrnl/amd_new_fully_open_instella_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jrnl/amd_new_fully_open_instella_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4v3fi</id>
    <title>Prompts for QwQ-32B</title>
    <updated>2025-03-06T13:10:23+00:00</updated>
    <author>
      <name>/u/drrros</name>
      <uri>https://old.reddit.com/user/drrros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday while searching for prompts for QwQ, I stumbled upon an interesting article: &lt;a href="https://www.researchgate.net/publication/389351923_Towards_Thinking-Optimal_Scaling_of_Test-Time_Compute_for_LLM_Reasoning"&gt;research&lt;/a&gt;&lt;br /&gt; TLDR: 3 options for QwQ system prompt:&lt;br /&gt; &lt;code&gt;Low Reasoning Eﬀort: You have extremely limited time to think and respond to the user’s query. Every&lt;/code&gt; &lt;code&gt;additional second of processing and reasoning incurs a signiﬁcant resource cost, which could aﬀect eﬃciency and eﬀectiveness. Your task is to prioritize speed without sacriﬁcing essential clarity or accuracy. Provide the most direct and concise answer possible. Avoid unnecessary steps, reﬂections, veriﬁcation, or reﬁnements UNLESS ABSOLUTELY NECESSARY. Your primary goal is to deliver a quick, clear and correct response.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Medium Reasoning Eﬀort: You have suﬃcient time to think and respond to the user’s query, allowing for a more thoughtful and in-depth answer. However, be aware that the longer you take to reason and process, the greater the associated resource costs and potential consequences. While you should not rush, aim to balance the depth of your reasoning with eﬃciency. Prioritize providing a well-thought-out response, but do not overextend your thinking if the answer can be provided with a reasonable level of analysis. Use your reasoning time wisely, focusing on what is essential for delivering an accurate response without unnecessary delays and overthinking.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;High Reasoning Eﬀort: You have unlimited time to think and respond to the user’s question. There is no need to worry about reasoning time or associated costs. Your only goal is to arrive at a reliable, correct ﬁnal answer. Feel free to explore the problem from multiple angles, and try various methods in your reasoning. This includes reﬂecting on reasoning by trying diﬀerent approaches, verifying steps from diﬀerent aspects, and rethinking your conclusions as needed. You are encouraged to take the time to analyze the problem thoroughly, reﬂect on your reasoning promptly and test all possible solutions. Only after a deep, comprehensive thought process should you provide the ﬁnal answer, ensuring it is correct and well-supported by your reasoning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Tried a High effort prompt and got some good results, may be someone would find them interesting.&lt;/p&gt; &lt;p&gt;Edit: fixed some copy-pasting issues in prompts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/drrros"&gt; /u/drrros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4s0o4</id>
    <title>QwQ-32B solves the o1-preview Cipher problem!</title>
    <updated>2025-03-06T09:50:28+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen QwQ 32B solves the Cipher problem first showcased in the &lt;a href="https://openai.com/index/learning-to-reason-with-llms/"&gt;OpenAI o1-preview Technical Paper&lt;/a&gt;. No other local model so far (at least on my 48Gb MacBook) has been able to solve this. Amazing performance from a 32B model (6-bit quantised too!). Now for the sad bit — it did take over 9000 tokens, and at 4t/s this took 33 minutes to complete.&lt;/p&gt; &lt;p&gt;Here's the full output, including prompt from llama.cpp:&lt;br /&gt; &lt;a href="https://gist.github.com/sunpazed/497cf8ab11fa7659aab037771d27af57"&gt;https://gist.github.com/sunpazed/497cf8ab11fa7659aab037771d27af57&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T09:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4r1iu</id>
    <title>A SOTA of hardware for LLM made by exolab creator</title>
    <updated>2025-03-06T08:35:04+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a quite long but &lt;a href="https://x.com/alexocheema/status/1897349404522078261"&gt;interesting thread&lt;/a&gt; made by Alex Cheema, the creator of exolabs.&lt;/p&gt; &lt;p&gt;With the release of the new Qwen and the fast pace of improvement, it seems that we will no longer need to buy maxed out machines to run a frontier model locally.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Apple's timing could not be better with this. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The M3 Ultra 512GB Mac Studio fits perfectly with massive sparse MoEs like DeepSeek V3/R1. &lt;/p&gt; &lt;p&gt;2 M3 Ultra 512GB Mac Studios with &lt;a href="https://x.com/exolabs"&gt;u/exolabs&lt;/a&gt; is all you need to run the full, unquantized DeepSeek R1 at home. &lt;/p&gt; &lt;p&gt;The first requirement for running these massive AI models is that they need to fit into GPU memory (in Apple's case, unified memory). Here's a quick comparison of how much that costs for different options (note: DIGITS is left out here since details are still unconfirmed): &lt;/p&gt; &lt;p&gt;NVIDIA H100: 80GB @ 3TB/s, $25,000, $312.50 per GB&lt;br /&gt; AMD MI300X: 192GB @ 5.3TB/s, $20,000, $104.17 per GB&lt;br /&gt; Apple M2 Ultra: 192GB @ 800GB/s, $5,000, $26.04 per GB&lt;br /&gt; Apple M3 Ultra: 512GB @ 800GB/s, $9,500, $18.55 per GB &lt;/p&gt; &lt;p&gt;That's a 28% reduction in $ per GB from the M2 Ultra - pretty good. &lt;/p&gt; &lt;p&gt;The concerning thing here is the memory refresh rate. This is the ratio of memory bandwidth to memory of the device. It tells you how many times per second you could cycle through the entire memory on the device. This is the dominating factor for the performance of single request (batch_size=1) inference. For a dense model that saturates all of the memory of the machine, the maximum theoretical token rate is bound by this number. Comparison of memory refresh rate: &lt;/p&gt; &lt;p&gt;NVIDIA H100 (80GB): 37.5/s&lt;br /&gt; AMD MI300X (192GB): 27.6/s&lt;br /&gt; Apple M2 Ultra (192GB): 4.16/s (9x less than H100)&lt;br /&gt; Apple M3 Ultra (512GB): 1.56/s (24x less than H100) &lt;/p&gt; &lt;p&gt;Apple is trading off more memory for less memory refresh frequency, now 24x less than a H100. Another way to look at this is to analyze how much it costs per unit of memory bandwidth. Comparison of cost per GB/s of memory bandwidth (cheaper is better): &lt;/p&gt; &lt;p&gt;NVIDIA H100 (80GB): $8.33 per GB/s&lt;br /&gt; AMD MI300X (192GB): $3.77 per GB/s&lt;br /&gt; Apple M2 Ultra (192GB): $6.25 per GB/s&lt;br /&gt; Apple M3 Ultra (512GB): $11.875 per GB/s &lt;/p&gt; &lt;p&gt;There are two ways Apple wins with this approach. Both are hierarchical model structures that exploit the sparsity of model parameter activation: MoE and Modular Routing. &lt;/p&gt; &lt;p&gt;MoE adds multiple experts to each layer and picks the top-k of N experts in each layer, so only k/N experts are active per layer. The more sparse the activation (smaller the ratio k/N) the better for Apple. DeepSeek R1 ratio is small: 8/256 = 1/32. Model developers could likely push this to be even smaller, potentially we might see a future where k/N is something like 8/1024 = 1/128 (&amp;lt;1% activated parameters). &lt;/p&gt; &lt;p&gt;Modular Routing includes methods like DiPaCo and dynamic ensembles where a gating function activates multiple independent models and aggregates the results into one single result. For this, multiple models need to be in memory but only a few are active at any given time. &lt;/p&gt; &lt;p&gt;Both MoE and Modular Routing require a lot of memory but not much memory bandwidth because only a small % of total parameters are active at any given time, which is the only data that actually needs to move around in memory. &lt;/p&gt; &lt;p&gt;Funny story... 2 weeks ago I had a call with one of Apple's biggest competitors. They asked if I had a suggestion for a piece of AI hardware they could build. I told them, go build a 512GB memory Mac Studio-like box for AI. Congrats Apple for doing this. Something I thought would still take you a few years to do you did today. I'm impressed. &lt;/p&gt; &lt;p&gt;Looking forward, there will likely be an M4 Ultra Mac Studio next year which should address my main concern since these Ultra chips use Apple UltraFusion to fuse Max dies. The M4 Max had a 36.5% increase in memory bandwidth compared to the M3 Max, so we should see something similar (or possibly more depending on the configuration) in the M4 Ultra.&lt;/p&gt; &lt;p&gt;AI generated TLDR:&lt;/p&gt; &lt;p&gt;Apple's new M3 Ultra Mac Studio with 512GB unified memory is ideal for massive sparse AI models like DeepSeek V3/R1, allowing users to run large models at home affordably compared to NVIDIA and AMD GPUs. While Apple's approach offers significantly cheaper memory capacity, it sacrifices memory bandwidth, resulting in lower memory refresh rates—crucial for dense model inference. However, sparse architectures like Mixture-of-Experts (MoE) and Modular Routing effectively utilize Apple's strengths by activating only a small portion of parameters at a time. Future Apple chips (e.g., M4 Ultra) may further improve memory bandwidth, addressing current performance limitations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4r1iu/a_sota_of_hardware_for_llm_made_by_exolab_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T08:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4gw91</id>
    <title>QwQ-32B seems to get the same quality final answer as R1 while reasoning much more concisely and efficiently</title>
    <updated>2025-03-05T23:04:05+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I will now switch over to using QwQ as my primary reasoning model instead of R1. In all my testing, it gets the same or superior quality answers as R1 does, while having its chain of thought be much more efficient, much more concise, and much more confident. In contrast, R1 feels like a bumbling idiot who happens to be really smart only because he tries every possible solution. And It's not particularly close either, QwQ takes like 4x fewer tokens than R1 on the same problem while both arriving at the same answer.&lt;/p&gt; &lt;p&gt;Adam was right when he said not all CoTs are equal, and in this case, I think Qwen trained their model to be more efficient without degrading quality at all.&lt;/p&gt; &lt;p&gt;But I'm curious to hear what everyone here thinks, because I'm sure others are more experienced than I am.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4vgk2</id>
    <title>Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo).</title>
    <updated>2025-03-06T13:29:37+00:00</updated>
    <author>
      <name>/u/FUS3N</name>
      <uri>https://old.reddit.com/user/FUS3N</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt; &lt;img alt="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." src="https://preview.redd.it/7js9zyilm2ne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=0a20857aad361d43a64a623575b753839bc12b77" title="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FUS3N"&gt; /u/FUS3N &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7js9zyilm2ne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4az6k</id>
    <title>Qwen/QwQ-32B · Hugging Face</title>
    <updated>2025-03-05T19:05:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt; &lt;img alt="Qwen/QwQ-32B · Hugging Face" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Qwen/QwQ-32B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4sjn5</id>
    <title>New AI breakthrough,Faster and Smaller low thinking token with "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation "</title>
    <updated>2025-03-06T10:28:07+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.21074"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4sjn5/new_ai_breakthroughfaster_and_smaller_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4sjn5/new_ai_breakthroughfaster_and_smaller_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T10:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b1t9</id>
    <title>QwQ-32B released, equivalent or surpassing full Deepseek-R1!</title>
    <updated>2025-03-05T19:08:01+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt; &lt;img alt="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" src="https://external-preview.redd.it/GjWMsqQ0sjAo2i1u3zMKBVF8QJTEurDWKLmSNIhLwOE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37675ad756a9c5a85511da4e75709d13466b2af3" title="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1897361654763151544"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4p3xw</id>
    <title>A few hours with QwQ and Aider - and my thoughts</title>
    <updated>2025-03-06T06:15:15+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a mini review. I'll be as brief as possible.&lt;/p&gt; &lt;p&gt;I tested QwQ using Q5 and Q6 from Bartowski. I didn't notice any major benefit from Q6.&lt;/p&gt; &lt;h2&gt;The Good&lt;/h2&gt; &lt;p&gt;It's very good. This model, if you can stomach the extra tokens, is stronger than Deepseek Distill R1 32B, no doubt about it. But &lt;em&gt;it needs to think more&lt;/em&gt; to achieve it. If you are sensitive to context size or inference speed, this may be a difficult trade off.&lt;/p&gt; &lt;h2&gt;The Great&lt;/h2&gt; &lt;p&gt;This model beat Qwen-Coder 32B, who has been the king of kings for coders in Aider for models of this size. It doesn't necessarily write better code, but it takes far less iterations. It catches your intentions and instructions on the first try and avoids silly syntax errors. &lt;strong&gt;The biggest strength is that I have to prompt way less using QwQ vs Qwen Coder&lt;/strong&gt; - but it should be noted that 1 prompt to QwQ will take 2-3x as many tokens as 3 iterative prompts to Qwen-Coder 32B &lt;/p&gt; &lt;h2&gt;The Bad&lt;/h2&gt; &lt;p&gt;As said above, it THINKS to be as smart as it is. And it thinks A LOT. I'm using 512GB/s entirely in VRAM and I found myself getting impatient.&lt;/p&gt; &lt;h2&gt;The Ugly&lt;/h2&gt; &lt;p&gt;Twice it randomly wrote perfect code for me (one shots) but then forgot to follow Aider's code-editing rules. This is a huge bummer after waiting for SO MANY thinking tokens to produce a result.&lt;/p&gt; &lt;h2&gt;Conclusion (so far)&lt;/h2&gt; &lt;p&gt;Those benchmarks beating Deepseek R1 (full fat) are definitely bogus. This model is not in that tier. But it's basically managed to become three iterative prompts to Qwen32B and Qwen-Coder32B in a single prompt, which is absolutely incredible. I think a lot of folks will get use out of this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4p3xw/a_few_hours_with_qwq_and_aider_and_my_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T06:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jpij</id>
    <title>M3 Ultra is a slightly weakened 3090 w/ 512GB</title>
    <updated>2025-03-06T01:19:01+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To conclude, you are getting a slightly weakened 3090 with 512GB at max config as it gets 114.688TFLOPS FP16 vs 142.32TFLOPS FP16 for 3090 and memory bandwidth of 819.2GB/s vs 936GB/s.&lt;/p&gt; &lt;p&gt;The only place I can find about M3 Ultra spec is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/"&gt;https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, it is highly vague about the spec. So I made an educated guess on the exact spec of M3 Ultra based on this article.&lt;/p&gt; &lt;p&gt;To achieve a GPU of 2x performance of M2 Ultra and 2.6x of M1 Ultra, you need to double the shaders per core from 128 to 256. That's what I guess is happening here for such big improvement.&lt;/p&gt; &lt;p&gt;I also made a guesstimate on what a M4 Ultra can be.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M3 Ultra&lt;/th&gt; &lt;th align="left"&gt;M2 Ultra&lt;/th&gt; &lt;th align="left"&gt;M1 Ultra&lt;/th&gt; &lt;th align="left"&gt;M4 Ultra?&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Core&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;76&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Shader&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;td align="left"&gt;9728&lt;/td&gt; &lt;td align="left"&gt;8192&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU GHz&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;1.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU FP16&lt;/td&gt; &lt;td align="left"&gt;114.688&lt;/td&gt; &lt;td align="left"&gt;54.4768&lt;/td&gt; &lt;td align="left"&gt;42.5984&lt;/td&gt; &lt;td align="left"&gt;137.6256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Type&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5X&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Speed&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;8533&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Controller&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Bandwidth&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;1092.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU P-Core&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU GHz&lt;/td&gt; &lt;td align="left"&gt;4.05&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;td align="left"&gt;3.2&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU FP16&lt;/td&gt; &lt;td align="left"&gt;3.1104&lt;/td&gt; &lt;td align="left"&gt;1.792&lt;/td&gt; &lt;td align="left"&gt;1.6384&lt;/td&gt; &lt;td align="left"&gt;3.456&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Apple is likely to be selling it at 10-15k. If 10k, I think it is quite a good deal as its performance is about 4xDIGITS and RAM is much faster. 15k is still not a bad deal either in that perspective.&lt;/p&gt; &lt;p&gt;There is also a possibility that there is no doubling of shader density and Apple is just playing with words. That would be a huge bummer. In that case, it is better to wait for M4 Ultra.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4u57l</id>
    <title>Hunyuan Image to Video released!</title>
    <updated>2025-03-06T12:16:11+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt; &lt;img alt="Hunyuan Image to Video released!" src="https://external-preview.redd.it/bjcyc3R4bnc5Mm5lMSVRu4OBDxIXZycPsoc4EtwnK4B2nYL7URskxFmP5hp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2734e75b58f76380d802fe5f62995c44dffdc8c0" title="Hunyuan Image to Video released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yck5cznw92ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T12:16:11+00:00</published>
  </entry>
</feed>
