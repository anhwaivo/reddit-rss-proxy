<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-28T13:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n1nnuw</id>
    <title>Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud.</title>
    <updated>2025-08-27T17:38:03+00:00</updated>
    <author>
      <name>/u/TeamEarly</name>
      <uri>https://old.reddit.com/user/TeamEarly</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"&gt; &lt;img alt="Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud." src="https://external-preview.redd.it/dW44NmZ0amZpbGxmMQuGtiBYImp4y5TE0Jp1MRc4u7lsWJVoGir-kxQWUhhu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=860470108798c3775427c269351d94be3f33b0f3" title="Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering putting Elmer on TestFlight. It's an iOS/Mac app combo that lets you use your locally-hosted AI models &amp;amp; services (Ollama, LM Studio, ComfyUI) from anywhere, using your iPhone. &lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Remote access to your local AI setup via secure CloudKit relay&lt;/li&gt; &lt;li&gt;Auto-discovery: Just run the Mac app, iPhone finds it automatically&lt;/li&gt; &lt;li&gt;Multi-service: Works with Ollama, LM Studio, ComfyUI, and custom endpoints&lt;/li&gt; &lt;li&gt;No port forwarding: Uses your personal iCloud for secure tunneling between devices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Perfect for when you want to access your local setup's compute while mobile, without the complexity of VPNs or exposing ports. I'm still working on it but thinking of doing a TesFlight soon! &lt;/p&gt; &lt;p&gt;I'm curious if anyone has opinion about the relay strategy? I considered options like Cloudflare Tunnels, but iCloud felt most private. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamEarly"&gt; /u/TeamEarly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e5knbtjfillf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n293h7</id>
    <title>OpenRouter - Privacy settings. Disable Logging</title>
    <updated>2025-08-28T10:58:31+00:00</updated>
    <author>
      <name>/u/shoeshineboy_99</name>
      <uri>https://old.reddit.com/user/shoeshineboy_99</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n293h7/openrouter_privacy_settings_disable_logging/"&gt; &lt;img alt="OpenRouter - Privacy settings. Disable Logging" src="https://b.thumbs.redditmedia.com/fO9Ta95NIcv4HBwZ89HM7O7fdavA8OiG_RfxfJQRpAU.jpg" title="OpenRouter - Privacy settings. Disable Logging" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/b071grmirqlf1.png?width=1748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a8a12515f3330272d5d651afaf2e4df62fd3bd6"&gt;Disable the loggin option if you are privacy focussed. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are using OpenRouter, remember to disable the Enable input/output logging option.&lt;/p&gt; &lt;p&gt;You can also enable the &amp;quot;Zero Data Retention&amp;quot; policy option (_just below the logging setting_) -- thanks to &lt;a href="/u/AxelFooley"&gt;u/AxelFooley&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shoeshineboy_99"&gt; /u/shoeshineboy_99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n293h7/openrouter_privacy_settings_disable_logging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n293h7/openrouter_privacy_settings_disable_logging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n293h7/openrouter_privacy_settings_disable_logging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T10:58:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n26ls5</id>
    <title>Has anyone implemented a concept-based reasoning system?</title>
    <updated>2025-08-28T08:23:29+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working on a chatbot right now and I've hit a pretty clear wall with simple keyword-based reasoning. No matter how complex I make the logic, it still feels like the bot's just fixated on a few words. It's not a fundamental solution.&lt;/p&gt; &lt;p&gt;To make an AI that thinks like a living organism, I think we need it to recognize &lt;strong&gt;concepts&lt;/strong&gt;, not just keywords.&lt;/p&gt; &lt;p&gt;For example, instead of treating words like 'travel', 'vacation', and 'flight' as separate things, the bot would group them all into a single &lt;strong&gt;'leisure concept'&lt;/strong&gt; vector. This way, if the conversation shifts from 'plane' to 'hotel', the AI doesn't lose the essence of the conversation because the core concept of 'leisure' is still active.&lt;/p&gt; &lt;p&gt;This is roughly how I'd approach the implementation, but has anyone here actually built something like this? How did you do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n26ls5/has_anyone_implemented_a_conceptbased_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n26ls5/has_anyone_implemented_a_conceptbased_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n26ls5/has_anyone_implemented_a_conceptbased_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T08:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2chrm</id>
    <title>Again where behemoth and reasoning model from meta ??</title>
    <updated>2025-08-28T13:39:03+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt; &lt;img alt="Again where behemoth and reasoning model from meta ??" src="https://preview.redd.it/xma7ru49krlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=214fa2574efffdfe39bf57c819059660b5a2a371" title="Again where behemoth and reasoning model from meta ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xma7ru49krlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T13:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n29o4d</id>
    <title>What is your method for creating large datasets?</title>
    <updated>2025-08-28T11:28:38+00:00</updated>
    <author>
      <name>/u/urmel42</name>
      <uri>https://old.reddit.com/user/urmel42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I finetune a model with data from several pdfs, I first use the &amp;quot;unstructured&amp;quot; library to make the data more easily readable for training and then run the gpt5 API to create a dataset out of the newly structured data. However, this way of doing it is more or less self-taught and I am pretty sure there are better and more efficient ways of doing this. So I would be thankful to learn from you guys :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/urmel42"&gt; /u/urmel42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n29o4d/what_is_your_method_for_creating_large_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n29o4d/what_is_your_method_for_creating_large_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n29o4d/what_is_your_method_for_creating_large_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T11:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1amux</id>
    <title>Hugging Face has reached two million models.</title>
    <updated>2025-08-27T07:35:26+00:00</updated>
    <author>
      <name>/u/sstainsby</name>
      <uri>https://old.reddit.com/user/sstainsby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"&gt; &lt;img alt="Hugging Face has reached two million models." src="https://preview.redd.it/6basw10amilf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e12973c2fb3ede2accf2e8ae76cc63010a6ac51" title="Hugging Face has reached two million models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sstainsby"&gt; /u/sstainsby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6basw10amilf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T07:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n20aef</id>
    <title>I made a Local LLM-based privacy filter for cloud LLM services, so that private data never leaves your machine</title>
    <updated>2025-08-28T02:17:44+00:00</updated>
    <author>
      <name>/u/cxu25</name>
      <uri>https://old.reddit.com/user/cxu25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n20aef/i_made_a_local_llmbased_privacy_filter_for_cloud/"&gt; &lt;img alt="I made a Local LLM-based privacy filter for cloud LLM services, so that private data never leaves your machine" src="https://preview.redd.it/xue7m3jk0olf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e79e8363d899326f8c7dac99c52620c7a6ac9f0" title="I made a Local LLM-based privacy filter for cloud LLM services, so that private data never leaves your machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The diagram should explain the idea well. The local middle layer intercepts data transferring between user and cloud, ensuring the user sees only raw message, and the cloud LLM sees only anonymizied text.&lt;br /&gt; It can work as a Python library / OpenAI SDK replacement / API Gatetway / Web Server. &lt;/p&gt; &lt;p&gt;Check &lt;a href="https://github.com/cxumol/promptmask"&gt;GitHub repo&lt;/a&gt; for technical details, and check my &lt;a href="https://xirtam.cxumol.com/promptmask-how-not-give-ai-secrets/"&gt;blog post&lt;/a&gt; for the full ideas around it.&lt;/p&gt; &lt;p&gt;I keep this post short because I wrote a longer post and it was removed as soon as submitted. I didn't know which word triggered the spam filter. Please leave a comment/suggestion if this idea/project sounds interesting to you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cxu25"&gt; /u/cxu25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xue7m3jk0olf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n20aef/i_made_a_local_llmbased_privacy_filter_for_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n20aef/i_made_a_local_llmbased_privacy_filter_for_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T02:17:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1n6wr</id>
    <title>Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!</title>
    <updated>2025-08-27T17:20:26+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"&gt; &lt;img alt="Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!" src="https://external-preview.redd.it/fQ68o495vGTQI0wPf3lbyqrUgPukCFvd0dIhZUXV0Is.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571a08519f9d8c16a4098391bb0cbae0b5a43e5b" title="Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Stop me if you have already seen this...&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1ece5</id>
    <title>TheDrummer is on fire!!!</title>
    <updated>2025-08-27T11:23:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="/u/TheLocalDrummer"&gt;u/TheLocalDrummer&lt;/a&gt; published lots of new models (finetunes) in the last days:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Behemoth-X-123B-v2-GGUF"&gt;https://huggingface.co/TheDrummer/Behemoth-X-123B-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF"&gt;https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF"&gt;https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF"&gt;https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are looking for something new to try - this is definitely the moment!&lt;/p&gt; &lt;p&gt;if you want more in progress models, please check discord and &lt;a href="https://huggingface.co/BeaverAI"&gt;https://huggingface.co/BeaverAI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T11:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1rssb</id>
    <title>[open source] We built a better reranker and open sourced it.</title>
    <updated>2025-08-27T20:13:26+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our research team just released the best performing and most efficient reranker out there, and it's available now as an open weight model on HuggingFace. Rerankers are critical in context engineering: they improve retrieval accuracy, and help you make the best use of limited context, whether for RAG or another use case.&lt;/p&gt; &lt;p&gt;Reranker v2 was designed specifically for agentic RAG, supports instruction following, and is multilingual.&lt;/p&gt; &lt;p&gt;Along with this, we're also open source our eval set, which allows you to reproduce our benchmark results. Back in March, when we introduced the world's first instruction-following reranker, it was SOTA on BEIR. After observing reranker use in production, we created an evaluation dataset that better matches real world use - focusing on QA-focused tests from several benchmarks. By releasing these datasets, we are also advancing instruction-following reranking evaluation, where high-quality benchmarks are currently limited.&lt;/p&gt; &lt;p&gt;Now all the weights for reranker V2 are live on HuggingFace: 1B, 2B, and 6B parameter models. I've been having fun building demos with earlier versions, like a reranker-based MCP server selector. Excited to try this out with the latest version!&lt;/p&gt; &lt;p&gt;Please give it a try and let us know what you think. Links to learn more in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T20:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2alye</id>
    <title>built a opensource tool that explores your files with deep research like workflow</title>
    <updated>2025-08-28T12:15:31+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt; &lt;img alt="built a opensource tool that explores your files with deep research like workflow" src="https://external-preview.redd.it/VjhzNXFqseZP5gshgU7f0eCX58wU9q276mVwsHcHWx8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae3fa637225d92705332fa1f59996eb6a85201b" title="built a opensource tool that explores your files with deep research like workflow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zzg6frcr4rlf1.png?width=1286&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=acb614d0a76dc0df628ef411b3066eae5f6eebbe"&gt;research workflow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/s7dx6h2t4rlf1.gif"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo - &lt;a href="https://github.com/Datalore-ai/deepdoc"&gt;https://github.com/Datalore-ai/deepdoc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I released a small open source project and the support it got honestly meant a lot. the feedback and love here keep me building more stuff so thank you for that.&lt;/p&gt; &lt;p&gt;recently I have been working on something new called &lt;strong&gt;DeepDoc&lt;/strong&gt;. it follows a deep research type workflow but on local resources instead of internet. the idea is simple. instead of digging through your own files manually, the tool explores them and hands back a clean report.&lt;/p&gt; &lt;p&gt;you just point it to directory containing local files like pdf, docx etc. it extracts the text, splits it into chunks, runs semantic search, builds a structure based on your instructions and then writes out a markdown report. each section is built step by step by exploring the right pieces, creating research queries, refining with reflection and finally stitching everything into a structured write up.&lt;/p&gt; &lt;p&gt;the result is something that feels like a researched report of your own documents without you having to scroll skim or copy paste.&lt;/p&gt; &lt;p&gt;still early but already works nicely on research papers, reports and even scanned files. planning to push it further soon.&lt;/p&gt; &lt;p&gt;if you want to see what the reports look like just drop a comment or dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1myth</id>
    <title>OpenAI has launched HealthBench on HuggingFace</title>
    <updated>2025-08-27T17:12:06+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"&gt; &lt;img alt="OpenAI has launched HealthBench on HuggingFace" src="https://b.thumbs.redditmedia.com/Cd1CiYSAFAOcmX901eEWMbsHfHloFXn3PMvvDhm3VkI.jpg" title="OpenAI has launched HealthBench on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/datasets/openai/healthbench"&gt;https://huggingface.co/datasets/openai/healthbench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n1myth"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2bdal</id>
    <title>Llama.cpp --verbose</title>
    <updated>2025-08-28T12:50:27+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed something a bit weird?&lt;/p&gt; &lt;p&gt;Qwen coder famously doesn't work in roo. I used --verbose on LCP to try and capture the exact failure but IT NEVER FAILS WHEN VERBOSE IS ON?!&lt;/p&gt; &lt;p&gt;In fact, it works flawlessly. So flawlessly, I believed Devstral had fixed the chat template for me in one prompt.&lt;/p&gt; &lt;p&gt;Now I feel silly.&lt;/p&gt; &lt;p&gt;How exactly is --verbose smoothing over the chat template difficulties? It feels like verbose enables something extra?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:50:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n25ygg</id>
    <title>Contextual AI Reranker v2 1B; SequenceClassification (single-logit) Converted Model</title>
    <updated>2025-08-28T07:40:28+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Contextual AI’s reranker v2 is a Qwen3-based multilingual reranker that already behaves like a classifier: the score is the last-token logit for vocab id 0 (next_logits[:, 0]), with BF16 numerics and left padding so the final position is aligned across a batch.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/sigridjineth/ctxl-rerank-v2-1b-seq-cls"&gt;https://huggingface.co/sigridjineth/ctxl-rerank-v2-1b-seq-cls&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That design is great for clarity, but the causal-LM interface still exposes a full vocab projection, which isn’t ideal for CrossEncoder pipelines or classification-style serving.A small conversion fixes that. The Qwen3 discussion by &lt;a href="https://www.linkedin.com/in/tomaarsen/"&gt;Tom Aarsen&lt;/a&gt; on “Converting a reranker model to a single label classification model” showed how to collapse a generative head into a classifier by mapping label-word logits; for reranker v2 it’s even simpler, the score lives in a single channel.I copy lm_head.weight[0] into a 1-logit SequenceClassification head (bias zero or the matching LM bias), propagate pad/eos/bos ids to config, enforce left padding, and verify strict parity by comparing the classifier logit to next_logits[:, 0] under the same prompt, with a BF16→FP32 readout.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.linkedin.com/posts/sigridjineth_sigridjinethctxl-rerank-v2-1b-seq-cls-activity-7366726911789629440-a0HT?utm_source=social_share_send&amp;amp;utm_medium=member_desktop_web&amp;amp;rcm=ACoAABRjkPcB873c2QQdGuFf5vmfAJXAqmBQOOQ"&gt;https://www.linkedin.com/posts/sigridjineth_sigridjinethctxl-rerank-v2-1b-seq-cls-activity-7366726911789629440-a0HT?utm_source=social_share_send&amp;amp;utm_medium=member_desktop_web&amp;amp;rcm=ACoAABRjkPcB873c2QQdGuFf5vmfAJXAqmBQOOQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is numerically identical scores, lower overhead (1×H instead of V×H), and drop-in compatibility with CrossEncoder and standard classification tooling.If that’s useful, try the converted model. It ships with the conversion and parity scripts; stars, issues, and PRs (including 2B/6B variants) are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n25ygg/contextual_ai_reranker_v2_1b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n25ygg/contextual_ai_reranker_v2_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n25ygg/contextual_ai_reranker_v2_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T07:40:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1kwy4</id>
    <title>Smuggling Nvidia GPUs to China</title>
    <updated>2025-08-27T15:56:51+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"&gt; &lt;img alt="Smuggling Nvidia GPUs to China" src="https://external-preview.redd.it/YjBh51k4f4z8_hT8ywyQGSPmo_QffcXvjGcCE4rlHns.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af98a8faa60a778d7426ad70272313ccf7bded36" title="Smuggling Nvidia GPUs to China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The assembly process works this way — Nvidia designs the silicon (done all over the world, but they’re headquartered in California), and TSMC manufactures and fabricates the silicon in Taiwan. Then, Chinese companies manufacture — and sometimes engineer through contract — the cooling solutions, the PCB (printed circuit board), and source all the capacitors and voltage regulator components. Everything that makes one of these devices — pretty much everything — is sourced in China.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Very insightful interview, especially for those who did not have time to watch the entire video.&lt;/p&gt; &lt;p&gt;Personally I find the repair/recycle capability (aka &amp;quot;keeping silicon in circulation&amp;quot; - the way Steve describes it) to be way more significant factor than export bans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/how-gpus-get-smuggled-to-china"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n27p5g</id>
    <title>Qwen3 rbit rl finetuned for stromger reasoning</title>
    <updated>2025-08-28T09:34:59+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;available now on hugging face and ollama &lt;a href="https://huggingface.co/adeelahmad/ReasonableQwen3-4B"&gt;adeelahmad/ReasonableQwen3-4B&lt;/a&gt; gguf and mlx&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/adeelahmad/ReasonableQwen3-4B"&gt;https://huggingface.co/adeelahmad/ReasonableQwen3-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/adeelahmad/ReasonableQwen3-4b"&gt;https://ollama.com/adeelahmad/ReasonableQwen3-4b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T09:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1vryp</id>
    <title>The True Story of ZLUDA: How CUDA Can Run on AMD &amp; Intel GPUs</title>
    <updated>2025-08-27T22:51:19+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1vryp/the_true_story_of_zluda_how_cuda_can_run_on_amd/"&gt; &lt;img alt="The True Story of ZLUDA: How CUDA Can Run on AMD &amp;amp; Intel GPUs" src="https://external-preview.redd.it/05PGfptuTJVeMItOO1eqztPAkFkahvrAYBUutp2gRMg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6d9cf5f6c38ee5250b3aadb13d9f1d0197d5c63" title="The True Story of ZLUDA: How CUDA Can Run on AMD &amp;amp; Intel GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got to appreciate the YT algorithm when it works. It suggested this interview with the creator of ZLUDA. It has 121 views only as I write this! He shares the back story of the project, how it came to be, how he got to AMD, why AMD let go of him and ZLUDA, and his roadmap for 2025 and 2026.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/2Kw_2fC9o80?si=u0sAqmmBbqFeXA9x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1vryp/the_true_story_of_zluda_how_cuda_can_run_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1vryp/the_true_story_of_zluda_how_cuda_can_run_on_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1uokl</id>
    <title>Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)</title>
    <updated>2025-08-27T22:05:58+00:00</updated>
    <author>
      <name>/u/Sufficient-Way8060</name>
      <uri>https://old.reddit.com/user/Sufficient-Way8060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"&gt; &lt;img alt="Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)" src="https://preview.redd.it/1k3v6nm7xmlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=329b15b8f182130c9637a712b1c2e26afd9107af" title="Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Just dropped something I think you'll find interesting - a series of small language models specifically trained for &lt;strong&gt;anonymizing personal data before it leaves your device&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;What these do&lt;/h1&gt; &lt;p&gt;Instead of sending &amp;quot;My name is Sarah and I work at Microsoft making $120k&amp;quot; to Claude/GPT, these models detect PII and replace it with semantically similar alternatives: &amp;quot;My name is Jessica and I work at TechCorp making $112k&amp;quot;. Query intent stays the same, but your real info stays private.&lt;/p&gt; &lt;h1&gt;The models&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;🏃‍♂️ Anonymizer-0.6B&lt;/strong&gt; - Mobile-optimized, &amp;lt;200ms inference&lt;br /&gt; &lt;strong&gt;⚖️ Anonymizer-1.7B&lt;/strong&gt; - Balanced (9.20/10 quality vs GPT-4.1's 9.77/10)&lt;br /&gt; &lt;strong&gt;🎯 Anonymizer-4B&lt;/strong&gt; - Highest accuracy (9.55/10 quality)&lt;/p&gt; &lt;p&gt;All based on Qwen3, trained with GRPO using GPT-4.1 as judge on ~30k anonymization samples.&lt;/p&gt; &lt;p&gt;Most &amp;quot;privacy&amp;quot; solutions either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Send your data to be anonymized (defeating the purpose)&lt;/li&gt; &lt;li&gt;Use simple regex replacement (breaks context)&lt;/li&gt; &lt;li&gt;Are way too heavy for real-time use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are lightweight enough to run as a preprocessing step before your main LLM calls, whether that's local or API-based.&lt;/p&gt; &lt;h1&gt;Currently powers &lt;a href="http://link.freysa.ai/appstore"&gt;Enchanted&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;We're using these in production for an iOS app where users want large open-source models and ChatGPT/Claude quality but with actual privacy. The 1.7B runs great on M-series MacBooks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-0.6B"&gt;Anonymizer-0.6B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-1.7B"&gt;Anonymizer-1.7B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-4B"&gt;Anonymizer-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.freysa.ai/blueprint/reinforcement-learning-for-privacy-training-local-models-on-the-anonymization-frontier"&gt;Blog post&lt;/a&gt; with more technical details&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear thoughts on the approach or if anyone's been working on similar privacy-preserving inference setups!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - Yes, I know there's some irony in using GPT-4.1 to train privacy models, but gotta start somewhere 😅&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Way8060"&gt; /u/Sufficient-Way8060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1k3v6nm7xmlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1y04u</id>
    <title>Using a local LLM as a privacy filter for GPT-4/5 &amp; other cloud models</title>
    <updated>2025-08-28T00:30:33+00:00</updated>
    <author>
      <name>/u/cxu25</name>
      <uri>https://old.reddit.com/user/cxu25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt; &lt;img alt="Using a local LLM as a privacy filter for GPT-4/5 &amp;amp; other cloud models" src="https://external-preview.redd.it/BorDJx5oDYIFbTOFfLwuJI6cZGAtIBXsCgjBUU7XqMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a731db6e945b9d6c39c93fae6a145bfa55796d93" title="Using a local LLM as a privacy filter for GPT-4/5 &amp;amp; other cloud models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The trade-off between local and cloud LLM is frustrating. Smarts or privacy, which side do you want to sacrifice? My answer is to use a small, fast local model as an intelligent privacy filter for the big cloud models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why the obvious regex redaction doesn't work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most redaction tools, like &lt;a href="https://langfuse.com/docs/observability/features/masking"&gt;https://langfuse.com/docs/observability/features/masking&lt;/a&gt;, rely on regex. It's fast but brittle. A regex for a US SSN is useless for its UK/Canada counterparts, and there are hundreds of countries with their own ID formats. And how do you write a regex for arbitrary passwords or weirdly formatted API keys? You can't.&lt;/p&gt; &lt;p&gt;Even if you &lt;em&gt;could&lt;/em&gt; perfectly redact everything, you run into a bigger problem. Most tools just swap your data with [REDACTED].&lt;/p&gt; &lt;p&gt;Let's say someone asks AI assistant about a legal document:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Summarize the dispute between &lt;em&gt;John Doe&lt;/em&gt; and &lt;em&gt;Jane Smith&lt;/em&gt; regarding the property at &lt;em&gt;123 Main St&lt;/em&gt;. &lt;em&gt;John&lt;/em&gt;'s wife, &lt;em&gt;Mary Doe&lt;/em&gt;, is also a witness.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Redaction creates this mess:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Summarize the dispute between [REDACTED] and [REDACTED] regarding the property at [REDACTED]. [REDACTED]'s wife, [REDACTED], is also a witness.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The context is destroyed, and the LLM is confused, and you get a garbage response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix: Local LLM as a Semantic Gatekeeper&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of regex, we can use a local model to do this intelligently. Here's the workflow I came up with:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Your message sending to cloud LLM is first intercepted locally, like &lt;code&gt;&amp;quot;My patient, Jensen Huang (ID: P12345), needs help...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If sensitive data is found, local LLM will create a JSON map, like &lt;code&gt;{&amp;quot;Jensen Huang&amp;quot;: &amp;quot;${PATIENT_NAME}&amp;quot;, &amp;quot;P12345&amp;quot;: &amp;quot;${PATIENT_ID}&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt;The actual message sent to cloud would be &lt;code&gt;&amp;quot;My patient, ${PATIENT_NAME} (ID: ${PATIENT_ID}), needs help...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Cloud AI assistant respond &lt;code&gt;&amp;quot;Here is what we need to do for ${PATIENT_NAME} ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;The response is intercepted locally, to restore back sensitive data placeholders &lt;code&gt;&amp;quot;Here is what we need to do for Jensen Huang ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;So you get the final response as &lt;code&gt;&amp;quot;Here is what we need to do for Jensen Huang ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0hdeej8j7olf1.png?width=3685&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d154c9bc8e83ba1a67915e12f71dbcd759558b3"&gt;diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this way, secrets never leave your machine. The cloud AI gets the semantic context it needs to be useful, but never sees the actual data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My implementation: PromptMask, a local LLM-based privacy filter for LLMs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It can be installed as a python package &lt;code&gt;pip install promptmask&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Aiming at seamless integration and user experience, I managed to implement two easy ways for use:&lt;/p&gt; &lt;p&gt;For python developer, it provides a drop-in replacement for the OpenAI SDK &lt;/p&gt; &lt;pre&gt;&lt;code&gt;from promptmask import OpenAIMasked as OpenAI &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For everyone else, if you use apps that connect to an OpenAI-compatible API, you can run a local API gateway.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install &amp;quot;promptmask[web]&amp;quot; promptmask-web &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This spins up a server on localhost:8000. Point your app's API endpoint to &lt;a href="http://localhost:8000/gateway/v1/chat/completions"&gt;http://localhost:8000/gateway/v1/chat/completions&lt;/a&gt;, and in the promptmask config file, add your cloud AI provider URL as upstream, it will automatically handle the masking/unmasking for any tool you use.&lt;/p&gt; &lt;p&gt;PromptMask itself does not include any LLM server, you will need to run a local model with Ollama, llama.cpp, vLLM, etc.&lt;/p&gt; &lt;p&gt;GitHub Repo (MIT Licensed): &lt;a href="https://github.com/cxumol/promptmask"&gt;https://github.com/cxumol/promptmask&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You &lt;strong&gt;don't need a 70B model&lt;/strong&gt; to spot passwords and passport numbers. Together with PromptMask, I built an eval framework and benchmarked a bunch of models. The results show that even ~1B models can do the job with good few-shot prompting. See &lt;a href="https://github.com/cxumol/promptmask/blob/master/eval/benchmark.md"&gt;https://github.com/cxumol/promptmask/blob/master/eval/benchmark.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---------&lt;/p&gt; &lt;p&gt;For a much deeper dive into the &amp;quot;why&amp;quot; and &amp;quot;how,&amp;quot; including the prompt engineering for small models and the benchmark setup, I wrote a full blog post about it here: &lt;a href="https://xirtam.cxumol.com/promptmask-how-not-give-ai-secrets/"&gt;https://xirtam.cxumol.com/promptmask-how-not-give-ai-secrets/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to get your feedback on this approach and the tool itself.&lt;/p&gt; &lt;p&gt;Edit: add diagram, formatting, fix typos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cxu25"&gt; /u/cxu25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T00:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n21tb6</id>
    <title>Using Qwen to generate 3D assets in Blender</title>
    <updated>2025-08-28T03:33:45+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"&gt; &lt;img alt="Using Qwen to generate 3D assets in Blender" src="https://external-preview.redd.it/9uCJCtXh3hjjUjvHiYJX5nSL5ngxbe-WclVIpprEgOA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9cd1fe81e5b3dea5c4706e7a6a97566c3d8f5c9" title="Using Qwen to generate 3D assets in Blender" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working on an AI agent that hooks up to Blender to generate low poly models. So far I'm impressed by Qwen's ability to generate and think usable code for this. Inspired by indie game dev where I constantly needed quick models for placeholders or prototyping. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/qzOMpqr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T03:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1paeu</id>
    <title>What you think it will be..</title>
    <updated>2025-08-27T18:37:50+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt; &lt;img alt="What you think it will be.." src="https://preview.redd.it/68wbznvowllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f674727eff3b01ce03cc3be22d7a1f41fa83009d" title="What you think it will be.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68wbznvowllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T18:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n28n3v</id>
    <title>Sparrow: Custom language model architecture for microcontrollers like the ESP32</title>
    <updated>2025-08-28T10:31:54+00:00</updated>
    <author>
      <name>/u/c-f_i</name>
      <uri>https://old.reddit.com/user/c-f_i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt; &lt;img alt="Sparrow: Custom language model architecture for microcontrollers like the ESP32" src="https://external-preview.redd.it/dnB2ZnpraGdrcWxmMXI7s2J-dYT-lngU-7I3sc5b7CKL3t5WhtAsvCq_0YDI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e51762114b8f90c87d5be2a46288d85439d46b36" title="Sparrow: Custom language model architecture for microcontrollers like the ESP32" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Above is a video of Sparrow LM running on 1 core of the ESP32S3 while another core dedicated to the webserver/webapp, to showcase a ChatGPT-like system, although of course the models can be used for anything from text to sentiment analysis, time series analysis and more, depending how it is trained.&lt;/p&gt; &lt;p&gt;I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.&lt;/p&gt; &lt;p&gt;The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.&lt;/p&gt; &lt;p&gt;The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.&lt;/p&gt; &lt;p&gt;I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.&lt;/p&gt; &lt;p&gt;It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.&lt;/p&gt; &lt;p&gt;Let me know what you think! I have a lot more videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms. &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8"&gt;Here's the above video&lt;/a&gt; in 4K on YouTube, and &lt;a href="https://youtu.be/waqzSlAR0iY?si=Qfms7pVaRm0RMAR5"&gt;here's &lt;/a&gt;another video of it running without the Webapp overhead on the ESP32P4. &lt;a href="https://youtube.com/shorts/4xjKu1FP1I4?si=SWXVUj898T9ThNAy"&gt;This YouTube Short&lt;/a&gt; showcases Sparrow on PC with a simple webapp design with Streamlit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c-f_i"&gt; /u/c-f_i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pefagkhgkqlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T10:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2b4et</id>
    <title>Qwen / Tongyi Lab launches GUI-Owl &amp; Mobile-Agent-v3</title>
    <updated>2025-08-28T12:39:15+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt; &lt;img alt="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" src="https://b.thumbs.redditmedia.com/iUG3obaGCmAVOp3wRA9NxR4f0cDoay4umqV_naioeDc.jpg" title="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/X-PLUG/MobileAgent"&gt;https://github.com/X-PLUG/MobileAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full Research Paper: &lt;a href="https://arxiv.org/abs/2508.15144"&gt;https://arxiv.org/abs/2508.15144&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2b4et"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n24utb</id>
    <title>RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)</title>
    <updated>2025-08-28T06:29:26+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt; &lt;img alt="RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)" src="https://external-preview.redd.it/eTdoNDByeThlcGxmMX--5rdiQuwxJ4jOINV8QPW9HN9UrvcxZxCYZhm1-TIi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264a3e65f2cd5a66911e7233d68932539c3879a6" title="RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created and released open source the ComfyUI Wrapper for VibeVoice.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Speaker Node&lt;/strong&gt; to simplify workflow management when using only one voice.&lt;/li&gt; &lt;li&gt;Ability to load text from a file. This allows you to generate speech for the equivalent of dozens of minutes. The longer the text, the longer the generation time (obviously).&lt;/li&gt; &lt;li&gt;I tested cloning my real voice. I only provided a 56-second sample, and the results were very positive. You can see them in the video.&lt;/li&gt; &lt;li&gt;From my tests (not to be considered conclusive): when providing voice samples in a language other than English or Chinese (e.g. Italian), the model can generate speech in that same language (Italian) with a decent success rate. On the other hand, when providing English samples, I couldn’t get valid results when trying to generate speech in another language (e.g. Italian).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Speakers&lt;/strong&gt; &lt;strong&gt;Node&lt;/strong&gt;, which allows up to 4 speakers (limit set by the Microsoft model). Results are decent only with the 7B model. The valid success rate is still much lower compared to single speaker generation. In short: the model looks very promising but still premature. The wrapper will still be adaptable to future updates of the model. Keep in mind the 7B model is still officially in &lt;em&gt;Preview&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How much VRAM is needed?&lt;/strong&gt; Right now I’m only using the official models (so, maximum quality). The 1.5B model requires about &lt;strong&gt;5GB VRAM&lt;/strong&gt;, while the 7B model requires about &lt;strong&gt;17GB VRAM&lt;/strong&gt;. I haven’t tested on low-resource machines yet. To reduce resource usage, we’ll have to wait for quantized models or, if I find the time, I’ll try quantizing them myself (no promises).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My thoughts on this model:&lt;/strong&gt;&lt;br /&gt; A big step forward for the Open Weights ecosystem, and I’m really glad Microsoft released it. At its current stage, I see single-speaker generation as very solid, while multi-speaker is still too immature. But take this with a grain of salt. I may not have fully figured out how to get the best out of it yet. The real difference is the success rate between single-speaker and multi-speaker.&lt;/p&gt; &lt;p&gt;This model is &lt;em&gt;heavily&lt;/em&gt; influenced by the seed. Some seeds produce fantastic results, while others are really bad. With images, such wide variation can be useful. For voice cloning, though, it would be better to have a more deterministic model where the seed matters less.&lt;/p&gt; &lt;p&gt;In practice, this means you have to experiment with several seeds before finding the perfect voice. That can work for some workflows but not for others.&lt;/p&gt; &lt;p&gt;With multi-speaker, the problem gets worse because a single seed drives the entire conversation. You might get one speaker sounding great and another sounding off.&lt;/p&gt; &lt;p&gt;Personally, I think I’ll stick to using single-speaker generation even for multi-speaker conversations unless a future version of the model becomes more deterministic.&lt;/p&gt; &lt;p&gt;That being said, it’s still a &lt;em&gt;huge&lt;/em&gt; step forward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;URL to ComfyUI Wrapper:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy7k60z8eplf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T06:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n22xbl</id>
    <title>HunyuanVideo-Foley is out, an open source text-video-to-audio model</title>
    <updated>2025-08-28T04:33:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt; &lt;img alt="HunyuanVideo-Foley is out, an open source text-video-to-audio model" src="https://external-preview.redd.it/dXU2amRweXd1b2xmMTawZyv5aMEWeESK9yBcqymop7gFK-DtVYY3rCRDUSQp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39348627bb256dad55b1e266f8a7ec0f5b4b62ff" title="HunyuanVideo-Foley is out, an open source text-video-to-audio model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;try HunyuanVideo-Foley: &lt;a href="https://hunyuan.tencent.com/video/zh?tabIndex=0"&gt;https://hunyuan.tencent.com/video/zh?tabIndex=0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/tencent/HunyuanVideo-Foley"&gt;https://huggingface.co/tencent/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley"&gt;https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://szczesnys.github.io/hunyuanvideo-foley/"&gt;https://szczesnys.github.io/hunyuanvideo-foley/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Research report: &lt;a href="https://arxiv.org/abs/2508.16930"&gt;https://arxiv.org/abs/2508.16930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jpjpqw2xuolf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T04:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
</feed>
