<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-17T16:24:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1koagwh</id>
    <title>Offline real-time voice conversations with custom chatbots using AI Runner</title>
    <updated>2025-05-16T20:06:51+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"&gt; &lt;img alt="Offline real-time voice conversations with custom chatbots using AI Runner" src="https://external-preview.redd.it/cSHoRfgGCxZ0WRaZwhiy2L4dmB2Ncgy7iEYxgxHsJTE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=506daa7e710caccec20b3c8724df06d9e0cf2a5c" title="Offline real-time voice conversations with custom chatbots using AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/n0SaEkXmeaA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T20:06:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kofuse</id>
    <title>ArchGW 0.2.8 is out 🚀 - unifying repeated "low-level" functionality in building LLM apps via a local proxy.</title>
    <updated>2025-05-17T00:11:49+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"&gt; &lt;img alt="ArchGW 0.2.8 is out 🚀 - unifying repeated &amp;quot;low-level&amp;quot; functionality in building LLM apps via a local proxy." src="https://preview.redd.it/gap0dbz2h81f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e218b966553c521564274d6134d94b9521e83f61" title="ArchGW 0.2.8 is out 🚀 - unifying repeated &amp;quot;low-level&amp;quot; functionality in building LLM apps via a local proxy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am thrilled about our latest release: &lt;a href="https://github.com/katanemo/archgw"&gt;Arch 0.2.8&lt;/a&gt;. Initially we handled calls made to LLMs - to unify key management, track spending consistently, improve resiliency and improve model choice - but we just added support for an ingress listener (on the same running process) to handle both ingress an egress functionality that is common and repeated in application code today - now managed by an intelligent local proxy (in a framework and language agnostic way) that makes building AI applications faster, safer and more consistently between teams. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in 0.2.8.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added support for bi-directional traffic as a first step to support Google's A2A&lt;/li&gt; &lt;li&gt;Improved &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat 3B&lt;/a&gt; LLM for fast routing and common tool calling scenarios&lt;/li&gt; &lt;li&gt;Support for LLMs hosted on Groq&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;🚦 Ro&lt;/code&gt;uting. Engineered with purpose-built &lt;a href="https://huggingface.co/collections/katanemo/arch-function-66f209a693ea8df14317ad68"&gt;LLMs&lt;/a&gt; for fast (&amp;lt;100ms) agent routing and hand-off&lt;/li&gt; &lt;li&gt;&lt;code&gt;⚡ Tools Use&lt;/code&gt;: For common agentic scenarios Arch clarifies prompts and makes tools calls&lt;/li&gt; &lt;li&gt;&lt;code&gt;⛨ Guardrails&lt;/code&gt;: Centrally configure and prevent harmful outcomes and enable safe interactions&lt;/li&gt; &lt;li&gt;&lt;code&gt;🔗 Access t&lt;/code&gt;o LLMs: Centralize access and traffic to LLMs with smart retries&lt;/li&gt; &lt;li&gt;&lt;code&gt;🕵 Observab&lt;/code&gt;ility: W3C compatible request tracing and LLM metrics&lt;/li&gt; &lt;li&gt;&lt;code&gt;🧱 Built on&lt;/code&gt; Envoy: Arch runs alongside app servers as a containerized process, and builds on top of &lt;a href="https://envoyproxy.io"&gt;Envoy's&lt;/a&gt; proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gap0dbz2h81f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T00:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kophr9</id>
    <title>Just benchmarked the 5060TI...</title>
    <updated>2025-05-17T10:05:41+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;Model Eval. Toks Resp. toks Total toks&lt;/code&gt;&lt;br /&gt; &lt;code&gt;mistral-nemo:12b-instruct-2407-q8_0 290.38 30.93 31.50&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llama3.1:8b-instruct-q8_0 563.90 46.19 47.53&lt;/code&gt; &lt;/p&gt; &lt;p&gt;I've had to change the process on vast cause with the 50 series I'm having reliability issues, some instances have very degraded performance, so I have to test on multiple instances and pick the most performant one then test 3 times to see if the results are reliable&lt;/p&gt; &lt;p&gt;It's about 30% faster than the 4060TI.&lt;/p&gt; &lt;p&gt;As usual I put the full list here&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kophr9/just_benchmarked_the_5060ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kophr9/just_benchmarked_the_5060ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kophr9/just_benchmarked_the_5060ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T10:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2mq7</id>
    <title>what happened to Stanford</title>
    <updated>2025-05-16T14:44:17+00:00</updated>
    <author>
      <name>/u/BoringAd6806</name>
      <uri>https://old.reddit.com/user/BoringAd6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt; &lt;img alt="what happened to Stanford" src="https://preview.redd.it/l9ap08t4p51f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99406294d0642388d4c739930b9569d685129d1" title="what happened to Stanford" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoringAd6806"&gt; /u/BoringAd6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l9ap08t4p51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1koggmm</id>
    <title>My voice dataset creator is now on Colab with a GUI</title>
    <updated>2025-05-17T00:43:45+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"&gt; &lt;img alt="My voice dataset creator is now on Colab with a GUI" src="https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2817183828c9747b960cb2e55c59cfa41f4f9ded" title="My voice dataset creator is now on Colab with a GUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My voice extractor tool is now on Google Colab with a GUI interface. Tested it with one minute of audio and it processed in about 5 minutes on Colab's CPU - much slower than with a GPU, but still works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://colab.research.google.com/github/ReisCook/Voice_Extractor_Colab/blob/main/Voice_Extractor_Colab.ipynb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T00:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kof8ni</id>
    <title>On the universality of BitNet models</title>
    <updated>2025-05-16T23:41:01+00:00</updated>
    <author>
      <name>/u/Automatic_Truth_6666</name>
      <uri>https://old.reddit.com/user/Automatic_Truth_6666</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt; &lt;img alt="On the universality of BitNet models" src="https://b.thumbs.redditmedia.com/OsDAcsjTSwHMN5CuEUlkPyEBrJv-5Ly3N_qkaYzNlas.jpg" title="On the universality of BitNet models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mrig6j2bc81f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=715576fa0b82337c43c5150bb06950c9a39e45d0"&gt;https://preview.redd.it/mrig6j2bc81f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=715576fa0b82337c43c5150bb06950c9a39e45d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the &amp;quot;novelty&amp;quot; of the recent Falcon-E release is that the checkpoints are universal, meaning they can be reverted back to bfloat16 format, llama compatible, with almost no performance degradation. e.g. you can test the 3B bf16 here: &lt;a href="https://chat.falconllm.tii.ae/"&gt;https://chat.falconllm.tii.ae/&lt;/a&gt; and the quality is very decent from our experience (especially on math questions)&lt;br /&gt; This also means in a single pre-training run you can get at the same time the bf16 model and the bitnet counterpart.&lt;br /&gt; This can be interesting from the pre-training perspective and also adoption perspective (not all people want bitnet format), to what extend do you think this &amp;quot;property&amp;quot; of Bitnet models can be useful for the community?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Truth_6666"&gt; /u/Automatic_Truth_6666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T23:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kon5wd</id>
    <title>Best LLM benchmark for Rust coding?</title>
    <updated>2025-05-17T07:19:45+00:00</updated>
    <author>
      <name>/u/vhthc</name>
      <uri>https://old.reddit.com/user/vhthc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know about a current good LLM benchmark for Rust code?&lt;/p&gt; &lt;p&gt;I have found these so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://leaderboard.techfren.net/"&gt;https://leaderboard.techfren.net/&lt;/a&gt; - can toggle to Rust - most current I found, but very small list of models, no qwq32, o4, claude 3.7, deepseek chat, etc. uses the aider polyglot benchmark which has 30 rust testcases.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://www.prollm.ai/leaderboard/stack-eval?type=conceptual,debugging,implementation,optimization&amp;amp;level=advanced,beginner,intermediate&amp;amp;tag=rust"&gt;https://www.prollm.ai/leaderboard/stack-eval?type=conceptual,debugging,implementation,optimization&amp;amp;level=advanced,beginner,intermediate&amp;amp;tag=rust&lt;/a&gt; - only 23 test cases. very current with models&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://www.prollm.ai/leaderboard/stack-unseen?type=conceptual,debugging,implementation,optimization,version&amp;amp;level=advanced,beginner,intermediate&amp;amp;tag=rust"&gt;https://www.prollm.ai/leaderboard/stack-unseen?type=conceptual,debugging,implementation,optimization,version&amp;amp;level=advanced,beginner,intermediate&amp;amp;tag=rust&lt;/a&gt; - only has 3 test cases. pointless :-(&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://llm.extractum.io/list/?benchmark=bc_lang_rust"&gt;https://llm.extractum.io/list/?benchmark=bc_lang_rust&lt;/a&gt; - although still being updated with models it is missing a ton - no qwen 3 or any deepseek model. I also find suspicious that qwen coder 2.5 32b has the same score as SqlCoder 8bit. I assume this means too small number of testcases&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard"&gt;https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard&lt;/a&gt; - needs to click on &amp;quot;view all columns&amp;quot; and select rust. no deepseek r1 or chat, no qwen 3, and from the ranking this one looks too like too few testcases&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When I compare &lt;a href="https://www.prollm.ai/leaderboard/stack-eval"&gt;https://www.prollm.ai/leaderboard/stack-eval&lt;/a&gt; to &lt;a href="https://leaderboard.techfren.net/"&gt;https://leaderboard.techfren.net/&lt;/a&gt; the ranking is so different that I trust neither.&lt;/p&gt; &lt;p&gt;So is there a better Rust benchmark out there? Or which one is the most reliable? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vhthc"&gt; /u/vhthc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kon5wd/best_llm_benchmark_for_rust_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kon5wd/best_llm_benchmark_for_rust_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kon5wd/best_llm_benchmark_for_rust_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T07:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kovobp</id>
    <title>What to do with extra PC</title>
    <updated>2025-05-17T15:29:42+00:00</updated>
    <author>
      <name>/u/PickleSavings1626</name>
      <uri>https://old.reddit.com/user/PickleSavings1626</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Work gives me $200/months stipend to buy whatever I want, mainly for happiness (they are big on mental health). Not knowing what to buy, I now have a maxed out mac mini and a 6750 XT GPU rig. They both just sit there. I usually use LM Studio on my Macbook Pro. Any suggestions on what to do with these? I don’t think I can link them up for faster LLM work or higher context windows. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PickleSavings1626"&gt; /u/PickleSavings1626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kovobp/what_to_do_with_extra_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T15:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1v1k</id>
    <title>I built a tiny Linux OS to make your LLMs actually useful on your machine</title>
    <updated>2025-05-16T14:12:00+00:00</updated>
    <author>
      <name>/u/iluxu</name>
      <uri>https://old.reddit.com/user/iluxu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt; &lt;img alt="I built a tiny Linux OS to make your LLMs actually useful on your machine" src="https://external-preview.redd.it/Opn0lWenfUSxX1FlZaKUoyxIpn8_sSk-rxtkMoj2byo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62958b7668163a6b32bc9aa0eddc4ec07f59c982" title="I built a tiny Linux OS to make your LLMs actually useful on your machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I’ve been working on llmbasedos, a minimal Arch-based Linux distro that turns your local environment into a first-class citizen for any LLM frontend (like Claude Desktop, VS Code, ChatGPT+browser, etc).&lt;/p&gt; &lt;p&gt;The problem: every AI app has to reinvent the wheel — file pickers, OAuth flows, plugins, sandboxing… The idea: expose local capabilities (files, mail, sync, agents) via a clean, JSON-RPC protocol called MCP (Model Context Protocol).&lt;/p&gt; &lt;p&gt;What you get: • An MCP gateway (FastAPI) that routes requests • Small Python daemons that expose specific features (FS, mail, sync, agents) • Auto-discovery via .cap.json — your new feature shows up everywhere • Optional offline mode (llama.cpp included), or plug into GPT-4o, Claude, etc.&lt;/p&gt; &lt;p&gt;It’s meant to be dev-first. Add a new capability in under 50 lines. Zero plugins, zero hacks — just a clean system-wide interface for your AI.&lt;/p&gt; &lt;p&gt;Open-core, Apache-2.0 license.&lt;/p&gt; &lt;p&gt;Curious to hear what features you’d build with it — happy to collab if anyone’s down!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iluxu"&gt; /u/iluxu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iluxu/llmbasedos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kopjp8</id>
    <title>You didn't asked, but I need to tell about going local on windows</title>
    <updated>2025-05-17T10:09:32+00:00</updated>
    <author>
      <name>/u/Nepherpitu</name>
      <uri>https://old.reddit.com/user/Nepherpitu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I want to share my experience about running LLMs locally on Windows 11 22H2 with 3x NVIDIA GPUs. I read a lot about how to serve LLM models at home, but almost always guide was about either &lt;code&gt;ollama pull&lt;/code&gt; or linux-specific or for dedicated server. So, I spent some time to figure out how to conveniently run it by myself.&lt;/p&gt; &lt;p&gt;My goal was to achieve 30+ tps for dense 30b+ models with support for all modern features.&lt;/p&gt; &lt;h1&gt;Hardware Info&lt;/h1&gt; &lt;p&gt;My motherboard is regular MSI MAG X670 with PCIe 5.0@x16 + 4.0@x1 (small one) + 4.0@x4 + 4.0@x2 slots. So I able to fit 3 GPUs with only one at full CPIe speed. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 7900X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 64GB DDR5 at 6000MHz&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPUs&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RTX 4090 (CUDA0)&lt;/strong&gt;: Used for gaming and desktop tasks. Also using it to play with diffusion models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2x RTX 3090 (CUDA1, CUDA2)&lt;/strong&gt;: Dedicated to inference. These GPUs are connected via PCIe 4.0. Before bifurcation, they worked at x4 and x2 lines with 35 TPS. Now, after x8+x8 bifurcation, performance is 43 TPS. Using vLLM nightly (v0.9.0) gives 55 TPS.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;PSU: 1600W with PCIe power cables for 4 GPUs, don't remember it's name and it's hidden in spaghetti.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tools and Setup&lt;/h1&gt; &lt;h1&gt;Podman Desktop with GPU passthrough&lt;/h1&gt; &lt;p&gt;I use Podman Desktop and pass GPU access to containers. &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; help target specific GPUs, because Podman can't pass specific GPUs on its own &lt;a href="https://podman-desktop.io/docs/podman/gpu"&gt;docs&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;vLLM Nightly Builds&lt;/h1&gt; &lt;p&gt;For Qwen3-32B, I use the &lt;a href="https://hub.docker.com/r/hanseware/vllm-nightly/tags"&gt;hanseware/vllm-nightly&lt;/a&gt; image. It achieves ~55 TPS. But why VLLM? Why not llama.cpp with speculative decoding? Because llama.cpp can't stream tool calls. So it don't work with continue.dev. But don't worry, continue.dev agentic mode is so broken it won't work with vllm either - &lt;a href="https://github.com/continuedev/continue/issues/5508"&gt;https://github.com/continuedev/continue/issues/5508&lt;/a&gt;. Also, &lt;code&gt;--split-mode row&lt;/code&gt; cripples performance for me. I don't know why, but tensor parallelism works for me only with VLLM and TabbyAPI. And TabbyAPI is a bit outdated, struggle with function calls and EXL2 has some weird issues with chinese characters in output if I'm using it with my native language.&lt;/p&gt; &lt;h1&gt;llama-swap&lt;/h1&gt; &lt;p&gt;Windows does not support vLLM natively, so containers are needed. Earlier versions of &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; could not stop Podman processes properly. The author added &lt;code&gt;cmdStop&lt;/code&gt; (like &lt;code&gt;podman stop vllm-qwen3-32b&lt;/code&gt;) to fix this after I asked for help (GitHub issue #130).&lt;/p&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Qwen3-32B-AWQ with vLLM achieved ~55 TPS for small context and goes down to 30 TPS when context growth to 24K tokens. With Llama.cpp I can't get more than 20.&lt;/li&gt; &lt;li&gt;Qwen3-30B-Q6 runs at 100 TPS with llama.cpp VULKAN, going down to 70 TPS at 24K.&lt;/li&gt; &lt;li&gt;Qwen3-30B-AWQ runs at 100 TPS with VLLM as well.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Configuration Examples&lt;/h1&gt; &lt;p&gt;Below are some snippets from my &lt;code&gt;config.yaml&lt;/code&gt;:&lt;/p&gt; &lt;h1&gt;Qwen3-30B with VULKAN (llama.cpp)&lt;/h1&gt; &lt;p&gt;This model uses the &lt;code&gt;script.ps1&lt;/code&gt; to lock GPU clocks at high values during model loading for ~15 seconds, then reset them. Without this, Vulkan loading time would be significantly longer. Ask it to write such script, it's easy using nvidia-smi.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;qwen3-30b&amp;quot;: cmd: &amp;gt; powershell -File ./script.ps1 -launch &amp;quot;./llamacpp/vulkan/llama-server.exe --jinja --reasoning-format deepseek --no-mmap --no-warmup --host 0.0.0.0 --port ${PORT} --metrics --slots -m ./models/Qwen3-30B-A3B-128K-UD-Q6_K_XL.gguf -ngl 99 --flash-attn --ctx-size 65536 -ctk q8_0 -ctv q8_0 --min-p 0 --top-k 20 --no-context-shift -dev VULKAN1,VULKAN2 -ts 100,100 -t 12 --log-colors&amp;quot; -lock &amp;quot;./gpu-lock-clocks.ps1&amp;quot; -unlock &amp;quot;./gpu-unlock-clocks.ps1&amp;quot; ttl: 0 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen3-32B with vLLM (Nightly Build)&lt;/h1&gt; &lt;p&gt;The &lt;code&gt;tool-parser-plugin&lt;/code&gt; is from &lt;a href="https://github.com/vllm-project/vllm/pull/18220"&gt;this unmerged PR&lt;/a&gt;. It works, but the path must be set manually to podman host machine filesystem, which is inconvenient.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;qwen3-32b&amp;quot;: cmd: | podman run --name vllm-qwen3-32b --rm --gpus all --init -e &amp;quot;CUDA_VISIBLE_DEVICES=1,2&amp;quot; -e &amp;quot;HUGGING_FACE_HUB_TOKEN=hf_XXXXXX&amp;quot; -e &amp;quot;VLLM_ATTENTION_BACKEND=FLASHINFER&amp;quot; -v /home/user/.cache/huggingface:/root/.cache/huggingface -v /home/user/.cache/vllm:/root/.cache/vllm -p ${PORT}:8000 --ipc=host hanseware/vllm-nightly:latest --model /root/.cache/huggingface/Qwen3-32B-AWQ -tp 2 --max-model-len 65536 --enable-auto-tool-choice --tool-parser-plugin /root/.cache/vllm/qwen_tool_parser.py --tool-call-parser qwen3 --reasoning-parser deepseek_r1 -q awq_marlin --served-model-name qwen3-32b --kv-cache-dtype fp8_e5m2 --max-seq-len-to-capture 65536 --rope-scaling &amp;quot;{\&amp;quot;rope_type\&amp;quot;:\&amp;quot;yarn\&amp;quot;,\&amp;quot;factor\&amp;quot;:4.0,\&amp;quot;original_max_position_embeddings\&amp;quot;:32768}&amp;quot; --gpu-memory-utilization 0.95 cmdStop: podman stop vllm-qwen3-32b ttl: 0 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen2.5-Coder-7B on CUDA0 (4090)&lt;/h1&gt; &lt;p&gt;This is a small model that auto-unloads after 600 seconds. It consume only 10-12 GB of VRAM on the 4090 and used for FIM completions.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;qwen2.5-coder-7b&amp;quot;: cmd: | ./llamacpp/cuda12/llama-server.exe -fa --metrics --host 0.0.0.0 --port ${PORT} --min-p 0.1 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 --temp 0.7 -m ./models/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf --no-mmap -ngl 99 --ctx-size 32768 -ctk q8_0 -ctv q8_0 -dev CUDA0 ttl: 600 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Thanks&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ggml-org/llama.cpp team&lt;/strong&gt; for llama.cpp :).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mostlygeek&lt;/strong&gt; for &lt;code&gt;llama-swap&lt;/code&gt; :)).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vllm team&lt;/strong&gt; for great vllm :))).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Anonymous person&lt;/strong&gt; who builds and hosts vLLM nightly Docker image – it is very helpful for performance. I tried to build it myself, but it's a mess with running around random errors. And each run takes 1.5 hours.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 32B&lt;/strong&gt; for writing this post. Yes, I've edited it, but still counts.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nepherpitu"&gt; /u/Nepherpitu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kopjp8/you_didnt_asked_but_i_need_to_tell_about_going/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kopjp8/you_didnt_asked_but_i_need_to_tell_about_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kopjp8/you_didnt_asked_but_i_need_to_tell_about_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T10:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko0khr</id>
    <title>Stanford has dropped AGI</title>
    <updated>2025-05-16T13:15:17+00:00</updated>
    <author>
      <name>/u/Abject-Huckleberry13</name>
      <uri>https://old.reddit.com/user/Abject-Huckleberry13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt; &lt;img alt="Stanford has dropped AGI" src="https://external-preview.redd.it/RLiqoJrn4RdLs0J4_egpcYM7T2LlLp_klpSUS3M3qFg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa5717d8c431adaa645d19436f3ab2adbc6cfc8" title="Stanford has dropped AGI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Huckleberry13"&gt; /u/Abject-Huckleberry13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Stanford/Rivermind-AGI-12B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1koccyx</id>
    <title>I just to give love to Mistral ❤️🥐</title>
    <updated>2025-05-16T21:27:29+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Of all the open models, Mistral's offerings (particularly Mistral Small) has to be the one of the most consistent in terms of just getting the task done. &lt;/p&gt; &lt;p&gt;Yesterday wanted to turn a 214 row, 4 column row into a list. Tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flash 2.5 - worked but stopped short a few times&lt;/li&gt; &lt;li&gt;Chatgpt 4.1 - asked a few questions to clarify,started and stopped&lt;/li&gt; &lt;li&gt;Meta llama 4 - did a good job, but stopped just slight short&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hit up Lè Chat , paste in CSV , seconds later , list done. &lt;/p&gt; &lt;p&gt;In my own experience, I have defaulted to Mistral Small in my chrome extension PromptPaul, and Small handles tools, requests and just about any of the circa 100 small jobs I throw it each day with ease.&lt;/p&gt; &lt;p&gt;Thank you Mistral.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T21:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko6hy7</id>
    <title>When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane.</title>
    <updated>2025-05-16T17:21:55+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"&gt; &lt;img alt="When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane." src="https://preview.redd.it/1fwbjz4zf61f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c05a4e8bffe7ec3f4e56031dc110d91c80808d7b" title="When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can remember, like a few months ago, I ran some of the smaller models with &amp;lt;7B parameters and couldn't even get coherent sentences. This 4B model runs super fast and answered this question perfectly. To be fair, it probably has seen a lot of these examples in it's training data but nonetheless - it's crazy. I only ran this prompt in English to show it here but initially it was in German. Also there, got very well expressed explanations for my question. Crazy that this comes from a 2.6GB file of structured numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1fwbjz4zf61f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T17:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1iob</id>
    <title>Ollama violating llama.cpp license for over a year</title>
    <updated>2025-05-16T13:57:38+00:00</updated>
    <author>
      <name>/u/op_loves_boobs</name>
      <uri>https://old.reddit.com/user/op_loves_boobs</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/op_loves_boobs"&gt; /u/op_loves_boobs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44003741"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1komb56</id>
    <title>Pivotal Token Search (PTS): Optimizing LLMs by targeting the tokens that actually matter</title>
    <updated>2025-05-17T06:21:59+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;Pivotal Token Search (PTS)&lt;/strong&gt;, a technique for identifying and targeting critical decision points in language model generations that I've just open-sourced.&lt;/p&gt; &lt;h1&gt;What is PTS and why should you care?&lt;/h1&gt; &lt;p&gt;Have you ever noticed that when an LLM solves a problem, there are usually just a few key decision points where it either stays on track or goes completely off the rails? That's what PTS addresses.&lt;/p&gt; &lt;p&gt;Inspired by the recent &lt;a href="https://arxiv.org/abs/2412.08905v1"&gt;Phi-4 paper from Microsoft&lt;/a&gt;, PTS identifies &amp;quot;pivotal tokens&amp;quot; - specific points in a generation where the next token dramatically shifts the probability of a successful outcome.&lt;/p&gt; &lt;p&gt;Traditional DPO treats all tokens equally, but in reality, a tiny fraction of tokens are responsible for most of the success or failure. By targeting these, we can get more efficient training and better results.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;PTS uses a binary search algorithm to find tokens that cause significant shifts in solution success probability:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We take a model's solution to a problem with a known ground truth&lt;/li&gt; &lt;li&gt;We sample completions from different points in the solution to estimate success probability&lt;/li&gt; &lt;li&gt;We identify where adding a single token causes a large jump in this probability&lt;/li&gt; &lt;li&gt;We then create DPO pairs focused &lt;em&gt;specifically&lt;/em&gt; on these pivotal decision points&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example, in a math solution, choosing &amp;quot;cross-multiplying&amp;quot; vs &amp;quot;multiplying both sides&amp;quot; might dramatically affect the probability of reaching the correct answer, even though both are valid operations.&lt;/p&gt; &lt;h1&gt;What's included in the repo&lt;/h1&gt; &lt;p&gt;The &lt;a href="https://github.com/codelion/pts"&gt;GitHub repository&lt;/a&gt; contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complete implementation of the PTS algorithm&lt;/li&gt; &lt;li&gt;Data generation pipelines&lt;/li&gt; &lt;li&gt;Examples and usage guides&lt;/li&gt; &lt;li&gt;Evaluation tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, we've released:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets?other=pts"&gt;Pre-generated datasets&lt;/a&gt; for multiple domains&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/models?other=pts"&gt;Pre-trained models&lt;/a&gt; fine-tuned with PTS-generated preference pairs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/codelion/pts"&gt;https://github.com/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Datasets: &lt;a href="https://huggingface.co/datasets?other=pts"&gt;https://huggingface.co/datasets?other=pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Models: &lt;a href="https://huggingface.co/models?other=pts"&gt;https://huggingface.co/models?other=pts&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear about your experiences if you try it out! What other applications can you think of for this approach? Any suggestions for improvements or extensions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1komb56/pivotal_token_search_pts_optimizing_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T06:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko27bi</id>
    <title>Did Standford HuggingFace account got Hacked?</title>
    <updated>2025-05-16T14:26:25+00:00</updated>
    <author>
      <name>/u/ObscuraMirage</name>
      <uri>https://old.reddit.com/user/ObscuraMirage</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObscuraMirage"&gt; /u/ObscuraMirage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0j4j7z8yl51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1koqlmm</id>
    <title>Best model for upcoming 128GB unified memory machines?</title>
    <updated>2025-05-17T11:19:22+00:00</updated>
    <author>
      <name>/u/woahdudee2a</name>
      <uri>https://old.reddit.com/user/woahdudee2a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen-3 32B at Q8 is likely the best local option for now at just 34 GB, but surely we can do better?&lt;/p&gt; &lt;p&gt;Maybe the Qwen-3 235B-A22B at Q3 is possible, though it seems quite sensitive to quantization, so Q3 might be too aggressive.&lt;/p&gt; &lt;p&gt;Isn't there a more balanced 70B-class model that would fit this machine better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woahdudee2a"&gt; /u/woahdudee2a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koqlmm/best_model_for_upcoming_128gb_unified_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T11:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kosz97</id>
    <title>Orin Nano finally arrived in the mail. What should I do with it?</title>
    <updated>2025-05-17T13:26:49+00:00</updated>
    <author>
      <name>/u/miltonthecat</name>
      <uri>https://old.reddit.com/user/miltonthecat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"&gt; &lt;img alt="Orin Nano finally arrived in the mail. What should I do with it?" src="https://b.thumbs.redditmedia.com/kqIcM98T4EAIxbeoEQuku-Ke2-Up08JpbmnrUxjlTfI.jpg" title="Orin Nano finally arrived in the mail. What should I do with it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking of running home assistant with a local voice model or something like that. Open to any and all suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/miltonthecat"&gt; /u/miltonthecat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kosz97"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosz97/orin_nano_finally_arrived_in_the_mail_what_should/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T13:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kojtwd</id>
    <title>Qwen is about to release a new model?</title>
    <updated>2025-05-17T03:47:46+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.10527"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kojtwd/qwen_is_about_to_release_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kojtwd/qwen_is_about_to_release_a_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T03:47:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kony6o</id>
    <title>Orpheus-TTS is now supported by chatllm.cpp</title>
    <updated>2025-05-17T08:14:32+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"&gt; &lt;img alt="Orpheus-TTS is now supported by chatllm.cpp" src="https://external-preview.redd.it/d2R5dTV2NnV2YTFmMbi8x691ZBFKYQvO7W9KNJH0CgcVBTuUP81YP-JSjSnu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4df49a2ab27c9a554035f635b2cd548a552a97be" title="Orpheus-TTS is now supported by chatllm.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy to share that &lt;a href="https://github.com/foldl/chatllm.cpp"&gt;chatllm.cpp&lt;/a&gt; now supports Orpheus-TTS models.&lt;/p&gt; &lt;p&gt;The demo audio is generated with this prompt: &lt;/p&gt; &lt;p&gt;```sh&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;build-vulkan\bin\Release\main.exe -m quantized\orpheus-tts-en-3b.bin -i --max&lt;em&gt;length 1000 _&lt;/em&gt;______ __ __ __ __ ___ / _&lt;strong&gt;&lt;em&gt;/ /&lt;/em&gt; __&lt;/strong&gt; &lt;em&gt;/ /&lt;/em&gt;/ / / / / |/ /_________ ____ / / / __ / __ `/ &lt;strong&gt;/ / / / / /|&lt;em&gt;/ // _&lt;/em&gt;&lt;em&gt;/ _&lt;/em&gt; / __ \ / /&lt;/strong&gt;&lt;em&gt;/ / / / /&lt;/em&gt;/ / /&lt;em&gt;/ /&lt;/em&gt;&lt;strong&gt;/ /&lt;/strong&gt;&lt;em&gt;/ / / // /&lt;/em&gt;&lt;em&gt;/ /&lt;/em&gt;/ / /&lt;em&gt;/ / \&lt;/em&gt;&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;/ /_/\&lt;/strong&gt;,&lt;em&gt;/\&lt;/em&gt;&lt;em&gt;/&lt;/em&gt;_&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;/&lt;em&gt;/ /&lt;/em&gt;(_)&lt;/strong&gt;&lt;em&gt;/ .&lt;/em&gt;&lt;strong&gt;/ .&lt;/strong&gt;&lt;em&gt;/ You are served by Orpheus-TTS, /&lt;/em&gt;/ /_/ with 3300867072 (3.3B) parameters.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Input &amp;gt; Orpheus-TTS is now supported by chatllm.cpp. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3lyipv6uva1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kony6o/orpheustts_is_now_supported_by_chatllmcpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T08:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kooyfx</id>
    <title>llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)</title>
    <updated>2025-05-17T09:27:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"&gt; &lt;img alt="llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)" src="https://b.thumbs.redditmedia.com/aDTETHCO_H8_4IRBitddp52LLLB3EW99KQXDlvzStXo.jpg" title="llama.cpp benchmarks on 72GB VRAM Setup (2x 3090 + 2x 3060)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Building a LocalLlama Machine – Episode 4:&lt;/strong&gt; &lt;strong&gt;I think I am done (for now!)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I added a second RTX 3090 and replaced 64GB of slower RAM with 128GB of faster RAM.&lt;br /&gt; I think my build is complete for now (unless we get new models in 40B - 120B range!). &lt;/p&gt; &lt;p&gt;GPU Prices:&lt;br /&gt; - 2x RTX 3090 - 6000 PLN&lt;br /&gt; - 2x RTX 3060 - 2500 PLN&lt;br /&gt; - for comparison: single RTX 5090 costs between 12,000 and 15,000 PLN &lt;/p&gt; &lt;p&gt;Here are benchmarks of my system: &lt;/p&gt; &lt;p&gt;Qwen2.5-72B-Instruct-Q6_K - 9.14 t/s&lt;br /&gt; &lt;strong&gt;Qwen3-235B-A22B-Q3_K_M&lt;/strong&gt; - &lt;strong&gt;10.41 t/s (maybe I should try Q4)&lt;/strong&gt;&lt;br /&gt; Llama-3.3-70B-Instruct-Q6_K_L - 11.03 t/s&lt;br /&gt; Qwen3-235B-A22B-Q2_K - 14.77 t/s&lt;br /&gt; nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q8_0 - 15.09 t/s&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q8_0 - 15.1 t/s&lt;br /&gt; &lt;strong&gt;Llama-3.3-70B-Instruct-Q4_K_M&lt;/strong&gt; - &lt;strong&gt;17.4 t/s (important big dense model family)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q6_K&lt;/strong&gt; - &lt;strong&gt;17.84 t/s (kind of improved 70B)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Qwen_Qwen3-32B-Q8_0&lt;/strong&gt; - &lt;strong&gt;22.2 t/s (my fav general model)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;google_gemma-3-27b-it-Q8_0&lt;/strong&gt; - &lt;strong&gt;25.08 t/s (complements Qwen 32B)&lt;/strong&gt;&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q5_K_M - 29.78 t/s&lt;br /&gt; google_gemma-3-12b-it-Q8_0 - 30.68 t/s&lt;br /&gt; &lt;strong&gt;mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q8_0&lt;/strong&gt; - &lt;strong&gt;32.09 t/s (lots of finetunes)&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Llama-4-Scout-17B-16E-Instruct-Q4_K_M&lt;/strong&gt; - &lt;strong&gt;38.75 t/s (fast, very underrated)&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-14B-Q8_0 - 49.47 t/s&lt;br /&gt; microsoft_Phi-4-reasoning-plus-Q8_0 - 50.16 t/s&lt;br /&gt; &lt;strong&gt;Mistral-Nemo-Instruct-2407-Q8_0&lt;/strong&gt; - &lt;strong&gt;59.12 t/s (most finetuned model ever?)&lt;/strong&gt;&lt;br /&gt; granite-3.3-8b-instruct-Q8_0 - 78.09 t/s&lt;br /&gt; Qwen_Qwen3-8B-Q8_0 - 83.13 t/s&lt;br /&gt; Meta-Llama-3.1-8B-Instruct-Q8_0 - 87.76 t/s&lt;br /&gt; Qwen_Qwen3-30B-A3B-Q8_0 - 90.43 t/s&lt;br /&gt; Qwen_Qwen3-4B-Q8_0 - 126.92 t/s &lt;/p&gt; &lt;p&gt;Please look at screenshots to understand how I run these benchmarks, it's not always obvious:&lt;br /&gt; - if you want to use RAM with MoE models, you need to learn how to use the &lt;strong&gt;--override-tensor&lt;/strong&gt; option&lt;br /&gt; - if you want to use different GPUs like I do, you'll need to get familiar with the &lt;strong&gt;--tensor-split&lt;/strong&gt; option &lt;/p&gt; &lt;p&gt;Depending on the model, I use different configurations:&lt;br /&gt; - Single 3090&lt;br /&gt; - Both 3090s&lt;br /&gt; - Both 3090s + one 3060&lt;br /&gt; - Both 3090s + both 3060s&lt;br /&gt; - Both 3090s + both 3060s + RAM/CPU &lt;/p&gt; &lt;p&gt;In my opinion &lt;strong&gt;Llama 4 Scout&lt;/strong&gt; is extremely underrated — it's fast and surprisingly knowledgeable. Maverick is too big for me.&lt;br /&gt; I hope we’ll see some finetunes or variants of this model eventually. I hope Meta will release a 4.1 Scout at some point. &lt;/p&gt; &lt;p&gt;Qwen3 models are awesome, but in general, Qwen tends to lack knowledge about Western culture (movies, music, etc). In that area, Llamas, Mistrals, and Nemotrons perform much better. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please post your benchmarks&lt;/strong&gt; so we could compare different setups &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kooyfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T09:27:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kompbk</id>
    <title>New New Qwen</title>
    <updated>2025-05-17T06:48:29+00:00</updated>
    <author>
      <name>/u/bobby-chan</name>
      <uri>https://old.reddit.com/user/bobby-chan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"&gt; &lt;img alt="New New Qwen" src="https://external-preview.redd.it/KnKOyLV6zthvubjnKd-6Nrxq-GYIVyUyXITDw76dq6k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18734097bc18deaa13d0e68101249a248ed7e211" title="New New Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobby-chan"&gt; /u/bobby-chan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/WorldPM-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kompbk/new_new_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T06:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kotssm</id>
    <title>I believe we're at a point where context is the main thing to improve on.</title>
    <updated>2025-05-17T14:05:40+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like language models have become incredibly smart in the last year or two. Hell even in the past couple months we've gotten Gemini 2.5 and Grok 3 and both are incredible in my opinion. This is where the problems lie though. If I send an LLM a well constructed message these days, it is very uncommon that it misunderstands me. Even the open source and small ones like Gemma 3 27b has understanding and instruction following abilities comparable to gemini but what I feel that every single one of these llms lack in is maintaining context over a long period of time. Even models like gemini that claim to support a 1M context window don't actually support a 1m context window coherently thats when they start screwing up and producing bugs in code that they can't solve no matter what etc. Even Llama 3.1 8b is a really good model and it's so small! Anyways I wanted to know what you guys think. I feel like maintaining context and staying on task without forgetting important parts of the conversation is the biggest shortcoming of llms right now and is where we should be putting our efforts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kotssm/i_believe_were_at_a_point_where_context_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T14:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kosbyy</id>
    <title>GLaDOS has been updated for Parakeet 0.6B</title>
    <updated>2025-05-17T12:55:20+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt; &lt;img alt="GLaDOS has been updated for Parakeet 0.6B" src="https://preview.redd.it/8rtph8367c1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=412a76d5c943b2ae78ee168ac871cf7d6391f4e9" title="GLaDOS has been updated for Parakeet 0.6B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while, but I've had a chance to make &lt;a href="https://github.com/dnhkng/GLaDOS"&gt;a big update to GLaDOS&lt;/a&gt;: A much improved ASR model!&lt;/p&gt; &lt;p&gt;The new &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Nemo Parakeet 0.6B model&lt;/a&gt; is smashing the &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;Huggingface ASR Leaderboard&lt;/a&gt;, both in accuracy (#1!), and also speed (&amp;gt;10x faster then Whisper Large V3).&lt;/p&gt; &lt;p&gt;However, if you have been following the project, you will know I really dislike adding in more dependencies... and Nemo from Nvidia is a huge download. Its great; but its a library designed to be able to run hundreds of models. I just want to be able to run the very best or fastest 'good' model available.&lt;/p&gt; &lt;p&gt;So, I have refactored our all the audio pre-processing into &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/mel_spectrogram.py"&gt;one simple file&lt;/a&gt;, and the full &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/tdt_asr.py"&gt;Token-and-Duration Transducer (TDT)&lt;/a&gt; or &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/ctc_asr.py"&gt;FastConformer CTC model&lt;/a&gt; inference code as a file each. Minimal dependencies, maximal ease in doing ASR!&lt;/p&gt; &lt;p&gt;So now to can easily run either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt_ctc-110m"&gt;Parakeet-TDT_CTC-110M&lt;/a&gt; - solid performance, 5345.14 RTFx&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Parakeet-TDT-0.6B-v2&lt;/a&gt; - best performance, 3386.02 RTFx&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;just by using my python modules from the GLaDOS source. Installing GLaDOS will auto pull all the models you need, or you can download them directly from the &lt;a href="https://github.com/dnhkng/GLaDOS/releases/tag/0.1"&gt;releases section&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The TDT model is great, much better than Whisper too, give it a go! Give the project a Star to keep track, there's more cool stuff in development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rtph8367c1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1konnx9</id>
    <title>Let's see how it goes</title>
    <updated>2025-05-17T07:54:06+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt; &lt;img alt="Let's see how it goes" src="https://preview.redd.it/ngy98tkusa1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=109911e4427c5bfba6bed05ca517063cd80c31ef" title="Let's see how it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ngy98tkusa1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T07:54:06+00:00</published>
  </entry>
</feed>
