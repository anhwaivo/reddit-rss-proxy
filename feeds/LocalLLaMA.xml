<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-08T19:05:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j6mwz5</id>
    <title>Is there an implementation to have a server-hosted web app (e.g. LLM) that uses client GPU?</title>
    <updated>2025-03-08T18:03:31+00:00</updated>
    <author>
      <name>/u/DahakaMVl</name>
      <uri>https://old.reddit.com/user/DahakaMVl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The setup would essentially be:&lt;/p&gt; &lt;p&gt;- Server running web application with backend and database but without GPU&lt;/p&gt; &lt;p&gt;- Client with GPU accesses web application&lt;/p&gt; &lt;p&gt;Is there any implementation for such a case? I don't care about downloading models or anything, just about not installing a GPU on my server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DahakaMVl"&gt; /u/DahakaMVl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mwz5/is_there_an_implementation_to_have_a_serverhosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mwz5/is_there_an_implementation_to_have_a_serverhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mwz5/is_there_an_implementation_to_have_a_serverhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6e4j7</id>
    <title>DNA-R1 (REASONING MODEL)</title>
    <updated>2025-03-08T10:07:28+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt; &lt;img alt="DNA-R1 (REASONING MODEL)" src="https://external-preview.redd.it/eCZla5i8qBbR-EcKbP_YjinL9_9ebCep5Gy1Q0SHWrA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ff94ebc41043fb178b0e03601e7c624ce8570" title="DNA-R1 (REASONING MODEL)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We introduce DNA-R1, a specialized reasoning model optimized for Korean language based on Microsoft's Phi-4. By applying large-scale reinforcement learning (RL) using the same methodology as DeepSeek-R1, we have significantly enhanced the model's Korean reasoning capabilities. This model demonstrates deep understanding of Korean text and exhibits exceptional reasoning abilities across mathematics, coding, and general reasoning tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/krkq6jlpwfne1.png?width=697&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94af500a4c9592dafc9daa7646789e3ce0071c17"&gt;https://preview.redd.it/krkq6jlpwfne1.png?width=697&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94af500a4c9592dafc9daa7646789e3ce0071c17&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7onilg2rwfne1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78d40f1ae4658605f5cba2f6eacc27eba16e2f09"&gt;https://preview.redd.it/7onilg2rwfne1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78d40f1ae4658605f5cba2f6eacc27eba16e2f09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/degflmwrwfne1.png?width=187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd79a3183e7a8688690cc100200f0c0172becb2"&gt;https://preview.redd.it/degflmwrwfne1.png?width=187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd79a3183e7a8688690cc100200f0c0172becb2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/dnotitia/DNA-R1"&gt;https://huggingface.co/dnotitia/DNA-R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6e4j7/dnar1_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T10:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6kkyv</id>
    <title>How do you store and organize your models?</title>
    <updated>2025-03-08T16:18:54+00:00</updated>
    <author>
      <name>/u/NewTestAccount2</name>
      <uri>https://old.reddit.com/user/NewTestAccount2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I started exploring local models about a year ago, and in that time, I've gone through quite a few. Some models I just tested and deleted (like specific-use-case fine-tunes), while others I kept—even if they're a bit old (Gemma2, for example, for multilingual tasks, even despite limited content). I've also held onto some models even after newer versions came out (like certain Llama models).&lt;/p&gt; &lt;p&gt;The problem is that I'm having a hard time keeping everything organized. With so many new models to download, test, and potentially keep, I was wondering how you all manage your model storage.&lt;/p&gt; &lt;p&gt;I'm not just talking about storing the model files themselves but also keeping track of important info, such as:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General details about the model&lt;/li&gt; &lt;li&gt;Presets and parameters that work well&lt;/li&gt; &lt;li&gt;Best use cases&lt;/li&gt; &lt;li&gt;Other relevant notes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Right now, I manually take notes on each model's &amp;quot;vibe,&amp;quot; along with the presets and parameters that work best. I also save general info from posts here and download the model's HF webpage as an HTML file. While this method works to some extent, it still feels cumbersome.&lt;/p&gt; &lt;p&gt;For reference, I use Ooba as the backend and ST as the frontend.&lt;/p&gt; &lt;p&gt;So, how do you store and organize your models along with all the relevant info? Thanks for any tips!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewTestAccount2"&gt; /u/NewTestAccount2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6kkyv/how_do_you_store_and_organize_your_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6kkyv/how_do_you_store_and_organize_your_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6kkyv/how_do_you_store_and_organize_your_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T16:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wzea</id>
    <title>New AMD Driver Yields Up To 11% Performance Increase In koboldcpp</title>
    <updated>2025-03-07T19:02:32+00:00</updated>
    <author>
      <name>/u/WokeCapitalist</name>
      <uri>https://old.reddit.com/user/WokeCapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7900-xt.html"&gt;AMD's Adrenalin 25.3.1 driver&lt;/a&gt; release mentioned &lt;strong&gt;&lt;em&gt;&amp;quot;AI Performance Improvements on AMD Radeon™ RX 7000 Series&amp;quot;&lt;/em&gt;&lt;/strong&gt; in the release notes along with some large percentage increases for applications like Adobe Lightroom Denoise or DaVinci Resolve. As I had their previous WHQL recommended driver already installed, I decided to test it out in koboldcpp. It turns out there was a nice performance bump there, too. Worth a download if you haven't done so already!&lt;/p&gt; &lt;h1&gt;Hardware Test Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Microsoft Windows 11 Professional (x64) Build 26100.3194 (24H2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core Ultra 7 265K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 7900 XT (20GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Z890I Nova WiFi (BIOS 2.22)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk:&lt;/strong&gt; Lexar SSD NM800PRO 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 64GB (2×32GB DDR5 6400 CL32)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Boot, BitLocker &amp;amp; HVCI Enabled&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software Test Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; koboldcpp 1.83.1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; phi-4-q4,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 16384&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BLAS Batch Size&lt;/strong&gt;: 512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Layers&lt;/strong&gt;: 43/43&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Backend: Vulkan&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;30.312 s&lt;/td&gt; &lt;td align="left"&gt;27.607 s&lt;/td&gt; &lt;td align="left"&gt;8.95% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;537.21 T/s&lt;/td&gt; &lt;td align="left"&gt;589.85 T/s&lt;/td&gt; &lt;td align="left"&gt;9.84% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.203 s&lt;/td&gt; &lt;td align="left"&gt;5.301 s&lt;/td&gt; &lt;td align="left"&gt;1.87% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;19.22 T/s&lt;/td&gt; &lt;td align="left"&gt;18.86 T/s&lt;/td&gt; &lt;td align="left"&gt;1.88% lower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;35.515 s&lt;/td&gt; &lt;td align="left"&gt;32.908 s&lt;/td&gt; &lt;td align="left"&gt;7.35% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Backend: ROCm&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;24.861 s&lt;/td&gt; &lt;td align="left"&gt;22.370 s&lt;/td&gt; &lt;td align="left"&gt;10.06% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;655.00 T/s&lt;/td&gt; &lt;td align="left"&gt;727.94 T/s&lt;/td&gt; &lt;td align="left"&gt;11.15% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.831 s&lt;/td&gt; &lt;td align="left"&gt;5.586 s&lt;/td&gt; &lt;td align="left"&gt;4.20% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;17.15 T/s&lt;/td&gt; &lt;td align="left"&gt;17.90 T/s&lt;/td&gt; &lt;td align="left"&gt;4.32% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;30.692 s&lt;/td&gt; &lt;td align="left"&gt;27.956 s&lt;/td&gt; &lt;td align="left"&gt;8.97% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WokeCapitalist"&gt; /u/WokeCapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6mcrd</id>
    <title>Before I try... has someone already done it? (Ordered Retrieval Content - ORC)</title>
    <updated>2025-03-08T17:38:56+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orcs aren't so smart... I learned this in Warcraft... but if you tell them to do a simple task they can get it done. So I thought... what if I broke down a giant context into digestible bits? Provide splitting characters(s) and a prompt and you immediately have multiple outputs run based on the splits. You could choose to provide the previous outputs along with the new section, or you could output each individually with no mixture. This idea is a rough approximation of RAG, but it's ordered and holistic.&lt;/p&gt; &lt;p&gt;For example, I want a book summary, but pasting the whole book crushes my system or I have to use a very degraded context precision and it results in extremely high level information about the entire context that doesn't consider each chapter, scene, paragraph or sentence individually. What if I could split the chapters by headings (hash tags), what if I split chapters into scenes (three asterisks ***), what if I split scenes into paragraphs, paragraphs into sentences or even sentences into words? Could I then have a list of prompts that were performed on each split! I could create a prompt that summarizes each chapter by a brief recap of scenes. I could have each paragraph in the book summarized into a sentence. I could have each sentence evaluated for passive voice. In theory I could transform an entire book sentence by sentence with the right prompt.&lt;/p&gt; &lt;p&gt;I was hoping to do something like this as an extension for Text Gen by Oobabooga. I might build off of the extension Twinbook by FartyPants. Before I even attempt it, I thought I'd ask. Has it been done before? Is there something similar like it? What would you change about this idea... what would you use it for? Love your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6i5u2</id>
    <title>What is the current status of apple silicon for AI purposes? How far it is?</title>
    <updated>2025-03-08T14:21:33+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What about anything which mps is good at? Inference, training..etc. asking from a general point of view, what is the hindrance for utilizing its computing power for modern applications?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i5u2/what_is_the_current_status_of_apple_silicon_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i5u2/what_is_the_current_status_of_apple_silicon_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i5u2/what_is_the_current_status_of_apple_silicon_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60wt1</id>
    <title>NVIDIA RTX "PRO" 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W</title>
    <updated>2025-03-07T21:34:40+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt; &lt;img alt="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" src="https://external-preview.redd.it/8nLxMIJQrz_2tTIhvuryMxrtnbjPwWSOP7OO-C_HgM0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8c481144f93be83af79ca767c97e43a6750c4ac" title="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-x-blackwell-leak-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6hbp9</id>
    <title>Is Intel Granite Rapids-AP now the fastest CPU for Deepseek R1?</title>
    <updated>2025-03-08T13:37:42+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to its AMX support that AMD still doesn't have? It also has 12 channel DDR5-6400. &lt;/p&gt; &lt;p&gt;Together with a GPU, you can get 255t/s prompt processing and 11t/s with 2x6454S and 4090D GPU with ktransformer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://kvcache-ai.github.io/ktransformers/en/DeepseekR1_V3_tutorial.html"&gt;https://kvcache-ai.github.io/ktransformers/en/DeepseekR1_V3_tutorial.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I presume it will be faster with 6952P or higher and 5080 GPU.&lt;/p&gt; &lt;p&gt;The only drawback is that the cheapest CPU is 11.4k.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6hbp9/is_intel_granite_rapidsap_now_the_fastest_cpu_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6hbp9/is_intel_granite_rapidsap_now_the_fastest_cpu_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6hbp9/is_intel_granite_rapidsap_now_the_fastest_cpu_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T13:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6noh8</id>
    <title>Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1...</title>
    <updated>2025-03-08T18:36:50+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt; &lt;img alt="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." src="https://external-preview.redd.it/7RbGl7PqZ_sGLzEC-up6MH5b7zJrrofblGxxk0WxBC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132e3f4ed63e0faee796888e40ee542ef9d2a07c" title="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/FlappyAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66mpo</id>
    <title>Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark</title>
    <updated>2025-03-08T02:05:50+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt; &lt;img alt="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" src="https://preview.redd.it/ig84dy8oidne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1ee2b26596ddc8e881d50f0eb5e76449003b478" title="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ig84dy8oidne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j681n3</id>
    <title>Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark</title>
    <updated>2025-03-08T03:22:26+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt; &lt;img alt="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" src="https://preview.redd.it/izw2ej1cwdne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73dec38ebaddd2720f9cf241a4ae7cf9de6d8481" title="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izw2ej1cwdne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nzrk</id>
    <title>New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s</title>
    <updated>2025-03-08T18:51:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt; &lt;img alt="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" src="https://preview.redd.it/wfkxh0q5iine1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63ab35ce1aa6589c56196d048a5e4231e07749f" title="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfkxh0q5iine1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nct7</id>
    <title>Estimating how much the new NVIDIA RTX PRO 6000 Blackwell GPU should cost</title>
    <updated>2025-03-08T18:22:47+00:00</updated>
    <author>
      <name>/u/asssuber</name>
      <uri>https://old.reddit.com/user/asssuber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No price released yet, so let's figure out how much that card should cost:&lt;/p&gt; &lt;p&gt;Extra GDDR6 costs less than $8 per GB for the end consumer &lt;a href="https://arstechnica.com/gadgets/2024/01/review-radeon-7600-xt-offers-peace-of-mind-via-lots-of-ram-remains-a-midrange-gpu/"&gt;when installed in a GPU clamshell style&lt;/a&gt; like Nvidia is using here. GDDR7 chips seems to carry a &lt;a href="https://www.trendforce.com/presscenter/news/20240627-12207.html"&gt;20-30% premium&lt;/a&gt; over GDDR6 which I'm going to generalize to all other costs and margins related to putting it in a card, so we get less than $10 per GB.&lt;/p&gt; &lt;p&gt;Using the $2000 MSRP of the 32GB RTX 5090 as basis, the NVIDIA RTX PRO 6000 Blackwell with 96GB &lt;strong&gt;should cost less than $2700&lt;/strong&gt; to the end consumer. Oh, the wonders of a competitive capitalistic market, free of monopolistic practices!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asssuber"&gt; /u/asssuber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6k84c</id>
    <title>Phi-4:mini didn't stop the inference till I force stopped it.</title>
    <updated>2025-03-08T16:02:38+00:00</updated>
    <author>
      <name>/u/Resident_Acadia_4798</name>
      <uri>https://old.reddit.com/user/Resident_Acadia_4798</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"&gt; &lt;img alt="Phi-4:mini didn't stop the inference till I force stopped it." src="https://external-preview.redd.it/bml0YTY0dnhuaG5lMU2Zb_cWf0SBlyMf0Audt_A4sf0T3APMzRNk1uB8Pzdo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=047fccc87602f037b9eb8ebccfc452ed20d2c911" title="Phi-4:mini didn't stop the inference till I force stopped it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Acadia_4798"&gt; /u/Resident_Acadia_4798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ee76j1vxnhne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T16:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ma8i</id>
    <title>Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!</title>
    <updated>2025-03-08T17:35:45+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt; &lt;img alt="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" src="https://external-preview.redd.it/OERDGiS518l9lA6nng9dhSyETZuedB7NMNyJJW94EgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e69639f639d057ebece140432310ca7d2b192b1" title="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab"&gt;https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re part of the open-source project &lt;a href="https://github.com/anemll/anemll"&gt;&lt;strong&gt;ANEMLL&lt;/strong&gt;&lt;/a&gt;, which is working to bring large language models (LLMs) to the Apple Neural Engine. This hardware has incredible potential, but there’s a catch—Apple hasn’t shared much about its inner workings, like memory speeds or detailed performance specs. That’s where you come in!&lt;/p&gt; &lt;p&gt;To help us understand the Neural Engine better, we’ve launched a new benchmark tool: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;&lt;strong&gt;anemll-bench&lt;/strong&gt;&lt;/a&gt;. It measures the Neural Engine’s bandwidth, which is key for optimizing LLMs on Apple’s chips.&lt;/p&gt; &lt;p&gt;We’re especially eager to see results from &lt;strong&gt;Ultra&lt;/strong&gt; models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;M1 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;And, if you’re one of the lucky few, &lt;strong&gt;M3 Ultra&lt;/strong&gt;!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(Max models like M2 Max, M3 Max, and M4 Max are also super helpful!)&lt;/p&gt; &lt;p&gt;If you’ve got one of these Macs, here’s how you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Clone the repo&lt;/strong&gt;: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;https://github.com/Anemll/anemll-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run the benchmark&lt;/strong&gt;: Just follow the README—it’s straightforward!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Share your results&lt;/strong&gt;: Submit your JSON result via a &amp;quot;issues&amp;quot; or email&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why contribute?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll help an open-source project make real progress.&lt;/li&gt; &lt;li&gt;You’ll get to see how your device stacks up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious about the bigger picture? Check out the main ANEMLL project: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks for considering this—every contribution helps us unlock the Neural Engine’s potential&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6iuyf</id>
    <title>NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp; 600W TBP</title>
    <updated>2025-03-08T14:56:53+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" src="https://external-preview.redd.it/ipqoihUxtH0AdjsoCf5u0QWlmwf7QkIL9jnTAAb3HTw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6333201abf11bb51d15493e0484c12b4cafa2d16" title="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-gpu-more-cores-than-rtx-5090-24064-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dryj</id>
    <title>Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes</title>
    <updated>2025-03-08T09:41:19+00:00</updated>
    <author>
      <name>/u/2TierKeir</name>
      <uri>https://old.reddit.com/user/2TierKeir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt; &lt;img alt="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" src="https://external-preview.redd.it/aGlvZGVrdDZzZm5lMc_Az5p3qLdEN__5qSL7XTQoE-2LI7eWZo3yGOsqXnkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7745bb1240348e2c2b8426f85b17a2fe6e2edeed" title="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2TierKeir"&gt; /u/2TierKeir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xkdwav2sfne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6f61q</id>
    <title>QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7</title>
    <updated>2025-03-08T11:24:32+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt; &lt;img alt="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" src="https://preview.redd.it/opow8do3agne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00cbc87c29904f9341ccf656e804f32edd07064a" title="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/opow8do3agne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6i1ma</id>
    <title>Can't believe it, but the RTX 4090 actually exists and it runs!!!</title>
    <updated>2025-03-08T14:15:34+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt; &lt;img alt="Can't believe it, but the RTX 4090 actually exists and it runs!!!" src="https://b.thumbs.redditmedia.com/Jyu8XHnVjN10hhVg2LdYg8XX4xiDcuuwK_tm4Uimz_g.jpg" title="Can't believe it, but the RTX 4090 actually exists and it runs!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX 4090 96G version&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2"&gt;https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1"&gt;https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dzai</id>
    <title>Real-time token graph in Open WebUI</title>
    <updated>2025-03-08T09:56:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt; &lt;img alt="Real-time token graph in Open WebUI" src="https://external-preview.redd.it/dm1rY2E3dWl1Zm5lMeNo1g2VbIy6NNGx_1T_ctYYVLkaFt3bwpFyaChfDLc3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c66a72211f8ad427c81d09e427101d7638dfd38" title="Real-time token graph in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zscr76uiufne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:56:58+00:00</published>
  </entry>
</feed>
