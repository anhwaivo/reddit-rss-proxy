<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-31T06:40:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n44xxi</id>
    <title>OpenWebUI lets you auto expand reasoning now!</title>
    <updated>2025-08-30T15:47:40+00:00</updated>
    <author>
      <name>/u/slpreme</name>
      <uri>https://old.reddit.com/user/slpreme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt; &lt;img alt="OpenWebUI lets you auto expand reasoning now!" src="https://preview.redd.it/ms5d125ng6mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb790039d8346bafae55cda40cbc7c0b88f33de8" title="OpenWebUI lets you auto expand reasoning now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure when they added this, but it was a pet peeve of mine so I wanted to share this is how you can turn on show reasoning content automatically. It's just in Settings &amp;gt; Interface &amp;gt; Always Expand Details. I'm guessing that also expands some other things but I don't use any tools so I don't know which.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slpreme"&gt; /u/slpreme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ms5d125ng6mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4g4lq</id>
    <title>Question: will inference engines such as sglang and vllm support 2bit (or 3,5,6 etc)?</title>
    <updated>2025-08-30T23:47:12+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Question: will inference engines such as sglang and vllm support 2bit? Or 1.93bpw, 3.., 5.., 6..bpw etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4g4lq/question_will_inference_engines_such_as_sglang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3u7qf</id>
    <title>How’s your experience with the GPT OSS models? Which tasks do you find them good at—writing, coding, or something else</title>
    <updated>2025-08-30T06:06:58+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T06:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4kpwh</id>
    <title>Bright LeaderBoard No1 ｜DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval</title>
    <updated>2025-08-31T03:42:42+00:00</updated>
    <author>
      <name>/u/Dazzling-Impact1075</name>
      <uri>https://old.reddit.com/user/Dazzling-Impact1075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4kpwh/bright_leaderboard_no1_diver_a_multistage/"&gt; &lt;img alt="Bright LeaderBoard No1 ｜DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval" src="https://external-preview.redd.it/hoa_Cx28ycqK8YnNuxj26zkY_FsOnnCgNnwo0xxMTAQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c6b33471fc44d76bfa04e3ae92685346e8c7552" title="Bright LeaderBoard No1 ｜DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To overcome the reasoning-intensive retrieval problem， we present the DIVER framework whick rank no1 on the Bright LeaderBoard. We have open-sourced our code, model, and paper. &lt;/p&gt; &lt;p&gt;code: &lt;a href="https://github.com/AQ-MedAI/Diver"&gt;https://github.com/AQ-MedAI/Diver&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model: &lt;a href="https://huggingface.co/AQ-MedAI/Diver-Retriever-4B"&gt;https://huggingface.co/AQ-MedAI/Diver-Retriever-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/pdf/2508.07995"&gt;https://arxiv.org/pdf/2508.07995&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m0gpgqe2z9mf1.png?width=1310&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703d4865021203414f276834a8bd59cc99860826"&gt;https://preview.redd.it/m0gpgqe2z9mf1.png?width=1310&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703d4865021203414f276834a8bd59cc99860826&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/306fiyg3z9mf1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=218561091c88a3689ebe9ef375dce30b11c5fcc0"&gt;https://preview.redd.it/306fiyg3z9mf1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=218561091c88a3689ebe9ef375dce30b11c5fcc0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dazzling-Impact1075"&gt; /u/Dazzling-Impact1075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4kpwh/bright_leaderboard_no1_diver_a_multistage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4kpwh/bright_leaderboard_no1_diver_a_multistage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4kpwh/bright_leaderboard_no1_diver_a_multistage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T03:42:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4matr</id>
    <title>how to use intel npu for lm studio</title>
    <updated>2025-08-31T05:12:05+00:00</updated>
    <author>
      <name>/u/TopSecre-t</name>
      <uri>https://old.reddit.com/user/TopSecre-t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt; &lt;img alt="how to use intel npu for lm studio" src="https://a.thumbs.redditmedia.com/oF_VLz5o0T7sHHvIj_JdH0QlI_y3btKgoFY02MsEXn0.jpg" title="how to use intel npu for lm studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have intel core ultra 7 255hx and rtx 5060 with 8gb vram&lt;/p&gt; &lt;p&gt;the intel cpu has an integrated npu and it have 18gb shared memory also the gpu tab shows total 26gb memory with its 8gb vram&lt;/p&gt; &lt;p&gt;but how do i use the 18gb shared memory? when i ask something on my local ai on lm studio it only uses the gpu vram of 8gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopSecre-t"&gt; /u/TopSecre-t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n4matr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n451ka</id>
    <title>GPT OSS Fine-tuning QAT</title>
    <updated>2025-08-30T15:51:53+00:00</updated>
    <author>
      <name>/u/Short_Struggle7803</name>
      <uri>https://old.reddit.com/user/Short_Struggle7803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read more about our (Nvidia) end to end example on GPT OSS fine tuning QAT + SGlang deployment 👉 &lt;a href="https://lmsys.org/blog/2025-08-28-gpt-oss-qat/"&gt;https://lmsys.org/blog/2025-08-28-gpt-oss-qat/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine-tuning QAT helps keep the original MXFP4 quantization of GPT OSS while adapting to downstream task.&lt;/p&gt; &lt;p&gt;We have some example results (and comparisons to Nvidia’s NVFP4 format) here :&lt;/p&gt; &lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/"&gt;https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do checkout 🙃!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short_Struggle7803"&gt; /u/Short_Struggle7803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n407lv</id>
    <title>Training a 11M language model for Raspberry Pi Pico - progress</title>
    <updated>2025-08-30T12:19:39+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I have been training a Language Model for the Raspberry Pi Pico (fast &amp;amp; smart). My research showed me that 11M parameters is the sweet spot (about 2-3 token/s) to start.&lt;/p&gt; &lt;p&gt;What I use: &lt;/p&gt; &lt;p&gt;- For training: PicoLM&lt;/p&gt; &lt;p&gt;- GPU: 5090 for Prime Intellect (from grants), H100 on &lt;a href="http://Ori.co"&gt;Ori.co&lt;/a&gt; (free credits), AWS GPU (free credits)&lt;/p&gt; &lt;p&gt;Progress:&lt;/p&gt; &lt;p&gt;- I have been training it on 5M rows of Dolma + Olmo 7B tokenizers&lt;/p&gt; &lt;p&gt;Checkpoints:&lt;/p&gt; &lt;p&gt;17k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 7: &amp;quot;In the year 2050,&amp;quot; **Response**: ``` we have been doing that it was just for the same time, he said, and I have a lot of my father, and it was my own. We had a very much of a little as I wanted to do it. I'm not sure, but the people just like to me and I've never been a lot of my blog, so that I am not the most likely was that she's not to be my face. I wanted to get to me, ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;75k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 1: &amp;quot;Hello, how are you?&amp;quot; **Response**: ``` I've been my own own name. I think I've got a couple of reasons. I think I was a lot of my mind and my own way I did not know my work. I got the way I took a little time with my first time. I am very sure I do. I like a good, I’m sure I did my car but I do have to be on my job. I've been with my friend and ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All benchmark results: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All datasets &amp;amp; code are available on my github:&lt;br /&gt; - Github: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero&lt;/a&gt;&lt;br /&gt; - Huggingface: &lt;a href="https://huggingface.co/ThomasTheMaker"&gt;https://huggingface.co/ThomasTheMaker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note:&lt;br /&gt; - I am not a ML scientist. Purely an AI startup founder with too much energy to just do normal engineering and be happy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3xxm5</id>
    <title>🌟Introducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinking🌟</title>
    <updated>2025-08-30T10:08:44+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3- &lt;strong&gt;Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to &amp;quot;think in rap lyrics&amp;quot; or &amp;quot;use bullet points to organize thoughts&amp;quot; and it will literally reason that way before giving you an answer.&lt;/p&gt; &lt;p&gt;You can check out the model on HuggingFace: &lt;a href="https://huggingface.co/AGI-0/Art-0-8B"&gt;https://huggingface.co/AGI-0/Art-0-8B&lt;/a&gt; (please leave a like in the repo if you like this model)&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card &lt;a href="https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab"&gt;https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T10:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3ldon</id>
    <title>Qwen3-coder is mind blowing on local hardware (tutorial linked)</title>
    <updated>2025-08-29T22:35:27+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt; &lt;img alt="Qwen3-coder is mind blowing on local hardware (tutorial linked)" src="https://external-preview.redd.it/MHAyYm12N3NjMW1mMWyTIaaq8py0BbLEXek7RrX8ohVlR1FrRoAdOlxuqQ67.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d298923517a79cfa7fc3e04c1533fbc4c70a8f3b" title="Qwen3-coder is mind blowing on local hardware (tutorial linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello!&lt;/p&gt; &lt;p&gt;I'm honestly blown away by how far local models have gotten in the past 1-2 months. Six months ago, local models were completely useless in Cline, which tbf is pretty heavyweight in terms of context and tool-calling demands. And then a few months ago I found one of the qwen models to actually be somewhat usable, but not for any real coding.&lt;/p&gt; &lt;p&gt;However, qwen3-coder-30B is really impressive. 256k context and is actually able to complete tool calls and diff edits reliably in Cline. I'm using the 4-bit quantized version on my 36GB RAM Mac.&lt;/p&gt; &lt;p&gt;My machine does turn into a bit of a jet engine after a while, but the performance is genuinely useful. My setup is LM Studio + Qwen3 Coder 30B + Cline (VS Code extension). There are some critical config details that can break it (like disabling KV cache quantization in LM Studio), but once dialed in, it just works.&lt;/p&gt; &lt;p&gt;This feels like the first time local models have crossed the threshold from &amp;quot;interesting experiment&amp;quot; to &amp;quot;actually useful coding tool.&amp;quot; I wrote a full technical walkthrough and setup guide: &lt;a href="https://cline.bot/blog/local-models"&gt;https://cline.bot/blog/local-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/75bfhw7sc1mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T22:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mprh</id>
    <title>How do I turn Reddit conversations into a dataset for fine-tuning?</title>
    <updated>2025-08-31T05:36:49+00:00</updated>
    <author>
      <name>/u/Cyber_Zilla</name>
      <uri>https://old.reddit.com/user/Cyber_Zilla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m trying to create a dataset for fine-tuning a chatbot, but I’m stuck on the data processing step. I already have raw Reddit data (posts with titles, selftext, and comments), and I want to convert it into a prompt → response format that works for fine-tuning (e.g., with Unsloth or HuggingFace).&lt;/p&gt; &lt;p&gt;Some questions I’m struggling with:&lt;/p&gt; &lt;p&gt;How do people usually map posts and comments into Q&amp;amp;A pairs? (e.g., use the post as the “user” and the top comment as the “assistant”?)&lt;/p&gt; &lt;p&gt;If there are multiple comments, should I take the best one, or merge them somehow?&lt;/p&gt; &lt;p&gt;Are there existing tools/pipelines that can help with this, or is it mostly a case of writing custom Python scripts?&lt;/p&gt; &lt;p&gt;Basically, I want to go from raw Reddit JSON → clean structured JSONL ready for fine-tuning.&lt;/p&gt; &lt;p&gt;If anyone has done something similar (general Reddit → dataset, not tied to a specific topic), I’d really appreciate advice, tips, or references!&lt;/p&gt; &lt;p&gt;Thanks 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyber_Zilla"&gt; /u/Cyber_Zilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mprh/how_do_i_turn_reddit_conversations_into_a_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mprh/how_do_i_turn_reddit_conversations_into_a_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mprh/how_do_i_turn_reddit_conversations_into_a_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4329n</id>
    <title>How do you people run GLM 4.5 locally ?</title>
    <updated>2025-08-30T14:30:45+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context i have a dual rtx 3090 rig with 128gb of ddr5 ram and no matter what i try i get around 6 tokens per second...&lt;br /&gt; On CPU only inference i get between 5 and 6 tokens while on partial GPU offload i get between 5.5 and 6.8 tokens.&lt;br /&gt; I tried 2 different versions the one from unsloth Q4_K_S (&lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt;) and the one from LovedHeart MXFP4 (&lt;a href="https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1%5C_M"&gt;https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1\_M&lt;/a&gt;)&lt;br /&gt; The one from unsloth is 1 token per second slower but still no story change.&lt;br /&gt; I changed literally all settings from lmstudio, even managed to get it to load with the full 131k context but still nowhere near the speed other users get on a single 3090 with offloading.&lt;br /&gt; I tried installing vllm but i get too much errors and i gave up.&lt;br /&gt; Is there another program i should try ? Have i chose the wrong models ?&lt;br /&gt; It's really frustrating and it's taking me too much hours to solve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4n9yx</id>
    <title>56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)</title>
    <updated>2025-08-31T06:10:19+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt; &lt;img alt="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" src="https://preview.redd.it/k0im592roamf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf3307b2439da45726bdc7e079057842ef0c0904" title="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally planned to put the 3090 in a lower x4 slot, but it wouldn't fit to PSU case clearance. Builder put the 3090 in the upper x16 slot instead, and the 5090 just barely fit in the second x16.&lt;br /&gt; Both cards running x8/x8 rather than the original planned x16/x4 configuration - but I'm cool with it. The 3090 fans are literally 1mm from the backplate of the 5090 yet the thermals are fine with 7x 140mm case fans. After the anxiety of my dream build I'm not doing heavy testing yet, but now looking to get into serious fine-tuning pretty soon. &lt;/p&gt; &lt;p&gt;I've the developer of a local AI app designed for dual GPU systems (&lt;a href="https://github.com/boneylizard/Eloquent"&gt;https://github.com/boneylizard/Eloquent&lt;/a&gt;) and I've found that with expanded capabilities comes expanded imagination. Haven't done a git push in a while and there's an issue I really need to get around to addressing, but that explains the build. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0im592roamf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4m30g</id>
    <title>ISO: VS Code + LocalLlama help</title>
    <updated>2025-08-31T04:59:22+00:00</updated>
    <author>
      <name>/u/1EvilSexyGenius</name>
      <uri>https://old.reddit.com/user/1EvilSexyGenius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt; &lt;img alt="ISO: VS Code + LocalLlama help" src="https://preview.redd.it/de87bjkbeamf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fccda791b6be5274998a0a9f3d601c60b70c374" title="ISO: VS Code + LocalLlama help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I connect vs code's latest default AI chat system to use my locally running llamacpp server that's currently running in OpenAI API mode? &lt;/p&gt; &lt;p&gt;Is this even possible? 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1EvilSexyGenius"&gt; /u/1EvilSexyGenius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/de87bjkbeamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n40ngf</id>
    <title>What is the slowest Token/sec you can live with?</title>
    <updated>2025-08-30T12:41:26+00:00</updated>
    <author>
      <name>/u/OrganicApricot77</name>
      <uri>https://old.reddit.com/user/OrganicApricot77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me:&lt;/p&gt; &lt;p&gt;5tok/s is the slowest I’ll accept&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicApricot77"&gt; /u/OrganicApricot77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mo1r</id>
    <title>Best Way to Use Qwen3-Coder for Local AI Coding?</title>
    <updated>2025-08-31T05:33:56+00:00</updated>
    <author>
      <name>/u/Creative-Scene-6743</name>
      <uri>https://old.reddit.com/user/Creative-Scene-6743</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for some guidance on setting up Qwen Coder models locally for AI-assisted coding work. Normally I’d dive in and figure it out myself, but between work and vacation, I’ve fallen behind and want to avoid a time-consuming rabbit hole.&lt;/p&gt; &lt;p&gt;I have a couple of key questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How close have you gotten Qwen Code to rival Claude's code capabilities?&lt;/strong&gt; I’m particularly interested in performance for actual dev work, not just benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What’s the best setup you’ve found so far?&lt;/strong&gt; Are you integrating Qwen into an existing Claude Code by swapping the model? Are you using a like cline integration or something else entirely?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any lessons learned or tips would be hugely appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Scene-6743"&gt; /u/Creative-Scene-6743 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n45lx2</id>
    <title>GLM-4.5V model for Computer Use</title>
    <updated>2025-08-30T16:15:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt; &lt;img alt="GLM-4.5V model for Computer Use" src="https://external-preview.redd.it/MjZwOTdqc3psNm1mMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f81993da62e75eef832974ce71abbca064f96f" title="GLM-4.5V model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either locally via Hugging Face or Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eicije20m6mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n48lcx</id>
    <title>10,000 $ Budget for a rig that will run ai (24/7)</title>
    <updated>2025-08-30T18:18:07+00:00</updated>
    <author>
      <name>/u/Holiday_Leg8427</name>
      <uri>https://old.reddit.com/user/Holiday_Leg8427</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I want to make myself at home a set-up that can run AI 24/7, i need it mainly to replace the general use of llms ( chatgpt, gemini etc...), was thinking and saw lots of posts and info aboout getting an macbook studio with maximum ram capacity, is that the best way?&lt;br /&gt; Thank you for your responses (in advance)!&lt;/p&gt; &lt;p&gt;Edit: Guys, i dont think i need a llm anymore, Imma just ask on reddit everything I need, and get result from you guys, thank for all the help and tips, and btw I have some sort of &amp;quot;credit&amp;quot; (and i can write it off basically entirely on my company) for a high end pc/pc parts, thats why I wanted to invest into something that can be as usefull as possible, and i work with many legal/medical teams in europe where gdpr is king&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Leg8427"&gt; /u/Holiday_Leg8427 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4hkar</id>
    <title>I tried almost every tts model on my ryzen 7 5000 series 16gb ram rtx 3060 laptop 6-8GB Vram</title>
    <updated>2025-08-31T00:58:19+00:00</updated>
    <author>
      <name>/u/This_is_difficult_0</name>
      <uri>https://old.reddit.com/user/This_is_difficult_0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I needed a decent voiceover for my projects and I am using minimax audio currently. It's really good. But I wanted to try some local tts and i tested around 5 of them and here are my results.&lt;/p&gt; &lt;h1&gt;1. Kokoro - 0.4-0.5 x realtime .Fast and decent quality&lt;/h1&gt; &lt;p&gt;Fast koko api one from github has lower quality than the hexgrad. &lt;/p&gt; &lt;h1&gt;2. Xtts v2 - 0.8-1x realtime tried it from pinokio ai : speed is little slower than kokoro but decent quality with cloning capabilities.&lt;/h1&gt; &lt;h1&gt;3. Chatterbox - if you have lower end gpu and cpu like me don't try ! It's slow (1.5-2x realtime but high quality , expressive. I found out when i wanted it to be neutral or soothing, it still exaggerated or sounded too robotic but not in the middle.&lt;/h1&gt; &lt;p&gt;Installation was painful for me. Took me whole day figuring out to improve the speed. I tried different forks from github and messed up file paths😂 Extremely slow , so not useful for me.&lt;/p&gt; &lt;h1&gt;4. Zonos : 2-3x realtime Slow... Uninstalled it pretty soon&lt;/h1&gt; &lt;h1&gt;5. Orpheus : yet to try. Looks expressive but mismatch the rythm frequently in the huggin space. Maybe it will be slow on my laptop too&lt;/h1&gt; &lt;h1&gt;6. F5tts : around 1x realtime tried it from pinokio ai. Looks decent but somewhere in the middle, it hallucinates , so not useful&lt;/h1&gt; &lt;p&gt;I am thinking of trying Piper next. I want a lightweight better than kokoro. For now , i would rank kokoro as the best option for my pc. Do you have any good suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_is_difficult_0"&gt; /u/This_is_difficult_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T00:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mxrj</id>
    <title>MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max.</title>
    <updated>2025-08-31T05:50:09+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt; &lt;img alt="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." src="https://preview.redd.it/co3taaq1mamf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a7f592ad4e744736d9c8af891ea21008331265e" title="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co3taaq1mamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46mk9</id>
    <title>LongCat-Flash-Chat is here, yet another Chinese open weight model</title>
    <updated>2025-08-30T16:56:47+00:00</updated>
    <author>
      <name>/u/MindlessScrambler</name>
      <uri>https://old.reddit.com/user/MindlessScrambler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt; &lt;img alt="LongCat-Flash-Chat is here, yet another Chinese open weight model" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="LongCat-Flash-Chat is here, yet another Chinese open weight model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Chat"&gt;https://github.com/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web: &lt;a href="https://longcat.ai"&gt;https://longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f"&gt;https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindlessScrambler"&gt; /u/MindlessScrambler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4lh7s</id>
    <title>GPT-OSS-120B on Single RTX 6000 PRO</title>
    <updated>2025-08-31T04:24:33+00:00</updated>
    <author>
      <name>/u/Ill_Recipe7620</name>
      <uri>https://old.reddit.com/user/Ill_Recipe7620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 120 token/second with a single RTX 6000 PRO. It took 33 seconds to crank out the following answer after thinking for just 3 seconds.&lt;/p&gt; &lt;p&gt;If people aren't impressed with this model then they cannot be impressed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &amp;quot;compare the heat transfer coefficient of liquid water at a high pressure to steam at these conditions and IGNORE the boiling part only convection, search the web if you ned any information&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heat‑transfer‑coefficient ( h ) comparison – liquid water vs. steam at the same high pressure&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(boiling/phase‑change is deliberately excluded – only convective transport is considered)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1. What “the same conditions” really means&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantity&lt;/th&gt; &lt;th align="left"&gt;Typical high‑pressure case&lt;/th&gt; &lt;th align="left"&gt;Why it matters for h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Pressure&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5–20 MPa (≈50–200 bar) – common in super‑critical boilers, geothermal wells, high‑pressure reactors&lt;/td&gt; &lt;td align="left"&gt;Raises density of both phases, compresses the gas, and moves steam toward the liquid side of the phase diagram.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Bulk temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;350–550 °C (well above the normal boiling point of water)&lt;/td&gt; &lt;td align="left"&gt;Determines fluid properties (µ, k, ρ, Cp, Pr) that appear in the dimensionless groups governing convection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Flow regime&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Forced convection in a pipe or channel, fully developed turbulent flow (Re &amp;gt; 10 000) – the regime that gives the largest, most useful h values.&lt;/td&gt; &lt;td align="left"&gt;The Nusselt number correlations (Dittus‑Boelter, Gnielinski, Sieder‑Tate…) are based on turbulent forced flow.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geometry&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Typically a circular tube, D = 10–30 mm; length long enough for fully‑developed flow.&lt;/td&gt; &lt;td align="left"&gt;Geometry enters the Reynolds number and the hydraulic diameter used in correlations.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mass‑flow rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Chosen so that the &lt;strong&gt;mean velocity&lt;/strong&gt; &lt;em&gt;U&lt;/em&gt; is the same for both fluids (e.g., 2 m s⁻¹). This isolates the effect of the fluid properties.&lt;/td&gt; &lt;td align="left"&gt;With the same U, the only differences in h arise from ρ, µ, k, Cp, Pr.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Governing dimensionless groups&lt;/h1&gt; &lt;p&gt;For turbulent forced convection in a smooth pipe the Nusselt number is usually expressed as&lt;/p&gt; &lt;p&gt;where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Re&lt;/em&gt;=&lt;em&gt;μρUD&lt;/em&gt;​ – Reynolds number (inertia/viscous)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Pr&lt;/em&gt;=&lt;em&gt;kcp&lt;/em&gt;​&lt;em&gt;μ&lt;/em&gt;​ – Prandtl number (momentum/thermal diffusivity)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Nu&lt;/em&gt;=&lt;em&gt;khD&lt;/em&gt;​ – Nusselt number (convective/ conductive)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The constants &lt;em&gt;C&lt;/em&gt;,&lt;em&gt;m&lt;/em&gt;,&lt;em&gt;n&lt;/em&gt; depend on the correlation (Dittus‑Boelter, Gnielinski, etc.).&lt;br /&gt; The heat‑transfer coefficient follows directly:&lt;/p&gt; &lt;p&gt;Thus &lt;strong&gt;differences in h&lt;/strong&gt; arise from the three property groups (&lt;em&gt;ρ&lt;/em&gt;,&lt;em&gt;μ&lt;/em&gt;,&lt;em&gt;k&lt;/em&gt;,&lt;em&gt;cp&lt;/em&gt;​) that appear in &lt;em&gt;Re&lt;/em&gt;, &lt;em&gt;Pr&lt;/em&gt; and the explicit factor &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;3. Thermophysical properties at high pressure&lt;/h1&gt; &lt;p&gt;Below is a representative data set for &lt;strong&gt;10 MPa (≈100 bar)&lt;/strong&gt; at &lt;strong&gt;500 °C&lt;/strong&gt; (the pressure is the same for both fluids; the temperature is above the critical temperature of water, 374 °C, so both fluids are in the &lt;em&gt;supercritical&lt;/em&gt; regime, but we still distinguish “liquid‑like” (dense) and “steam‑like” (less dense) states).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Property&lt;/th&gt; &lt;th align="left"&gt;Liquid‑like water (ρ ≈ 650 kg m⁻³)&lt;/th&gt; &lt;th align="left"&gt;Steam‑like water (ρ ≈ 120 kg m⁻³)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density, ρ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;650 kg m⁻³&lt;/td&gt; &lt;td align="left"&gt;120 kg m⁻³&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Dynamic viscosity, µ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.4 × 10⁻⁴ Pa s&lt;/td&gt; &lt;td align="left"&gt;1.6 × 10⁻⁴ Pa s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity, k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.70 W m⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;0.45 W m⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Specific heat, cₚ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.1 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;2.4 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number, Pr = cₚµ/k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 7.3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 0.85&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Sources&lt;/em&gt;: NIST REFPROP 10.0, IAPWS‑95 formulation, extrapolated to 10 MPa and 500 °C.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the numbers tell us&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Density&lt;/strong&gt; – liquid‑like water is ~5 × denser, giving a Reynolds number ~5 × larger for the same velocity and pipe diameter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Viscosity&lt;/strong&gt; – steam is slightly less viscous, which &lt;em&gt;increases&lt;/em&gt; its Reynolds number a little, but the dominant factor is density.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal conductivity&lt;/strong&gt; – liquid water conducts heat ≈ 55 % better.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prandtl number&lt;/strong&gt; – liquid water has a &lt;strong&gt;high Pr&lt;/strong&gt; (≈ 7) while steam has a &lt;strong&gt;low Pr&lt;/strong&gt; (≈ 0.85). A high Pr means the thermal boundary layer is thinner than the velocity boundary layer, boosting h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Quantitative h‑comparison (forced turbulent flow)&lt;/h1&gt; &lt;h1&gt;4.1. Chosen flow parameters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pipe diameter &lt;em&gt;D&lt;/em&gt;=0.020m (20 mm)&lt;/li&gt; &lt;li&gt;Mean velocity &lt;em&gt;U&lt;/em&gt;=2.0m s−1 – typical for high‑pressure steam turbines or heat‑exchanger tubes.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4.2. Reynolds numbers&lt;/h1&gt; &lt;p&gt;Both are &lt;strong&gt;turbulent&lt;/strong&gt; (Re &amp;gt; 4 000).&lt;/p&gt; &lt;h1&gt;4.3. Nusselt numbers (Gnielinski correlation – valid for 0.1&amp;lt;Pr&amp;lt;2000 and 3000&amp;lt;Re&amp;lt;5×106)&lt;/h1&gt; &lt;p&gt;Carrying out the calculation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Fluid&lt;/th&gt; &lt;th align="left"&gt;Friction factor &lt;em&gt;f&lt;/em&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;em&gt;Nu&lt;/em&gt; (Gnielinski)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Liquid‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.014&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;liq​≈1.0×103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Steam‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.028&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;steam​≈3.1×102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;4.4. Heat‑transfer coefficient&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;under the same pressure, temperature, geometry and bulk velocity, liquid water transfers heat by convection roughly five times more efficiently than steam&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;5. Why the liquid is so much better (physics insight)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;Liquid water (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Steam (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Effect on h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density (ρ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5× larger → larger momentum → higher Re → thinner velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Low ρ → smaller Re → thicker velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Higher Re → higher Nu&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity (k)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;55 % larger → more conductive path across the thermal BL&lt;/td&gt; &lt;td align="left"&gt;Smaller k → poorer conduction&lt;/td&gt; &lt;td align="left"&gt;Directly multiplies Nu in h = Nu·k/D&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number (Pr)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;≈ 7 (thermal diffusion slower than momentum) → thermal BL &lt;strong&gt;much thinner&lt;/strong&gt; than velocity BL&lt;/td&gt; &lt;td align="left"&gt;≈ 0.85 (thermal diffusion faster) → thermal BL comparable to or thicker than velocity BL&lt;/td&gt; &lt;td align="left"&gt;Thin thermal BL → larger temperature gradient at wall → larger h&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Viscosity (µ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Slightly higher, but the density effect dominates the Reynolds number&lt;/td&gt; &lt;td align="left"&gt;Lower µ but low ρ keeps Re low&lt;/td&gt; &lt;td align="left"&gt;Net: liquid still gets larger Re&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Compressibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Near‑incompressible; flow properties change little with pressure&lt;/td&gt; &lt;td align="left"&gt;Gas‑like compressibility reduces effective velocity and thickens BL&lt;/td&gt; &lt;td align="left"&gt;Keeps liquid’s convective performance high&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;6. Influence of pressure itself&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pressure change&lt;/th&gt; &lt;th align="left"&gt;Liquid water&lt;/th&gt; &lt;th align="left"&gt;Steam&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Increasing pressure&lt;/strong&gt; (while staying sub‑critical for the liquid)&lt;/td&gt; &lt;td align="left"&gt;ρ ↑, µ ↓ (slightly) → &lt;strong&gt;Re ↑&lt;/strong&gt;; k ↑ modestly; Pr ↑ → &lt;strong&gt;h rises&lt;/strong&gt; (often 10‑30 % per 100 bar).&lt;/td&gt; &lt;td align="left"&gt;ρ ↑ dramatically (≈ 5–10× as you approach the critical point), µ ↓, k ↑, Pr ↑ from ~0.7 toward 1‑2. When steam becomes &lt;em&gt;dense&lt;/em&gt; (near‑critical), its Re and Pr increase enough that &lt;strong&gt;h can climb 2–4×&lt;/strong&gt;. However, even at 20 MPa the steam’s Pr never reaches the ~7 of liquid water, so &lt;em&gt;liquid still beats steam&lt;/em&gt; unless you cross into the &lt;em&gt;supercritical fluid&lt;/em&gt; regime where the distinction blurs.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Very near the critical point&lt;/strong&gt; (≈ 22 MPa, 374 °C)&lt;/td&gt; &lt;td align="left"&gt;Water is a &lt;em&gt;supercritical fluid&lt;/em&gt; with properties intermediate between liquid and gas; h can be &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt; – comparable to high‑pressure liquid.&lt;/td&gt; &lt;td align="left"&gt;Same supercritical fluid – the “steam” label loses meaning; h values become similar to the liquid side.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Take‑away:&lt;/strong&gt; Pressure improves h for both phases, but because liquid water already starts with a much higher baseline, the &lt;em&gt;ratio&lt;/em&gt; &lt;em&gt;h&lt;/em&gt;liq​/&lt;em&gt;h&lt;/em&gt;steam​ stays in the &lt;strong&gt;3–6 range&lt;/strong&gt; for most engineering‑relevant pressures (5–20 MPa). Only when you go extremely close to the critical point does the gap shrink dramatically.&lt;/p&gt; &lt;h1&gt;7. Practical implications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Application&lt;/th&gt; &lt;th align="left"&gt;Typical operating pressure &amp;amp; temperature&lt;/th&gt; &lt;th align="left"&gt;Which phase gives the larger convective h?&lt;/th&gt; &lt;th align="left"&gt;Design note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;High‑pressure water‑to‑steam heat exchangers (e.g., nuclear steam generators)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15 MPa, 300 °C liquid → 15 MPa, 350 °C steam&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Liquid side&lt;/em&gt; (primary) has &lt;em&gt;h&lt;/em&gt; 4–6× larger than the steam side.&lt;/td&gt; &lt;td align="left"&gt;The steam side is usually the limiting resistance; designers boost its turbulence (twisted tapes, ribbed tubes) to compensate.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical CO₂ heat recovery (analogy)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.5 MPa, 500 °C (fluid is supercritical)&lt;/td&gt; &lt;td align="left"&gt;The fluid behaves like a dense gas; h is &lt;strong&gt;lower&lt;/strong&gt; than a comparable liquid but &lt;strong&gt;higher&lt;/strong&gt; than low‑pressure CO₂ gas.&lt;/td&gt; &lt;td align="left"&gt;Use high‑Re flow (large velocities, small hydraulic diameters) to raise h.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geothermal wells (high‑P hot water, 10 MPa, 250 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fluid is liquid‑like&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Liquid&lt;/strong&gt; h ≈ 2–3 × 10⁴ W m⁻² K⁻¹, far exceeding what the same well would deliver if the fluid flashed to steam.&lt;/td&gt; &lt;td align="left"&gt;Maintaining liquid conditions (prevent flashing) maximises convective heat extraction.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical water oxidation reactors (SCWO, &amp;gt; 25 MPa, &amp;gt; 400 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Water is supercritical (no distinct liquid/steam)&lt;/td&gt; &lt;td align="left"&gt;h values of &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt;; similar to high‑pressure liquid water.&lt;/td&gt; &lt;td align="left"&gt;The high h is a key advantage for rapid heat removal in exothermic oxidation.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;8. Summary (concise bullet list)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Liquid water at high pressure&lt;/strong&gt; has &lt;strong&gt;much larger density&lt;/strong&gt; and &lt;strong&gt;higher Prandtl number&lt;/strong&gt; than steam at the same pressure/temperature.&lt;/li&gt; &lt;li&gt;These property differences raise the Reynolds number and shrink the thermal boundary layer, leading to &lt;strong&gt;higher Nusselt numbers&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Quantitatively, for a 20 mm tube, 2 m s⁻¹ flow, 10 MPa and 500 °C:→ &lt;strong&gt;Liquid water convects roughly 5 × faster.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;h_liquid ≈ 3–4 × 10⁴ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;h_steam ≈ 6–8 × 10³ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Raising pressure improves h for &lt;strong&gt;both&lt;/strong&gt; fluids, but the &lt;strong&gt;ratio stays between 3 and 6&lt;/strong&gt; over the usual engineering range (5–20 MPa).&lt;/li&gt; &lt;li&gt;Near the critical point the distinction blurs; steam becomes a dense supercritical fluid and its h rises dramatically, approaching that of the liquid side.&lt;/li&gt; &lt;li&gt;In design practice the &lt;strong&gt;steam side is usually the thermal bottleneck&lt;/strong&gt; in high‑pressure heat exchangers; engineers therefore add turbulence promoters, increase velocity, or operate at higher pressure to mitigate the lower h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Bottom line&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Recipe7620"&gt; /u/Ill_Recipe7620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n49hcg</id>
    <title>New AMD unified memory product - 512 bit bus = ~512GB/s memory bandwidth</title>
    <updated>2025-08-30T18:54:22+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent AMD leak hints at a new 512 bit memory bus for their unified memory systems. If so, a successor to the AI max would likely have 2x the memory bandwidth.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship"&gt;https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4dsym</id>
    <title>128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow.</title>
    <updated>2025-08-30T21:59:06+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt; &lt;img alt="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." src="https://preview.redd.it/ld3rckf8b8mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad5697c140ab4e2a23aa8df66f12cc106ee3b653" title="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ld3rckf8b8mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T21:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4garp</id>
    <title>Creating the brain behind dumb models</title>
    <updated>2025-08-30T23:55:53+00:00</updated>
    <author>
      <name>/u/ChristopherLyon</name>
      <uri>https://old.reddit.com/user/ChristopherLyon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt; &lt;img alt="Creating the brain behind dumb models" src="https://external-preview.redd.it/MWFtYzE2NDZ3OG1mMdkNi0cONUhOlI53Ev_Op4Ah4qsUiMoYY-mZ8OWM03HP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d27b6fcdb4c16b3d3a61e3f5d15d95327d5ebe8" title="Creating the brain behind dumb models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been fascinated by model intelligence enhancement and trying to deploy super tiny models like gemma3:270m in niche domains with high levels of success... &lt;/p&gt; &lt;p&gt;My latest implementation is a &amp;quot;community nested&amp;quot; relational graph knowledgebase pipeline that gives both top down context on knowledge sub-domains, but also a traditional bottom-up search (essentially regular semantic embedding cosine similarity) with a traversal mechanism to grab context from nodes that are not semantically similar but still referentially linked. Turns out there is a LOT of context that does not get picked up through regular embedding based RAG.&lt;/p&gt; &lt;p&gt;I created a quick front-end with nextjs and threejs to visualize how my knowledge base hangs together, and to quickly identify if I had a high level of overall coherence (i.e. number of isolated/disconnected clusters) and to get a better feeling for what context the LLM loads into memory for any given user query in real time (I'm a visual learner)&lt;/p&gt; &lt;p&gt;The KB you can see in the video is from a single 160 page PDF on Industrial Design, taking you anywhere from notable people, material science to manufacturing techniques. I was pleasantly surprised to see that the node for &amp;quot;ergonomics&amp;quot; was by far the most linked and overall strongly referenced in the corpus - essentially linking the &amp;quot;human factor&amp;quot; to some significant contribution to great product design.&lt;/p&gt; &lt;p&gt;If anyone hasn't gotten into graph based retrieval augmented generation I found the best resource and starter to be from Microsoft: &lt;a href="https://github.com/microsoft/graphrag"&gt;https://github.com/microsoft/graphrag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^ pip install graphrag and use the init and index commands to create your first graph in minutes.&lt;/p&gt; &lt;p&gt;Anyone else been in my shoes and already know what the NEXT step will be? Let me know.&lt;/p&gt; &lt;p&gt;It's 2 am so a quick video shot on my mobile is all I have right now, but I can't sleep thinking about this so thought I'd post what I have. I need to work some more on it and add the local LLM interface for querying the KB through the front end, but I don't mind open sourcing it if anyone is interested. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChristopherLyon"&gt; /u/ChristopherLyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bwyft136w8mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46ify</id>
    <title>Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)</title>
    <updated>2025-08-30T16:52:00+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt; &lt;img alt="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" src="https://preview.redd.it/1wl79kpjs6mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967356c2a208d26c7d657a1b535e795c0332f304" title="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wl79kpjs6mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
