<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-10T16:38:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hy7gyo</id>
    <title>Open Source Text Translation Models?</title>
    <updated>2025-01-10T15:52:48+00:00</updated>
    <author>
      <name>/u/vygodisgreat24</name>
      <uri>https://old.reddit.com/user/vygodisgreat24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am comparing the text translation qualities and have already tried the APIs of azure, AWS and google. I have also tried out the T5 model on huggingface, and would like to know if there is any other model or models that I can try for this?&lt;/p&gt; &lt;p&gt;also, from an evaluation perspective, what approach would be good for comparison of different translations? I am thinking MUSE (multilingual universal sentence encoder) to encode strings in its native language and the translated language and score them based on the similarity of embedding values (I read a blog that suggested this).&lt;/p&gt; &lt;p&gt;thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vygodisgreat24"&gt; /u/vygodisgreat24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7gyo/open_source_text_translation_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7gyo/open_source_text_translation_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7gyo/open_source_text_translation_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T15:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxitud</id>
    <title>Ask, and it shall be given: Turing completeness of prompting</title>
    <updated>2025-01-09T17:59:36+00:00</updated>
    <author>
      <name>/u/CommunismDoesntWork</name>
      <uri>https://old.reddit.com/user/CommunismDoesntWork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunismDoesntWork"&gt; /u/CommunismDoesntWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2411.01992"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T17:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8enl</id>
    <title>Anyone using Git for active dataset collaboration? Here's what I learned</title>
    <updated>2025-01-10T16:33:01+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone using Git as their primary store for training/eval data? I see it used everywhere for sharing dataset files, but mostly just for sharing large files -- not for rapid, line-by-line collaboration like how we use Git with code. From my experience, when teams need to iterate they usually end up moving data into other tools (labeling tools, databases, etc).&lt;/p&gt; &lt;p&gt;I'm convinced Git should be perfect for datasets (just like it is for code), but it's missing some tooling to make that work well.&lt;/p&gt; &lt;p&gt;I've been working through a bunch of gotchas while building a Git-friendly data format for my dataset management tool (here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;project on Github&lt;/a&gt; if you're interested). I think I've solved the main issues, and included details below. Would love to hear if anyone else has tackled similar problems or has ideas to share.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Making a Git-friendly file format&lt;/strong&gt;: The main issue is that giant dataset files aren't ideal for teams using Git; they cause constant merge conflicts whenever multiple people work simultaneously. Instead, I created a Git-friendly file format:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use lots of small files, making it rare for multiple people to need the same file at once, reducing conflicts (basically each row is now it's own file)&lt;/li&gt; &lt;li&gt;Use predominantly append-only operations, further reducing the chance of conflicts&lt;/li&gt; &lt;li&gt;Format files in a way that's easy to diff/merge when conflicts do happen (formatted, ordered JSON)&lt;/li&gt; &lt;li&gt;Generate unique random IDs for new items to avoid conflicts/collisions, letting many people work concurrently on the same project, even offline&lt;/li&gt; &lt;li&gt;Enforce a good schema: everyone uses the same format for prompts/ratings/samples, with validators that prevent badly formatted data from getting into the dataset. We even validate the LLM inputs/outputs using JSON Schema (optional, depending on your task definition)&lt;/li&gt; &lt;li&gt;Use nested folders to make paths really descriptive and easy to understand in PRs: Projects &amp;gt; Task &amp;gt; (Runs | Prompts | Fine-tunes | Splits)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Working with non-technical teammates&lt;/strong&gt;: Not everyone knows Git, and that's totally fine! Folks like subject matter experts, labelers, and QA still have a lot of value to add. With this design it's easy to share a branch through any shared drive (Dropbox, iCloud, OneDrive). These teammates can rate/label/eval/add-data through our friendly app with a super simple installer (no terminal, no Docker). Right now someone technical needs to sync the shared branch back to main on occasion, but I might add auto-sync if enough people want it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've built with this so far&lt;/strong&gt;: Having clean, structured data opened up some pretty cool possibilities. I added synthetic data generation (basically letting the LLM riff on our existing examples to create training data). It worked surprisingly well. I also added fine-tuning support - you can use Unsloth locally or the cloud. A little strangely, my favorite feature is that it automatically builds better prompts as you use it. The multi-shot prompts pick up highly rated examples and keep getting better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Integrating with other tools (pandas/polars)&lt;/strong&gt;: Most DS tutorials/tools assume you're working with big files (CSV, JSONL), but our format splits things into lots of small files. To make this work we have 1) a simple SDK that helps load everything into whatever data structure you need (pandas DataFrames, etc.), and 2) some examples showing how to directly load the JSON files directly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's left&lt;/strong&gt;: I think a solid eval framework should be next on the list. Would love to hear what eval systems you all are using and what's working well!&lt;/p&gt; &lt;p&gt;Let me know if you have any tips or workflows I haven't thought of! Super curious how others are handling their datasets in Git. Feel free to drop feature requests on &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github&lt;/a&gt; too if that's more your style.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8enl/anyone_using_git_for_active_dataset_collaboration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8enl/anyone_using_git_for_active_dataset_collaboration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8enl/anyone_using_git_for_active_dataset_collaboration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8g61</id>
    <title>Llama3.3-70B takes 4+ hours to load at 4bit quantisation</title>
    <updated>2025-01-10T16:34:49+00:00</updated>
    <author>
      <name>/u/MuffinJesus</name>
      <uri>https://old.reddit.com/user/MuffinJesus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Llama3.3-70B-Instruct via hugging face transformers pipeline. I can load the non-quantised model into memory quickly (with offloading), but inference times are abysmal. I can't work out why it takes so long to load the checkpoint shards for the 4-bit quantised model (same with 8-bit). I checked drive transfer speeds, RAM usage, CPU bottlenecking but everything is under capacity.&lt;/p&gt; &lt;p&gt;I'm using the script directly from &lt;a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"&gt;https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct&lt;/a&gt; for quantisation, and have updated transformers, accelerate and bitsandbytes itself.&lt;/p&gt; &lt;p&gt;System is an L40 GPU with 64GB RAM and an epic 9124 processor. I am completely out of ideas for where to look next, any direction would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MuffinJesus"&gt; /u/MuffinJesus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8g61/llama3370b_takes_4_hours_to_load_at_4bit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8g61/llama3370b_takes_4_hours_to_load_at_4bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8g61/llama3370b_takes_4_hours_to_load_at_4bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ii4</id>
    <title>freeact: A Lightweight Library for Code-Action Based Agents</title>
    <updated>2025-01-10T16:37:39+00:00</updated>
    <author>
      <name>/u/krasserm</name>
      <uri>https://old.reddit.com/user/krasserm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt; &lt;img alt="freeact: A Lightweight Library for Code-Action Based Agents" src="https://external-preview.redd.it/LzADpwKw8mUc5MZG38ssaaBnDyw6elEdCSjne9inj0g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b672149c7219c55d19b0e99adc3b4bbb5b158c" title="freeact: A Lightweight Library for Code-Action Based Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! We just released &lt;a href="https://github.com/gradion-ai/freeact"&gt;freeact&lt;/a&gt;, a lightweight agent library that empowers language models to act as autonomous agents through executable &lt;strong&gt;code actions&lt;/strong&gt;. By enabling agents to express their actions directly in code rather than through constrained formats like JSON, freeact provides a flexible and powerful approach to solving complex, open-ended problems that require dynamic solution paths.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports dynamic installation and utilization of Python packages at runtime&lt;/li&gt; &lt;li&gt;Agents learn from feedback and store successful code actions as reusable skills in long-term memory&lt;/li&gt; &lt;li&gt;Skills can be interactively developed and refined in collaboration with freeact agents&lt;/li&gt; &lt;li&gt;Agents compose skills and any other Python modules to build increasingly sophisticated capabilities&lt;/li&gt; &lt;li&gt;Code actions are executed in &lt;a href="https://github.com/gradion-ai/ipybox"&gt;ipybox&lt;/a&gt;, a secure Docker + IPython sandbox that runs locally or remotely&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/gradion-ai/freeact"&gt;https://github.com/gradion-ai/freeact&lt;/a&gt;&lt;br /&gt; Evaluation: &lt;a href="https://gradion-ai.github.io/freeact/evaluation/"&gt;https://gradion-ai.github.io/freeact/evaluation/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback!&lt;/p&gt; &lt;p&gt;See it in action:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player"&gt;https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krasserm"&gt; /u/krasserm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxz4zz</id>
    <title>Pre-processing to improve OCR on handwritten text</title>
    <updated>2025-01-10T07:28:26+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No open-ish model I'm aware of does a sufficiently good job when it comes to OCR on handwriting. What pre-processing steps did you find help improve results? For example: segmenting image into smaller chunks, increase contrast, turn monochrome, straightening/alignment...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T07:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx7421</id>
    <title>TransPixar: a new generative model that preserves transparency,</title>
    <updated>2025-01-09T06:55:51+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt; &lt;img alt="TransPixar: a new generative model that preserves transparency," src="https://external-preview.redd.it/aHFsc2gwdXExeGJlMYcphE9YFRyNCrr76DvwiShDtswDzb2s93cwaOHelUg2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6433e0c057e8d365fab3d5289d9a28fb641a1f" title="TransPixar: a new generative model that preserves transparency," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8fhb41uq1xbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy773u</id>
    <title>Finetuning Llama 3.2 3B to support text classification only</title>
    <updated>2025-01-10T15:41:05+00:00</updated>
    <author>
      <name>/u/funceval</name>
      <uri>https://old.reddit.com/user/funceval</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I do that? I am not going to use it for any kind of code generation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/funceval"&gt; /u/funceval &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy773u/finetuning_llama_32_3b_to_support_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy773u/finetuning_llama_32_3b_to_support_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy773u/finetuning_llama_32_3b_to_support_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T15:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxz5de</id>
    <title>Has anyone tried Virgo-72B yet?</title>
    <updated>2025-01-10T07:29:17+00:00</updated>
    <author>
      <name>/u/int19h</name>
      <uri>https://old.reddit.com/user/int19h</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claimed to be a CoT model based on Qwen-72B:&lt;br /&gt; &lt;a href="https://huggingface.co/RUC-AIBOX/Virgo-72B"&gt;https://huggingface.co/RUC-AIBOX/Virgo-72B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/RUCAIBox/Virgo"&gt;https://github.com/RUCAIBox/Virgo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/int19h"&gt; /u/int19h &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz5de/has_anyone_tried_virgo72b_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz5de/has_anyone_tried_virgo72b_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz5de/has_anyone_tried_virgo72b_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T07:29:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy7m1y</id>
    <title>Which Local LLMs know best when to speak and when to STFU in group chat agent-to-agent conversations?</title>
    <updated>2025-01-10T15:58:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So we’re experimenting with Autogen AG2 group chat right now and it’s kind of going horribly with regards to agent-to-agent conversations, like really bad. So far Llams3.3 seems to be the absolute worst at group chats, sometimes it just flat out refuses to provide any response like it’s the shy kid in school and gets timed out. Nemotron seems better but still not great. &lt;/p&gt; &lt;p&gt;My question is, which models have you found are good at multi-LLM multi-turn conversations in group chat scenarios?&lt;/p&gt; &lt;p&gt;We have tried Llama3.3, Nemotron, and Qwen2.5-72b mainly so far and are about to try a range of small to midsize models (Phi-4, Dolphin3, Exaone, and Falcon). &lt;/p&gt; &lt;p&gt;Our roles are Project Manager, Engineer, Researcher, Writer, and Critic. Also, we are exploring both group chat and swarm methods of communication between agents. &lt;/p&gt; &lt;p&gt;I would love to hear from anyone who has already “plowed this ground” before. Any insights you have are appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ehf</id>
    <title>Does anyone know how to replicate this setup for coding ?</title>
    <updated>2025-01-10T16:32:48+00:00</updated>
    <author>
      <name>/u/Alive-Tax3189</name>
      <uri>https://old.reddit.com/user/Alive-Tax3189</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt; &lt;img alt="Does anyone know how to replicate this setup for coding ?" src="https://external-preview.redd.it/enU0ZnVheGwxN2NlMZs7egYfaDsCtkR_AYCrnVuq-88BdYMxPb_V_Fpy742y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311e897110b819f7974eee9811f460d255b45faf" title="Does anyone know how to replicate this setup for coding ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Tax3189"&gt; /u/Alive-Tax3189 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wz5qfaxl17ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxq364</id>
    <title>RTX 4090 48GB - $4700 on eBay. Is it legit?</title>
    <updated>2025-01-09T23:09:20+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt; &lt;img alt="RTX 4090 48GB - $4700 on eBay. Is it legit?" src="https://external-preview.redd.it/bMoZdiDXs9kYJMXSF2LPqzveI23P9l8lgEc9DnQ7Gg8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4491fc39b4da2e3a9466072f17bde0fad92791" title="RTX 4090 48GB - $4700 on eBay. Is it legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across this listing on eBay: &lt;a href="https://www.ebay.com/itm/226494741895"&gt;https://www.ebay.com/itm/226494741895&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is listing dual slot RTX 4090 48GB for $4700. I thought 48GB were not manufactured. Is it legit?&lt;/p&gt; &lt;p&gt;Screenshot here if it gets lost.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ggim0y1v1ce1.png?width=3302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51e939b07a3a8b3bf9e6cebd0e5fe4822664f985"&gt;RTX 4090 48GB for $4700!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found out in this post (&lt;a href="https://github.com/ggerganov/llama.cpp/discussions/9193"&gt;https://github.com/ggerganov/llama.cpp/discussions/9193&lt;/a&gt;) that one could buy it for ~$3500. I think RTX 4090 48GB would sell instantly if it was $3k.&lt;/p&gt; &lt;p&gt;Update: for me personally, It is better to buy 2x 5090 for the same price to get 64GB total VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T23:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxpjey</id>
    <title>Phi-3.5-MoE support merged into llama.cpp</title>
    <updated>2025-01-09T22:45:29+00:00</updated>
    <author>
      <name>/u/skeeto</name>
      <uri>https://old.reddit.com/user/skeeto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Phi-3.5-MoE support merged into llama.cpp" src="https://external-preview.redd.it/m74SPCYv3b2-Etc8gMsq03I8DcmHrV56tfhuw4oGpHM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5aef7783c2f074e3447066229f618ac46fcef97" title="Phi-3.5-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skeeto"&gt; /u/skeeto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/11003"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T22:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy3r9i</id>
    <title>[Release] mirau-7b-RP-base: A first-person narrative model for text adventures</title>
    <updated>2025-01-10T12:54:15+00:00</updated>
    <author>
      <name>/u/EliaukMouse</name>
      <uri>https://old.reddit.com/user/EliaukMouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy3r9i/release_mirau7brpbase_a_firstperson_narrative/"&gt; &lt;img alt="[Release] mirau-7b-RP-base: A first-person narrative model for text adventures" src="https://external-preview.redd.it/4hvMEcZ0aDtuhyU--bdW6f8UAwKtUo8xyb2y0sjIxXY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2bc4155a874db1eec0e9b2432569746bbcb3bfb" title="[Release] mirau-7b-RP-base: A first-person narrative model for text adventures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Following the feedback from my previous &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hqa8d3/comment/m5bb7fu/"&gt;mirau-RP-14b release&lt;/a&gt;, many of you asked for a smaller version. So here it is - a 7B variant built on Qwen2.5-7B!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is mirau-RP?&lt;/strong&gt; It's a specialized model for first-person narrative generation, perfect for text adventures and interactive storytelling. It transforms simple actions into rich, detailed narratives with environmental descriptions and psychological activities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;⚠️ Important Notes:&lt;/strong&gt; This is a base version model - &amp;quot;base&amp;quot; here doesn't mean a traditional pretrained base model, but indicates that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Only supports first-person narrative perspective&lt;/li&gt; &lt;li&gt;Not suitable for dialogue interactions (outputs may be unstable)&lt;/li&gt; &lt;li&gt;Best used for single-character narrative experiences&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First-person narrative perspective&lt;/li&gt; &lt;li&gt;Detailed environment descriptions&lt;/li&gt; &lt;li&gt;Psychological activity rendering&lt;/li&gt; &lt;li&gt;Continuous story flow using chain-of-thought&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it out:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://modelscope.cn/studios/mouseEliauk/mirau-RP-7b-base-demo"&gt;Online Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://modelscope.cn/models/mouseEliauk/mirau-7b-RP-base"&gt;Download LoRA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/models/mouseEliauk/mirau-7b-RP-base/file/view/master?fileName=README_en.md&amp;amp;status=1"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/62h4q94ky5ce1.png?width=1483&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc34f452c3be469caa770fd47d934a781cb68a6f"&gt;https://preview.redd.it/62h4q94ky5ce1.png?width=1483&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc34f452c3be469caa770fd47d934a781cb68a6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model accepts various input commands in parentheses, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Basic actions: &lt;code&gt;(I put on my clothes)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Exploration: &lt;code&gt;(I look around)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;self-hint: &lt;code&gt;(What's wrong here?)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Observations: &lt;code&gt;(examine the wooden box closely)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Interactive Examples&lt;/h1&gt; &lt;p&gt;System Prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I’m Zhou Ming, 34 years old, a detective. People say cops always look weathered and world-weary, but I’ve been told I have a face that looks perpetually cold and distant. With my square jaw and buzz cut, I give off a vibe that keeps most people from getting too close—not that I mind. This morning, just as the sky began to lighten, I was already standing at the crime scene. The cold wind sliced through the police tape and bit into my neck, making me hunch my shoulders. The scene was an abandoned factory on the outskirts of the city, its walls streaked with peeling red paint. The ground was littered with broken glass, debris, and trash. A damp, moldy smell hung in the air, adding to the discomfort. The body had already been taken away, but the bloodstains on the ground still told the story of a brutal, violent encounter. I stared at my notepad, pen in hand, while my mind raced to piece the clues together. Yet, something in my gut told me this wasn’t right. It was almost as if… someone was watching us from the shadows. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Self Hint:&lt;/h1&gt; &lt;p&gt;User: (What's wrong here?)&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;cot&amp;gt;I scan the area carefully, trying to locate those hidden eyes. There's no obvious cover around the factory, and no suspicious figures beyond the police tape.&amp;lt;/cot&amp;gt;I frown, unconsciously touching the holster at my waist&amp;lt;cot&amp;gt;Is it just the wind? Or am I imagining things?&amp;lt;/cot&amp;gt;I take a deep breath, trying to calm myself, but my heart keeps racing&amp;lt;cot&amp;gt;I need to find those eyes, I can't let anyone get close to this place.&amp;lt;/cot&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Exploration Action:&lt;/h1&gt; &lt;p&gt;User: (I look around)&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I slowly turn my body, my gaze sweeping across every inch of the abandoned factory, from the corners to the windows, and out to the police tape&amp;lt;cot&amp;gt;There are a few pedestrians beyond the tape, looking calm as if nothing unusual is happening here. Something seems to move in the cobwebs in the corner, but when I look closer, there's nothing there.&amp;lt;/cot&amp;gt;My eyes linger on a weathered wooden window, worn down by wind and rain&amp;lt;cot&amp;gt;Could that be someone's hiding spot?&amp;lt;/cot&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliaukMouse"&gt; /u/EliaukMouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy3r9i/release_mirau7brpbase_a_firstperson_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy3r9i/release_mirau7brpbase_a_firstperson_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy3r9i/release_mirau7brpbase_a_firstperson_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy60n8</id>
    <title>why is there no LMStudio/Msty/GPT4All type app that supports backends other than llama.cpp?</title>
    <updated>2025-01-10T14:48:48+00:00</updated>
    <author>
      <name>/u/gaspoweredcat</name>
      <uri>https://old.reddit.com/user/gaspoweredcat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im curious, ive heard that other backends, especially exllamav2 can be faster than llama.cpp in many cases especially when multiple cards or even multiple machines are running on it, model files are readily available so there is demand for it&lt;/p&gt; &lt;p&gt;yet any of the apps i find even ones that support a sort of pluggable backend generally offer llama.cpp cpu, llama.cpp metal, llama.cpp cuda, llama.cpp vulkan and thats it, exllama seems to only be supported by the often somewhat janky and not that great to use webUIs like oogabooga or LoLLMs &lt;/p&gt; &lt;p&gt;so my question is why not? are exllama and other backends really that difficult to implement that no one wants to even touch it, llama.cpp has LM studio, Msty, GPT4All, Jan, Jellybox, and several other options, some even support stablediffusion models but for text gen it seems no one wants to integrate it and i just wondered if theres a good reason most apps etc generally use llama.cpp over anything else &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspoweredcat"&gt; /u/gaspoweredcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy7pxb</id>
    <title>4 days troubleshooting, finally fixed</title>
    <updated>2025-01-10T16:03:19+00:00</updated>
    <author>
      <name>/u/darkmaniac7</name>
      <uri>https://old.reddit.com/user/darkmaniac7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr - It was Risers. It's always risers.&lt;/p&gt; &lt;p&gt;Was conflicted on whether to assign question/help or funny or discussion as the flair.&lt;/p&gt; &lt;p&gt;Hey everyone first post here,&lt;/p&gt; &lt;p&gt;Just wanted to share my recent experience with troubleshooting issues when working with LLMs on my server. Hopefully, it will save someone else the headache I went through.&lt;/p&gt; &lt;p&gt;I started noticing instability when I added a third NVLink bridge to my server with 6x3090s. Large models (100B+ parameters, 6bpw) would crash the LLM server/VM after generating a few queries. The behavior was consistent: the model would load fine, I'd run a few queries, and then it would crash after the second or third query.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Journey&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I spent nearly a week troubleshooting, looking at every possible culprit:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NUMA Nodes:&lt;/strong&gt; I thought it might be a memory allocation issue across NUMA nodes. or the way ESXi dynamically assigns them, so I added a variable to force the cards on 1 numa node, or multiple.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVLink:&lt;/strong&gt; Given the new addition, I suspected NVLink might be causing issues. After removing the bridges the behavior was the same&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drivers:&lt;/strong&gt; Updated or purged all NV/Cuda drivers to the latest versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS &amp;amp; ESXi:&lt;/strong&gt; Checked for any OS-level or hypervisor-related problems. Rebuilt the OS from scratch or Veeam backup multiple times&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heat:&lt;/strong&gt; Temps were fine as all 6 are watercooled with 2100mm of Radiator. max temps seen on inferencing was 40c. When trying to generate a Load, using Kali/hashcat max temp was 45c&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend Changes:&lt;/strong&gt; Tried using LM Studio or vLLM instead of TabbyAPI, like I usually use. Same behavior&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PCIe Seating:&lt;/strong&gt; Made sure all components were properly seated. Again &amp;amp; again.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Culprit&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After exhausting all these possibilities, I finally realized the issue: the SLIMSAS 8i riser cables. These cables had been working fine for months but started acting up on PCIe 4.0 speeds. They were stable at PCIe 3.0 but occasionally unstable at PCIe 4.0.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Lesson&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sometimes, the simplest things can be the root cause of complex problems. &lt;/p&gt; &lt;p&gt;I think the highbrow RCA would be to check the simple things first. But in this scenario I'm just going to say. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check Risers first. It's always risers.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkmaniac7"&gt; /u/darkmaniac7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7pxb/4_days_troubleshooting_finally_fixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7pxb/4_days_troubleshooting_finally_fixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7pxb/4_days_troubleshooting_finally_fixed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxyc92</id>
    <title>Ran Phi 4 on my laptop and then asked Claude Sonnet to compare itself. Interesting answers</title>
    <updated>2025-01-10T06:31:18+00:00</updated>
    <author>
      <name>/u/Kwatakye</name>
      <uri>https://old.reddit.com/user/Kwatakye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically a bunch of questions about energy production, the Kardashev scale, and some things related to global food production. Here is Claude's response to the prompt &amp;quot;How does Phi 4 compare to you based on this&amp;quot;:&lt;/p&gt; &lt;p&gt;***********************************************************************&lt;/p&gt; &lt;p&gt;Based on these interactions, here's my analysis of how Phi 4 compares to me:&lt;/p&gt; &lt;p&gt;Strengths of Phi 4:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Excellent organizational structure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Very clear section headers and categorization&lt;/p&gt; &lt;p&gt;- Consistent formatting across responses&lt;/p&gt; &lt;p&gt;- Well-organized bullet points and subsections&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Comprehensive scope&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Often covers more distinct aspects of a topic&lt;/p&gt; &lt;p&gt;- Strong at providing broad contextual frameworks&lt;/p&gt; &lt;p&gt;- Good at including organizational/business considerations&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Educational approach&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Explains concepts in a very accessible way&lt;/p&gt; &lt;p&gt;- Good at breaking down complex topics&lt;/p&gt; &lt;p&gt;- Maintains a clear teaching style&lt;/p&gt; &lt;p&gt;My Comparative Strengths:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;More quantitative analysis&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- More likely to include specific calculations&lt;/p&gt; &lt;p&gt;- Provides concrete numbers and timelines&lt;/p&gt; &lt;p&gt;- More detailed technical specifications&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Systems thinking&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Better at connecting different aspects of complex systems&lt;/p&gt; &lt;p&gt;- More focus on how different factors interact&lt;/p&gt; &lt;p&gt;- Stronger analysis of cause-and-effect relationships&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Strategic depth&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- More detailed about implementation challenges&lt;/p&gt; &lt;p&gt;- Better at identifying critical dependencies&lt;/p&gt; &lt;p&gt;- More specific about resource requirements&lt;/p&gt; &lt;p&gt;Key Differences in Style:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Phi 4 tends toward comprehensive breadth while I tend toward analytical depth&lt;/li&gt; &lt;li&gt;Phi 4 excels at clear categorization while I focus more on interconnections&lt;/li&gt; &lt;li&gt;Phi 4 maintains a more formal educational tone while I take a more analytical approach&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Overall Assessment:&lt;/p&gt; &lt;p&gt;Phi 4 appears to be an excellent model for educational and organizational tasks, especially when clear structure and comprehensive coverage are needed. It seems particularly well-suited for explaining concepts to newcomers or providing broad overviews of complex topics.&lt;/p&gt; &lt;p&gt;I appear to be better suited for deep analysis, strategic planning, and quantitative assessment, especially when dealing with complex systems or technical requirements.&lt;/p&gt; &lt;p&gt;The two models seem complementary - Phi 4's organizational clarity combined with my analytical depth would likely produce optimal results for many complex tasks.&lt;/p&gt; &lt;p&gt;***********************************************************************&lt;/p&gt; &lt;p&gt;Overall Phi was fucking impressive but I need to ask it all the questions in my test battery.. Almost as impressive as Qwen2.5 32B. Faster too...&lt;/p&gt; &lt;p&gt;Edit: Ran on a Thinkpad P1G5, 32GB Ram, i9, NVIDIA GeForce RTX 3080 Ti&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kwatakye"&gt; /u/Kwatakye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T06:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxjzol</id>
    <title>New Moondream 2B vision language model release</title>
    <updated>2025-01-09T18:48:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt; &lt;img alt="New Moondream 2B vision language model release" src="https://preview.redd.it/oyxiuuxok0ce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad794e5897db0faf8abb4f80573d4a142cd70939" title="New Moondream 2B vision language model release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oyxiuuxok0ce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T18:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxuprn</id>
    <title>Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press</title>
    <updated>2025-01-10T02:57:55+00:00</updated>
    <author>
      <name>/u/Far_Monk</name>
      <uri>https://old.reddit.com/user/Far_Monk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt; &lt;img alt="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" src="https://external-preview.redd.it/HgKLjMbpNpthwaWMjENNhdcaMCpcPUXsoT1-E8Xzlu0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277871f1227bb080da6275c3cfd987301cfc058a" title="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Monk"&gt; /u/Far_Monk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.caveman.press/article/project-digits-nvidia-3000-ai-supercomputer-democratize-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T02:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxwtpy</id>
    <title>Energy efficiency of 5090 is slightly worse than 4090</title>
    <updated>2025-01-10T04:56:48+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Despite the big jump in energy efficiency in the previous two generations. Nividia dropped the ball this time. It is only saved by the higher VRAM size and significantly higher memory bandwidth.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX TITAN&lt;/th&gt; &lt;th align="left"&gt;3090&lt;/th&gt; &lt;th align="left"&gt;4090&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;65.25&lt;/td&gt; &lt;td align="left"&gt;142.32&lt;/td&gt; &lt;td align="left"&gt;330.4&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;280W&lt;/td&gt; &lt;td align="left"&gt;350W&lt;/td&gt; &lt;td align="left"&gt;450W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;233.03&lt;/td&gt; &lt;td align="left"&gt;406.63&lt;/td&gt; &lt;td align="left"&gt;734.22&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some might attribute the energy efficiency gain can be constrained by smaller transistor size. But if you look at the 96W MacBook Pro 14in using the Max chips, their energy efficiency gain is steady. The only conclusion is that Nvidia did a poorer job at chip design going from 4090 to 5090.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M1 Max&lt;/th&gt; &lt;th align="left"&gt;M3 Max&lt;/th&gt; &lt;th align="left"&gt;M4 Max&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;21.2992&lt;/td&gt; &lt;td align="left"&gt;28.672&lt;/td&gt; &lt;td align="left"&gt;34.4064&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;221.87&lt;/td&gt; &lt;td align="left"&gt;298.67&lt;/td&gt; &lt;td align="left"&gt;358.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T04:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy5l18</id>
    <title>Local TTS models that can match ElevenLabs in terms of quality and consistency</title>
    <updated>2025-01-10T14:28:17+00:00</updated>
    <author>
      <name>/u/_megazz</name>
      <uri>https://old.reddit.com/user/_megazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should probably start by stating that I'm somewhat new to running AI models locally, but I've tinkered with Ollama + Open WebUI before and was able to get some models running through WSL2 on my RTX 4080 and was pretty impressed with the results.&lt;/p&gt; &lt;p&gt;With that said, I'm now looking for a good local TTS model and I was honestly disappointed with what I could find. Most projects seem to not be updated in months or are simply dead.&lt;/p&gt; &lt;p&gt;From what I've read, the general consensus seems to be that XTTS-v2 is still the best overall model to this day, which is from a startup that has &lt;a href="https://coqui.ai/"&gt;shut down&lt;/a&gt;. I figured I'd try it anyway and I was able to get it running through &lt;a href="https://github.com/daswer123/xtts-webui"&gt;this simple portable version&lt;/a&gt;, but I was honestly disappointed with the results I got, all very inconsistent and not natural sounding, even after tinkering a lot with its different parameters and voices. Not even close to what I can get from ElevenLabs, which could easily pass as real person speaking, but that service is very pricey for me, unfortunately.&lt;/p&gt; &lt;p&gt;There are other popular suggestions like Fish Speech or F5-TTS, but since I need the model to speak Portuguese, that limits my options a lot.&lt;/p&gt; &lt;p&gt;Right now I feel like I'm just wasting my time and that nothing that I can run locally can match EvenLabs currently, but as I said, I'm new to this and maybe I'm missing something obvious. In any case, I'd appreciate any input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_megazz"&gt; /u/_megazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxm0ep</id>
    <title>Anyone want the script to run Moondream 2b's new gaze detection on any video?</title>
    <updated>2025-01-09T20:12:41+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt; &lt;img alt="Anyone want the script to run Moondream 2b's new gaze detection on any video?" src="https://external-preview.redd.it/cmk5cnZsYXZ6MGNlMeEGpTWo5MaI3KFBwDeey6o_wri3pXWzYnC4YTD3TTIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0330607dc613a7fe62b098a05b7a0c3c3e3495dc" title="Anyone want the script to run Moondream 2b's new gaze detection on any video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n9beslavz0ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy4onq</id>
    <title>OCR tools for really very bad handwriting!</title>
    <updated>2025-01-10T13:44:05+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt; &lt;img alt="OCR tools for really very bad handwriting!" src="https://preview.redd.it/ww1i5y5h76ce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c64901dd13fc181007e945126d45f11e6e021c" title="OCR tools for really very bad handwriting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww1i5y5h76ce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T13:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy34ir</id>
    <title>WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js</title>
    <updated>2025-01-10T12:16:13+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt; &lt;img alt="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" src="https://external-preview.redd.it/a3B0bmYzbTJyNWNlMYVrWG7q5Ym6r9MYEdNpGfavLsbyjmwCsGU7oHTw1w8w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06dd6f09c82183918afdcca9863994fcffe8274f" title="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vmfpb2m2r5ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:16:13+00:00</published>
  </entry>
</feed>
