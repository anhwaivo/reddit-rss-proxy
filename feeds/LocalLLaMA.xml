<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-11T14:06:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hyuf4n</id>
    <title>awesome-mcp-clients: A collection of MCP clients.</title>
    <updated>2025-01-11T11:59:44+00:00</updated>
    <author>
      <name>/u/punkpeye</name>
      <uri>https://old.reddit.com/user/punkpeye</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyuf4n/awesomemcpclients_a_collection_of_mcp_clients/"&gt; &lt;img alt="awesome-mcp-clients: A collection of MCP clients." src="https://external-preview.redd.it/4KBz9ekZU5wO9l68R_foyo2COnEy6xlRXL3HdC_atBs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6065d85b256d56cfd941b3a9d65722df035a3cf" title="awesome-mcp-clients: A collection of MCP clients." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/punkpeye"&gt; /u/punkpeye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/punkpeye/awesome-mcp-clients/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyuf4n/awesomemcpclients_a_collection_of_mcp_clients/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyuf4n/awesomemcpclients_a_collection_of_mcp_clients/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyqcir</id>
    <title>stable-diffusion.cpp context size</title>
    <updated>2025-01-11T06:56:43+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody using this tool? I noticed the context size being clipped to some given size when the inference starts. I wonder if anybody figured how to control that parameter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T06:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ehf</id>
    <title>Does anyone know how to replicate this setup for coding ?</title>
    <updated>2025-01-10T16:32:48+00:00</updated>
    <author>
      <name>/u/Alive-Tax3189</name>
      <uri>https://old.reddit.com/user/Alive-Tax3189</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt; &lt;img alt="Does anyone know how to replicate this setup for coding ?" src="https://external-preview.redd.it/enU0ZnVheGwxN2NlMZs7egYfaDsCtkR_AYCrnVuq-88BdYMxPb_V_Fpy742y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311e897110b819f7974eee9811f460d255b45faf" title="Does anyone know how to replicate this setup for coding ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Tax3189"&gt; /u/Alive-Tax3189 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wz5qfaxl17ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvgtw</id>
    <title>training STT model for my local language</title>
    <updated>2025-01-11T13:05:37+00:00</updated>
    <author>
      <name>/u/Alive-Professor5944</name>
      <uri>https://old.reddit.com/user/Alive-Professor5944</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;training STT model for my local language&lt;/p&gt; &lt;p&gt;Guys how can i fine tune STT, to build and ai voice chat that ables to understand my language so i can help my people learn english while they speak to the Ai with their local language please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Professor5944"&gt; /u/Alive-Professor5944 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjhch</id>
    <title>Any good LLM benchmarks that rank ability to document code and explain code?</title>
    <updated>2025-01-11T00:29:02+00:00</updated>
    <author>
      <name>/u/palindsay</name>
      <uri>https://old.reddit.com/user/palindsay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems current coding benchmarks like Aider and bigcode, etc. focus on code refactoring and generation. What about strength in code documentation and explanation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palindsay"&gt; /u/palindsay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy4onq</id>
    <title>OCR tools for really very bad handwriting!</title>
    <updated>2025-01-10T13:44:05+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt; &lt;img alt="OCR tools for really very bad handwriting!" src="https://preview.redd.it/ww1i5y5h76ce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c64901dd13fc181007e945126d45f11e6e021c" title="OCR tools for really very bad handwriting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww1i5y5h76ce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T13:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyrke5</id>
    <title>The ASRock Radeon RX 7900 XTX Creator</title>
    <updated>2025-01-11T08:27:26+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People building AI PCs with multiple GPUs on a budget love the RTX 3090 2-slot &amp;quot;Turbo&amp;quot;/&amp;quot;Aero&amp;quot;/&amp;quot;Classic&amp;quot; blower cards that pretty much disappeared from production shortly after the launch of the chip.&lt;/p&gt; &lt;p&gt;That's why i'm surprised these same people (hi!) aren't talking more about the ASRock Radeon RX 7900 XTX Creator card. It's a 2-slot blower card with a single fan. It's 1100â‚¬ new so 18% more expensive than the cheapest RX 7900 XTX cards. With a Threadripper mainboard you can easily stick four of these cards (96GB VRAM) into a large PC case without having to deal with PCIe port extenders that can cause instability.&lt;/p&gt; &lt;p&gt;Has someone already done this and want to share? How hard is it to get them cooled? Which case did you use? Which software is best for inferencing with multiple of these AMD GPUs? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyio5w</id>
    <title>Beginner Guide - Creating LLM Datasets with Python</title>
    <updated>2025-01-10T23:50:58+00:00</updated>
    <author>
      <name>/u/0xlisykes</name>
      <uri>https://old.reddit.com/user/0xlisykes</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xlisykes"&gt; /u/0xlisykes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://toolworks.dev/docs/Guides/creating-llm-datasets-python"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy5l18</id>
    <title>Local TTS models that can match ElevenLabs in terms of quality and consistency</title>
    <updated>2025-01-10T14:28:17+00:00</updated>
    <author>
      <name>/u/_megazz</name>
      <uri>https://old.reddit.com/user/_megazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should probably start by stating that I'm somewhat new to running AI models locally, but I've tinkered with Ollama + Open WebUI before and was able to get some models running through WSL2 on my RTX 4080 and was pretty impressed with the results.&lt;/p&gt; &lt;p&gt;With that said, I'm now looking for a good local TTS model and I was honestly disappointed with what I could find. Most projects seem to not be updated in months or are simply dead.&lt;/p&gt; &lt;p&gt;From what I've read, the general consensus seems to be that XTTS-v2 is still the best overall model to this day, which is from a startup that has &lt;a href="https://coqui.ai/"&gt;shut down&lt;/a&gt;. I figured I'd try it anyway and I was able to get it running through &lt;a href="https://github.com/daswer123/xtts-webui"&gt;this simple portable version&lt;/a&gt;, but I was honestly disappointed with the results I got, all very inconsistent and not natural sounding, even after tinkering a lot with its different parameters and voices. Not even close to what I can get from ElevenLabs, which could easily pass as real person speaking, but that service is very pricey for me, unfortunately.&lt;/p&gt; &lt;p&gt;There are other popular suggestions like Fish Speech or F5-TTS, but since I need the model to speak Portuguese, that limits my options a lot.&lt;/p&gt; &lt;p&gt;Right now I feel like I'm just wasting my time and that nothing that I can run locally can match EvenLabs currently, but as I said, I'm new to this and maybe I'm missing something obvious. In any case, I'd appreciate any input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_megazz"&gt; /u/_megazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvcrg</id>
    <title>Are embeddings invariant to translation?</title>
    <updated>2025-01-11T12:59:33+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. Is the embedding of a sentence close to the embedding of its translation (e.g. EN and DE) in the embedding space? Which embedding models are better in handling multiple languages, and also representing the same semantic in multiple languages?&lt;/p&gt; &lt;p&gt;Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:59:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyruya</id>
    <title>Where is everyone sourcing their hardware?</title>
    <updated>2025-01-11T08:49:26+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After looking for awhile I finally decided to purchase 3 identical refurbished 3090s through a manufacturer refurb online store and all of them have turned out to be unstable.&lt;br /&gt; One of them locking up the system within a few minutes of being turned on. I thought that by getting a manufacturer refurbished card directly from them that it would be less likely to be an issue. I looked around a lot before purchasing and this seemed like the safest option for a reasonable price ($699 per card).&lt;/p&gt; &lt;p&gt;I am in the process of RMAâ€™ing them, but where does everyone else get their hardware? Has anyone else had issues with bad video cards? Any tips on good places to order from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyukc2</id>
    <title>GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;D</title>
    <updated>2025-01-11T12:09:03+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt; &lt;img alt="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" src="https://external-preview.redd.it/2MaaUSNtf5DLbq6ZpF876OWYQdcOtASsj6e_pAKWpKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec156925c4109c28028bb52b1517ca8eb977cd5a" title="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tegridydev/dnd-llm-game?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu07n</id>
    <title>Localhost LLM Benchmark</title>
    <updated>2025-01-11T11:29:56+00:00</updated>
    <author>
      <name>/u/05032-MendicantBias</name>
      <uri>https://old.reddit.com/user/05032-MendicantBias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to benchmark my self hosted LLM, I want to run benchmarks like MMLU to evaluate the acceleration and accuracy of various quants against my GPU limitations&lt;/p&gt; &lt;p&gt;I tried the below tool, but it doesn't hit the API at all&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="http://github.com/EleutherAI/lm-evaluation-harness"&gt;github.com/EleutherAI/lm-evaluation-harness&lt;/a&gt;&lt;/p&gt; &lt;p&gt;lm_eval --model local-chat-completions --tasks gsm8k --model_args base_url=http://localhost:8000 --apply_chat_template&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;EDIT: As a sanity check I made a quick python program to query my self hosted LLM and it works, so the problem is not the LLM hosting:&lt;/p&gt; &lt;p&gt;CODE:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests # Define the URL for the LLM endpoint url = &amp;quot;http://localhost:8000/v1/completions&amp;quot; # Define the payload with your input data payload = { &amp;quot;prompt&amp;quot;: &amp;quot;Once upon a time&amp;quot;, &amp;quot;max_tokens&amp;quot;: 100 } # Set the headers (if needed) headers = { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer YOUR_API_KEY&amp;quot; # Replace 'YOUR_API_KEY' with your actual API key if required } # Send the POST request to the LLM endpoint response = requests.post(url, json=payload, headers=headers) # Check if the request was successful if response.status_code == 200: # Print the response from the LLM completion = response.json() print(&amp;quot;Completion:&amp;quot;, completion[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;]) else: # Print an error message if the request was not successful print(&amp;quot;Error:&amp;quot;, response.status_code, response.text) import requests # Define the URL for the LLM endpoint url = &amp;quot;http://localhost:8000/v1/completions&amp;quot; # Define the payload with your input data payload = { &amp;quot;prompt&amp;quot;: &amp;quot;Once upon a time&amp;quot;, &amp;quot;max_tokens&amp;quot;: 100 } # Set the headers (if needed) headers = { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer YOUR_API_KEY&amp;quot; # Replace 'YOUR_API_KEY' with your actual API key if required } # Send the POST request to the LLM endpoint response = requests.post(url, json=payload, headers=headers) # Check if the request was successful if response.status_code == 200: # Print the response from the LLM completion = response.json() print(&amp;quot;Completion:&amp;quot;, completion[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;]) else: # Print an error message if the request was not successful print(&amp;quot;Error:&amp;quot;, response.status_code, response.text) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ANSWER&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-01-11 12:53:43 [INFO] [LM STUDIO SERVER] Running completion on text: Once upon a time 2025-01-11 12:53:43 [INFO] [LM STUDIO SERVER] Processing... 2025-01-11 12:53:44 [INFO] Generated prediction: { &amp;quot;id&amp;quot;: &amp;quot;cmpl-kelaozz9r929wvhnt1vse&amp;quot;, &amp;quot;object&amp;quot;: &amp;quot;text_completion&amp;quot;, &amp;quot;created&amp;quot;: 1736596423, &amp;quot;model&amp;quot;: &amp;quot;qwen2.5-7b-instruct&amp;quot;, &amp;quot;choices&amp;quot;: [ { &amp;quot;index&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;, in the middle ages, there was a great king. He had many friends, but one of them he loved more than all his other friends combined. One day this friend of the king told him that he wanted to be the next king.\n\nThe king was a little upset by this, but he agreed to make the request formal in writing and signed it.\n\nA few days later, the friend of the king sent back the document with some notes written on the side, indicating what parts were acceptable&amp;quot;, &amp;quot;logprobs&amp;quot;: null, &amp;quot;finish_reason&amp;quot;: &amp;quot;length&amp;quot; } ], &amp;quot;usage&amp;quot;: { &amp;quot;prompt_tokens&amp;quot;: 4, &amp;quot;completion_tokens&amp;quot;: 99, &amp;quot;total_tokens&amp;quot;: 103 } } 2025-01-11 12:53:44 [INFO] [LM STUDIO SERVER] Client disconnected. Stopping generation.. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/05032-MendicantBias"&gt; /u/05032-MendicantBias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyh9y3</id>
    <title>DeepSeek-V3 imatrix quants by team mradermacher</title>
    <updated>2025-01-10T22:47:05+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt; &lt;img alt="DeepSeek-V3 imatrix quants by team mradermacher" src="https://external-preview.redd.it/m-G04wn3IB1jswcKbDUS8jJlKetCzX6HK1WoeuTcULY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059108de9020787af36b4f6d446ccbfc92d4ba7e" title="DeepSeek-V3 imatrix quants by team mradermacher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/DeepSeek-V3-i1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T22:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy91m1</id>
    <title>0.5B Distilled QwQ, runnable on IPhone</title>
    <updated>2025-01-10T16:59:44+00:00</updated>
    <author>
      <name>/u/Lord_of_Many_Memes</name>
      <uri>https://old.reddit.com/user/Lord_of_Many_Memes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt; &lt;img alt="0.5B Distilled QwQ, runnable on IPhone" src="https://external-preview.redd.it/hOvT7Zh2EDTGcuqajUYbM7IboIMuAwdCFsY0UWAS0pU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85274e9584cd8dc27f3835483f32b47ea48f28f0" title="0.5B Distilled QwQ, runnable on IPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lord_of_Many_Memes"&gt; /u/Lord_of_Many_Memes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/kz919/Mini-QwQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyapzu</id>
    <title>Phi-4 Finetuning - now with &gt;128K context length + Bug Fix Details</title>
    <updated>2025-01-10T18:09:05+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt; &lt;img alt="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Phi-4 with &amp;gt;128K context lengths using &lt;a href="https://github.com/unslothai/unsloth/"&gt;Unsloth&lt;/a&gt;! That's 12x longer than Hugging Face + FA2â€™s 11K on a 48GB GPU.&lt;/p&gt; &lt;p&gt;Phi-4 Finetuning Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also previously announced bug fixes for Phi-4 and so weâ€™ll reveal the details.&lt;/p&gt; &lt;p&gt;But, before we do, some of you were curious if our fixes actually worked? Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4"&gt;Open LLM Leaderboard&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e"&gt;https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you even tested it to show greatly improved results in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 1: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/"&gt;Multiple-choice tasks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316"&gt;https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/"&gt;ASCII art generation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada"&gt;https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Bug Fix Details&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Tokenizer Fix: Phi-4 incorrectly uses &amp;lt;|endoftext|&amp;gt; as EOS instead of &amp;lt;|im_end|&amp;gt;.&lt;/li&gt; &lt;li&gt;Finetuning Fix: Use a proper padding token (e.g., &amp;lt;|dummy_87|&amp;gt;).&lt;/li&gt; &lt;li&gt;Chat Template Fix: Avoid adding an assistant prompt unless specified to prevent serving issues.&lt;/li&gt; &lt;li&gt;More in-depth in our blog: &lt;a href="https://unsloth.ai/blog/phi4"&gt;https://unsloth.ai/blog/phi4&lt;/a&gt; or &lt;a href="https://twitter.com/danielhanchen/status/1877781452818968615"&gt;tweet&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For all other model uploads, see &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;our docs&lt;/a&gt;&lt;br /&gt; I know this post was a bit long, but I hope it was informative and please ask any questions!! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T18:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy34ir</id>
    <title>WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js</title>
    <updated>2025-01-10T12:16:13+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt; &lt;img alt="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" src="https://external-preview.redd.it/a3B0bmYzbTJyNWNlMYVrWG7q5Ym6r9MYEdNpGfavLsbyjmwCsGU7oHTw1w8w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06dd6f09c82183918afdcca9863994fcffe8274f" title="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vmfpb2m2r5ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hydavt</id>
    <title>New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b</title>
    <updated>2025-01-10T19:56:16+00:00</updated>
    <author>
      <name>/u/iamephemeral</name>
      <uri>https://old.reddit.com/user/iamephemeral</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt; &lt;img alt="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" src="https://external-preview.redd.it/r4CGqgcRPLr1eA9JfvNHSBaN_-4tgT5j575hGH0pgUU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239946d045e3a552b2d863b9157de34884befd7f" title="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamephemeral"&gt; /u/iamephemeral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Goodfire/Llama-3.3-70B-Instruct-SAE-l50"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T19:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyf1pf</id>
    <title>Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro?</title>
    <updated>2025-01-10T21:09:12+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt; &lt;img alt="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjoau</id>
    <title>This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)</title>
    <updated>2025-01-11T00:38:10+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt; &lt;img alt="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" src="https://b.thumbs.redditmedia.com/niNscGOj9hur8A-QVwFzrElx4sAsFt-GLXQ2A5RCLGw.jpg" title="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hyjoau"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyomxu</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push</title>
    <updated>2025-01-11T05:04:42+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push?leadSource=reddit_wall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvfjq</id>
    <title>What do you think of AI employees?</title>
    <updated>2025-01-11T13:03:35+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seeing a surge in start-ups and large enterprises building AI employees.&lt;/p&gt; &lt;p&gt;A good number of well-funded start-ups are building AI SDRs, SWEs, marketing agents, and Customer success agents. Even Salesforce is working on AgentForce to create no-code salesforce automation agents.&lt;/p&gt; &lt;p&gt;This trend is growing faster than I thought; dozens of start-ups are probably in YC this year.&lt;/p&gt; &lt;p&gt;Iâ€™m not sure if any of them are in production doing the jobs in the real world, and also, these agents may require a dozen integrations to be anywhere close to being functional.&lt;/p&gt; &lt;p&gt;As much as I like LLMs, they still donâ€™t seem capable of handling edge cases in real-world jobs. They may be suitable for building automated pipelines for tightly scoped tasks, but replacing humans seems far-fetched.&lt;/p&gt; &lt;p&gt;Salesforce Chairman Mark Benioff even commented on not hiring human employees anymore; though it could be their sneaky marketing, it shows their intent.&lt;/p&gt; &lt;p&gt;What do you think of this AI employee in general the present and future? I would love to hear your thoughts if youâ€™re building something simillar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu2dh</id>
    <title>LocalGLaDOS - running on a real LLM-rig</title>
    <updated>2025-01-11T11:34:21+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt; &lt;img alt="LocalGLaDOS - running on a real LLM-rig" src="https://external-preview.redd.it/EfE2n_bbhcmfaS9RbA5FtQq7jGIahU2UIGm8g-a1Uag.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ce4ca891cbd89dfa15f29ba5ffa968064f42e85" title="LocalGLaDOS - running on a real LLM-rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-GHKTocDF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks â€” trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks â€” trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks â€” trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X: &lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt;hf: &lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview&lt;/a&gt; blog: &lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;https://novasky-ai.github.io/posts/sky-t1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
</feed>
