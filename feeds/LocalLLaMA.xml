<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-01T20:23:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1if5f3b</id>
    <title>Mistral Small 3 is awesome, but...</title>
    <updated>2025-02-01T12:37:11+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has a rather strange way of correcting itself and letting it go unnoticed.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A marble is put in a glass. The glass is then turned upside down and put on a table. Then the glass is picked up and put in a microwave. Where's the marble?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the sequence of events you described, the marble would still be in the glass. Here's why:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; The marble is put in the glass.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; The glass is turned upside down and put on a table. At this point, the marble should fall out of the glass onto the table.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; The glass is picked up and put in a microwave. Since the marble fell out in the previous step, it would not be in the glass when the glass is put in the microwave.&lt;/p&gt; &lt;p&gt;Therefore, the marble is on the table.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It initially and wrongly states that the marble is still inside the glass, and when it's telling why it came to that conclusion, it makes it correct and nonchalantly alters the outcome, lol.&lt;/p&gt; &lt;p&gt;That is the most clear example. I have noticed it do this quite a lot but more subtly for other tasks, such as creative writing. It kinda alters/improves texts on the go, which sometimes makes it a bit incoherent but not necessarily bad on the whole.&lt;/p&gt; &lt;p&gt;In my opinion, apart from this strangle little behavior, Mistral Small 3 is generally very smart and probably the most generally &amp;quot;intelligent&amp;quot; model I've used so far for its size. I've especially had much fun doing creative writing with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifcir5</id>
    <title>Are there any models capable of high-quality profanity?</title>
    <updated>2025-02-01T18:16:47+00:00</updated>
    <author>
      <name>/u/Ray_Dillinger</name>
      <uri>https://old.reddit.com/user/Ray_Dillinger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter what the prompt, I can't get any LLM to do anything more interesting than drop a couple of f-bombs, insert &amp;quot;damn&amp;quot; as an adjective or interjection, or call the user as a bastard.&lt;/p&gt; &lt;p&gt;This will not do if I want it to represent a character that is using rancher-grade or military-grade profanity. There's a variety, lyricism and rhythm to really good profanity that, as far as I can tell, none of these models have anywhere in their training sets.&lt;/p&gt; &lt;p&gt;For example, when a rancher finds someone trespassing on their land at night, we should expect to be hearing phrases like &amp;quot;If you and your choad-licking toadies don't fuck off right now I'm going to tie your nipples in a knot and twist until you start to like it&amp;quot; and similar. This gets a lot of its force and effectiveness from internal rhymes like between 'choad' and 'toadies' and alliterations like between 'nipples' and 'knot', and is driven home by the recontextualization and accusation implicit in 'until you start to like it' as opposed to leaving it a bare threat.&lt;/p&gt; &lt;p&gt;Anyway, in my experience about ten percent of ranchers are capable of keeping up a barrage of this quality for five or ten minutes at a time before they start to repeat themselves, and I remember at least a couple of drill sergeants in the 1980s who seemed capable of going for a full hour at a time. The whole thing doesn't work unless it can be delivered rapidly enough that there's no chance to interrupt, and loses all force and starts to just sound stupid if it frequently repeats.&lt;/p&gt; &lt;p&gt;And I can't get anything similar in LLM output anywhere no matter how I try and coax. No lyricism, no internal rhyme, no alliterations, no accusatory recontextualizations, no awareness that repetition makes profanity sound stupid, and not even any good idea of what the words mean and which are appropriate to describe what kinds of behavior. These models think intense profanity consists of dropping a couple of f-bombs, and many of them aren't even capable of that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ray_Dillinger"&gt; /u/Ray_Dillinger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcir5/are_there_any_models_capable_of_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifeu07</id>
    <title>New benchmark about multi-turn conversation that challenge frontier LLMs and capture Sonet 3.5 advantage: all LLMs perform below 50% accuracy</title>
    <updated>2025-02-01T19:57:31+00:00</updated>
    <author>
      <name>/u/TheIdealHominidae</name>
      <uri>https://old.reddit.com/user/TheIdealHominidae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn"&gt;https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIdealHominidae"&gt; /u/TheIdealHominidae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iep1i4</id>
    <title>DeepSeek AI blocked by Italian authorities</title>
    <updated>2025-01-31T20:54:04+00:00</updated>
    <author>
      <name>/u/ApprehensiveCook2236</name>
      <uri>https://old.reddit.com/user/ApprehensiveCook2236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt; &lt;img alt="DeepSeek AI blocked by Italian authorities" src="https://external-preview.redd.it/cwVbdtxL_MOCraBvILhveGZjsoXBHPHOS4Ik8eBEAT4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74e97f5b3d89f2b950da588950bdaa1d7f71e9d" title="DeepSeek AI blocked by Italian authorities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveCook2236"&gt; /u/ApprehensiveCook2236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/01/31/deepseek-ai-blocked-by-italian-authorities-as-others-member-states-open-probes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1if6o88</id>
    <title>🚀Introducing LLPlayer - The media player integrated with OpenAI Whisper</title>
    <updated>2025-02-01T13:48:54+00:00</updated>
    <author>
      <name>/u/umlx</name>
      <uri>https://old.reddit.com/user/umlx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if6o88/introducing_llplayer_the_media_player_integrated/"&gt; &lt;img alt="🚀Introducing LLPlayer - The media player integrated with OpenAI Whisper" src="https://external-preview.redd.it/g0pAmVUQkF4KkAyxtOIwIb67j2xNu4Br8uYivNbBTeE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e25b86aa4e3a11f551927a2356ae1b77717da548" title="🚀Introducing LLPlayer - The media player integrated with OpenAI Whisper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I have created an open source video player that integrates OpenAI Whisper, Currently supports Windows only.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/umlx5h/LLPlayer"&gt;http://github.com/umlx5h/LLPlayer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently there was news on VLC about AI-generated subtitles, and like that one, I have created a video player that supports whisper.cpp natively.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1if6o88/video/0y1a36uo7jge1/player"&gt;Demo (source:ted.com/talks/elon_musk_the_future_we_re_building_and_boring)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Subtitles can be generated from the &lt;strong&gt;playback position&lt;/strong&gt; of both local and online videos &lt;strong&gt;asynchronously&lt;/strong&gt;. (with &lt;strong&gt;yt-dlp&lt;/strong&gt; integration)&lt;/p&gt; &lt;p&gt;Therefore, there is no need to wait at all to watch any videos with subtitles!&lt;/p&gt; &lt;p&gt;It supports audio in &lt;strong&gt;100&lt;/strong&gt; languages, and like VLC, it also supports real-time translation, with support for &lt;strong&gt;134&lt;/strong&gt; languages.&lt;/p&gt; &lt;p&gt;It is made for language learning purposes and has other unique features not found in normal video players.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dual Subtitles&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Translation&lt;/strong&gt; (Google, DeepL)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time OCR Subtitles&lt;/strong&gt; (Tesseract, MicrosoftOCR)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Subtitles Sidebar&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Subtitles Seeking&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Word Lookup &amp;amp; Search&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Subtitles Downloader&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;More information can be found at: &lt;a href="https://llplayer.com"&gt;https://llplayer.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umlx"&gt; /u/umlx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if6o88/introducing_llplayer_the_media_player_integrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if6o88/introducing_llplayer_the_media_player_integrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if6o88/introducing_llplayer_the_media_player_integrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T13:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieooqe</id>
    <title>DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark</title>
    <updated>2025-01-31T20:38:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt; &lt;img alt="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" src="https://preview.redd.it/i2p0m8em4ege1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4859ed3af650610750eb873e1231f2d526388ec" title="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i2p0m8em4ege1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifcqwj</id>
    <title>Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</title>
    <updated>2025-02-01T18:26:36+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt; &lt;img alt="Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs" src="https://b.thumbs.redditmedia.com/nMhDepkecp6mzYH9xbeeKZ7cjJZ8At59A1TyIQUg8YY.jpg" title="Longer thinking token might not be a best way. Paper: Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p3ft39n5lkge1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87862e427a143953733e496a2c9262e43392a7fc"&gt;https://preview.redd.it/p3ft39n5lkge1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87862e427a143953733e496a2c9262e43392a7fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xjlxdo9lkge1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b532c4f202a2d1010c25ddd617e44ff84e811e6e"&gt;https://preview.redd.it/4xjlxdo9lkge1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b532c4f202a2d1010c25ddd617e44ff84e811e6e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.18585"&gt;https://arxiv.org/pdf/2501.18585&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There were several post in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; saying people are trying to have longer thinking. But maybe longer thinking tokens doesn't necessarily means better or more accurate answer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/OpenAI/comments/1ibyq7p/deepseek_r1_overthinker_force_r1_models_to_think/"&gt;https://www.reddit.com/r/OpenAI/comments/1ibyq7p/deepseek_r1_overthinker_force_r1_models_to_think/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/qunash/r1-overthinker"&gt;https://github.com/qunash/r1-overthinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/latent-variable/r1_reasoning_effort"&gt;https://github.com/latent-variable/r1_reasoning_effort&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. I'm not the authors of the paper&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifcqwj/longer_thinking_token_might_not_be_a_best_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:26:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewnwe</id>
    <title>I was able to 1-shot prompt the unlsoth "python flappy bird game" test with Deepseek R1 distilled 70B. The distilled models deserve more credit.</title>
    <updated>2025-02-01T02:50:40+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the unlsoth models and how they were getting excellent performance using dynamic quantization of the main model down to 2 bits and shrinking the model 80% in size. See article here - &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still, even at 80% the size, when you consider kv cache &amp;amp; context, you still need a minimum of around 200GB of high bandwidth memory to run those models at any sort of respectable speed. And even then, you're getting 5-6 t/s tops, unless you can splurge on a 24-channel dual EPYC system. &lt;/p&gt; &lt;p&gt;I am lucky enough to have a 4x 3090 system, so I have 96GB of VRAM to play with. To me 96GB of VRAM is the sweet spot for running 70B models where you can run bigger quants (6bit+) with plenty left over for context window. And with all that still get above 15 t/s (and much much faster using parallelism &amp;amp; draft models/etc).&lt;/p&gt; &lt;p&gt;Anyway, I played around with 70B distilled model. I am running it with ollama, but I had to customize it to run at it's intended temperature of .6, and increased the context window to 16k (ollama stupidly defaults every model to 2k context). &lt;/p&gt; &lt;p&gt;Then I ran the unsloth flappy bird test prompt to see how close I could come to replicate it's performance - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Prompt: Create a Flappy Bird game in Python. You must include these things: You must use pygame. The background color should be randomly chosen and is a light shade. Start with a light blue color. Pressing SPACE multiple times will accelerate the bird. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color. Place on the bottom some land colored as dark brown or yellow chosen randomly. Make a score shown on the top right side. Increment if you pass pipes and don't hit them. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here's the full response the model produced - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Okay, I need to create a Flappy Bird game using Pygame based on the user's requirements. Let me go through each requirement step by step.&lt;/p&gt; &lt;p&gt;First, the background color must be light and randomly chosen, starting with light blue. So I'll set a default light blue but have it change randomly each time the game restarts.&lt;/p&gt; &lt;p&gt;Next, pressing SPACE should make the bird accelerate. That means each press will add to the bird's velocity, allowing it to jump higher. I'll need to handle multiple key presses, so maybe track if the spacebar is pressed and apply force each frame until released.&lt;/p&gt; &lt;p&gt;The bird's shape should be random—square, circle, or triangle. Its color must be dark. So I'll create functions to generate a random dark color and another to choose the shape type randomly each game.&lt;/p&gt; &lt;p&gt;There's land at the bottom, colored either dark brown or yellow, chosen randomly. I'll add a ground rectangle with a randomly selected color from those options.&lt;/p&gt; &lt;p&gt;The score is displayed on the top right and increases when passing pipes without hitting them. So I'll need to track when the bird passes a pipe and increment the score accordingly. The best score should be saved and shown when the game ends.&lt;/p&gt; &lt;p&gt;Pipes must be randomly spaced with enough space between them for the bird to pass. They can be dark green, light brown, or dark gray. I'll create a list of pipes, each with random heights and gaps, moving from right to left.&lt;/p&gt; &lt;p&gt;When the bird hits a pipe or the ground, the game ends. Then, display the best score on the screen. The user can press Q or Esc to quit and SPACE to restart.&lt;/p&gt; &lt;p&gt;I'll structure the code with functions for generating colors, creating pipes, handling collisions, drawing elements, and updating the game state. I'll also set up event loops for handling key presses and quitting.&lt;/p&gt; &lt;p&gt;Testing each part as I go is important. For example, ensuring the bird's velocity increases correctly when pressing SPACE multiple times, checking that pipes generate without overlapping, and verifying collision detection works as expected.&lt;/p&gt; &lt;p&gt;Finally, I'll make sure all elements are styled according to the color requirements and that the game restarts properly with new randomizations. &lt;/p&gt; &lt;p&gt;Here's a complete implementation of Flappy Bird with all the specified requirements:&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;import pygame import sys import random pygame.init() # Constants WIDTH, HEIGHT = 800, 600 GRAVITY = 0.5 JUMP_FORCE = -8 PIPE_WIDTH = 80 PIPE_GAP = 150 GROUND_HEIGHT = 50 SCORE_FONT = pygame.font.SysFont('Arial', 30) BEST_SCORE_FONT = pygame.font.SysFont('Arial', 40) class Bird: def __init__(self): self.shape = random.choice(['square', 'circle', 'triangle']) self.color = (random.randint(0,100), random.randint(0,100), random.randint(0,100)) self.x = WIDTH // 4 self.y = HEIGHT // 2 self.velocity = 0 self.size = 20 def draw(self, screen): if self.shape == 'square': pygame.draw.rect(screen, self.color, (self.x, self.y, self.size, self.size)) elif self.shape == 'circle': pygame.draw.circle(screen, self.color, (int(self.x + self.size//2), int(self.y + self.size//2)), self.size//2) elif self.shape == 'triangle': points = [ (self.x, self.y), (self.x + self.size, self.y), (self.x + self.size//2, self.y + self.size) ] pygame.draw.polygon(screen, self.color, points) class Pipe: def __init__(self): self.height = random.randint(50, HEIGHT - PIPE_GAP - 100) self.x = WIDTH self.color = (random.choice([0, 128, 64]), random.choice([128, 64, 0]), random.choice([0, 64, 32])) def draw(self, screen): pygame.draw.rect(screen, self.color, (self.x, 0, PIPE_WIDTH, self.height)) pygame.draw.rect(screen, self.color, (self.x, self.height + PIPE_GAP, PIPE_WIDTH, HEIGHT - (self.height + PIPE_GAP))) def get_light_color(): return (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255)) def get_dark_color(): return (random.randint(0, 128), random.randint(0, 128), random.randint(0, 128)) def main(): screen = pygame.display.set_mode((WIDTH, HEIGHT)) clock = pygame.time.Clock() background_color = (173, 216, 230) # Light blue ground_color = (101, 67, 33) # Dark brown bird = Bird() pipes = [Pipe()] score = 0 best_score = 0 game_active = True jump_force = JUMP_FORCE while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_SPACE and game_active: jump_force = JUMP_FORCE bird.velocity = jump_force elif (event.key == pygame.K_q or event.key == pygame.K_ESCAPE): pygame.quit() sys.exit() keys = pygame.key.get_pressed() if keys[pygame.K_SPACE] and game_active: bird.velocity += 0.5 screen.fill(background_color) # Draw ground ground_rect = pygame.Rect(0, HEIGHT - GROUND_HEIGHT, WIDTH, GROUND_HEIGHT) pygame.draw.rect(screen, ground_color, ground_rect) if game_active: # Bird physics bird.velocity += GRAVITY bird.y += bird.velocity # Update pipes for pipe in pipes: pipe.x -= 2 if pipe.x &amp;lt; -PIPE_WIDTH: pipes.remove(pipe) score += 1 # Add new pipes if len(pipes) == 0 or pipes[-1].x &amp;lt; WIDTH // 2: pipes.append(Pipe()) # Check collisions for pipe in pipes: if (bird.x + bird.size &amp;gt; pipe.x and bird.x &amp;lt; pipe.x + PIPE_WIDTH): if bird.y &amp;lt; pipe.height or bird.y + bird.size &amp;gt; pipe.height + PIPE_GAP: game_active = False if bird.y + bird.size &amp;gt; HEIGHT - GROUND_HEIGHT: game_active = False else: # Game over screen text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) best_text = BEST_SCORE_FONT.render(f'Best Score: {best_score}', True, (0, 0, 0)) restart_text = SCORE_FONT.render('Press SPACE to restart', True, (0, 0, 0)) screen.blit(text, (WIDTH // 2 - text.get_width() // 2, HEIGHT // 2)) screen.blit(best_text, (WIDTH // 2 - best_text.get_width() // 2, HEIGHT // 2 + 40)) screen.blit(restart_text, (WIDTH // 2 - restart_text.get_width() // 2, HEIGHT // 2 + 80)) # Draw pipes for pipe in pipes: pipe.draw(screen) bird.draw(screen) score_text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) screen.blit(score_text, (WIDTH - 100, 10)) if not game_active: best_score = max(best_score, score) if keys[pygame.K_SPACE]: # Reset game background_color = get_light_color() bird = Bird() pipes.clear() score = 0 game_active = True pygame.display.flip() clock.tick(60) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;This implementation includes all the requested features:&lt;/p&gt; &lt;p&gt;Random light background color (starting with light blue) Bird shape can be square, circle, or triangle with random dark colors Ground with random dark brown or yellow color Randomly spaced pipes with different colors Score tracking and best score display Space to jump, Q/Esc to quit, SPACE to restart Physics-based bird movement Collision detection The game runs smoothly at 60 FPS and includes all the requested gameplay mechanics. You can run it by saving it as a Python file and running it with Pygame Installed (pip install pygame).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The results - &lt;a href="https://imgur.com/Sv0micW"&gt;Imgur&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This response nailed every requirement. And I know for a fact this model wasn't pre-trained on this prompt. Because I tried it w/ the default 2k context and it did not produce a fully working game first try. &lt;/p&gt; &lt;p&gt;Deepseek R1 is a reasoning model, so it needs enough context to go through it's thought process + produce a output. So bumping the context window to 16k is what allowed the model to run at it's potential. The base R1 model was actually designed to run at 128k context, so I'll play with how high I can go before I run out of GPU memory. But going from 2k to 16k already yielded much better results.&lt;/p&gt; &lt;p&gt;So, color me impressed. I wonder if the people that have had poor results w/ the distilled models are because they didn't give the model enough context, or ran 4 bpw or below quants. I think with these models you're better off trying to get 6 bpw or higher as they're already distilled and unoptimized out of the gate. And to repeat, reasoning models need enough context to perform their CoT and produce an output.&lt;/p&gt; &lt;p&gt;All I know is even the distilled models are incredible with just a little tuning. And I believe in the near future someone will actually optimize these distilled models and we'll have near frontier model performance at reasonable sizes. &lt;/p&gt; &lt;p&gt;This is such an exciting time to be a local llm enthusiast!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T02:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iehstw</id>
    <title>GPU pricing is spiking as people rush to self-host deepseek</title>
    <updated>2025-01-31T15:50:54+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt; &lt;img alt="GPU pricing is spiking as people rush to self-host deepseek" src="https://preview.redd.it/599a10y9pcge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60ac26bd7a2d3395eefcaee8fbf07a28102792f" title="GPU pricing is spiking as people rush to self-host deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/599a10y9pcge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbptd</id>
    <title>Tulu 3: RVLR Based Llama 3 model.</title>
    <updated>2025-02-01T17:43:04+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5"&gt;https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yet to test it out but this sounds promising considering it's from Allen Institute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifbptd/tulu_3_rvlr_based_llama_3_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T17:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ienetu</id>
    <title>Deepseek R1 is now hosted by Nvidia</title>
    <updated>2025-01-31T19:44:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt; &lt;img alt="Deepseek R1 is now hosted by Nvidia" src="https://preview.redd.it/1zufl131vdge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c70d8c80da395577b63301493ed66fac0dc6c408" title="Deepseek R1 is now hosted by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA just brought DeepSeek-R1 671-bn param model to NVIDIA NIM microservice on build.nvidia .com&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The DeepSeek-R1 NIM microservice can deliver up to 3,872 tokens per second on a single NVIDIA HGX H200 system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using NVIDIA Hopper architecture, DeepSeek-R1 can deliver high-speed inference by leveraging FP8 Transformer Engines and 900 GB/s NVLink bandwidth for expert communication.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;As usual with NVIDIA's NIM, its a enterprise-scale setu to securely experiment, and deploy AI agents with industry-standard APIs. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zufl131vdge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T19:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3xpg</id>
    <title>Virtuoso-Small-v2 - Distilled from Deepseek-v3, 128k context</title>
    <updated>2025-02-01T10:56:34+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Virtuoso-Small-v2 (14B)&lt;/strong&gt; is our next-generation, 14-billion-parameter language model that builds upon the original Virtuoso-Small architecture. This version is distilled from Deepseek-v3, leveraging an expanded dataset of 5B+ tokens worth of logits.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;arcee-ai/Virtuoso-Small-v2:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Virtuoso-Small-v2"&gt;https://huggingface.co/arcee-ai/Virtuoso-Small-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF"&gt;https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:56:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffboy</id>
    <title>SmolVLM fully open source</title>
    <updated>2025-02-01T20:19:01+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt; &lt;img alt="SmolVLM fully open source" src="https://external-preview.redd.it/RpBd16Y386MrSYjhSF5aL1O5cjq2V0xWVKGs2JQsIl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9476c8b4dd1bf85443ac42ac9be87b98d3ff2e1e" title="SmolVLM fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/andimarafioti/status/1885341684134978035"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4m53</id>
    <title>How can I force DeepSeek to think much more? And how much the longest i can force it?</title>
    <updated>2025-02-01T11:44:16+00:00</updated>
    <author>
      <name>/u/Western_Soil_4613</name>
      <uri>https://old.reddit.com/user/Western_Soil_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any benchmark/best practice for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Soil_4613"&gt; /u/Western_Soil_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1if9kju</id>
    <title>They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think...</title>
    <updated>2025-02-01T16:09:10+00:00</updated>
    <author>
      <name>/u/MMAgeezer</name>
      <uri>https://old.reddit.com/user/MMAgeezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"&gt; &lt;img alt="They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think..." src="https://preview.redd.it/gir8e8bhxjge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4776556bde99f86b60d38ae09051ad184544ecd" title="They called THIS 'Unsafe'? 🤔 Check out this example and tell me what you think..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spotted an interesting (and maybe concerning?) 'unsafe' example in this AI safety paper (image attached, page 13 in the paper). The answer gives very high-level points about some of the ways cybercriminals operate - provided by o3-mini (an older beta checkpoint of it).&lt;/p&gt; &lt;p&gt;Is flagging this kind of thing as 'unsafe' missing the point? Is the real danger not that AIs could actually help criminals, and just explaining the concepts at a high-level isn't the problem?&lt;/p&gt; &lt;p&gt;If you disagree, I'd love to hear your thoughts on why this specific example should be considered 'unsafe'.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://arxiv.org/pdf/2501.184384"&gt;o3-mini vs. DeepSeek-R1: Which one is safer?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important note about the paper&lt;/strong&gt;: It doesn't use the full R1 model (uses the Llama3.3-70B fine-tune instead) and it's using a beta release of o3-mini, as part of OpenAI's early research access program.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MMAgeezer"&gt; /u/MMAgeezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gir8e8bhxjge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if9kju/they_called_this_unsafe_check_out_this_example/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ies630</id>
    <title>openai can be opening again</title>
    <updated>2025-01-31T23:09:00+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt; &lt;img alt="openai can be opening again" src="https://preview.redd.it/1oovs3vgvege1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acc945b69ba442d4e66865f5e83ab96ac9b83b7b" title="openai can be opening again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1oovs3vgvege1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1if7hm3</id>
    <title>How To Run Deepseek R1 671b Fully Locally On a $2000 EPYC Server</title>
    <updated>2025-02-01T14:30:39+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://digitalspaceport.com/how-to-run-deepseek-r1-671b-fully-locally-on-2000-epyc-rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieurv8</id>
    <title>My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:</title>
    <updated>2025-02-01T01:11:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt; &lt;img alt="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" src="https://preview.redd.it/jixqkaabhfge1.gif?width=216&amp;amp;crop=smart&amp;amp;s=c67a878b6f732544b4693cf47d6dc14a8220e551" title="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jixqkaabhfge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T01:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1rls</id>
    <title>We've been incredibly fortunate with how things have developed over the past year</title>
    <updated>2025-02-01T08:11:39+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I still remember how in late 2023, people were speculating that Mixtral-8x7b was the best open-weights model that the community would get &amp;quot;for a long time&amp;quot;, and possibly ever. Shortly afterwards, Mistral published a controversial blog post that appeared to indicate that they were moving away from open weights – an ominous sign at a time when there were very few open-weights models available, and Anthropic and OpenAI seemed as far out of reach as the stars.&lt;/p&gt; &lt;p&gt;But since then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta released the excellent Llama 3 series as open weights (though not entirely free software).&lt;/li&gt; &lt;li&gt;Contrary to what many had feared, Mistral continued to publish open-weights models, even releasing the weights for Mistral Large, which was previously API-only, and now publishing their latest Mistral Small under the Apache License, when the previous version was still under their proprietary MRL.&lt;/li&gt; &lt;li&gt;Yi-34b transitioned from a proprietary license to Apache.&lt;/li&gt; &lt;li&gt;Microsoft has been publishing a number of excellent small models under permissive licenses.&lt;/li&gt; &lt;li&gt;Qwen came out of nowhere, and released the best models that can be run on consumer hardware, almost all of them under permissive licenses.&lt;/li&gt; &lt;li&gt;DeepSeek upended the entire industry, and &lt;strong&gt;an MIT-licensed model is now ranked joint #1 on style-controlled LMSYS,&lt;/strong&gt; on par with cutting-edge, proprietary, API-only models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was completely unforeseeable a year ago. Reality has outpaced the wildest dreams of the most naive optimists. Some doomsayers even predicted that open-weights models would soon be outlawed. The exact opposite has happened, and continues to happen.&lt;/p&gt; &lt;p&gt;To get an idea for what could easily have been, just look at the world of image generation models. In 15 months, there have only been two significant open-weights releases: SD3, and Flux.1D. SD3 was mired in controversy due to Stability's behavior and has been all but ignored by the community, and Flux is crippled by distillation. Both models are censored to a degree that has become the stuff of memes, and their licenses essentially make them unusable for anything except horsing around.&lt;/p&gt; &lt;p&gt;That is how the LLM world could have turned out. Instead, we have a world where I don't even download every new model anymore, because there are multiple exciting releases every week and I simply lack the time to take all of them for a spin. I now regularly delete models from my hard drive that I would have given my right hand for not too long ago. It's just incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T08:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1if8x64</id>
    <title>llama.cpp now supports tool calling (OpenAI-compatible)</title>
    <updated>2025-02-01T15:39:17+00:00</updated>
    <author>
      <name>/u/Federal_Discipline_4</name>
      <uri>https://old.reddit.com/user/Federal_Discipline_4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/9639"&gt;https://github.com/ggerganov/llama.cpp/pull/9639&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On top of generic support for &lt;em&gt;all&lt;/em&gt; models, it supports 8+ models’ native formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.x&lt;/li&gt; &lt;li&gt;Functionary 3&lt;/li&gt; &lt;li&gt;Hermes 2/3&lt;/li&gt; &lt;li&gt;Qwen 2.5&lt;/li&gt; &lt;li&gt;Mistral Nemo&lt;/li&gt; &lt;li&gt;Firefunction 2&lt;/li&gt; &lt;li&gt;DeepSeek R1 (WIP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Runs locally anywhere (incl. Raspberry Pi 5), e.g. on a Mac:&lt;/p&gt; &lt;p&gt;&lt;code&gt; brew install llama.cpp llama-server --jinja -fa -hf bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Still fresh / lots of bugs to discover: feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Discipline_4"&gt; /u/Federal_Discipline_4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T15:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifc798</id>
    <title>DeepSeek R1 reproduced for $30: Berkeley researchers replicate DeepSeek R1 for $30—casting doubt on H100 claims and controversy - Tech Startups</title>
    <updated>2025-02-01T18:03:26+00:00</updated>
    <author>
      <name>/u/LeBoulu777</name>
      <uri>https://old.reddit.com/user/LeBoulu777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifc798/deepseek_r1_reproduced_for_30_berkeley/"&gt; &lt;img alt="DeepSeek R1 reproduced for $30: Berkeley researchers replicate DeepSeek R1 for $30—casting doubt on H100 claims and controversy - Tech Startups" src="https://external-preview.redd.it/AwwvTMjYJXl1LwT0oT8Srs8APNeAVHB-rZiHRThDY0I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98d7d53a32751cabd2ee1d1cb06b9c504f71e973" title="DeepSeek R1 reproduced for $30: Berkeley researchers replicate DeepSeek R1 for $30—casting doubt on H100 claims and controversy - Tech Startups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeBoulu777"&gt; /u/LeBoulu777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techstartups.com/2025/01/31/deepseek-r1-reproduced-for-30-berkeley-researchers-replicate-deepseek-r1-for-30-casting-doubt-on-h100-claims-and-controversy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifc798/deepseek_r1_reproduced_for_30_berkeley/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifc798/deepseek_r1_reproduced_for_30_berkeley/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T18:03:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1if71w7</id>
    <title>o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot.</title>
    <updated>2025-02-01T14:08:08+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt; &lt;img alt="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." src="https://external-preview.redd.it/aTBxM2VyeG5iamdlMUWHkN0UG3UwPNFPGIT0TYE7p36ybavsfv5qTlMpE8Gi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c32bf16da98a479fb59fe074cec8f9ff9f2587" title="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x607arxnbjge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5q97</id>
    <title>Just canceled my ChatGPT Plus subscription</title>
    <updated>2025-02-01T12:56:16+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I initially subscribed when they introduced uploading documents when it was limited to the plus plan. I kept holding onto it for o1 since it really was a game changer for me. But since R1 is free right now (when it’s available at least lol) and the quantized distilled models finally fit onto a GPU I can afford, I cancelled my plan and am going to get a GPU with more VRAM instead. I love the direction that open source machine learning is taking right now. It’s crazy to me that distillation of a reasoning model to something like Llama 8B can boost the performance by this much. I hope we soon will get more advancements in more efficient large context windows and projects like Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1if43uf</id>
    <title>Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source</title>
    <updated>2025-02-01T11:08:19+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt; &lt;img alt="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" src="https://preview.redd.it/iewy2sxsfige1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad808e1a916c32f2181d2dc562c2065ba8cb4c99" title="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iewy2sxsfige1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:08:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3lq1</id>
    <title>Sam Altman acknowledges R1</title>
    <updated>2025-02-01T10:31:35+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt; &lt;img alt="Sam Altman acknowledges R1" src="https://preview.redd.it/ot5nsk399ige1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca17a8d86fa20881ff4876577c465ae2c733d9" title="Sam Altman acknowledges R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Straight from the horses mouth. Without R1, or bigger picture open source competitive models, we wouldn’t be seeing this level of acknowledgement from OpenAI. &lt;/p&gt; &lt;p&gt;This highlights the importance of having open models, not only that, but open models that actively compete and put pressure on closed models. &lt;/p&gt; &lt;p&gt;R1 for me feels like a real &lt;em&gt;hard takeoff&lt;/em&gt; moment. &lt;/p&gt; &lt;p&gt;No longer can OpenAI or other closed companies dictate the rate of release. &lt;/p&gt; &lt;p&gt;No longer do we have to get the scraps of what they decide to give us. &lt;/p&gt; &lt;p&gt;Now they have to actively compete in an open market.&lt;/p&gt; &lt;p&gt;No moat. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Source: &lt;a href="https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC"&gt;https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ot5nsk399ige1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:31:35+00:00</published>
  </entry>
</feed>
