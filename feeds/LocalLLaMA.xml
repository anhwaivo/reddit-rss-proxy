<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-06T11:05:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lsfmcj</id>
    <title>I created this tool I named ReddSummary.com – just paste a link and boom you got the summary</title>
    <updated>2025-07-05T17:46:40+00:00</updated>
    <author>
      <name>/u/Himanshu507</name>
      <uri>https://old.reddit.com/user/Himanshu507</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt; &lt;img alt="I created this tool I named ReddSummary.com – just paste a link and boom you got the summary" src="https://preview.redd.it/2exxosoue3bf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c1d1e13fd17ff4381b38e36072d089c11c07e48" title="I created this tool I named ReddSummary.com – just paste a link and boom you got the summary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Himanshu507"&gt; /u/Himanshu507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2exxosoue3bf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsv7j1</id>
    <title>Using local LLM for anonymizing prompts before sending to cloud LLM - are there any open source solutions?</title>
    <updated>2025-07-06T07:13:58+00:00</updated>
    <author>
      <name>/u/cesarean722</name>
      <uri>https://old.reddit.com/user/cesarean722</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:&lt;/p&gt; &lt;p&gt;local model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;detects personal or business specific data in prompts,&lt;/li&gt; &lt;li&gt;creates mapping dictionary&lt;/li&gt; &lt;li&gt;warns if replace is not feasible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;proxy app:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;executes string replace according to rules in dictionary&lt;/li&gt; &lt;li&gt;routes requests to cloud LLM api&lt;/li&gt; &lt;li&gt;passes LLM warnings to user&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cesarean722"&gt; /u/cesarean722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T07:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdxc2</id>
    <title>New app for locally running AI models on Android your smartphone</title>
    <updated>2025-07-05T16:32:35+00:00</updated>
    <author>
      <name>/u/RomanKryvolapov</name>
      <uri>https://old.reddit.com/user/RomanKryvolapov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt; &lt;img alt="New app for locally running AI models on Android your smartphone" src="https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba60f6210ef2ca39ebd034795d1371f991f9f7d" title="New app for locally running AI models on Android your smartphone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I create Android application for locally running AI models on smartphone&lt;/p&gt; &lt;p&gt;I am interested in your opinion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher"&gt;https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5"&gt;https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RomanKryvolapov"&gt; /u/RomanKryvolapov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsx20</id>
    <title>How RAG actually works — a toy example with real math</title>
    <updated>2025-07-04T20:44:15+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt; &lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt; &lt;h1&gt;Step 1: Chunk&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot; S1: &amp;quot;Poach an egg&amp;quot; S2: &amp;quot;How to change a tire&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Embed&lt;/h1&gt; &lt;p&gt;After the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.&lt;/p&gt; &lt;p&gt;Toy demo values:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10] # “Boil an egg” V1 = [ 0.88, 0.12, 0.00, 0.09] # “Poach an egg” V2 = [-0.20, 0.40, 0.80, 0.10] # “How to change a tire” &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt; &lt;h1&gt;Step 3: Normalize&lt;/h1&gt; &lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors V0̂ = [ 0.988, 0.110, 0.000, 0.110] # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1 V1̂ = [ 0.986, 0.134, 0.000, 0.101] # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1 V2̂ = [-0.217, 0.434, 0.868, 0.108] # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 4: Index&lt;/h1&gt; &lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br /&gt; Keep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt; &lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br /&gt; “Best way to cook an egg?”&lt;/p&gt; &lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we need to find the vector that’s &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br /&gt; The most common way is cosine similarity — often written as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But since we already normalized all vectors,&lt;br /&gt; ‖A‖ = ‖B‖ = 1 → so the formula becomes just:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = A ⋅ B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br /&gt; If two vectors are exactly the same, dot product = 1.&lt;br /&gt; So we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt; &lt;p&gt;Let’s calculate the scores (example, not real)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110) ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999 Vi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101) ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999 Vi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108) ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)&lt;br /&gt; are both very close to the user input.&lt;/p&gt; &lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br /&gt; Now the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsvff1</id>
    <title>Help choosing LLM</title>
    <updated>2025-07-06T07:29:01+00:00</updated>
    <author>
      <name>/u/BESTHARSH004</name>
      <uri>https://old.reddit.com/user/BESTHARSH004</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heelo, im making a project where llm might have to deal with geospatial data, raster like. Dealing with formalts like Map Tiles, geojason etc. (Algo RAG implementations) for this i need an LLM but an so confused which one to use. Llama and Mistral both have so many models that im confused.&lt;br /&gt; It must be free to use via api or downloadable locally through ollama (light enough to run well on a gaming laptop).&lt;/p&gt; &lt;p&gt;If someone has exp with using LLMs for similar tasks i need ur help 😬 &lt;/p&gt; &lt;p&gt;This LLM will be the frontface for the user. There wl be other chains to perform operations on the data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BESTHARSH004"&gt; /u/BESTHARSH004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T07:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsvsw0</id>
    <title>Creating a Knowledge Base for Agentic Research Architect</title>
    <updated>2025-07-06T07:54:49+00:00</updated>
    <author>
      <name>/u/dew_chiggi</name>
      <uri>https://old.reddit.com/user/dew_chiggi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this sounds dumb lol&lt;/p&gt; &lt;p&gt;My organisation is researching/attempting to create AI agents that can act as software architects and help in designing softwares. This is an already established product and we get a lot of new feature requests on top of it. &lt;/p&gt; &lt;p&gt;So basically, this agent would need the understanding of the current product - lots of code, PDFs, Word documents, excel sheets (configuration files).&lt;/p&gt; &lt;p&gt;I am wondering what should be my starting point? &lt;/p&gt; &lt;p&gt;Vector Databases, Knowledge Graphs, hybrid approach? &lt;/p&gt; &lt;p&gt;Any pointers should help. Let me know if this is too ambitious as well. Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dew_chiggi"&gt; /u/dew_chiggi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvsw0/creating_a_knowledge_base_for_agentic_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvsw0/creating_a_knowledge_base_for_agentic_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsvsw0/creating_a_knowledge_base_for_agentic_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T07:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls70r2</id>
    <title>Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance</title>
    <updated>2025-07-05T10:43:05+00:00</updated>
    <author>
      <name>/u/d5dq</name>
      <uri>https://old.reddit.com/user/d5dq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt; &lt;img alt="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" src="https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a" title="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d5dq"&gt; /u/d5dq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T10:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lswhaj</id>
    <title>Upgrade for my 4060ti</title>
    <updated>2025-07-06T08:41:21+00:00</updated>
    <author>
      <name>/u/Former-Tangerine-723</name>
      <uri>https://old.reddit.com/user/Former-Tangerine-723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello people. I have a 4060ti for local Inference. The card is doing just fine considering the allocated budget. I'm thinking a second card to pair with it so I can utilize longer context and/or bigger models. The two options I consider is a second 4060ti or a 5060ti (my budget is tight) What do you think? Any other suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Former-Tangerine-723"&gt; /u/Former-Tangerine-723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswhaj/upgrade_for_my_4060ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswhaj/upgrade_for_my_4060ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lswhaj/upgrade_for_my_4060ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsx9pn</id>
    <title>Advice Needed: Building an In-House LLM System Using Latest Tech — Recommendations?</title>
    <updated>2025-07-06T09:35:48+00:00</updated>
    <author>
      <name>/u/No_Edge2098</name>
      <uri>https://old.reddit.com/user/No_Edge2098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently working on setting up an &lt;strong&gt;in-house Large Language Model (LLM) system&lt;/strong&gt; for internal organizational projects. Given the rapid advancements in AI technology, I’d greatly value your professional insights and recommendations to ensure we're leveraging the latest tools and methods effectively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's our current plan and key considerations:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Model Selection:&lt;/strong&gt; We're considering open-source models such as GPT-3 (EleutherAI), T5, or FLAN-T5. Are there any standout alternatives or specific models you've successfully implemented lately?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Data Pipeline:&lt;/strong&gt; We’re using Apache Kafka for real-time data ingestion and Apache Spark for batch processing. Have you come across any newer or more efficient tools and practices beneficial for handling large-scale datasets?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Training &amp;amp; Fine-Tuning:&lt;/strong&gt; Planning to utilize Ray Tune and Weights &amp;amp; Biases for hyperparameter optimization and experiment tracking. GPU costs remain a concern—any advice on cost-effective or emerging platforms for fine-tuning large models?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Deployment &amp;amp; Serving:&lt;/strong&gt; Considering Kubernetes, Docker, and FastAPI for deployment. Would you recommend NVIDIA Triton Server or TensorRT for better performance? What has your experience been?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Performance &amp;amp; Scalability:&lt;/strong&gt; Ensuring real-time scalability and minimal latency is crucial. How do you efficiently manage scalability and parallel inference when deploying multiple models concurrently?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6. Ethics &amp;amp; Bias Mitigation:&lt;/strong&gt; Effective bias detection and mitigation frameworks are essential for us. Can you suggest recent effective tools or methods for ethical AI deployment?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We'd appreciate your input on:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key tools or strategies that significantly improved your LLM workflows in 2025.&lt;/li&gt; &lt;li&gt;Recommendations for cost-effective GPU management and training setups.&lt;/li&gt; &lt;li&gt;Preferred tools for robust monitoring, logging, and performance analysis (e.g., Prometheus, Grafana).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Edge2098"&gt; /u/No_Edge2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T09:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsi0gj</id>
    <title>Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control</title>
    <updated>2025-07-05T19:31:10+00:00</updated>
    <author>
      <name>/u/Old_Mathematician107</name>
      <uri>https://old.reddit.com/user/Old_Mathematician107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt; &lt;img alt="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" src="https://b.thumbs.redditmedia.com/RwkhySplb6CWm6KR-CbkXw8jpoNWLPRz3r_cADWabdw.jpg" title="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.&lt;/p&gt; &lt;p&gt;deki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.&lt;/p&gt; &lt;p&gt;All the code/information is available on GitHub: &lt;a href="https://github.com/RasulOs/deki"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have also uploaded the model to Hugging Face:&lt;br /&gt; Space: &lt;a href="https://huggingface.co/spaces/orasul/deki"&gt;orasul/deki&lt;/a&gt;&lt;br /&gt; (Check the analyze-and-get-yolo endpoint)&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/orasul/deki-yolo"&gt;orasul/deki-yolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Mathematician107"&gt; /u/Old_Mathematician107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lsi0gj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T19:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsye88</id>
    <title>Is that possible built a local gemini-cli totally in local and workable?</title>
    <updated>2025-07-06T10:51:26+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which means it has to fullfill 2 requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;small, as it needs runing local, ideally no more than 2B;&lt;/li&gt; &lt;li&gt;able to do agents work, means it shouldn't be very dumb;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;eventhough you might ask why not using cloud api, well, it's a typical question about data sensetive and price.&lt;/p&gt; &lt;p&gt;Just wanna talk about if this is a trend, or do we nearly this situation which can do agents, that can just work in local, with bareable speed and free price.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsye88/is_that_possible_built_a_local_geminicli_totally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsye88/is_that_possible_built_a_local_geminicli_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsye88/is_that_possible_built_a_local_geminicli_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T10:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls5b89</id>
    <title>Powerful 4B Nemotron based finetune</title>
    <updated>2025-07-05T08:43:38+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt; &lt;img alt="Powerful 4B Nemotron based finetune" src="https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637" title="Powerful 4B Nemotron based finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \ adventure finetunes at its size category.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt; &lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here"&gt;See the examples in the logs&lt;/a&gt; 🙂&lt;/li&gt; &lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels 'in the moment', kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it's a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt; &lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt; &lt;li&gt;Very decent assistant.&lt;/li&gt; &lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt; &lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \ Adventure:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"&gt;Horde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T08:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsuje6</id>
    <title>Anyone building a local coding cli or coding agent?</title>
    <updated>2025-07-06T06:30:07+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just broke the ground on mine. I used copilot a bit 2 years ago when it was pretty new but preferred cut &amp;amp; paste, then I did &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; a bit, then back to cut &amp;amp; paste. Did aider a bit, then ...&lt;/p&gt; &lt;p&gt;None of them really hit the sweet spot for me, so I decided to roll my own, might not be as good as the commercial ones, but it's always a fun learning exercise. If you are cooking up one as well, let me know, looking to bounce ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsuje6/anyone_building_a_local_coding_cli_or_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T06:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lswkv4</id>
    <title>What are some good in-browser inference tools for small LLMs? (Use case: JSON to Chart.js config)</title>
    <updated>2025-07-06T08:48:35+00:00</updated>
    <author>
      <name>/u/callmedevilthebad</name>
      <uri>https://old.reddit.com/user/callmedevilthebad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I’m exploring some ideas around running small LLMs entirely &lt;strong&gt;in the browser&lt;/strong&gt;, and wanted to ask for suggestions or experiences with lightweight inference frameworks.&lt;/p&gt; &lt;p&gt;The main use case I’m playing with is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;(Priority)&lt;/strong&gt; Taking a JSON object and generating a valid &lt;a href="https://www.chartjs.org/"&gt;Chart.js&lt;/a&gt; config to visualize it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;(Secondary)&lt;/strong&gt; Producing a natural language explanation of the data — like a brief summary or insight.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd like the whole thing to run locally in the browser — no backend — so I'm looking for tools or runtimes that support:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small quantized models (ideally &amp;lt;100MB)&lt;/li&gt; &lt;li&gt;WebGPU or WASM support&lt;/li&gt; &lt;li&gt;Quick startup and decent performance for structured JSON reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve started looking into &lt;a href="https://mlc.ai"&gt;MLC.ai&lt;/a&gt;, which seems promising, but curious if anyone here has:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tried MLC.ai recently for browser-based LLM tasks?&lt;/li&gt; &lt;li&gt;Found any newer/easier runtimes that support small models?&lt;/li&gt; &lt;li&gt;Used models that are particularly good at structured JSON-to-JSON transformations?&lt;/li&gt; &lt;li&gt;Prompting tips for clean Chart.js output?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;sales&amp;quot;: [100, 200, 300], &amp;quot;months&amp;quot;: [&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;, &amp;quot;Mar&amp;quot;] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Expected output: A full Chart.js config for a bar or line chart. Bonus: An optional summary like &lt;em&gt;“Sales increased steadily from January to March.”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what folks have tried or recommend for running small models client-side. Thanks!&lt;/p&gt; &lt;p&gt;Edit: Anything under 500mb is good Edit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/callmedevilthebad"&gt; /u/callmedevilthebad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsyd4g</id>
    <title>After Huawei Pangu LLM faced plagiarism allegations, an anonymous insider shares their side of the story</title>
    <updated>2025-07-06T10:49:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: This post is a translation of a long piece written by an anonymous user claiming to be a former Huawei Noah’s Ark Lab employee involved in the development of the Pangu LLM. The authenticity of the claims cannot be independently verified, so please read with caution.&lt;/p&gt; &lt;p&gt;The original GitHub repo: &lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;A user claiming to be part of the Pangu LLM team at Huawei’s Noah’s Ark Lab has posted a detailed, emotionally charged account describing what they call the “bitterness and darkness” behind Pangu’s development. The story spans from technical failures and internal conflicts to allegations of misconduct at both technical and leadership levels. Here are the main takeaways:&lt;/p&gt; &lt;p&gt;Team Structure and Work Conditions: The Pangu development was allegedly conducted under the &amp;quot;Siye&amp;quot; (四野) organizational umbrella, with multiple vertical teams (e.g., 4th and 16th). Developers were relocated to Suzhou for long periods, working weekends under extreme pressure. Despite some corporate perks (like afternoon tea), burnout and separation from families were widespread.&lt;/p&gt; &lt;p&gt;From Research to Delivery: Originally envisioned as a research-oriented lab, the Noah team became a delivery team with constant internal reviews and product integration deadlines (with XiaoYi &amp;quot;Huawei Voice Assistant&amp;quot;, Huawei Cloud, etc.). Many members left due to overwork and disillusionment.&lt;/p&gt; &lt;p&gt;Training Struggles and Tokenizer Failures: Early models like the 71B and 135B dense versions allegedly suffered from extremely inefficient tokenizers—each symbol or character took one token. Attempts to switch tokenizers (borrowing from the smaller model lab) resulted in major performance regressions and bugs.&lt;/p&gt; &lt;p&gt;Allegations of “Shelling” from Other Models: The author accuses an internal “small model lab,” led by a director named Wang Yunhe, of repeatedly &amp;quot;wrapping&amp;quot; external models like Qwen 1.5 and DeepSeek-v3, tweaking them slightly, and passing them off as self-developed Huawei models. Specific examples include:&lt;/p&gt; &lt;p&gt;The so-called “Pangu 135B v2,” alleged to be a lightly modified Qwen-110B.&lt;/p&gt; &lt;p&gt;A 72B MoE model claimed to be derived from Qwen 2.5 14B.&lt;/p&gt; &lt;p&gt;The supposed 718B MoE training effort, which allegedly just fine-tuned a frozen DeepSeek-v3 checkpoint without changing file paths or model code names.&lt;/p&gt; &lt;p&gt;Internal Resistance and Third-Gen Efforts: Some engineers, including the author, pushed for a clean-slate, from-scratch training approach using Huawei’s Ascend chips. The 38B v3 and later 135B v3 were described as genuine efforts with stable training curves, no loss spikes, and results comparable to domestic competitors in 2024. These efforts were seen as a point of pride and a rare case of “real” innovation.&lt;/p&gt; &lt;p&gt;Internal Politics and Misattribution: Despite the work of the main team, the small model lab allegedly took credit, absorbed code and data, and bypassed internal governance mechanisms (e.g., bloodline tracking, version control). The author says this created a two-tier system where political connections outweighed technical contributions.&lt;/p&gt; &lt;p&gt;Final Disillusionment: The whistleblower decided to resign and requested their name be removed from published technical reports, calling them a permanent stain. The person admits their complicity in earlier decisions and expresses regret for not standing up sooner.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;It seems the author was in great emotional distress while writing this. Regardless of the story’s authenticity, I wish him/her peace and clarity in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T10:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lswnto</id>
    <title>gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code.</title>
    <updated>2025-07-06T08:54:21+00:00</updated>
    <author>
      <name>/u/PieBru</name>
      <uri>https://old.reddit.com/user/PieBru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt; &lt;img alt="gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code." src="https://b.thumbs.redditmedia.com/nT92tKjMkTLoLC9uXWz050-DcHqgcFo8BpiZvSZgM8I.jpg" title="gemini-cli: falling back to gemini-flash is the best marketing strategy Anthropic could have dreamed of for claude-code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5"&gt;https://preview.redd.it/9n9zbmftv7bf1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=442e283d1f39378d1799de954c2c1806310333f5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm a huge open source fan, but I think the gemini-cli fallback from &amp;quot;pro&amp;quot; to &amp;quot;flash&amp;quot; will divert more &amp;quot;real&amp;quot; coders to claude-code than convince them to get a gemini-pro subscription.&lt;/p&gt; &lt;p&gt;The gemini-cli doc states that &amp;quot;To ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&amp;quot;. That's good, but it doesn't mention the throttling from pro to flash. When I try to build something out of the Erathostene Sieve, the throttling causes a code mess and soon reaches the limits (err 429) without a useful solution, because of the flash incapacity to solve &amp;quot;real&amp;quot; coding problems.&lt;/p&gt; &lt;p&gt;gemini-cli at this early stage can't compare to claude-code, so loosing &amp;quot;real&amp;quot; community devs isn't the best strategy to win the battle, IMO.&lt;/p&gt; &lt;p&gt;At the end, I'm looking for alternative solutions, without discarding the auto-build of a similar tool that with some agentic LLM routing can substitute closed-source and cloud solutions.&lt;/p&gt; &lt;p&gt;Meanwhile, the above solutions + context engineering may be used to build some &amp;quot;private&amp;quot; solution.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PieBru"&gt; /u/PieBru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lswnto/geminicli_falling_back_to_geminiflash_is_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbhzs</id>
    <title>When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?</title>
    <updated>2025-07-05T14:44:47+00:00</updated>
    <author>
      <name>/u/spiritxfly</name>
      <uri>https://old.reddit.com/user/spiritxfly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt; &lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spiritxfly"&gt; /u/spiritxfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T14:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdjnb</id>
    <title>Llama-4-Maverick 402B on a oneplus 13</title>
    <updated>2025-07-05T16:15:49+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt; &lt;img alt="Llama-4-Maverick 402B on a oneplus 13" src="https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcc6e776cfcd25daabf167946f5f41fbb1c23e70" title="Llama-4-Maverick 402B on a oneplus 13" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt; &lt;p&gt;Here's the command used: &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/QwkaFHf"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt; &lt;p&gt;- Green: in RAM&lt;/p&gt; &lt;p&gt;- Red: read from DISC&lt;/p&gt; &lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt; &lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tletuj5ov2bf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsw9vz</id>
    <title>Run Large LLMs on RunPod with text-generation-webui – Full Setup Guide + Template</title>
    <updated>2025-07-06T08:26:43+00:00</updated>
    <author>
      <name>/u/abandonedexplorer</name>
      <uri>https://old.reddit.com/user/abandonedexplorer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I usually rent GPUs from the cloud since I don’t want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.&lt;/p&gt; &lt;p&gt;For LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.&lt;/p&gt; &lt;p&gt;So here you go, a complete tutorial with a one-click template included:&lt;/p&gt; &lt;h1&gt;Source code and instructions:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md"&gt;https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;RunPod template:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo"&gt;https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you're good to go. The template was inspired by TheBloke's now-deprecated &lt;a href="https://github.com/TheBlokeAI/dockerLLM"&gt;dockerLLM&lt;/a&gt; project, which I’ve completely refactored.&lt;/p&gt; &lt;p&gt;A quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like &lt;a href="https://github.com/vllm-project/vllm"&gt;VLLM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why I use RunPod:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Relatively cheap – I can get 48 GB VRAM for just $0.40/hour&lt;/li&gt; &lt;li&gt; Easy multi-GPU support – I can stack cheap GPUs to run big models (like Mistral Large) at a low cost&lt;/li&gt; &lt;li&gt; Simple templates – very little tinkering needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I see renting GPUs as a solid privacy middle ground. Ideally, I’d run everything locally, but I don’t want to invest in expensive hardware. While I cannot audit RunPod's privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).&lt;/p&gt; &lt;p&gt;The README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abandonedexplorer"&gt; /u/abandonedexplorer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T08:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsxxo2</id>
    <title>Python Implementation of Google's MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</title>
    <updated>2025-07-06T10:20:49+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sigridjineth/muvera-py"&gt;https://github.com/sigridjineth/muvera-py&lt;/a&gt;&lt;br /&gt; I have created the Python implementation was created to make the FDE algorithm more accessible while maintaining complete fidelity to the original C++ implementation. Every function and parameter has been carefully mapped to ensure identical behavior.&lt;/p&gt; &lt;h1&gt;What is FDE (Read below)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fixed-Dimensional Encoding (FDE) solves a fundamental problem in modern search systems: how to efficiently search through billions of documents when each document is represented by hundreds of vectors (as in ColBERT-style models).&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Traditional search&lt;/strong&gt;: Document = 1 vector → Fast but inaccurate&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modern multi-vector search&lt;/strong&gt;: Document = 100s of vectors → Accurate but extremely slow&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The FDE Solution&lt;/h1&gt; &lt;p&gt;FDE transforms multiple vectors into a single fixed-size vector while preserving the similarity relationships. The magic is that the dot product between two FDE vectors approximates the original Chamfer similarity between the multi-vector sets.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T10:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsofwq</id>
    <title>All i said was hello lol</title>
    <updated>2025-07-06T00:35:44+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt; ollama run phi4-mini-reasoning:3.8b&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hello&lt;/p&gt; &lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Okay, let's see what the user needs here. The message says &amp;quot;You will be given a problem.&amp;quot; but&lt;/p&gt; &lt;p&gt;then it just has &amp;quot;hello&amp;quot;. Hmm, maybe there was a typo or the problem didn't get sent&lt;/p&gt; &lt;p&gt;correctly. Let me check again.&lt;/p&gt; &lt;p&gt;Wait, the user's name is Phi, an AI math expert from Microsoft. Then there's a greeting&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot; and that's it. So perhaps the actual problem wasn't included properly. Maybe the user&lt;/p&gt; &lt;p&gt;intended to send a math problem but only wrote &amp;quot;hello&amp;quot;. I should prompt them to provide the&lt;/p&gt; &lt;p&gt;problem they need help with. But according to the instructions, I need to first think through&lt;/p&gt; &lt;p&gt;solving the problem before giving the answer. However, there's no problem here.&lt;/p&gt; &lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is the problem itself? That seems unlikely. Maybe it's a test or&lt;/p&gt; &lt;p&gt;a mistake. Let me consider if &amp;quot;hello&amp;quot; could be part of a math question. For example,&lt;/p&gt; &lt;p&gt;converting letters to numbers using ASCII values. Let's try that.&lt;/p&gt; &lt;p&gt;H in ASCII is 72, E is 69, L is 76, another L is 76, and O is 79. So adding those up: 72 + 69&lt;/p&gt; &lt;p&gt;+ 76 + 76 + 79. Let me calculate that. 72 + 69 is 141. 141 + 76 is 217. 217 + 76 is 293. 293&lt;/p&gt; &lt;p&gt;+ 79 is 372. So the total would be 372. But I'm not sure if that's what the user wants. They&lt;/p&gt; &lt;p&gt;might have intended a different approach.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a cipher problem, like Caesar cipher where each letter is shifted.&lt;/p&gt; &lt;p&gt;For example, shifting letters by a certain number. But without more context, it's hard to&lt;/p&gt; &lt;p&gt;tell.&lt;/p&gt; &lt;p&gt;Since the user mentioned being an AI math expert and the initial instruction was about&lt;/p&gt; &lt;p&gt;solving a problem, but the input only has &amp;quot;hello&amp;quot;, I think the best course is to ask for&lt;/p&gt; &lt;p&gt;clarification. However, since the task requires providing an answer within a box, maybe I&lt;/p&gt; &lt;p&gt;should state that the problem is missing or assume a common problem. Wait, perhaps the user&lt;/p&gt; &lt;p&gt;intended to send a problem but it didn't go through, so responding by pointing out the&lt;/p&gt; &lt;p&gt;missing problem would be better. But according to the example given in the query, sometimes&lt;/p&gt; &lt;p&gt;even if there's confusion, you have to proceed.&lt;/p&gt; &lt;p&gt;Alternatively, maybe &amp;quot;hello&amp;quot; is a greeting and the actual problem is forthcoming. But the&lt;/p&gt; &lt;p&gt;structure of the query shows that after &amp;quot;You will be given a problem,&amp;quot; the user input is&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot;. So perhaps there was an error in submission. In that case, my answer should note&lt;/p&gt; &lt;p&gt;that the problem seems missing and ask for it. But since I need to provide a boxed answer,&lt;/p&gt; &lt;p&gt;maybe there's a default response. Hmm.&lt;/p&gt; &lt;p&gt;Alternatively, if this is a test of handling errors, then the correct response would be to&lt;/p&gt; &lt;p&gt;inform the user that no problem was detected and request the problem statement. However,&lt;/p&gt; &lt;p&gt;given the setup here, perhaps the assistant is supposed to generate a sample problem and&lt;/p&gt; &lt;p&gt;solution? But that's speculative.&lt;/p&gt; &lt;p&gt;Wait, looking back at the original instructions: &amp;quot;You will be given a problem. Please reason&lt;/p&gt; &lt;p&gt;step by step, and put your final answer within \boxed{}.&amp;quot; The user's input after that is&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot;. So maybe the actual problem was meant to follow &amp;quot;hello&amp;quot; but didn't. Therefore, my&lt;/p&gt; &lt;p&gt;conclusion is that there's a missing problem here. Since I need to provide an answer, perhaps&lt;/p&gt; &lt;p&gt;I should state that the problem is missing and ask for it, but in the boxed answer? But the&lt;/p&gt; &lt;p&gt;example shows mathematical answers in boxes. Alternatively, maybe it's a trick question where&lt;/p&gt; &lt;p&gt;&amp;quot;hello&amp;quot; translates to something else.&lt;/p&gt; &lt;p&gt;If we consider &amp;quot;hello&amp;quot; as a hexadecimal number, H isn't valid hex (only 0-9 and A-F). So&lt;/p&gt; &lt;p&gt;that's not possible. If using base64 decoding, but again, letters are allowed. But without&lt;/p&gt; &lt;p&gt;more context, this is all guesswork.&lt;/p&gt; &lt;p&gt;Given that the user is an AI math expert, maybe they intended to send a problem but it didn't&lt;/p&gt; &lt;p&gt;come through. Since I need to follow the instructions strictly, perhaps I should answer based&lt;/p&gt; &lt;p&gt;on &amp;quot;hello&amp;quot; being the input. If we take &amp;quot;hello&amp;quot; as a string and compute something related to&lt;/p&gt; &lt;p&gt;its letters, like the sum of their positions in the alphabet: H=8, E=5, L=12, L=12, O=15. So&lt;/p&gt; &lt;p&gt;8+5+12+12+15 = 52. Maybe that's the answer? \boxed{52}&lt;/p&gt; &lt;p&gt;Alternatively, product: 8*5*12*12*15. Let's compute that: 8*5=40, 40*12=480, 480*12=5760,&lt;/p&gt; &lt;p&gt;5760*15=86400. That's a big number, but maybe not likely.&lt;/p&gt; &lt;p&gt;Alternatively, count the letters: &amp;quot;hello&amp;quot; has 5 letters. So \boxed{5}. But that's too simple.&lt;/p&gt; &lt;p&gt;Another angle: &amp;quot;HELLO&amp;quot; in different languages? Not sure. Or Morse code: H is comma, E is dot&lt;/p&gt; &lt;p&gt;slash, L are double dash, O is hollow exclamation. But converting that to numbers or&lt;/p&gt; &lt;p&gt;something else... Maybe the total dashes and dots. H (comma) has 3 symbols, but maybe not&lt;/p&gt; &lt;p&gt;relevant.&lt;/p&gt; &lt;p&gt;Alternatively, if it's a calculus problem, like finding the derivative of &amp;quot;hello&amp;quot;, which&lt;/p&gt; &lt;p&gt;doesn't make sense.&lt;/p&gt; &lt;p&gt;Given all this uncertainty, perhaps the safest assumption is that the user intended to send a&lt;/p&gt; &lt;p&gt;math problem but it didn't come through correctly. Therefore, my answer should indicate that&lt;/p&gt; &lt;p&gt;the problem is missing. However, since I need to provide a boxed answer as per instructions,&lt;/p&gt; &lt;p&gt;and there's no way to know, maybe I'll go with the sum of the alphabetical positions:&lt;/p&gt; &lt;p&gt;8+5+12+12+15=52. So \boxed{52}.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;The problem statement appears to be incomplete or incorrectly submitted. However, if we&lt;/p&gt; &lt;p&gt;interpret &amp;quot;hello&amp;quot; by converting each letter to its position in the alphabet (H=8, E=5, L=12,&lt;/p&gt; &lt;p&gt;O=15) and summing them:&lt;/p&gt; &lt;p&gt;\[&lt;/p&gt; &lt;p&gt;8 + 5 + 12 + 12 + 15 = 52&lt;/p&gt; &lt;p&gt;\]&lt;/p&gt; &lt;p&gt;\boxed{52}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T00:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsmtzr</id>
    <title>Is Codestral 22B still the best open LLM for local coding on 32–64 GB VRAM?</title>
    <updated>2025-07-05T23:13:57+00:00</updated>
    <author>
      <name>/u/One-Stress-6734</name>
      <uri>https://old.reddit.com/user/One-Stress-6734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt; &lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt; &lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Stress-6734"&gt; /u/One-Stress-6734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T23:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lskb8k</id>
    <title>Should I buy an appartment or 4 H100s</title>
    <updated>2025-07-05T21:14:39+00:00</updated>
    <author>
      <name>/u/InfiniteEjaculation</name>
      <uri>https://old.reddit.com/user/InfiniteEjaculation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are they so expensive, has anybody here ever tested them? How many rtx 5090s are needed to match it's performance? What llm can we run entirely on one h100 with as much RAM as required?&lt;/p&gt; &lt;p&gt;Naive questions but I am very confused&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteEjaculation"&gt; /u/InfiniteEjaculation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lskb8k/should_i_buy_an_appartment_or_4_h100s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T21:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsgtvy</id>
    <title>Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)</title>
    <updated>2025-07-05T18:39:10+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt; &lt;img alt="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" src="https://external-preview.redd.it/qkqp42ZUQM3ud5lixeAGPwFgRk03v_VKVBUbGROr91s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66839b47c62157c843227bea591a16264454d62a" title="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a PC was always one of those &amp;quot;someday&amp;quot; projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn't just slow, it was a hard bottleneck.&lt;/p&gt; &lt;p&gt;So, I started mapping out what this new machine needed to be:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;32GB VRAM as the baseline.&lt;/strong&gt; I'm really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.&lt;br /&gt; - &lt;strong&gt;128GB of RAM as the baseline.&lt;/strong&gt; Essential for wrangling the large datasets that come with the territory.&lt;br /&gt; - &lt;strong&gt;A clean, consumer-desk look.&lt;/strong&gt; I don't want a rugged, noisy server rack.&lt;br /&gt; - &lt;strong&gt;AI inference as the main job,&lt;/strong&gt; but I didn't want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.&lt;br /&gt; - &lt;strong&gt;Room to grow.&lt;/strong&gt; I wanted a foundation I could build on later.&lt;br /&gt; - And the big one: &lt;strong&gt;Keep it under $1500.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.&lt;/p&gt; &lt;p&gt;Just as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn't record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.&lt;/p&gt; &lt;p&gt;But here was the catch: these are server cards. Even though they have a display port, it doesn't actually work. That would have killed my &amp;quot;all-rounder&amp;quot; requirement.&lt;/p&gt; &lt;p&gt;I started digging deep, trying to find a workaround. That's when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...&lt;/p&gt; &lt;p&gt;...until the kind-hearted &lt;a href="/u/Accurate_Ad4323"&gt;u/Accurate_Ad4323&lt;/a&gt; from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.&lt;/p&gt; &lt;p&gt;I still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger's advice? And with ROCm's reputation for being a bit tricky, I didn't want to make my life even harder.&lt;/p&gt; &lt;p&gt;In the end, I decided to pull the trigger. Worst-case scenario? I'd have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.&lt;/p&gt; &lt;p&gt;I found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.&lt;/p&gt; &lt;p&gt;Here’s what I ordered from them:&lt;/p&gt; &lt;p&gt;- Supermicro X11DPI-N -&amp;gt; $320&lt;br /&gt; - Dual Xeon 6148 CPUs -&amp;gt; 27 * 2 = $54&lt;br /&gt; - 2x CPU Coolers -&amp;gt; $62&lt;br /&gt; - 2x MI50 32GB GPUs -&amp;gt; $137 * 2 = $274&lt;br /&gt; - 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&amp;gt; $124&lt;br /&gt; - 10x 120mm RGB fans -&amp;gt; $32&lt;br /&gt; - 6x 140mm RGB fans -&amp;gt; $27&lt;br /&gt; - 2x custom cooling shrouded fans for MI50s -&amp;gt; $14&lt;br /&gt; - Shipping + Duties -&amp;gt; $187&lt;/p&gt; &lt;p&gt;I know people get skeptical about Alibaba, but in my opinion, you're safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.&lt;/p&gt; &lt;p&gt;When the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).&lt;/p&gt; &lt;p&gt;I also bought these peripherals brand-new:&lt;/p&gt; &lt;p&gt;- Phanteks Enthoo Pro 2 Server Edition -&amp;gt; $200&lt;br /&gt; - ProLab 1200W 80Plus Gold PSU -&amp;gt; $100&lt;br /&gt; - 2TB NVMe SSD (For Ubuntu) -&amp;gt; $100&lt;br /&gt; - 1TB 2.5 SSD (For Windows) -&amp;gt; $50&lt;/p&gt; &lt;p&gt;All in, I spent exactly &lt;strong&gt;$1544.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now for the two final hurdles:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Assembling everything without breaking it!&lt;/strong&gt; As a first-timer, it took me about three very careful days, but I'm so proud of how it turned out.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing that custom VBIOS. Did I get the &amp;quot;bonus&amp;quot;?&lt;/strong&gt; After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, to answer the questions I had for myself about the VBIOS cross-flash:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is it stable?&lt;/strong&gt; Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set &amp;quot;VGA Priority&amp;quot; to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to &amp;quot;Onboard&amp;quot; fixes it. This doesn't happen at all in Ubuntu with ROCm.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does the flash hurt AI performance?&lt;/strong&gt; Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I've seen zero difference. If anything weird pops up, I'll be sure to update.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can it game?&lt;/strong&gt; Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.&lt;/p&gt; &lt;p&gt;I ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)&lt;/p&gt; &lt;p&gt;Now, I'll end this off with a couple pictures of the build and some benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790"&gt;https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71"&gt;https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The build is still a work-in-progress with regards to cable management :facepalm)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A power limit of 150W was imposed on both GPUs for all these tests.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-128K-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Magistral-Small-2506-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gemma-3-27b-it-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama-3.3-70B-Instruct-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-128K-UD-Q2_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2_K_XL-00001-of-00002.gguf -ot '(4-7+).ffn_._exps.=CPU' -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | ot | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Furmark scores post VBIOS flash and NimeZ drivers on Windows:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f"&gt;https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764"&gt;https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall, this whole experience has been an adventure, but it's been overwhelmingly positive. I thought I'd share it for anyone else thinking about a similar build.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Noticed a lot of requests to post the seller. Here you go: &lt;a href="https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html"&gt;https://www.alibaba.com/product-detail/Best-Price-Graphics-Cards-MI50-32GB_1601432581416.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T18:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lspzn3</id>
    <title>128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.</title>
    <updated>2025-07-06T01:59:10+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt; &lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt; &lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt; &lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt; &lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt; &lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;3014.18 ± 1.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;191.63 ± 0.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1289.11 ± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;91.46 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;357.71 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;249.45 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.24 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;300.02 ± 0.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.39 ± 0.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;48.92 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.05 ± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;56.33 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;16.00 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;1023.81 ± 3.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;63.87 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;238.17 ± 0.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;25.17 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;202.50 ± 0.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.17 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt; &lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt; &lt;p&gt;Now, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output token throughput (tok/s) (256)&lt;/th&gt; &lt;th align="left"&gt;Prompt processing t/s (4096)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.68&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.76&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;25.96&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;27.26&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;32.3&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;38.55&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;36.96&lt;/td&gt; &lt;td align="left"&gt;350&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt; &lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T01:59:10+00:00</published>
  </entry>
</feed>
