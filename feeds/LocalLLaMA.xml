<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-15T13:30:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kmi59x</id>
    <title>Stable Audio Open Small - new fast audio generation model</title>
    <updated>2025-05-14T15:31:25+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt;: &lt;a href="https://huggingface.co/stabilityai/stable-audio-open-small"&gt;https://huggingface.co/stabilityai/stable-audio-open-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.08175"&gt;https://arxiv.org/abs/2505.08175&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arm learning path&lt;/strong&gt;: &lt;a href="https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/run-stable-audio-open-small-with-lite-rt"&gt;https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/run-stable-audio-open-small-with-lite-rt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The last link has some demos, they claim 30% faster than realtime!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi59x/stable_audio_open_small_new_fast_audio_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kme2c4</id>
    <title>Build DeepSeek architecture from scratch | 20 high quality video lectures</title>
    <updated>2025-05-14T12:36:36+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt; &lt;img alt="Build DeepSeek architecture from scratch | 20 high quality video lectures" src="https://external-preview.redd.it/KAbXE4K5sDdk4MosCKTIZy94mD_n03QyKwLpBwLHH7s.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d99eb54310442088ed7f01364f74ed3363b88f" title="Build DeepSeek architecture from scratch | 20 high quality video lectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/of6lxo00sq0f1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 20 lectures covering everything from Multi-Head Latent Attention to Mixture of Experts. &lt;/p&gt; &lt;p&gt;It took me 2 months to finish recording these lectures. &lt;/p&gt; &lt;p&gt;One of the most challenging (and also rewarding) thing I have done this year. &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 20 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings: &lt;a href="https://youtu.be/rP0CoTxe5gU"&gt;https://youtu.be/rP0CoTxe5gU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings: &lt;a href="https://youtu.be/bQCQ7VO-TWU"&gt;https://youtu.be/bQCQ7VO-TWU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings: &lt;a href="https://youtu.be/a17DlNxkv2k"&gt;https://youtu.be/a17DlNxkv2k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE: &lt;a href="https://youtu.be/m1x8vA_Tscc"&gt;https://youtu.be/m1x8vA_Tscc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction: &lt;a href="https://youtu.be/v7U21meXd6Y"&gt;https://youtu.be/v7U21meXd6Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration: &lt;a href="https://youtu.be/yw6fpYPJ7PI"&gt;https://youtu.be/yw6fpYPJ7PI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques: &lt;a href="https://youtu.be/nRadcspta_8"&gt;https://youtu.be/nRadcspta_8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: Multi-Token Prediction (MTP) and Fine-grained quantization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kme2c4/build_deepseek_architecture_from_scratch_20_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T12:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmq7gx</id>
    <title>We need llama-4-maverick-03-26-experimental.</title>
    <updated>2025-05-14T20:52:37+00:00</updated>
    <author>
      <name>/u/PuppyGirlEfina</name>
      <uri>https://old.reddit.com/user/PuppyGirlEfina</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmq7gx/we_need_llama4maverick0326experimental/"&gt; &lt;img alt="We need llama-4-maverick-03-26-experimental." src="https://b.thumbs.redditmedia.com/pa7bHZFqHKuUv-GA3knVGPP3wZuwEYBgGa4xZQnPkAY.jpg" title="We need llama-4-maverick-03-26-experimental." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been spending a lot of time looking into the differences between the Llama-4 Maverick we got and the `llama-4-maverick-03-26-experimental` version, and honestly, I'm starting to feel like we seriously missed out.&lt;/p&gt; &lt;p&gt;From my own personal testing with the `03-26-experimental`, the emotional intelligence is genuinely striking. It feels more nuanced, more understanding, and less like it is just pattern-matching empathy. It's a qualitative difference that really stands out.&lt;/p&gt; &lt;p&gt;And it's not just my anecdotal experience. This post (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the%5C_experimental%5C_version%5C_of%5C_llama4%5C_maverick%5C_on/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the\_experimental\_version\_of\_llama4\_maverick\_on/&lt;/a&gt;) highlights how the LMArena version is significantly more creative and a better coder than the model that eventually got the official release.&lt;/p&gt; &lt;p&gt;Now, I know the counter-argument: &amp;quot;Oh, it was just better at 'glazing' or producing overly long, agreeable responses.&amp;quot; But I don't think that tells the whole story. If you look at the LMSys blog post on sentiment control (&lt;a href="https://blog.lmarena.ai/blog/2025/sentiment-control/"&gt;https://blog.lmarena.ai/blog/2025/sentiment-control/&lt;/a&gt;), it's pretty clear. When they account for the verbosity and &amp;quot;glazing,&amp;quot; the `llama-4-maverick-03-26-experimental` model &lt;em&gt;still&lt;/em&gt; significantly outperforms the released version. In their charts, the experimental model is shown as being above Gemma 3 27B, while the released version actually dips &lt;em&gt;below&lt;/em&gt; it. That's a difference in underlying capability, not just surface-level agreeableness.&lt;/p&gt; &lt;p&gt;And then there's the infamous &amp;quot;ball in the heptagon&amp;quot; test. The released Llama-4 Maverick was a complete trainwreck on this, as painfully detailed here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsl37d/im%5C_incredibly%5C_disappointed%5C_with%5C_llama4/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jsl37d/im\_incredibly\_disappointed\_with\_llama4/&lt;/a&gt;. It was a real letdown for many. But the `03-26-experimental` version? It actually handles the heptagon test surprisingly well, demonstrating a level of coding the released version just doesn't seem to have.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o0p3m0kj8t0f1.gif"&gt;Sorry, if it seems slow at the start. That isn't in the actual thing, it's just the webm -&amp;gt; gif conversion.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, what gives? It feels like the `llama-4-maverick-03-26-experimental` was a more aligned that actually possessed superior core capabilities in several key areas. While the released version might be more polished in some respects, it seems to have worse actual intelligence and usefulness for more complex tasks.&lt;/p&gt; &lt;p&gt;I really hope there's a chance we can see this experimental version released, or at least get more insight into why such a capable version was seemingly left behind. It feels like the community is missing out on a much better model.&lt;/p&gt; &lt;p&gt;What are your thoughts? Has anyone else tested or seen results from `llama-4-maverick-03-26-experimental` that align with this? (It's still up on LMArena for direct chat.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The `llama-4-maverick-03-26-experimental` version seems demonstrably better in emotional intelligence, creativity, coding, and even raw benchmark performance (once &amp;quot;glazing&amp;quot; is accounted for) and reasoning (heptagon test) than the released Llama-4 Maverick. We want access to &lt;em&gt;that&lt;/em&gt; model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuppyGirlEfina"&gt; /u/PuppyGirlEfina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmq7gx/we_need_llama4maverick0326experimental/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmq7gx/we_need_llama4maverick0326experimental/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmq7gx/we_need_llama4maverick0326experimental/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T20:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1klx9q2</id>
    <title>Real-time webcam demo with SmolVLM using llama.cpp</title>
    <updated>2025-05-13T20:59:50+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt; &lt;img alt="Real-time webcam demo with SmolVLM using llama.cpp" src="https://external-preview.redd.it/OHg0YjZidWQ0bTBmMduXqqISYSTmhZJt9j6zzJp3o5OEqUQPvF7tZjxvn6li.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbb3d3b1a7db42b1a83c7e14926531c1ab78b9f" title="Real-time webcam demo with SmolVLM using llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81evi7ud4m0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T20:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmhr87</id>
    <title>Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!</title>
    <updated>2025-05-14T15:15:31+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!" src="https://external-preview.redd.it/vaSJWfDvrVhyb2X2lFu4a2nMHg68l5zMzNqYLj2vNZ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059aaa73054b57a14497284f4a9f9a7d64c69435" title="Drummer's Snowpiercer 15B v1 - Trudge through the winter with a finetune of Nemotron 15B Thinker!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmhr87/drummers_snowpiercer_15b_v1_trudge_through_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn69mp</id>
    <title>How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?</title>
    <updated>2025-05-15T11:54:06+00:00</updated>
    <author>
      <name>/u/coconautico</name>
      <uri>https://old.reddit.com/user/coconautico</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm trying to build a solution to &lt;strong&gt;analyze a set of PDF files&lt;/strong&gt; (5-10) using an LLM.&lt;/p&gt; &lt;p&gt;My current approach is to perform a &lt;strong&gt;high-quality OCR&lt;/strong&gt; (using Docling) and then, dump all this information as the &lt;strong&gt;context for my prompt&lt;/strong&gt;. However, I doubt this is the best strategy nowadays.&lt;/p&gt; &lt;p&gt;Playing around with Gemini, I've noticed it handles PDF files extremely well*, even showing the &lt;strong&gt;tokens it contains&lt;/strong&gt;. So I was wondering if the model is &amp;quot;&lt;strong&gt;reading&lt;/strong&gt;&amp;quot; the PDF file &lt;strong&gt;directly&lt;/strong&gt; (native vision), or is there a preliminary step where it converts the PDF to pure text using &lt;strong&gt;OCR before processing&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I'm also wondering if a &lt;strong&gt;Retrieval Augmented Generation (RAG) strategy&lt;/strong&gt; is involved in how it interacts with the document content once uploaded.&lt;/p&gt; &lt;p&gt;If anyone knows more about this process, it would be interesting to hear.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;*It was able to perfectly process a PDF of images with handwritten text and equations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coconautico"&gt; /u/coconautico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmg1ht</id>
    <title>Wan-AI/Wan2.1-VACE-14B · Hugging Face (Apache-2.0)</title>
    <updated>2025-05-14T14:06:06+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"&gt; &lt;img alt="Wan-AI/Wan2.1-VACE-14B · Hugging Face (Apache-2.0)" src="https://external-preview.redd.it/TmzIWNNChRov_gA4HjoE6PO2tsdMf2f2ESAHN00wPOY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c439fad5f75361eaa3eb176a04dfd9733c3e274" title="Wan-AI/Wan2.1-VACE-14B · Hugging Face (Apache-2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Wan2.1&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/VACE"&gt;VACE&lt;/a&gt;, an all-in-one model for video creation and editing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.1-VACE-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmg1ht/wanaiwan21vace14b_hugging_face_apache20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T14:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn75zx</id>
    <title>What are the current best models for keeping a roles of real word scenarios in low size.</title>
    <updated>2025-05-15T12:39:26+00:00</updated>
    <author>
      <name>/u/SomeRandomGuuuuuuy</name>
      <uri>https://old.reddit.com/user/SomeRandomGuuuuuuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am looking for model to prompt it to imitate human in specific real word situations like receptionist or medical professionals and make them stick to role.&lt;br /&gt; I looked for some time and test different models around and find only this source regarding it&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/flowers-team/StickToYourRoleLeaderboard"&gt;https://huggingface.co/spaces/flowers-team/StickToYourRoleLeaderboard&lt;/a&gt; but it don't seem that updated.&lt;br /&gt; And used this &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/"&gt;https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/&lt;/a&gt; I tested these models around 10 GB VRAM but so far llama seems best but not perfect do you guy suggest other models or resources or specific prompt techniques. i experimented with prompt injection and so on.&lt;/p&gt; &lt;p&gt;&lt;code&gt;google_gemma-3-12b-it-Q6_K_L.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Meta-Llama-3-1-8B-Instruct-Q8_0.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;phi-4.Q5_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Qwen2.5-14B-Instruct-1M-GGUF&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeRandomGuuuuuuy"&gt; /u/SomeRandomGuuuuuuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75zx/what_are_the_current_best_models_for_keeping_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75zx/what_are_the_current_best_models_for_keeping_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75zx/what_are_the_current_best_models_for_keeping_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:39:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn57h0</id>
    <title>MLX version of Qwen3:235B for an 128GB RAM Mac Studio wanted</title>
    <updated>2025-05-15T10:51:22+00:00</updated>
    <author>
      <name>/u/EmergencyLetter135</name>
      <uri>https://old.reddit.com/user/EmergencyLetter135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am looking for an MLX version of Qwen 3 in the 235B-A22B version for a Mac Studio with 128 GB Ram. I use LM Studio and have already tested the following models of huggingface on the Mac Studio without success:&lt;/p&gt; &lt;p&gt;mlx-community/Qwen3-235B-A22B-mixed-3-4bit&lt;/p&gt; &lt;p&gt;mlx-community/Qwen3-235B-A22B-3bit&lt;/p&gt; &lt;p&gt;Alternatively to the MLX Modells, the following GGUF model from Unsloth will work:&lt;/p&gt; &lt;p&gt;Qwen3-235B-A22B-UD-Q2_K_XL (88.02gb)(17.77 t/s)&lt;/p&gt; &lt;p&gt;I am looking forward to your experience with an Apple computer with 128 GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmergencyLetter135"&gt; /u/EmergencyLetter135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn57h0/mlx_version_of_qwen3235b_for_an_128gb_ram_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn57h0/mlx_version_of_qwen3235b_for_an_128gb_ram_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn57h0/mlx_version_of_qwen3235b_for_an_128gb_ram_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T10:51:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmmdm9</id>
    <title>My Local LLM Chat Interface: Current Progress and Vision</title>
    <updated>2025-05-14T18:18:12+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmdm9/my_local_llm_chat_interface_current_progress_and/"&gt; &lt;img alt="My Local LLM Chat Interface: Current Progress and Vision" src="https://external-preview.redd.it/bHduMTFnZmNoczBmMQXdlzCcXrRSF6QNtR-5LXsr8naKnLiD8pPE0dCTLYFs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c19d5850a365956aabd16ca86f020e48357e7ae" title="My Local LLM Chat Interface: Current Progress and Vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, my first reddit post ever! I’ve been building a fully local, offline LLM chat interface designed around actual daily use, fast performance, and a focus on clean, customizable design. It started as a personal challenge and has grown into something I use constantly and plan to evolve much further.&lt;/p&gt; &lt;p&gt;Here’s what I’ve implemented so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complete markdown renderer for clean message formatting&lt;/li&gt; &lt;li&gt;Chat minimization to keep long conversations tidy&lt;/li&gt; &lt;li&gt;In-chat search to quickly find messages by keyword&lt;/li&gt; &lt;li&gt;Text-to-speech (TTS) support for LLM responses&lt;/li&gt; &lt;li&gt;User message editing and forking&lt;/li&gt; &lt;li&gt;Switching between different versions of user and LLM messages&lt;/li&gt; &lt;li&gt;Experimental quoting system for LLM outputs (early stage)&lt;/li&gt; &lt;li&gt;Polished front-end with custom theme and color tuning&lt;/li&gt; &lt;li&gt;Multiple theme switching for different moods and use cases&lt;/li&gt; &lt;li&gt;Beautifully crafted UI with attention to user experience&lt;/li&gt; &lt;li&gt;Glassmorphism effects for a modern, layered visual look&lt;/li&gt; &lt;li&gt;Initial memory feature to help the LLM retain context across interactions, in future I will make it global and local memory as well&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The current version feels fast, snappy, and very enjoyable to use. But I’m only at the start. The next phase will focus on expanding real functionality: integrating task-oriented agents, adding deep document research and knowledge exploration, enabling thinking UIs and visual canvases, providing code analysis and explanations, introducing full voice-driven control with fallback to text, and even allowing generation of audio summaries or podcast-like outputs from chats and documents. The aim is to turn this into a complete local research, thinking, and workflow assistant.&lt;/p&gt; &lt;p&gt;I built this for myself, but if people show interest, I’ll consider releasing it. I genuinely want feedback: what am I missing, what could be better, and which features would you prioritize if you were using something like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0az6hifchs0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmdm9/my_local_llm_chat_interface_current_progress_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmdm9/my_local_llm_chat_interface_current_progress_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T18:18:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmmq6d</id>
    <title>Base Models That Can Still Complete Text in an Entertaining Way</title>
    <updated>2025-05-14T18:32:10+00:00</updated>
    <author>
      <name>/u/Soft-Ad4690</name>
      <uri>https://old.reddit.com/user/Soft-Ad4690</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back during the LLaMa-1 to Mistral-7B era, it used to be a lot of fun to just download a base model, give it a ridiculous prompt, and let it autocomplete. The results were often less dry and more entertaining than asking the corresponding instruct models to do it.&lt;/p&gt; &lt;p&gt;But today's models, even the base ones, seem to be heavily trained on synthetic, dry, reasoning-heavy data, and that approach just doesn't work anymore.&lt;/p&gt; &lt;p&gt;Do you know of any current models (or maybe fine-tunes) that still work well for this purpose?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soft-Ad4690"&gt; /u/Soft-Ad4690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmq6d/base_models_that_can_still_complete_text_in_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmq6d/base_models_that_can_still_complete_text_in_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmmq6d/base_models_that_can_still_complete_text_in_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T18:32:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmqmr8</id>
    <title>Nous Psyche, distributed training of a new 40B base model</title>
    <updated>2025-05-14T21:09:52+00:00</updated>
    <author>
      <name>/u/discr</name>
      <uri>https://old.reddit.com/user/discr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/discr"&gt; /u/discr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://psyche.network/runs/consilience-40b-1/0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmqmr8/nous_psyche_distributed_training_of_a_new_40b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmqmr8/nous_psyche_distributed_training_of_a_new_40b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T21:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi3ra</id>
    <title>AMD Strix Halo (Ryzen AI Max+ 395) GPU LLM Performance</title>
    <updated>2025-05-14T15:29:44+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing some (ongoing) testing on a Strix Halo system recently and with a bunch of desktop systems coming out, and very few advanced/serious GPU-based LLM performance reviews out there, I figured it might be worth sharing a few notes I've made on the current performance and state of software.&lt;/p&gt; &lt;p&gt;This post will primarily focus on LLM inference with the Strix Halo GPU on Linux (but the llama.cpp testing should be pretty relevant for Windows as well).&lt;/p&gt; &lt;p&gt;This post gets rejected with too many links so I'll just leave a single link for those that want to dive deeper: &lt;a href="https://llm-tracker.info/_TOORG/Strix-Halo"&gt;https://llm-tracker.info/_TOORG/Strix-Halo&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance&lt;/h1&gt; &lt;p&gt;In terms of raw compute specs, the Ryzen AI Max 395's Radeon 8060S has 40 RDNA3.5 CUs. At a max clock of 2.9GHz this should have a peak of &lt;strong&gt;59.4 FP16/BF16 TFLOPS&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt; 512 ops/clock/CU * 40 CU * 2.9e9 clock / 1e12 = 59.392 FP16 TFLOPS &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This peak value requires either WMMA or wave32 VOPD otherwise the max is halved.&lt;/p&gt; &lt;p&gt;Using mamf-finder to test, without hipBLASLt, it takes about 35 hours to test and only gets to &lt;strong&gt;5.1 BF16 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;lt;9%&lt;/strong&gt; max theoretical).&lt;/p&gt; &lt;p&gt;However, when run with hipBLASLt, this goes up to &lt;strong&gt;36.9 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;gt;60%&lt;/strong&gt; max theoretical) which is comparable to MI300X efficiency numbers.&lt;/p&gt; &lt;p&gt;On the memory bandwidth (MBW) front, &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives about &lt;strong&gt;212 GB/s&lt;/strong&gt; peak bandwidth (DDR5-8000 on a 256-bit bus gives a theoretical peak MBW of &lt;strong&gt;256 GB/s&lt;/strong&gt;). This is roughly in line with the max MBW tested by ThePhawx, jack stone, and others on various Strix Halo systems.&lt;/p&gt; &lt;p&gt;One thing &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives you is also CPU to GPU speed, which is &lt;strong&gt;~84 GB/s&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The system I am using is set to almost all of its memory dedicated to GPU - 8GB GART and 110 GB GTT and has a very high PL (&amp;gt;100W TDP).&lt;/p&gt; &lt;h1&gt;llama.cpp&lt;/h1&gt; &lt;p&gt;What most people probably want to know is how these chips perform with llama.cpp for bs=1 inference. &lt;/p&gt; &lt;p&gt;First I'll test with the standard TheBloke/Llama-2-7B-GGUF Q4_0 so you can easily compare to other tests like my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/"&gt;previous compute and memory bandwidth efficiency tests across architectures&lt;/a&gt; or the official llama.cpp Apple Silicon M-series performance thread.&lt;/p&gt; &lt;p&gt;I ran with a number of different backends, and the results were actually pretty surprising:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;294.64 ± 0.58&lt;/td&gt; &lt;td align="left"&gt;28.94 ± 0.04&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU + FA&lt;/td&gt; &lt;td align="left"&gt;294.36 ± 3.13&lt;/td&gt; &lt;td align="left"&gt;29.42 ± 0.03&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;348.96 ± 0.31&lt;/td&gt; &lt;td align="left"&gt;48.72 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;4219&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;331.96 ± 0.41&lt;/td&gt; &lt;td align="left"&gt;45.78 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;4245&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;322.63 ± 1.34&lt;/td&gt; &lt;td align="left"&gt;48.40 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;343.91 ± 0.60&lt;/td&gt; &lt;td align="left"&gt;50.88 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;881.71 ± 1.71&lt;/td&gt; &lt;td align="left"&gt;52.22 ± 0.05&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;884.20 ± 6.23&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;52.73 ± 0.07&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The HIP version performs &lt;strong&gt;far&lt;/strong&gt; below what you'd expect in terms of tok/TFLOP efficiency for prompt processing even vs other RDNA3 architectures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gfx1103&lt;/code&gt; Radeon 780M iGPU gets 14.51 tok/TFLOP. At that efficiency you'd expect the about 850 tok/s that the Vulkan backend delivers.&lt;/li&gt; &lt;li&gt;&lt;code&gt;gfx1100&lt;/code&gt; Radeon 7900 XTX gets 25.12 tok/TFLOP. At that efficiency you'd expect almost 1500 tok/s, almost double what the Vulkan backend delivers, and &amp;gt;4X what the current HIP backend delivers.&lt;/li&gt; &lt;li&gt;HIP pp512 barely beats out CPU backend numbers. I don't have an explanation for this.&lt;/li&gt; &lt;li&gt;Just for a reference of how bad the HIP performance is, an 18CU M3 Pro has ~12.8 FP16 TFLOPS (4.6X less compute than Strix Halo) and delivers about the same pp512. Lunar Lake Arc 140V has 32 FP16 TFLOPS (almost 1/2 Strix Halo) and has a pp512 of 657 tok/s (1.9X faster)&lt;/li&gt; &lt;li&gt;With the Vulkan backend pp512 is about the same as an M4 Max and tg128 is about equivalent to an M4 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing a similar system with Linux 6.14 vs 6.15 showed a 15% performance difference so it's possible future driver/platform updates will improve/fix Strix Halo's ROCm/HIP compute efficiency problems.&lt;/p&gt; &lt;p&gt;So that's a bit grim, but I did want to point out one silver lining. With the recent fixes for Flash Attention with the llama.cpp Vulkan backend, I did some higher context testing, and here, the HIP + rocWMMA backend actually shows some strength. It has basically &lt;strong&gt;no decrease in either pp or tg performance at 8K context&lt;/strong&gt; and uses the least memory to boot:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;245.59 ± 0.10&lt;/td&gt; &lt;td align="left"&gt;12.43 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10591&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;190.86 ± 0.49&lt;/td&gt; &lt;td align="left"&gt;30.01 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;7+8089&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;230.10 ± 0.70&lt;/td&gt; &lt;td align="left"&gt;12.37 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10590&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;368.77 ± 1.22&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;50.97 ± 0.00&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7+8062&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;487.69 ± 0.83&lt;/td&gt; &lt;td align="left"&gt;7.54 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;7761+1180&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;490.18 ± 4.89&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;32.03 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;7767+1180&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;You need to have &lt;code&gt;rocmwmma&lt;/code&gt; installed - many distros have packages but you need gfx1151 support is very new (#PR 538) from last week) so you will probably need to build your own rocWMMA from source&lt;/li&gt; &lt;li&gt;You should then rebuild llama.cpp with &lt;code&gt;-DGGML_HIP_ROCWMMA_FATTN=ON&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you mostly do 1-shot inference, then the Vulkan + FA backend is actually probably the best and is the most cross-platform/easy option. If you frequently have longer conversations then HIP + WMMA + FA is probalby the way to go, even if prompt processing is much slower than it should be right now.&lt;/p&gt; &lt;p&gt;I also ran some tests with Qwen3-30B-A3B UD-Q4_K_XL. Larger MoEs is where these large unified memory APUs really shine. &lt;/p&gt; &lt;p&gt;Here are Vulkan results. One thing worth noting, and this is particular to the Qwen3 MoE and Vulkan backend, but using &lt;code&gt;-b 256&lt;/code&gt; significantly improves the pp512 performance:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;70.03 ± 0.18&lt;/td&gt; &lt;td align="left"&gt;75.32 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan b256&lt;/td&gt; &lt;td align="left"&gt;118.78 ± 0.64&lt;/td&gt; &lt;td align="left"&gt;74.76 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While the pp512 is slow, tg128 is as speedy as you'd expect for 3B activations.&lt;/p&gt; &lt;p&gt;This is still only a 16.5 GB model though, so let's go bigger. Llama 4 Scout is 109B parameters and 17B activations and the UD-Q4_K_XL is 57.93 GiB.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;102.61 ± 1.02&lt;/td&gt; &lt;td align="left"&gt;20.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While Llama 4 has had a rocky launch, this is a model that performs about as well as Llama 3.3 70B, but tg is 4X faster, and has SOTA vision as well, so having this speed for tg is a real win.&lt;/p&gt; &lt;p&gt;I've also been able to successfully RPC llama.cpp to test some truly massive (Llama 4 Maverick, Qwen 235B-A22B models, but I'll leave that for a future followup).&lt;/p&gt; &lt;p&gt;Besides romWMMA, I was able to build a ROCm 6.4 image for Strix Halo (gfx1151) using &lt;a href="/u/scottt"&gt;u/scottt&lt;/a&gt;'s dockerfiles. These docker images have hipBLASLt built with gfx1151 support.&lt;/p&gt; &lt;p&gt;I was also able to build AOTriton without too much hassle (it takes about 1h wall time on Strix Halo if you restrict to just the gfx1151 GPU_TARGET).&lt;/p&gt; &lt;p&gt;Composable Kernel (CK) has gfx1151 support now as well and builds in about 15 minutes.&lt;/p&gt; &lt;p&gt;PyTorch was a huge PITA to build, but with a fair amount of elbow grease, I was able to get HEAD (2.8.0a0) compiling, however it still has problems with Flash Attention not working even with &lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL&lt;/code&gt; set.&lt;/p&gt; &lt;p&gt;There's a lot of active work ongoing for PyTorch. For those interested, I'd recommend checking out my linked docs.&lt;/p&gt; &lt;p&gt;I won't bother testing training or batch inference engines until at least PyTorch FA is sorted. Current testing shows fwd/bwd pass to be in the &lt;strong&gt;~1 TFLOPS&lt;/strong&gt; ballpark (very bad)...&lt;/p&gt; &lt;p&gt;This testing obviously isn't very comprehensive, but since there's very little out there, I figure I'd at least share some of the results, especially with the various Chinese Strix Halo mini PCs beginning to ship and with Computex around the corner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmnsol</id>
    <title>AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms</title>
    <updated>2025-05-14T19:14:49+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"&gt; &lt;img alt="AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms" src="https://preview.redd.it/pj1r83skrs0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7100be7b0bc1cceb9f30d390f47de6dcbfbabcec" title="AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google announced AlphaEvolve, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our Gemini models with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas.&lt;/p&gt; &lt;p&gt;AlphaEvolve enhanced the efficiency of Google's data centers, chip design and AI training processes — including training the large language models underlying &lt;strong&gt;AlphaEvolve itself&lt;/strong&gt;. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pj1r83skrs0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T19:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2weg</id>
    <title>LLM for Translation locally</title>
    <updated>2025-05-15T08:12:32+00:00</updated>
    <author>
      <name>/u/yayita2500</name>
      <uri>https://old.reddit.com/user/yayita2500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ! I need to translate some texts..I have been doint Gcloud Trasnlate V3 and also Vertex, but the cost is absolutely high..I do have a 4070 with 12Gb. which model you suggest using Ollama to use a translator that support asian and western languages? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yayita2500"&gt; /u/yayita2500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmrfoo</id>
    <title>MLA optimization with flashattention for llama.cpp,MLA + FA now only uses K-cache - 47% saving on KV-cache size</title>
    <updated>2025-05-14T21:42:55+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13529"&gt;MLA + FA now only uses K-cache - 47% saving on KV-cache size (only for use with #13435 for now) by jukofyork · Pull Request #13529 · ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: kv_size = 163840, type_k = 'f16', type_v = 'f16', n_layer = 61, can_shift = 0, padding = 256&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: CUDA0 KV buffer size = 10980.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: KV self size = 10980.00 MiB, K (f16): 10980.00 MiB, V (f16): 0.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The full context of 160k tokens now takes up less than 11GB without kquants&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T21:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2gsa</id>
    <title>Is neural engine on mac a wasted opportunity?</title>
    <updated>2025-05-15T07:41:38+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the point of having a 32-core neural engine on the new mac studio if you can’t use it for LLM or image/video generation tasks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi6vl</id>
    <title>I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU.</title>
    <updated>2025-05-14T15:33:15+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt; &lt;img alt="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." src="https://external-preview.redd.it/Z3l2NXpmczhucjBmMUwcvEt1gWTYtmZHqUwsIc9aRH3JKfTLJ5UHo4J1H4An.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d172895901d0b35dab0f76eb10b4c4648b8f5c" title="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/&lt;/a&gt;, I decided to update the llama.cpp server demo so that it runs 100% locally in-browser on WebGPU, using Transformers.js. This means you can simply visit the link and run the demo, without needing to install anything locally. &lt;/p&gt; &lt;p&gt;I hope you like it! &lt;a href="https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu"&gt;https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PS: The source code is a single index.html file you can find in the &amp;quot;Files&amp;quot; section on the demo page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/or5b3ks8nr0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmyr7h</id>
    <title>Qwen3-235B-A22B not measuring up to DeepseekV3-0324</title>
    <updated>2025-05-15T03:45:07+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep trying to get it to behave, but q8 is not keeping up with my deepseekv3_q3_k_xl. what gives? am I doing something wrong or is it just all hype? it's a capable model and I'm sure for those that have not been able to run big models, this is a shock and great, but for those of us who have been able to run huge models, it's feel like a waste of bandwidth and time. it's not a disaster like llama-4 yet I'm having a hard time getting it into rotation of my models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T03:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn75q8</id>
    <title>PDF input merged into llama.cpp</title>
    <updated>2025-05-15T12:39:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmlu2y</id>
    <title>Qwen3-30B-A6B-16-Extreme is fantastic</title>
    <updated>2025-05-14T17:57:00+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme"&gt;https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quants:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone recently mentioned this model here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; and I gave it a try. For me it is the best model I can run locally with my 36GB CPU only setup. In my view it is a lot smarter than the original A3B model. &lt;/p&gt; &lt;p&gt;It uses 16 experts instead of 8 and when watching it thinking I can see that it thinks a step further/deeper than the original model. Speed is still great. &lt;/p&gt; &lt;p&gt;I wonder if anyone else has tried it. A 128k context version is also available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T17:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6427</id>
    <title>Llamafile 0.9.3 Brings Support For Qwen3 &amp; Phi4</title>
    <updated>2025-05-15T11:45:30+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt; &lt;img alt="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" src="https://external-preview.redd.it/Cj4HZCrFxF1ZWikVE2EGwsOPpKF5ST6n_sC3VWnurnI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee9e6c0ea1c9f1b1be02252a698b00e32a60cbe" title="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Llamafile-0.9.3-Released"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2aay</id>
    <title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title>
    <updated>2025-05-15T07:28:36+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt; &lt;img alt="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" src="https://preview.redd.it/ww4aygc1ew0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18baa07396402b906dd387ccabc4f5bab873fba3" title="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.09343"&gt;https://arxiv.org/abs/2505.09343&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww4aygc1ew0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn542r</id>
    <title>Introducing A.I.T.E Ball</title>
    <updated>2025-05-15T10:45:28+00:00</updated>
    <author>
      <name>/u/tonywestonuk</name>
      <uri>https://old.reddit.com/user/tonywestonuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt; &lt;img alt="Introducing A.I.T.E Ball" src="https://external-preview.redd.it/NXllMTcxNDFkeDBmMcTQf63cMAAIN-71fn86oCbnKUR2tA_D5RmS947R5l7-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf3613bb545a67e6ba0ae442a8d9fddc761c89a7" title="Introducing A.I.T.E Ball" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a totally self contained (no internet) AI powered 8ball.&lt;/p&gt; &lt;p&gt;Its running on an Orange pi zero 2w, with whisper.cpp to do the text-2-speach, and llama.cpp to do the llm thing, Its running Gemma 3 1b. About as much as I can do on this hardware. But even so.... :-) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonywestonuk"&gt; /u/tonywestonuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/scyofz31dx0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2mv9</id>
    <title>LLMs Get Lost In Multi-Turn Conversation</title>
    <updated>2025-05-15T07:53:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt; &lt;img alt="LLMs Get Lost In Multi-Turn Conversation" src="https://b.thumbs.redditmedia.com/MIMwMQ4O4HnoFjzXbBTjShTxVfai2B_u3_lcuHpfKVk.jpg" title="LLMs Get Lost In Multi-Turn Conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://arxiv.org/abs/2505.06120"&gt;paper&lt;/a&gt; found that the performance of open and closed LLMs drops significantly in multi-turn conversations. Most benchmarks focus on single-turn, fully-specified instruction settings. They found that LLMs often make (incorrect) assumptions in early turns, on which they rely going forward and never recover from.&lt;/p&gt; &lt;p&gt;They concluded that when a multi-turn conversation doesn't yield the desired results, it might help to restart with a fresh conversation, putting all the relevant information from the multi-turn conversation into the first turn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836"&gt;https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Sharded&amp;quot; means they split an original fully-specified single-turn instruction into multiple tidbits of information that they then fed the LLM turn by turn. &amp;quot;Concat&amp;quot; is a comparison as a baseline where they fed all the generated information pieces in the same turn. Here are examples on how they did the splitting:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2"&gt;https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:53:58+00:00</published>
  </entry>
</feed>
