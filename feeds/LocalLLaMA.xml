<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-31T12:27:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jndsj5</id>
    <title>We built a website where you can vote on Minecraft structures generated by AI</title>
    <updated>2025-03-30T14:37:03+00:00</updated>
    <author>
      <name>/u/civilunhinged</name>
      <uri>https://old.reddit.com/user/civilunhinged</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/civilunhinged"&gt; /u/civilunhinged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://mcbench.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnr4i2</id>
    <title>How could I help improve llama.cpp?</title>
    <updated>2025-03-31T00:41:40+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm a Computer Engineering student. I have some experience with C and C++, but I've never worked on open-source projects as large as llama.cpp.&lt;br /&gt; I'd like to know how I could contribute and what would be the best way to get started.&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnr4i2/how_could_i_help_improve_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbhdl</id>
    <title>I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ☺️</title>
    <updated>2025-03-30T12:36:19+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had similar experience with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnrfpp</id>
    <title>New llama model "themis" on lmarena</title>
    <updated>2025-03-31T00:58:08+00:00</updated>
    <author>
      <name>/u/Shyvadi</name>
      <uri>https://old.reddit.com/user/Shyvadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its hidden and only available in battle but it said it was llama could this be llama 4?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shyvadi"&gt; /u/Shyvadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnrfpp/new_llama_model_themis_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32b is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnhuy3</id>
    <title>Llama 3.2 going insane on Facebook</title>
    <updated>2025-03-30T17:39:40+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt; &lt;img alt="Llama 3.2 going insane on Facebook" src="https://b.thumbs.redditmedia.com/NqDMwH6Bz4GQgIvs8QbIfAzCKcWDnZaM3TyMCLpxkoc.jpg" title="Llama 3.2 going insane on Facebook" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It kept going like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnhuy3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T17:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnegrp</id>
    <title>3 new Llama models inside LMArena (maybe LLama 4?)</title>
    <updated>2025-03-30T15:08:49+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt; &lt;img alt="3 new Llama models inside LMArena (maybe LLama 4?)" src="https://b.thumbs.redditmedia.com/dn-wlWwvH94ULQ168bBoDHch1sjJK-d3SZVT2HvBWwc.jpg" title="3 new Llama models inside LMArena (maybe LLama 4?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnegrp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngj5u</id>
    <title>I built a coding agent that allows qwen2.5-coder to use tools</title>
    <updated>2025-03-30T16:41:24+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt; &lt;img alt="I built a coding agent that allows qwen2.5-coder to use tools" src="https://preview.redd.it/1erih6euuure1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5447b0990d64ea3d82b01889605650baf3b6948d" title="I built a coding agent that allows qwen2.5-coder to use tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1erih6euuure1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnc9rd</id>
    <title>It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x</title>
    <updated>2025-03-30T13:21:39+00:00</updated>
    <author>
      <name>/u/madaerodog</name>
      <uri>https://old.reddit.com/user/madaerodog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt; &lt;img alt="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" src="https://b.thumbs.redditmedia.com/xZgN0CnCg9_dkwL0g3ohDgCJu3nIHgZs9DZKGJ0a-FQ.jpg" title="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaerodog"&gt; /u/madaerodog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnc9rd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnsfb3</id>
    <title>Bailing Moe is now supported in llama.cpp</title>
    <updated>2025-03-31T01:51:45+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking forward to this one, finally a new small MOE model. &lt;/p&gt; &lt;p&gt;Ling comes in 3 variants Lite (16.8B total 2.75B active), Lite Coder (16.8B total 2.75B active) and Plus (290B total 28.8B active).&lt;/p&gt; &lt;p&gt;With the small size they are perfectly suited for CPU inference.&lt;/p&gt; &lt;p&gt;It will be interesting to see how these compare to Qwen 3 MOE once that releases. &lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32"&gt;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;info about model: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;pull request: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12634#pullrequestreview-2727983571"&gt;https://github.com/ggml-org/llama.cpp/pull/12634#pullrequestreview-2727983571&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnsfb3/bailing_moe_is_now_supported_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T01:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjrdk</id>
    <title>Benchmark: RTX 3090, 4090, and even 4080 are surprisingly strong for 1-person QwQ-32B inference. (but 5090 not yet)</title>
    <updated>2025-03-30T19:01:34+00:00</updated>
    <author>
      <name>/u/fxtentacle</name>
      <uri>https://old.reddit.com/user/fxtentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't want to send all of my code to any outside company, but I still want to use AI code completion. Accordingly, I was curious how fast various GPUs would be for hosting when there's only 1 user: me. I used vLLM and &lt;code&gt;QwQ-32B-Q4_K_M&lt;/code&gt; for benchmarking.&lt;/p&gt; &lt;p&gt;&lt;code&gt;median_ttft_ms&lt;/code&gt; measures how long it takes for the GPU to handle the context and parse my request. And then &lt;code&gt;median_otps&lt;/code&gt; is how many output tokens the GPU can generate per second. (OTPS = Output Tokens Per Second) Overall, the &lt;code&gt;median_ttft_ms&lt;/code&gt; values were all &amp;lt;1s unless the card was overloaded and I think they will rarely matter in practice. That means the race is on for the highest OTPS.&lt;/p&gt; &lt;p&gt;As expected, a H200 is fast with 334ms + 30 OTPS. The H100 NVL is still fast with 426ms + 23 OTPS. The &amp;quot;old&amp;quot; H100 with HBM3 is similar at 310ms + 22 OTPS.&lt;/p&gt; &lt;p&gt;But I did not expect 2x RTX 4080 to score 383ms + 33 OTPS, which is really close to the H200 and that's somewhat insane if you consider that I'm comparing a 34000€ datacenter product with a 1800€ home setup. An old pair of 2x RTX 3090 is also still pleasant at 564ms + 28 OTPS. And a (watercooled and gently overclocked) RTX 3090 TI rocked the ranking with 558ms + 36 OTPS. You can also clearly see that vLLM is not fully optimized for the RTX 5090 yet, because there the official docker image did not work (yet) and I had to compile from source and, still, the results were somewhat meh with 517ms + 18 TOPS, which is slightly slower than a single 4090.&lt;/p&gt; &lt;p&gt;You'll notice that the consumer GPUs are slower in the initial context and request parsing. That makes sense because that task is highly parallel, i.e. what datacenter products were optimized for. But due to higher clock speeds and more aggressive cooling, consumer GPUs outcompete both H100 and H200 at output token generation, which is the sequential part of the task.&lt;/p&gt; &lt;p&gt;Here's my raw result JSONs from &lt;code&gt;vllm/benchmarks/benchmark_serving.py&lt;/code&gt; and a table with even more hardware variations: &lt;a href="https://github.com/DeutscheKI/llm-performance-tests"&gt;https://github.com/DeutscheKI/llm-performance-tests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, my take-aways from this would be:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAM clock dominates everything. OC for the win!&lt;/li&gt; &lt;li&gt;Go with 2x 4080 over a single 4090 or 5090.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fxtentacle"&gt; /u/fxtentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T19:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnd6px</id>
    <title>LLMs over torrent</title>
    <updated>2025-03-30T14:08:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt; &lt;img alt="LLMs over torrent" src="https://preview.redd.it/8z6t2vvu3ure1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ade8fa1e4ff10e2d71461fdb60f942583a4d442f" title="LLMs over torrent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just messing around with an idea - serving LLM models over torrent. I’ve uploaded Qwen2.5-VL-3B-Instruct to a seedbox sitting in a neutral datacenter in the Netherlands (hosted via Feralhosting).&lt;/p&gt; &lt;p&gt;If you wanna try it out, grab the torrent file here and load it up in any torrent client:&lt;/p&gt; &lt;p&gt;👉 &lt;a href="http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent"&gt;http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment - no promises about uptime, speed, or anything really. It might work, it might not 🤷&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Some random thoughts / open questions: 1. Only models with redistribution-friendly licenses (like Apache-2.0) can be shared this way. Qwen is cool, Mistral too. Stuff from Meta or Google gets more legally fuzzy - might need a lawyer to be sure. 2. If we actually wanted to host a big chunk of available models, we’d need a ton of seedboxes. Huggingface claims they store 45PB of data 😅 📎 &lt;a href="https://huggingface.co/docs/hub/storage-backends"&gt;https://huggingface.co/docs/hub/storage-backends&lt;/a&gt; 3. Binary deduplication would help save space. Bonus points if we can do OTA-style patch updates to avoid re-downloading full models every time. 4. Why bother? AI’s getting more important, and putting everything in one place feels a bit risky long term. Torrents could be a good backup layer or alt-distribution method.&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Anyway, curious what people think. If you’ve got ideas, feedback, or even some storage/bandwidth to spare, feel free to join the fun. Let’s see what breaks 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8z6t2vvu3ure1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnwxw3</id>
    <title>I had Claude and Gemini Pro collaborate on a game. The result? 2048 Ultimate Edition</title>
    <updated>2025-03-31T06:30:54+00:00</updated>
    <author>
      <name>/u/eposnix</name>
      <uri>https://old.reddit.com/user/eposnix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like both Claude and Gemini for coding, but for different reasons, so I had the idea to just put them in a loop and let them work with each other on a project. The prompt: &amp;quot;Make an amazing version of 2048.&amp;quot; They deliberated for about 10 minutes straight, bouncing ideas back and forth, and 2900+ lines of code later, output &lt;strong&gt;2048 Ultimate Edition&lt;/strong&gt; (they named it themselves).&lt;/p&gt; &lt;p&gt;The final version of their 2048 game boasted these features (none of which I asked for):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth animations&lt;/li&gt; &lt;li&gt;Difficulty settings&lt;/li&gt; &lt;li&gt;Adjustable grid sizes&lt;/li&gt; &lt;li&gt;In-game stats tracking (total moves, average score, etc.)&lt;/li&gt; &lt;li&gt;Save/load feature&lt;/li&gt; &lt;li&gt;Achievements system&lt;/li&gt; &lt;li&gt;Clean UI with keyboard &lt;em&gt;and&lt;/em&gt; swipe controls&lt;/li&gt; &lt;li&gt;Light/Dark mode toggle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to try it out here: &lt;a href="https://www.eposnix.com/AI/2048.html"&gt;https://www.eposnix.com/AI/2048.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, you can read their collaboration here: &lt;a href="https://pastebin.com/yqch19yy"&gt;https://pastebin.com/yqch19yy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While this doesn't necessarily involve local models, this method can easily be adapted to use local models instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eposnix"&gt; /u/eposnix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwxw3/i_had_claude_and_gemini_pro_collaborate_on_a_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T06:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnplb1</id>
    <title>MLX fork with speculative decoding in server</title>
    <updated>2025-03-30T23:22:58+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I forked mlx-lm and ported the speculative decoding from the generate command to the server command, so now we can launch an OpenAI compatible completions endpoint with it enabled. I’m working on tidying the tests up to submit PR to upstream but wanted to announce here in case anyone wanted this capability now. I get a 90% speed increase when using qwen coder 0.5 as draft model and 32b as main model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; mlx_lm.server --host localhost --port 8080 --model ./Qwen2.5-Coder-32B-Instruct-8bit --draft-model ./Qwen2.5-Coder-0.5B-8bit &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm"&gt;https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T23:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo0wf9</id>
    <title>RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)</title>
    <updated>2025-03-31T11:25:49+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt; &lt;img alt="RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)" src="https://b.thumbs.redditmedia.com/nA7D2jhOOKLXACZ-SNAxVn1wByf8iBUa2MWnQMWgGaY.jpg" title="RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cgpfkci6e0se1.jpg?width=868&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8fbbd40cc6fe111c3913c2bb4f76d623a6ae9a02"&gt;https://www.proshop.fi/Naeytoenohjaimet/NVIDIA-RTX-PRO-6000-Blackwell-Bulk-96GB-GDDR7-RAM-Naeytoenohjaimet/3358883&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Proshop is a decently sized retailer and Nvidia's partner for selling Founders Edition cards in several European countries so the listing is definitely legit.&lt;/p&gt; &lt;p&gt;NVIDIA RTX PRO 5000 Blackwell 48GB listed at ~4000€ + some more listings for those curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.proshop.fi/?s=rtx+pro+blackwell&amp;amp;o=2304"&gt;https://www.proshop.fi/?s=rtx+pro+blackwell&amp;amp;o=2304&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T11:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo1d2k</id>
    <title>I'm building extension that gets you free and unlimited usage of Gemini 2.5 Pro</title>
    <updated>2025-03-31T11:53:44+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo1d2k/im_building_extension_that_gets_you_free_and/"&gt; &lt;img alt="I'm building extension that gets you free and unlimited usage of Gemini 2.5 Pro" src="https://external-preview.redd.it/OTM4b3M1bGxrMHNlMQztlUbXx4HmVNCbcZJT8RQ5gRkadeJUcw9XZgxVzRZr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f9e6bab0955956521e4b561e3d9c3de2b5cb0fa" title="I'm building extension that gets you free and unlimited usage of Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wtvj15llk0se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo1d2k/im_building_extension_that_gets_you_free_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo1d2k/im_building_extension_that_gets_you_free_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T11:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnzjnb</id>
    <title>[MERGED] Adding Qwen3 and Qwen3MoE · Pull Request #36878 · huggingface/transformers</title>
    <updated>2025-03-31T09:54:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzjnb/merged_adding_qwen3_and_qwen3moe_pull_request/"&gt; &lt;img alt="[MERGED] Adding Qwen3 and Qwen3MoE · Pull Request #36878 · huggingface/transformers" src="https://external-preview.redd.it/9ERrHYa5ukfAsMPJAg_yCQX0ifla2bBT23y1hLKjU0Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2f3d576fb5075da4dd20a88f0d5c6f99060a7e2" title="[MERGED] Adding Qwen3 and Qwen3MoE · Pull Request #36878 · huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The pull request that adds Qwen3 and Qwen3MoE support to HuggingFace's Transformers library got merged today!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzjnb/merged_adding_qwen3_and_qwen3moe_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzjnb/merged_adding_qwen3_and_qwen3moe_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T09:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnxx49</id>
    <title>I made a Grammarly alternative without clunky UI. Completely free with Gemini Nano (in-browser AI). Helps you with writing emails, articles, social media posts, etc.</title>
    <updated>2025-03-31T07:46:24+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxx49/i_made_a_grammarly_alternative_without_clunky_ui/"&gt; &lt;img alt="I made a Grammarly alternative without clunky UI. Completely free with Gemini Nano (in-browser AI). Helps you with writing emails, articles, social media posts, etc." src="https://external-preview.redd.it/bXc4cGxsNDdjenJlMRg_TmPcBoSM13pUYzKlWo7qhuAMWmP4IKxV8h55ZV-h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83787ad90557457cbbb4143689f2ddd0c11ee905" title="I made a Grammarly alternative without clunky UI. Completely free with Gemini Nano (in-browser AI). Helps you with writing emails, articles, social media posts, etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wbpq5l47czre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxx49/i_made_a_grammarly_alternative_without_clunky_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxx49/i_made_a_grammarly_alternative_without_clunky_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T07:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnfpnr</id>
    <title>It’s been 1000 releases and 5000 commits in llama.cpp</title>
    <updated>2025-03-30T16:04:30+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt; &lt;img alt="It’s been 1000 releases and 5000 commits in llama.cpp" src="https://external-preview.redd.it/wyCM1fHzTa-IIqHgS1QTxdSYNXn668elDj0WmYMPf_k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0d58c9a49c1e9ce629e5b31dce17b727d8c6ab8" title="It’s been 1000 releases and 5000 commits in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1000th release of llama.cpp&lt;/p&gt; &lt;p&gt;Almost 5000 commits. (4998)&lt;/p&gt; &lt;p&gt;It all started with llama 1 leak.&lt;/p&gt; &lt;p&gt;Thanks you team. Someone tag ‘em if you know their handle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnxlbn</id>
    <title>Warning: Fake deepseek v3.1 blog post</title>
    <updated>2025-03-31T07:20:42+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"&gt; &lt;img alt="Warning: Fake deepseek v3.1 blog post" src="https://b.thumbs.redditmedia.com/BFmws_fTdBtz2dhc2H_CT86cSjHSljP3ufbMOqyZaME.jpg" title="Warning: Fake deepseek v3.1 blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There has been this blog post recently circulating about the release of an alleged &amp;quot;Deepseek V3.1&amp;quot;, and after looking into the website, it seems like it is totally fake. Remember, deepseek does not have any official blog.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vbo8mdtxczre1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d0abb44c73705312c551b3cc0d00bc3df150621"&gt;https://preview.redd.it/vbo8mdtxczre1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d0abb44c73705312c551b3cc0d00bc3df150621&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnxlbn/warning_fake_deepseek_v31_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T07:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvhkd</id>
    <title>The diminishing returns of larger models, perhaps you don't need to spend big on hardware for inference</title>
    <updated>2025-03-31T04:50:11+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been tracking the recent performance of models like Gemma 27B, QwQ 32B, and Mistral Small, and I'm starting to believe we're hitting a point of diminishing returns with the really large (70B+) LLMs. For a while, scaling to larger parameters was the path to better overall performance. But the gap is shrinking – and shrinking fast.&lt;/p&gt; &lt;p&gt;Gemma3 27B consistently punches above its weight, often rivaling or exceeding Llama 3.3 70B on many benchmarks, especially when considering cost/performance. QwQ 32B is another excellent example. These aren't just &amp;quot;good for their size&amp;quot; – they're legitimately competitive.&lt;/p&gt; &lt;p&gt;Why is this happening? A few factors:&lt;/p&gt; &lt;p&gt;- Distillation: We're getting really good at distilling knowledge from larger models into smaller ones.&lt;/p&gt; &lt;p&gt;- Architecture Improvements: Innovations in attention mechanisms, routing, and other architectural details are making smaller models more efficient.&lt;/p&gt; &lt;p&gt;- Data Quality: Better curated and more focused training datasets are allowing smaller models to learn more effectively.&lt;/p&gt; &lt;p&gt;- Diminishing Returns: Each doubling in parameter count yields a smaller and smaller improvement in performance. Going from 7B to 30B is a bigger leap than going from 30B to 70B and from 70 to 400B.&lt;/p&gt; &lt;p&gt;What does this mean for inference?&lt;/p&gt; &lt;p&gt;If you’re currently shelling out for expensive GPU time to run 70B+ models, consider this: the performance gap is closing. Investing in a ton of hardware today might only give you a marginal advantage that disappears in a few months.&lt;/p&gt; &lt;p&gt;If you can be patient, the advances happening in the 30B-50B range will likely deliver a lot of the benefits of larger models without the massive hardware requirements. What requires an H100 today may happily run on an RTX 4090 , or even more modest GPU, in the near future.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;p&gt;TL;DR: Gemma, QwQ, and others are showing that smaller LLMs can be surprisingly competitive with larger ones. Don't overspend on hardware now – the benefits of bigger models are rapidly becoming accessible in smaller packages.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvhkd/the_diminishing_returns_of_larger_models_perhaps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T04:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnwh90</id>
    <title>We used AlphaMaze idea to train a robotics control model!</title>
    <updated>2025-03-31T05:57:54+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"&gt; &lt;img alt="We used AlphaMaze idea to train a robotics control model!" src="https://external-preview.redd.it/YXg2dnh6d3VzeXJlMdEk5BYEIzAqHbVyhNzLaxnyvsN1SHVgxmelOVR9PzS5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9536f905923cbcb5516b647645a0f579b0ee3023" title="We used AlphaMaze idea to train a robotics control model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it’s me again, from Menlo Research (aka homebrew aka Jan)! We just launched a new experiment: AlphaSpace – a robotics model that operates purely on semantic tokens, with no hardcoded rules or modality encoding!&lt;/p&gt; &lt;p&gt;In the previous release, AlphaSpace demonstrated spatial reasoning in a 2D (5x5) maze. The model's reasoning improved when applying GRPO. More importantly, the entire project was built by representing the maze using semantic tokens—without relying on modality encoding or encoders!&lt;/p&gt; &lt;p&gt;However, this experiment raises some key questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How far can semantic tokens take us?&lt;/li&gt; &lt;li&gt;If 5x5 is too small, can this tokenization method scale to 100x100, or even 1000x1000?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To explore this, we conducted a new experiment called AlphaSpace, building on some ideas from AlphaMaze but with significant changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Larger reasoning space: From 2D 5x5 to 3D 100x100x30.&lt;/li&gt; &lt;li&gt;No traditional visual representation—instead, we generate synthetic reasoning data more systematically.&lt;/li&gt; &lt;li&gt;Testing the model on a robotics benchmark.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What makes AlphaSpace exciting?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Represents space purely through semantic tokens, without step-by-step planning.&lt;/li&gt; &lt;li&gt;No dependence on a modality encoder, making it easier to integrate into various systems without end-to-end training.&lt;/li&gt; &lt;li&gt;100% synthetic dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out more details here:&lt;br /&gt; Paper: &lt;a href="https://arxiv.org/abs/2503.18769"&gt;https://arxiv.org/abs/2503.18769&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/homebrewltd/AlphaSpace-1.5B"&gt;https://huggingface.co/homebrewltd/AlphaSpace-1.5B&lt;/a&gt;&lt;br /&gt; Dataset: &lt;a href="https://huggingface.co/datasets/Menlo/Pick-Place-Table-Reasoning-local-pos-v0.2"&gt;https://huggingface.co/datasets/Menlo/Pick-Place-Table-Reasoning-local-pos-v0.2&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/menloresearch/space-thinker"&gt;https://github.com/menloresearch/space-thinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://alphaspace.menlo.ai/"&gt;https://alphaspace.menlo.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SPOILER:&lt;br /&gt; - As much as we want to this model development has been halted a bit early and there are still many things we didn't account for when training the model, so just treat it as a small and fun experiment&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw0asvwusyre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnwh90/we_used_alphamaze_idea_to_train_a_robotics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T05:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnzq51</id>
    <title>PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s</title>
    <updated>2025-03-31T10:06:50+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"&gt; &lt;img alt="PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s" src="https://external-preview.redd.it/mjqF-7sslVFdngpzQezzzTUL6oX500j7ElXyhrnVXck.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65cdc44f4fd50dd2535768214841267fd3ed7cf8" title="PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watch as I build a monster PC to run Deepseek-V3-0324:671b-Q8 locally at 6-8 tokens per second. I'm using dual EPYC 9355 processors and 768Gb of 5600mhz RDIMMs 24x32Gb on a MZ73-LM0 Gigabyte motherboard. I flash the BIOS, install Ubuntu 24.04.2 LTS, ollama, Open WebUI, and more, step by step!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/v4810MVGhog"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T10:06:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvqsg</id>
    <title>why is no one talking about Qwen 2.5 omni?</title>
    <updated>2025-03-31T05:06:57+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems crazy to me the first multimodal with voice, image, and text gen open sourced and no one is talking about it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T05:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnzdvp</id>
    <title>Qwen3 support merged into transformers</title>
    <updated>2025-03-31T09:42:25+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T09:42:25+00:00</published>
  </entry>
</feed>
