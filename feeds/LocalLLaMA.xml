<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-15T10:24:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m0d6ry</id>
    <title>Can I fine-tune Qwen3 with DPO? How do I handle &lt;thinking&gt; tokens?</title>
    <updated>2025-07-15T09:25:52+00:00</updated>
    <author>
      <name>/u/pragmojo</name>
      <uri>https://old.reddit.com/user/pragmojo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm attempting to fine-tune Qwen3-8B for a specific domain. Since this model produces thinking tokens, I'm a bit unsure how to handle them during training.&lt;/p&gt; &lt;p&gt;I'm attempting to use &lt;code&gt;DPOConfig&lt;/code&gt; and &lt;code&gt;DPOTrainer&lt;/code&gt; from &lt;code&gt;trl&lt;/code&gt;, with &lt;code&gt;Lora&lt;/code&gt; for lower VRAM usage.&lt;/p&gt; &lt;p&gt;For training, do I include the &lt;code&gt;&amp;lt;thinking&amp;gt;&lt;/code&gt; tokens in the &lt;code&gt;chosen&lt;/code&gt; and &lt;code&gt;rejected&lt;/code&gt; outputs for the training data? It's a bit unclear to me how to handle these.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pragmojo"&gt; /u/pragmojo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T09:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m081hm</id>
    <title>Test MNN Chat for Android</title>
    <updated>2025-07-15T04:09:10+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/"&gt; &lt;img alt="Test MNN Chat for Android" src="https://b.thumbs.redditmedia.com/6GJVWwujc9y0RxTiQQge_lZ6RODHcNochQCqPhXQNwQ.jpg" title="Test MNN Chat for Android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are alpha test the google play version of MNN Chat. looking for feedback from users like you.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, join our Google Group:&lt;a href="https://groups.google.com/g/mnn-chat/"&gt;MNN Chat Testers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Then, download the app from the Play Store:&lt;a href="https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release"&gt;Get MNN Chat&lt;/a&gt; or visit &lt;a href="https://play.google.com/apps/testing/com.alibaba.mnnllm.android.release"&gt;WebPage&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice Chat:&lt;/strong&gt; Talk directly to any AI model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pinned Models:&lt;/strong&gt; Keep your favorite models just a tap away.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Filtering:&lt;/strong&gt; Easily sort and find models by size.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Tool:&lt;/strong&gt; Test how fast different models run on your device.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd"&gt;https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283"&gt;https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T04:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0c569</id>
    <title>AI Agent tutorial in TS from the basics to building multi-agent teams</title>
    <updated>2025-07-15T08:15:22+00:00</updated>
    <author>
      <name>/u/necati-ozmen</name>
      <uri>https://old.reddit.com/user/necati-ozmen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/"&gt; &lt;img alt="AI Agent tutorial in TS from the basics to building multi-agent teams" src="https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44638bb3ecbfd7937f6eabdd3c0d40e4ea668e61" title="AI Agent tutorial in TS from the basics to building multi-agent teams" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We published a step by step tutorial for building AI agents that actually do things, not just chat. Each section adds a key capability, with runnable code and examples.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915"&gt;https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tutorial: &lt;a href="https://voltagent.dev/tutorial/introduction/"&gt;https://voltagent.dev/tutorial/introduction/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/voltagent/voltagent"&gt;https://github.com/voltagent/voltagent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tutorial Source Code: &lt;a href="https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial"&gt;https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’ve been building OSS dev tools for over 7 years. From that experience, we’ve seen that tutorials which combine key concepts with hands-on code examples are the most effective way to understand the why and how of agent development.&lt;/p&gt; &lt;p&gt;What we implemented:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1 – The Chatbot Problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why most chatbots are limited and what makes AI agents fundamentally different.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2 – Tools: Give Your Agent Superpowers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let your agent do real work: call APIs, send emails, query databases, and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3 – Memory: Remember Every Conversation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Persist conversations so your agent builds context over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4 – MCP: Connect to Everything&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using MCP to integrate GitHub, Slack, databases, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5 – Subagents: Build Agent Teams&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Create specialized agents that collaborate to handle complex tasks.&lt;/p&gt; &lt;p&gt;It’s all built using VoltAgent, our TypeScript-first open-source AI agent framework.(I'm maintainer) It handles routing, memory, observability, and tool execution, so you can focus on logic and behavior.&lt;/p&gt; &lt;p&gt;Although the tutorial uses VoltAgent, the core ideas tools, memory, coordination are framework-agnostic. So even if you’re using another framework or building from scratch, the steps should still be useful.&lt;/p&gt; &lt;p&gt;We’d love your feedback, especially from folks building agent systems. If you notice anything unclear or incomplete, feel free to open an issue or PR. It’s all part of the open-source repo. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/necati-ozmen"&gt; /u/necati-ozmen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:15:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzzcje</id>
    <title>MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation</title>
    <updated>2025-07-14T21:36:24+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"&gt; &lt;img alt="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation" src="https://b.thumbs.redditmedia.com/irH_IxYHAQEjBGgXU_AOln_EsxdXVQKIs5z4JjXsVfc.jpg" title="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.&lt;/p&gt; &lt;p&gt;The benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.​​​​​​​​​​​​​​​​&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://mmluprox.github.io"&gt;https://mmluprox.github.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.10497"&gt;https://arxiv.org/abs/2503.10497&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/weihao1115/MMLU-ProX"&gt;https://github.com/weihao1115/MMLU-ProX&lt;/a&gt; (still empty)&lt;/li&gt; &lt;li&gt;Full dataset: &lt;a href="https://huggingface.co/datasets/li-lab/MMLU-ProX"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Lite dataset: &lt;a href="https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lzzcje"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m06nhe</id>
    <title>Non-reasoning models adopting reasoning behavior from previous messages</title>
    <updated>2025-07-15T02:58:19+00:00</updated>
    <author>
      <name>/u/Thedudely1</name>
      <uri>https://old.reddit.com/user/Thedudely1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thedudely1"&gt; /u/Thedudely1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T02:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzfhhq</id>
    <title>Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman</title>
    <updated>2025-07-14T06:48:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt; &lt;img alt="Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman" src="https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8" title="Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt; &lt;p&gt;I don't know how the French and European authorities could accept this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syyfccpldscf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T06:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0dqgh</id>
    <title>Open source and free iOS app to chat with your LLMs when you are away from home.</title>
    <updated>2025-07-15T10:00:21+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones. &lt;/p&gt; &lt;p&gt;I find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.&lt;br /&gt; So I made these two companion apps (one for iOS and one for Mac). You just install them and they work. &lt;/p&gt; &lt;p&gt;The Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).&lt;br /&gt; The iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle…&lt;br /&gt; The UI is super intuitive for anyone who has ever used a chatbot. &lt;/p&gt; &lt;p&gt;They don't need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment. &lt;/p&gt; &lt;p&gt;The only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.&lt;/p&gt; &lt;p&gt;The iOS app is called LLM Pigeon and this is the link:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The MacOS app is called LLM Pigeon Server and this is the link:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T10:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0cdle</id>
    <title>PydanticAI is GOAT for building agents in Python</title>
    <updated>2025-07-15T08:31:52+00:00</updated>
    <author>
      <name>/u/-lq_pl-</name>
      <uri>https://old.reddit.com/user/-lq_pl-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/"&gt; &lt;img alt="PydanticAI is GOAT for building agents in Python" src="https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e990c8a8d39910f731443529ec7ac5bc68d1a0d6" title="PydanticAI is GOAT for building agents in Python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not affiliated with the project, this is my unbiased opinion.&lt;/p&gt; &lt;p&gt;I wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.&lt;/p&gt; &lt;p&gt;I tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.&lt;/p&gt; &lt;p&gt;Making an agent that can keep track of characters in the story is as simple as this:&lt;/p&gt; &lt;p&gt;```py class Character(BaseModel): &amp;quot;&amp;quot;&amp;quot;Character model with stats and description.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; name: str appearance: str = Field(description=&amp;quot;Physical appearance and decorative clothing&amp;quot;) personality: str = Field(description=&amp;quot;Personality traits and behavior&amp;quot;) money: int = Field(ge=0, description=&amp;quot;Amount of money the character carries&amp;quot;) # skipping other attributes... agent = Agent(...) # dictionary of all characters in the story npcs = {} # This automatically generates a tool signature that the LLM understands u/agent.tool_plain def add_character( character: Character ) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot; Add a new character to the story. Use this tool for every new named character in the story. &amp;quot;&amp;quot;&amp;quot; if character.name in state_manager.state.npcs: return f&amp;quot;Character {character.name!r} already exists in the story.&amp;quot; npcs[character.name] = character return f&amp;quot;Added character {character.name!r} to the story.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note how you don't have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.&lt;/p&gt; &lt;p&gt;PydanticAI is the first of these libraries that is actually enjoyable to use.&lt;/p&gt; &lt;p&gt;I use Mistral Small 3.2 in my tests and it doesn't work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-lq_pl-"&gt; /u/-lq_pl- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.pydantic.dev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0c7am</id>
    <title>Open source LLMs leaderboard</title>
    <updated>2025-07-15T08:19:11+00:00</updated>
    <author>
      <name>/u/oh_my_right_leg</name>
      <uri>https://old.reddit.com/user/oh_my_right_leg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Is there a leaderboard for open source LLMs? I know &lt;a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard"&gt;this&lt;/a&gt; one for VLMs and there used to be one from HuggingFace, but I think that &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;amp;official=true"&gt;one&lt;/a&gt; is no longer maintained.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oh_my_right_leg"&gt; /u/oh_my_right_leg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m013ou</id>
    <title>Moonshot AI’s open source Kimi K2 outperforms GPT-4 in key benchmarks</title>
    <updated>2025-07-14T22:46:29+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T22:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m041m4</id>
    <title>Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes</title>
    <updated>2025-07-15T00:55:39+00:00</updated>
    <author>
      <name>/u/sunshinecheung</name>
      <uri>https://old.reddit.com/user/sunshinecheung</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/"&gt; &lt;img alt="Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" src="https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b646755e2c619c761965d3179957a1ab2c65c147" title="Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week, a small group of top members of the lab, including Alexandr Wang, 28, Meta’s new chief A.I. officer, discussed abandoning the company’s most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.&lt;/p&gt; &lt;p&gt;Meta had finished feeding in data to improve its Behemoth model, a process known as “training,” but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model — which is known as a “frontier” model — stopped running new tests on it, one of the people said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunshinecheung"&gt; /u/sunshinecheung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3f68h6pzrxcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T00:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzv16g</id>
    <title>Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes</title>
    <updated>2025-07-14T18:54:00+00:00</updated>
    <author>
      <name>/u/showmeufos</name>
      <uri>https://old.reddit.com/user/showmeufos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"&gt; &lt;img alt="Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" src="https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731" title="Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/showmeufos"&gt; /u/showmeufos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T18:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzw6yu</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-07-14T19:36:39+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;📊 &lt;strong&gt;Feature&lt;/strong&gt;s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt; &lt;li&gt;50+ File extensions supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ &lt;strong&gt;Podcast&lt;/strong&gt;s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Extensio&lt;/strong&gt;n&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T19:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzm645</id>
    <title>After Kimi K2 Is Released: No Longer Just a ChatBot</title>
    <updated>2025-07-14T13:18:06+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author’s insights genuinely thought-provoking. The original Chinese version is &lt;a href="https://bigeagle.me/2025/07/kimi-k2/"&gt;here&lt;/a&gt;—feel free to read it in full (and of course you can use Kimi K2 as your translator). Here’s my own distilled summary of the main points:&lt;/p&gt; &lt;p&gt;• Beyond chatbots: Kimi K2 experiments with an “artifact-first” interaction model that has the AI immediately build interactive front-end deliverables—PPT-like pages, diagrams, even mini-games—rather than simply returning markdown text.&lt;/p&gt; &lt;p&gt;• Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt; &lt;p&gt;• What makes an agentic model: A minimal loop—think, choose tools, observe results, iterate—can be learned from synthetic trajectories. Today’s agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt; &lt;p&gt;• Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit “hacky” hidden pipelines, forcing genuinely strong, general models—exactly what an AGI-oriented startup needs.&lt;/p&gt; &lt;p&gt;• Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1’s viral rise proved that raw model quality markets itself and validates the “foundation-model-first” path.&lt;/p&gt; &lt;p&gt;• Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt; &lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A while ago, ‘Agent’ products were all the rage. I kept hearing people say that Kimi shouldn’t compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don’t keep pushing that ceiling higher, I won’t stay here a single extra day.&lt;/p&gt; &lt;p&gt;Chasing AGI is an extremely narrow, perilous bridge—there’s no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‘As an investor, I care about the ROI of AI applications.’ In that moment I knew the company he founded wouldn’t last long.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T13:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m04ic2</id>
    <title>Grok no more model Open-source?</title>
    <updated>2025-07-15T01:16:48+00:00</updated>
    <author>
      <name>/u/Brilliant_Stock_5137</name>
      <uri>https://old.reddit.com/user/Brilliant_Stock_5137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. &lt;/p&gt; &lt;p&gt;Edit : Sam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn't Open-source any model (e.g Grok-2 or Grok-3) and now.&lt;/p&gt; &lt;p&gt;Me : xAI promise Open-source grok-2 or Grok-3?&lt;/p&gt; &lt;p&gt;Sam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant_Stock_5137"&gt; /u/Brilliant_Stock_5137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T01:16:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0cgmc</id>
    <title>Cognition, maker of the AI coding agent Devin, acquires Windsurf</title>
    <updated>2025-07-15T08:37:43+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"&gt; &lt;img alt="Cognition, maker of the AI coding agent Devin, acquires Windsurf" src="https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=045e7a9473212511b88ba864670fee8b5d269a71" title="Cognition, maker of the AI coding agent Devin, acquires Windsurf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The announcement comes just days after Google hired away Windsurf’s CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup’s 250-person team behind. Google’s deal occurred just hours after OpenAI’s $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m03sh9</id>
    <title>A very nice overview on how llama.cpp quantization works</title>
    <updated>2025-07-15T00:43:58+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/vW30o4U9BFE"&gt;https://youtu.be/vW30o4U9BFE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T00:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0cgnl</id>
    <title>Kimi K2: cheap and fast API access for those who can't run locally</title>
    <updated>2025-07-15T08:37:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"&gt; &lt;img alt="Kimi K2: cheap and fast API access for those who can't run locally" src="https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8de1b4b36c00b224fb29471c6864b8730dd4f7f2" title="Kimi K2: cheap and fast API access for those who can't run locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you can't run &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;kimi-k2&lt;/a&gt; locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around ~250 tokens per second:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepinfra.com/moonshotai/Kimi-K2-Instruct"&gt;https://deepinfra.com/moonshotai/Kimi-K2-Instruct&lt;/a&gt; ($0.55/$2.20 in/out Mtoken)&lt;/li&gt; &lt;li&gt;&lt;a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct"&gt;https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct&lt;/a&gt; ($1/$3 in/out Mtoken, but very fast)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!&lt;/p&gt; &lt;p&gt;It also shows the power of an open weights model with an permissive license: Even if you can't run it yourself, there's a lot more options in API access.&lt;/p&gt; &lt;p&gt;See all providers on OpenRouter: &lt;a href="https://openrouter.ai/moonshotai/kimi-k2"&gt;https://openrouter.ai/moonshotai/kimi-k2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; There's also a free variant, but I don't know the details: &lt;a href="https://openrouter.ai/moonshotai/kimi-k2:free"&gt;https://openrouter.ai/moonshotai/kimi-k2:free&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/moonshotai/kimi-k2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T08:37:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzps3b</id>
    <title>Kimi K2 1.8bit Unsloth Dynamic GGUFs</title>
    <updated>2025-07-14T15:41:16+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt; &lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt; &lt;p&gt;You need to use either &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14654"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href="https://github.com/unslothai/llama.cpp"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt; &lt;p&gt;The suggested parameters are:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.6 min_p = 0.01 (set it to a small number) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Docs has more details: &lt;a href="https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T15:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzl5zk</id>
    <title>UTCP: A safer, scalable tool-calling alternative to MCP</title>
    <updated>2025-07-14T12:33:01+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt; &lt;img alt="UTCP: A safer, scalable tool-calling alternative to MCP" src="https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95" title="UTCP: A safer, scalable tool-calling alternative to MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wv84vx7h3ucf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T12:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0115d</id>
    <title>Meta on track to be first lab with a 1GW supercluster</title>
    <updated>2025-07-14T22:43:36+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt; &lt;img alt="Meta on track to be first lab with a 1GW supercluster" src="https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f" title="Meta on track to be first lab with a 1GW supercluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/584vdadc4xcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T22:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m021nx</id>
    <title>Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)</title>
    <updated>2025-07-14T23:25:45+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt; &lt;img alt="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" src="https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=427166a43aad977ff4e628d5d89073bd9fd90280" title="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl35mhaybxcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T23:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzywie</id>
    <title>Kimi K2 tops creative writing benchmark</title>
    <updated>2025-07-14T21:19:11+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt; &lt;img alt="Kimi K2 tops creative writing benchmark" src="https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855" title="Kimi K2 tops creative writing benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q48f55vcpwcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0d0vz</id>
    <title>Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)</title>
    <updated>2025-07-15T09:15:07+00:00</updated>
    <author>
      <name>/u/yingyn</name>
      <uri>https://old.reddit.com/user/yingyn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt; &lt;img alt="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" src="https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg" title="Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here're the findings!&lt;/p&gt; &lt;p&gt;If there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yingyn"&gt; /u/yingyn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m0d0vz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T09:15:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m04a20</id>
    <title>EXAONE 4.0 32B</title>
    <updated>2025-07-15T01:06:15+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"&gt; &lt;img alt="EXAONE 4.0 32B" src="https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18202842c69b787ccdb604277c8c0ce21247e4d3" title="EXAONE 4.0 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T01:06:15+00:00</published>
  </entry>
</feed>
