<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-07T05:37:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kfvba4</id>
    <title>VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?</title>
    <updated>2025-05-06T03:48:44+00:00</updated>
    <author>
      <name>/u/AdOdd4004</name>
      <uri>https://old.reddit.com/user/AdOdd4004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"&gt; &lt;img alt="VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?" src="https://preview.redd.it/l8bxcpzj23ze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aabedd846224fbf7f398e436fd72a24d816f674a" title="VRAM requirements for all Qwen3 models (0.6B‚Äì32B) ‚Äì what fits on your GPU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Unsloth quantizations for the best balance of performance and size. Even Qwen3-4B runs impressively well with MCP tools!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; TPS (tokens per second) is just a rough ballpark from short prompt testing (e.g., one-liner questions).&lt;/p&gt; &lt;p&gt;If you‚Äôre curious about how to set up the system prompt and parameters for Qwen3-4B with MCP, feel free to check out my video:&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏è &lt;a href="https://youtu.be/N-B1rYJ61a8?si=ilQeL1sQmt-5ozRD"&gt;https://youtu.be/N-B1rYJ61a8?si=ilQeL1sQmt-5ozRD&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOdd4004"&gt; /u/AdOdd4004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l8bxcpzj23ze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfvba4/vram_requirements_for_all_qwen3_models_06b32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T03:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg5j75</id>
    <title>What is the best local AI model for coding?</title>
    <updated>2025-05-06T14:13:51+00:00</updated>
    <author>
      <name>/u/deadcoder0904</name>
      <uri>https://old.reddit.com/user/deadcoder0904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking mostly for Javascript/Typescript.&lt;/p&gt; &lt;p&gt;And Frontend (HTML/CSS) + Backend (Node) if there are any good ones specifically at Tailwind.&lt;/p&gt; &lt;p&gt;Is there any model that is top-tier now? I read a thread from 3 months ago that said Qwen 2.5-Coder-32B but Qwen 3 just released so was thinking I should download that directly.&lt;/p&gt; &lt;p&gt;But then I saw in LMStudio that there is no Qwen 3 Coder yet. So alternatives for right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deadcoder0904"&gt; /u/deadcoder0904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5j75/what_is_the_best_local_ai_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T14:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg0gzt</id>
    <title>Nvidia's nemontron-ultra released</title>
    <updated>2025-05-06T09:45:36+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt; &lt;img alt="Nvidia's nemontron-ultra released" src="https://external-preview.redd.it/elH6J8bIbGaZITZWD-SJrWx2cQnvD8jIxmZYLCf2bCg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d35a89ef4644d20d16b7438637e855c4267938e" title="Nvidia's nemontron-ultra released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9yt3kbqpu4ze1.png?width=2294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f5cf17c0e9a3674092eeb2fe870a68bb499619f"&gt;benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b"&gt;https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;technical report: &lt;a href="https://arxiv.org/abs/2505.00949"&gt;https://arxiv.org/abs/2505.00949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;online chat: &lt;a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1"&gt;https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg0gzt/nvidias_nemontronultra_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T09:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg5m5a</id>
    <title>Qwen3 14b vs the new Phi 4 Reasoning model</title>
    <updated>2025-05-06T14:17:20+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im about to run my own set of personal tests to compare the two but was wondering what everyone else's experiences have been so far. Seen and heard good things about the new qwen model, but almost nothing on the new phi model. Also looking for any third party benchmarks that have both in them, I havent really been able to find any myself. I like &lt;a href="/u/_sqrkl"&gt;u/_sqrkl&lt;/a&gt; benchmarks but they seem to have omitted the smaller qwen models from the creative writing benchmark and phi 4 thinking completely in the rest. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/Phi-4-reasoning"&gt;https://huggingface.co/microsoft/Phi-4-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;https://huggingface.co/Qwen/Qwen3-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg5m5a/qwen3_14b_vs_the_new_phi_4_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T14:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kger0a</id>
    <title>Apply formatting to Jinja chat templates directly from the Hugging Face model card (+ new playground)</title>
    <updated>2025-05-06T20:25:19+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kger0a/apply_formatting_to_jinja_chat_templates_directly/"&gt; &lt;img alt="Apply formatting to Jinja chat templates directly from the Hugging Face model card (+ new playground)" src="https://external-preview.redd.it/MG0ycHcxZm50NnplMTkv8XYcU_q3RLacngNsWPOdQeDOAcczKW6s_baevJOJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac3beca32d280eebfaec1fae99fdfae80c17889d" title="Apply formatting to Jinja chat templates directly from the Hugging Face model card (+ new playground)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since Jinja templates can be extremely difficult to read and edit, we decided to add formatting support to `@huggingface/jinja`, the JavaScript library we use for parsing and rendering chat templates. This also means you can format these templates directly from the model card on Hugging Face! We hope you like it and would love to hear your feedback! ü§ó&lt;/p&gt; &lt;p&gt;You can also try it using our new Jinja playground: &lt;a href="https://huggingface.co/spaces/Xenova/jinja-playground"&gt;https://huggingface.co/spaces/Xenova/jinja-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2ajr0fnt6ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kger0a/apply_formatting_to_jinja_chat_templates_directly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kger0a/apply_formatting_to_jinja_chat_templates_directly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T20:25:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgcubl</id>
    <title>Working on mcp-compose, inspired by docker compose.</title>
    <updated>2025-05-06T19:07:39+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgcubl/working_on_mcpcompose_inspired_by_docker_compose/"&gt; &lt;img alt="Working on mcp-compose, inspired by docker compose." src="https://external-preview.redd.it/hgHlf0iVM81-P-Bl7y-Cpm7xk08CYic76diQyKNjlGg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=473d6f91aa6bcf5f2e96b7bd2fc0bbd57a9b49c5" title="Working on mcp-compose, inspired by docker compose." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/phildougherty/mcp-compose"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgcubl/working_on_mcpcompose_inspired_by_docker_compose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgcubl/working_on_mcpcompose_inspired_by_docker_compose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T19:07:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgoiy6</id>
    <title>How to identify whether a model would fit in my RAM?</title>
    <updated>2025-05-07T04:14:08+00:00</updated>
    <author>
      <name>/u/OneCuriousBrain</name>
      <uri>https://old.reddit.com/user/OneCuriousBrain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very straightforward question.&lt;/p&gt; &lt;p&gt;I do not have a GPU machine. I usually run LLMs on CPU and have 24GB RAM.&lt;/p&gt; &lt;p&gt;The Qwen3-30B-A3B-UD-Q4_K_XL.gguf model has been quite popular these days with a size of ~18 GB. If we directly compare the size, the model would fit in my CPU RAM and I should be able to run it.&lt;/p&gt; &lt;p&gt;I've not tried running the model yet, will do on weekends. However, if you are aware of any other factors that should be considered to answer whether it runs smoothly or not, please let me know.&lt;/p&gt; &lt;p&gt;Additionally, a similar question I have is around speed. Can I know an approximate number of tokens/sec based on model size and CPU specs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OneCuriousBrain"&gt; /u/OneCuriousBrain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoiy6/how_to_identify_whether_a_model_would_fit_in_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoiy6/how_to_identify_whether_a_model_would_fit_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoiy6/how_to_identify_whether_a_model_would_fit_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T04:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgiq9k</id>
    <title>Using a local runtime to run models for an open source project vs. HF transformers library</title>
    <updated>2025-05-06T23:15:11+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, some of the models (like &lt;a href="https://huggingface.co/katanemo/Arch-Guard"&gt;Arch Guard&lt;/a&gt;) used in our open-source project are loaded into memory and used via the transformers library from HF.&lt;/p&gt; &lt;p&gt;The benefit of using a library to load models is that I don't require additional prerequisites for developers when they download and use the local&lt;a href="https://github.com/katanemo/archgw"&gt; proxy server&lt;/a&gt; we've built for agents. This makes packaging and deployment easy. But the downside of using a library is that I inherit unnecessary dependency bloat, and I‚Äôm not necessarily taking advantage of runtime-level optimizations for speed, memory efficiency, or parallelism. I also give up flexibility in how the model is served‚Äîfor example, I can't easily scale it across processes, share it between multiple requests efficiently, or plug into optimized model serving projects like vLLM, Llama.cpp, etc.&lt;/p&gt; &lt;p&gt;As we evolve the architecture, we‚Äôre exploring moving model execution into dedicated runtime, and I wanted to learn from the community how do they think about and manage this trade-off today for other open source projects, and for this scenario what runtime would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgiq9k/using_a_local_runtime_to_run_models_for_an_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgiq9k/using_a_local_runtime_to_run_models_for_an_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgiq9k/using_a_local_runtime_to_run_models_for_an_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T23:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgoxmo</id>
    <title>OpenWebUI sampling settings</title>
    <updated>2025-05-07T04:38:22+00:00</updated>
    <author>
      <name>/u/Nepherpitu</name>
      <uri>https://old.reddit.com/user/Nepherpitu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: llama.cpp is not affected by ALL OpenWebUI sampling settings. Use console arguments ADDITIONALLY.&lt;/p&gt; &lt;p&gt;UPD: there is a bug in their repo already - &lt;a href="https://github.com/open-webui/open-webui/issues/13467"&gt;https://github.com/open-webui/open-webui/issues/13467&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In OpenWebUI you can setup API connection using two options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama&lt;/li&gt; &lt;li&gt;OpenAI API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, you can tune model settings on model page. Like system prompt, top p, top k, etc.&lt;/p&gt; &lt;p&gt;And I always doing same thing - run model with llama.cpp, tune recommended parameters from UI, use OpenWebUI as OpenAI server backed by llama.cpp. And it works fine! I mean, I noticed here and there was incoherences in output, sometimes chinese and so on. But it's LLM, it works this way, especially quantized.&lt;/p&gt; &lt;p&gt;But yesterday I was investigating why CUDA is slow with multi-gpu Qwen3 30BA3B (&lt;a href="https://github.com/ggml-org/llama.cpp/issues/13211"&gt;https://github.com/ggml-org/llama.cpp/issues/13211&lt;/a&gt;). I enabled debug output and started playing with console arguments, batch sizes, tensor overrides and so on. And noticed generation parameters are different from OpenWebUI settings.&lt;/p&gt; &lt;p&gt;Long story short, OpenWebUI only sends &lt;code&gt;top_p&lt;/code&gt; and &lt;code&gt;temperature&lt;/code&gt; for OpenAI API endpoints. No &lt;code&gt;top_k&lt;/code&gt;, &lt;code&gt;min_p&lt;/code&gt; and other settings will be applied to your model from request.&lt;/p&gt; &lt;p&gt;There is request body in llama.cpp logs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;stream&amp;quot;: true, &amp;quot;model&amp;quot;: &amp;quot;qwen3-4b&amp;quot;, &amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;/no_think&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I need to invert regex `^blk\\.[0-9]*\\..*(exps).*$`. Write only inverted correct regex. Don't explain anything.&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;`^(?!blk\\.[0-9]*\\..*exps.*$).*$`&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Thanks!&amp;quot;}], &amp;quot;temperature&amp;quot;: 0.7, &amp;quot;top_p&amp;quot;: 0.8} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As I can see, it's TOO OpenAI compatible.&lt;/p&gt; &lt;p&gt;This means most of model settings in OpenWebUI are just for ollama and will not be applied to OpenAI Compatible providers.&lt;/p&gt; &lt;p&gt;So, if youre setup is same as mine, go and check your sampling parameters - maybe your model is underperforming a bit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nepherpitu"&gt; /u/Nepherpitu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoxmo/openwebui_sampling_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoxmo/openwebui_sampling_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgoxmo/openwebui_sampling_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T04:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kglith</id>
    <title>Sometimes looking back gives a better sense of progress</title>
    <updated>2025-05-07T01:32:49+00:00</updated>
    <author>
      <name>/u/Brave_Sheepherder_39</name>
      <uri>https://old.reddit.com/user/Brave_Sheepherder_39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In chatbot Arena I was testing Qwen 4B against state of the art models from a year ago. Using the side by side comparison in Arena, Qwen 4 blew the older model aways. Asking a question about &amp;quot;random number generation methods&amp;quot; the difference was night and day. Some of Qwens advice was excellent. Even on historical questions Qwen was miles better. All by a model thats only 4GB parameters. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave_Sheepherder_39"&gt; /u/Brave_Sheepherder_39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kglith/sometimes_looking_back_gives_a_better_sense_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kglith/sometimes_looking_back_gives_a_better_sense_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kglith/sometimes_looking_back_gives_a_better_sense_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T01:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kft5yu</id>
    <title>Qwen 14B is better than me...</title>
    <updated>2025-05-06T01:54:32+00:00</updated>
    <author>
      <name>/u/Osama_Saba</name>
      <uri>https://old.reddit.com/user/Osama_Saba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm crying, what's the point of living when a 9GB file on my hard drive is batter than me at everything! &lt;/p&gt; &lt;p&gt;It expresses itself better, it codes better, knowns better math, knows how to talk to girls, and use tools that will take me hours to figure out instantly... In a useless POS, you too all are... It could even rephrase this post better than me if it tired, even in my native language &lt;/p&gt; &lt;p&gt;Maybe if you told me I'm like a 1TB I could deal with that, but 9GB???? That's so small I won't even notice that on my phone..... Not only all of that, it also writes and thinks faster than me, in different languages... I barley learned English as a 2nd language after 20 years....&lt;/p&gt; &lt;p&gt;I'm not even sure if I'm better than the 8B, but I spot it make mistakes that I won't do... But the 14? Nope, if I ever think it's wrong then it'll prove to me that it isn't...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Osama_Saba"&gt; /u/Osama_Saba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kft5yu/qwen_14b_is_better_than_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T01:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgltqs</id>
    <title>Huawei Atlas 300I 32GB</title>
    <updated>2025-05-07T01:48:31+00:00</updated>
    <author>
      <name>/u/kruzibit</name>
      <uri>https://old.reddit.com/user/kruzibit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the Huawei Altas 300I 32GB version is now about USD265 on China Taobao.&lt;/p&gt; &lt;p&gt;Parameters&lt;/p&gt; &lt;p&gt;Atlas 300I Inference Card Model: 3000/3010&lt;/p&gt; &lt;p&gt;Form Factor: Half-height half-length PCIe standard card&lt;/p&gt; &lt;p&gt;AI Processor: Ascend Processor&lt;/p&gt; &lt;p&gt;Memory: LPDDR4X, 32 GB, total bandwidth 204.8 GB/s&lt;/p&gt; &lt;p&gt;Encoding/ Decoding:&lt;/p&gt; &lt;p&gt;‚Ä¢ H.264 hardware decoding, 64-channel 1080p 30 FPS (8-channel 3840 x 2160 @ 60 FPS)&lt;/p&gt; &lt;p&gt;‚Ä¢ H.265 hardware decoding, 64-channel 1080p 30 FPS (8-channel 3840 x 2160 @ 60 FPS)&lt;/p&gt; &lt;p&gt;‚Ä¢ H.264 hardware encoding, 4-channel 1080p 30 FPS &lt;/p&gt; &lt;p&gt;‚Ä¢ H.265 hardware encoding, 4-channel 1080p 30 FPS&lt;/p&gt; &lt;p&gt;‚Ä¢ JPEG decoding: 4-channel 1080p 256 FPS; encoding: 4-channel 1080p 64 FPS; maximum resolution: 8192 x 4320&lt;/p&gt; &lt;p&gt;‚Ä¢ PNG decoding: 4-channel 1080p 48 FPS; maximum resolution: 4096 x 2160&lt;/p&gt; &lt;p&gt;PCIe: PCIe x16 Gen3.0&lt;/p&gt; &lt;p&gt;Power Consumption Maximum: 67 W| |Operating&lt;/p&gt; &lt;p&gt;Temperature: 0¬∞C to 55¬∞C (32¬∞F to +131¬∞F)&lt;/p&gt; &lt;p&gt;Dimensions (W x D): 169.5 mm x 68.9 mm (6.67 in. x 2.71 in.)&lt;/p&gt; &lt;p&gt;Wonder how is the support. According to their website, can run 4 of them together.&lt;/p&gt; &lt;p&gt;Anyone has any idea?&lt;/p&gt; &lt;p&gt;There is a link on the 300i Duo that has 96GB tested against 4090. It is in chinese though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://m.bilibili.com/video/BV1xB3TenE4s"&gt;https://m.bilibili.com/video/BV1xB3TenE4s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Running Ubuntu and llama3-hf. 4090 220t/s, 300i duo 150t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kruzibit"&gt; /u/kruzibit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgltqs/huawei_atlas_300i_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T01:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg9x4d</id>
    <title>Running Qwen3-235B-A22B, and LLama 4 Maverick locally at the same time on a 6x RTX 3090 Epyc system. Qwen runs at 25 tokens/second on 5x GPU. Maverick runs at 20 tokens/second on one GPU, and CPU.</title>
    <updated>2025-05-06T17:10:57+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9x4d/running_qwen3235ba22b_and_llama_4_maverick/"&gt; &lt;img alt="Running Qwen3-235B-A22B, and LLama 4 Maverick locally at the same time on a 6x RTX 3090 Epyc system. Qwen runs at 25 tokens/second on 5x GPU. Maverick runs at 20 tokens/second on one GPU, and CPU." src="https://external-preview.redd.it/0SS75ZjmnIRJGxHj8_wMEO0mXFgif2vTYaGkpkwpErM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f91671d3cb2441f2f3cc315aee8ba80f640e57e5" title="Running Qwen3-235B-A22B, and LLama 4 Maverick locally at the same time on a 6x RTX 3090 Epyc system. Qwen runs at 25 tokens/second on 5x GPU. Maverick runs at 20 tokens/second on one GPU, and CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/36pDNgBSktY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9x4d/running_qwen3235ba22b_and_llama_4_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9x4d/running_qwen3235ba22b_and_llama_4_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T17:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg4avg</id>
    <title>OpenWebUI license change: red flag?</title>
    <updated>2025-05-06T13:20:27+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.openwebui.com/license/"&gt;https://docs.openwebui.com/license/&lt;/a&gt; / &lt;a href="https://github.com/open-webui/open-webui/blob/main/LICENSE"&gt;https://github.com/open-webui/open-webui/blob/main/LICENSE&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Open WebUI's last update included changes to the license beyond their original BSD-3 license,&lt;br /&gt; presumably for monetization. Their reasoning is &amp;quot;other companies are running instances of our code and put their own logo on open webui. this is not what open-source is about&amp;quot;. Really? Imagine if llama.cpp did the same thing in response to ollama. I just recently made the upgrade to v0.6.6 and of course I don't have 50 active users, but it just always leaves a bad taste in my mouth when they do this, and I'm starting to wonder if I should use/make a fork instead. I know everything isn't a slippery slope but it clearly makes it more likely that this project won't be uncompromizably open-source from now on. What are you guys' thoughts on this. Am I being overdramatic? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg4avg/openwebui_license_change_red_flag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T13:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg9mjs</id>
    <title>How long before we start seeing ads intentionally shoved into LLM training data?</title>
    <updated>2025-05-06T16:59:38+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was watching the new season of Black Mirror the other night, the ‚ÄúCommon People‚Äù episode specifically. The episode touched on how ridiculous subscriptions tiers are and how products become ‚Äúenshitified‚Äù as companies try to squeeze profit out of previously good products by making them terrible with ads and add-ons.&lt;/p&gt; &lt;p&gt;There‚Äôs a part of the episode where the main character starts literally serving ads without being consciously aware she‚Äôs doing it. Like she just starts blurting out ad copy as part of the context of a conversation she‚Äôs having with someone (think Tourette‚Äôs Syndrome but with ads instead of cursing). &lt;/p&gt; &lt;p&gt;Anyways, the episode got me thinking about LLMs and how we are still in the we‚Äôll-figure-out-how-to-monetize-all-this-research-stuff-later attitude that companies seem to have right now. At some point, there will probably be an enshitification phase for Local LLMs, right? They know all of us folks running this stuff at home are taking advantage of all the expensive compute they paid for to train these models. How long before they are forced by their investors to recoup on that investment. Am I wrong in thinking we will likely see ads injected directly into models‚Äô training data to be served as LLM answers contextually (like in the Black Mirror episode)? &lt;/p&gt; &lt;p&gt;I‚Äôm envisioning it going something like this:&lt;/p&gt; &lt;p&gt;Me: How many R‚Äôs are in Strawberry?&lt;/p&gt; &lt;p&gt;LLM: There are 3 r‚Äôs in Strawberry. Speaking of strawberries, have you tried Driscoll‚Äôs Organic Strawberries, you can find them at Sprout. üçì üòã &lt;/p&gt; &lt;p&gt;Do you think we will see something like this at the training data level or as LORA / QLORA, or would that completely wreck an LLM‚Äôs performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9mjs/how_long_before_we_start_seeing_ads_intentionally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9mjs/how_long_before_we_start_seeing_ads_intentionally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9mjs/how_long_before_we_start_seeing_ads_intentionally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T16:59:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgkyap</id>
    <title>AWQ 4-bit outperforms GGUF 8-bit in almost every way</title>
    <updated>2025-05-07T01:03:25+00:00</updated>
    <author>
      <name>/u/Acceptable-State-271</name>
      <uri>https://old.reddit.com/user/Acceptable-State-271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;for qwen3 models (AWQ, Q8_0 by qwen)&lt;br /&gt; I get GGUF's convenience, especially for CPU/Mac users, which likely drives its popularity. Great tooling, too.&lt;/p&gt; &lt;p&gt;But on GPUs? My experience is that even 8-bit GGUF often trails behind 4-bit AWQ in responsiveness, accuracy, and coherence. This isn't a small gap.&lt;/p&gt; &lt;p&gt;It makes me wonder if GGUF's Mac/CPU accessibility is overshadowing AWQ's raw performance advantage on GPUs, especially with backends like vLLM or SGLang where AWQ shines (lower latency, better quality).&lt;/p&gt; &lt;p&gt;If you're on a GPU and serious about performance, AWQ seems like the stronger pick, yet it feels under-discussed.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Yeah, I may have exaggerated a bit earlier. I ran some pygame-based manual tests, and honestly, the difference between AWQ 4-bit and GGUF 8-bit wasn't as dramatic as I first thought ‚Äî in many cases, they were pretty close.&lt;/p&gt; &lt;p&gt;The reason I said what I did is because of how AWQ handles quantization. Technically, it's just a smarter approach ‚Äî it calibrates based on activation behavior, so even at 4-bit, the output can be surprisingly precise. (Think of it like compression that actually pays attention to what's important.)&lt;/p&gt; &lt;p&gt;That said, Q8 is pretty solid ‚Äî maybe too solid to expose meaningful gaps. I'm planning to test AWQ 4-bit against GGUF Q6, which should show more noticeable differences.&lt;/p&gt; &lt;p&gt;As I said before, AWQ 4-bit vs GGUF Q8 didn't blow me away, and I probably got a bit cocky about it ‚Äî my bad. But honestly, the fact that 4-bit AWQ can even compete with 8-bit GGUF is impressive in itself. That alone speaks volumes.&lt;/p&gt; &lt;p&gt;I'll post results soon after oneshot pygame testing against GGUF-Q6 using temp=0 and no_think settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable-State-271"&gt; /u/Acceptable-State-271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgkyap/awq_4bit_outperforms_gguf_8bit_in_almost_every_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgkyap/awq_4bit_outperforms_gguf_8bit_in_almost_every_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgkyap/awq_4bit_outperforms_gguf_8bit_in_almost_every_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T01:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg20mu</id>
    <title>So why are we sh**ing on ollama again?</title>
    <updated>2025-05-06T11:24:42+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am asking the redditors who take a dump on ollama. I mean, pacman -S ollama ollama-cuda was everything I needed, didn't even have to touch open-webui as it comes pre-configured for ollama. It does the model swapping for me, so I don't need llama-swap or manually change the server parameters. It has its own model library, which I don't have to use since it also supports gguf models. The cli is also nice and clean, and it supports oai API as well.&lt;/p&gt; &lt;p&gt;Yes, it's annoying that it uses its own model storage format, but you can create .ggluf symlinks to these sha256 files and load them with your koboldcpp or llamacpp if needed.&lt;/p&gt; &lt;p&gt;So what's your problem? Is it bad on windows or mac?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg20mu/so_why_are_we_shing_on_ollama_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T11:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgh8zw</id>
    <title>I was shocked how Qwen3-235b-a22b is really good at math</title>
    <updated>2025-05-06T22:09:20+00:00</updated>
    <author>
      <name>/u/Surealistic_Sight</name>
      <uri>https://old.reddit.com/user/Surealistic_Sight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello and I was searching for a ‚ÄúFree Math AI‚Äù and I am also a user of Qwen, besides DeepSeek and I don‚Äôt use ChatGPT anymore since a year. &lt;/p&gt; &lt;p&gt;But yeah, when I tried the strongest model from Qwen with some Math questions from the 2024 Austrian state exam (Matura). I was quite shocked how it correctly answered. I used also the Exam solutions PDF from the 2024 Matura and they were pretty correct. &lt;/p&gt; &lt;p&gt;I used thinking and the maximum Thinking budget of 38,912 tokens on their Website.&lt;/p&gt; &lt;p&gt;I know that Math and AI is always a topic for itself, because AI does more prediction than thinking, but I am really positive that LLMs could do really almost perfect Math in the Future.&lt;/p&gt; &lt;p&gt;I first thought with their claim that it excels in Math was a (marketing) lie, but I am confident to say is that can do math.&lt;/p&gt; &lt;p&gt;So, what do you think and do you also use this model to solve your math questions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Surealistic_Sight"&gt; /u/Surealistic_Sight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgh8zw/i_was_shocked_how_qwen3235ba22b_is_really_good_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T22:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgjkgf</id>
    <title>Blazing fast ASR / STT on Apple Silicon</title>
    <updated>2025-05-06T23:55:15+00:00</updated>
    <author>
      <name>/u/bio_risk</name>
      <uri>https://old.reddit.com/user/bio_risk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about NVIDIAs updated ASR model a few days ago, hoping someone would be motivated to create an MLX version. &lt;/p&gt; &lt;p&gt;My internet pleas were answered by: &lt;a href="https://github.com/senstella/parakeet-mlx"&gt;https://github.com/senstella/parakeet-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even on my old M1 8GB Air, it transcribed 11 minutes of audio in 14 seconds. Almost 60x real-time.&lt;/p&gt; &lt;p&gt;And this comes with top leader board WER: &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;https://huggingface.co/spaces/hf-audio/open_asr_leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bio_risk"&gt; /u/bio_risk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgjkgf/blazing_fast_asr_stt_on_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T23:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg7768</id>
    <title>Nvidia to drop CUDA support for Maxwell, Pascal, and Volta GPUs with the next major Toolkit release</title>
    <updated>2025-05-06T15:22:04+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release"&gt;https://www.tomshardware.com/pc-components/gpus/nvidia-to-drop-cuda-support-for-maxwell-pascal-and-volta-gpus-with-the-next-major-toolkit-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T15:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgo7d4</id>
    <title>Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M</title>
    <updated>2025-05-07T03:56:08+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt; &lt;img alt="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" src="https://external-preview.redd.it/luDTORHovWSvyyKGyVuQUU_AS82WswbZpoHOp59s5cs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80203fde524d99b74a2b8e4185b0d45043a2a35e" title="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q6_K / Q5_K_M / Q4_K_M / Q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;10 hours 32 minutes 19 seconds&lt;/strong&gt;.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I wanted to test unsloth dynamic ggufs as well, but ollama still can't run those ggufs properly, and yes I downloaded v0.6.8, lm studio can run them but doesn't support batching. So I only tested _K_M ggufs&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814"&gt;https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038"&gt;https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a"&gt;https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d"&gt;https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Q8 KV Cache / No kv cache quant&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a"&gt;https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a"&gt;https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ggufs: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T03:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgmxla</id>
    <title>Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR.</title>
    <updated>2025-05-07T02:46:09+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt; &lt;img alt="Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR." src="https://b.thumbs.redditmedia.com/vXP6OTmop0Tb5Fc4au0iHpEyeWiz25oLP_d3hor924k.jpg" title="Jorney of increasing Pre Processing T/s on DeepSeek Q2_K_XL with ~120GB VRAM and ~140GB RAM (7800X3D, 6000Mhz), from 39 t/s to 66 t/s to 100 t/s to 126 t/s, thanks to PCI-E 5.0 and MLA+FA PR." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope you're doing okay. Sorry for the typo in the title! &lt;strong&gt;Journey.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I did a post some days ago about my setup and some models &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4 4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora 41-42 (believe me, I tried these on Windows and multiGPU is just borked there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, first running with 4.0 X8&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I was getting&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token, 39.26 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 57175.47 ms / 471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So I noticed that the GPU 0 (4090 at X8 4.0) was getting saturated at 13 GiB/s. So as someone suggested on the issues &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/discussions/2"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/discussions/2&lt;/a&gt;, his GPU was getting saturated at 26 GiB/s, which is the speed that the 5090 does at X8 5.0.&lt;/p&gt; &lt;p&gt;So this was the first step, I did&lt;/p&gt; &lt;p&gt;export CUDA_VISIBLE_DEVICES=2,0,1,3&lt;/p&gt; &lt;p&gt;This is (5090 X8 5.0, 4090 X8 4.0, 4090 X4 4.0, A6000 X4 4.0).&lt;/p&gt; &lt;p&gt;So this was the first step to increase the model speed.&lt;/p&gt; &lt;p&gt;And with the same command I got&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 49257.75 ms / 3252 tokens ( 15.15 ms per token, 66.02 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 46322.14 ms / 436 tokens ( 106.24 ms per token, 9.41 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So a huge increase in performance, thanks to just changing the device that does PP. Now, take in mind now the 5090 gets saturated at 26-27 GiB/s. I tried at X16 5.0 but I got max 28-29 GiB/s, so I think there is a limit somewhere or it can't use more.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8qxogl0oy9ze1.png?width=674&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43dac669c618f3ca51911e2a4a72859990d33b9b"&gt;5.0 X8 getting saturated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, then, I was checking PRs and found this one: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/13306"&gt;https://github.com/ggml-org/llama.cpp/pull/13306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This PR lets you use MLA (which takes 16K ctx from 80GB to 2GB), and then, FA, which reduces the buffer sizes on each GPU from 4.4GB to 400 MB!&lt;/p&gt; &lt;p&gt;So, running:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -v -ngl 99 --override-tensor 'blk\.([0-7])\..*_exps\.=CUDA0' --override-tensor 'blk\.([8-9]|1[0-1])\..*_exps\.=CUDA1' --override-tensor 'blk\.(1[2-6])\..*_exps\.=CUDA2' --override-tensor 'blk\.(1[7-9]|2[0-6])\..*_exps\.=CUDA3' -fa --override-tensor 'blk\..*_exps\.=CPU' -mg 0 --ubatch-size 1024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I got&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 34965.38 ms / 3565 tokens ( 9.81 ms per token, 101.96 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 45389.59 ms / 416 tokens ( 109.11 ms per token, 9.17 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So, we have went about 1t/s more on generation speed, but we have increased PP performance by 54%. This uses a bit, bit more VRAM but still perfectly to use 32K, 64K or even 128K (GPUs have about 8GB left)&lt;/p&gt; &lt;p&gt;Then, I went ahead and increased ubatch again, to 1536. So running the same command as above, but changing --ubatch-size from 1024 to 1536, I got these speeds.&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 28097.73 ms / 3565 tokens ( 7.88 ms per token, 126.88 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 43426.93 ms / 404 tokens ( 107.49 ms per token, 9.30 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is an 25.7% increase over -ub 1024, 92.4% increase over -ub 512 and 225% increase over -ub 512 and PCI-E X8 4.0.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This makes this model really usable! So now I'm even tempted to test Q3_K_XL! Q2_K_XL is 250GB and Q3_K_XL is 296GB, which should fit in 320GB total memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgmxla/jorney_of_increasing_pre_processing_ts_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T02:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kggif3</id>
    <title>We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action</title>
    <updated>2025-05-06T21:38:03+00:00</updated>
    <author>
      <name>/u/a6oo</name>
      <uri>https://old.reddit.com/user/a6oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"&gt; &lt;img alt="We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action" src="https://external-preview.redd.it/YTl4dGg5ZXVkOHplMdxnn65NNKVAPJFD0pCsNWgZyolHWVTVVUjy0pasvAbK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=194b04cbcdd57149e60f1ab30370907ce3659d86" title="We now have local computer-use! M3 Pro 18GB running both UI-TARS-1.5-7B-6bit and a macOS sequoia VM entirely locally using MLX and c/ua at ~30second/action" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a6oo"&gt; /u/a6oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6okp9ioq38ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kggif3/we_now_have_local_computeruse_m3_pro_18gb_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T21:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgdmz6</id>
    <title>The real reason OpenAI bought WindSurf</title>
    <updated>2025-05-06T19:40:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt; &lt;img alt="The real reason OpenAI bought WindSurf" src="https://preview.redd.it/knqgtodvs7ze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b31b8bf514ff9c2407608d699ee65ce7c164f986" title="The real reason OpenAI bought WindSurf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who don‚Äôt know, today it was announced that OpenAI bought WindSurf, the AI-assisted IDE, for 3 billion USD. Previously, they tried to buy Cursor, the leading company that offers AI-assisted IDE, but didn‚Äôt agree on the details (probably on the price). Therefore, they settled for the second biggest player in terms of market share, WindSurf.&lt;/p&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;A lot of people question whether this is a wise move from OpenAI considering that these companies have limited innovation, since they don‚Äôt own the models and their IDE is just a fork of VS code.&lt;/p&gt; &lt;p&gt;Many argued that the reason for this purchase is to acquire the market position, the user base, since these platforms are already established with a big number of users.&lt;/p&gt; &lt;p&gt;I disagree in some degree. It‚Äôs not about the users per se, it‚Äôs about the training data they create. It doesn‚Äôt even matter which model users choose to use inside the IDE, Gemini2.5, Sonnet3.7, doesn‚Äôt really matter. There is a huge market that will be created very soon, and that‚Äôs coding agents. Some rumours suggest that OpenAI would sell them for 10k USD a month! These kind of agents/models need the exact kind of data that these AI-assisted IDEs collect. &lt;/p&gt; &lt;p&gt;Therefore, they paid the 3 billion to buy the training data they‚Äôd need to train their future coding agent models.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knqgtodvs7ze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T19:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg9jkq</id>
    <title>New SOTA music generation model</title>
    <updated>2025-05-06T16:56:14+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt; &lt;img alt="New SOTA music generation model" src="https://external-preview.redd.it/N2dybzhkY2h6NnplMUATahysLltY5LFjwkyeKdWeoWJNo8-MZQBD68gR6Fn5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54585ce0978377749da79c9379f3519be85eac15" title="New SOTA music generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ace-step is a multilingual 3.5B parameters music generation model. They released training code, LoRa training code and will release more stuff soon.&lt;/p&gt; &lt;p&gt;It supports 19 languages, instrumental styles, vocal techniques, and more.&lt;/p&gt; &lt;p&gt;I‚Äôm pretty exited because it‚Äôs really good, I never heard anything like it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://ace-step.github.io/"&gt;https://ace-step.github.io/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B"&gt;https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gf0uynfhz6ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T16:56:14+00:00</published>
  </entry>
</feed>
