<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-16T17:22:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iqehm9</id>
    <title>Multilingual creative writing ranking</title>
    <updated>2025-02-15T23:25:20+00:00</updated>
    <author>
      <name>/u/MadScientist-1214</name>
      <uri>https://old.reddit.com/user/MadScientist-1214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested various LLMs for their ability to generate creative writing in German. Here's how I conducted the evaluation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Task: Each model was asked to write a 400-word story in German&lt;/li&gt; &lt;li&gt;Evaluation: Both Claude and ChatGPT assessed each story for: &lt;ul&gt; &lt;li&gt;Language quality (grammar, vocabulary, fluency)&lt;/li&gt; &lt;li&gt;Content quality (creativity, coherence, engagement)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Testing environment: &lt;ul&gt; &lt;li&gt;Some models were tested via Huggingface Spaces: &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/CohereForAI/c4ai-command"&gt;https://huggingface.co/spaces/CohereForAI/c4ai-command&lt;/a&gt;&lt;/li&gt; &lt;li&gt;huggingface.co/chat&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Others were run locally with minor parameter tuning (temperature and min_p). And some I tested twice.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Ø Language&lt;/th&gt; &lt;th&gt;Ø Content&lt;/th&gt; &lt;th&gt;Average Ø&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;nvidia/Llama-3.1-Nemotron-70B-Instruct-HF&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;arcee-ai/SuperNova-Medius&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gghfez/Writer-Large-2411-v2.1-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;stelterlab/Mistral-Small-24B-Instruct-2501-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;google/gemma-2-27b-it&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NousResearch/Hermes-3-Llama-3.1-8B&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CohereForAI/c4ai-command-r-plus-08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R 08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;aya-expanse-32B&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistralai/Mistral-Nemo-Instruct-2407&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct-AWQ&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;c4ai-command-r-08-2024-awq&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/Gemma-2-Ataraxy-9B-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/gemma-2-9b-it-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R7B 12-2024&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Finally, I took a closer look at nvidia/Llama-3.1-Nemotron-70B-Instruct-HF, which got a perfect grammar score. While its German skills are pretty impressive, I wouldn’t quite agree with the perfect score. The model usually gets German right, but there are a couple of spots where the phrasing feels a bit off (maybe 2-3 instances in every 400 words).&lt;/p&gt; &lt;p&gt;I hope this helps anyone. If you have any other model suggestions, feel free to share them. I’d also be interested in seeing results in other languages from native speakers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadScientist-1214"&gt; /u/MadScientist-1214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:25:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqo4kt</id>
    <title>Best local vision model for technical drawings?</title>
    <updated>2025-02-16T08:50:12+00:00</updated>
    <author>
      <name>/u/Mundane_Maximum5795</name>
      <uri>https://old.reddit.com/user/Mundane_Maximum5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I think the title says it all, but maybe some context. I work for a small industrial company and we deal with technical drawings on a daily basis. One of our problems is that due to our small size we often lack the time to do some checks on customer and internal drawings before they go in production. I have played with Chatgpt and reading technical drawings and have been blown away with the quality of the analysis, but these were for completely fake drawings to ensure privacy. I have looked at different local llms to replace this, but none come even remotely close to what I need, frequently hallucinating answers. Anybody have a great model/prompt combo that works? Needs to be completely local for infosec reasons...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Maximum5795"&gt; /u/Mundane_Maximum5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqo4kt/best_local_vision_model_for_technical_drawings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T08:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqwmwj</id>
    <title>Made a game where an LLM judges your drawings as famous fictional characters</title>
    <updated>2025-02-16T16:58:10+00:00</updated>
    <author>
      <name>/u/Infrared12</name>
      <uri>https://old.reddit.com/user/Infrared12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This started as a hobby project that I intended to get done over a weekend. It was an attempt to get a quick and fun game that involved LLMs somehow. I actually had the idea floating around for a while but never got to develop it until recently.&lt;/p&gt; &lt;p&gt;Built an initial version in a ~week, played with some of our friends and they actually liked it, their feedback drove me to actually put more time and re-write the entire game, which is what you are seeing now.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img vkngr4kvbcje1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img oh51bdexbcje1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img fc3zrbqzbcje1...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infrared12"&gt; /u/Infrared12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqwmwj/made_a_game_where_an_llm_judges_your_drawings_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqwmwj/made_a_game_where_an_llm_judges_your_drawings_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqwmwj/made_a_game_where_an_llm_judges_your_drawings_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T16:58:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipvp2h</id>
    <title>LLMs make flying 1000x better</title>
    <updated>2025-02-15T06:45:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally I hate flying, internet is flaky and it's hard to get things done. I've found that i can get a lot of what I want the internet for on a local model and with the internet gone I don't get pinged and I can actually head down and focus. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T06:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqx3n7</id>
    <title>Fine Tune LLM on Unstructured Data</title>
    <updated>2025-02-16T17:17:27+00:00</updated>
    <author>
      <name>/u/West_League1850</name>
      <uri>https://old.reddit.com/user/West_League1850</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have huge amount of unstructured data specifically github repos, code samples and documentation. How do I put this data in the right format of instruction, input and output (reason for reasoning models) automatically because manually doing this will require a lot of time, since I do not have entire context of codebase and documentation. Can I use LLM to prepare this dataset for me? So, the fine tuned model will be a chatbot that provides source code and answers to questions related to code base documentation. Please do not recommend RAG since that is already done and we are looking into how well fine tuning performs compared to RAG implementation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West_League1850"&gt; /u/West_League1850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqx3n7/fine_tune_llm_on_unstructured_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqx3n7/fine_tune_llm_on_unstructured_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqx3n7/fine_tune_llm_on_unstructured_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T17:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqe6wv</id>
    <title>Have you guys tried DeepSeek-R1-Zero?</title>
    <updated>2025-02-15T23:10:59+00:00</updated>
    <author>
      <name>/u/CodeMurmurer</name>
      <uri>https://old.reddit.com/user/CodeMurmurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading R1 paper and their pure RL model DeepSeek-R1-Zero got 86.7% on AIME 2024. I wasn't able to find any service hosting the model. Deepseek-R1 got 79.8 on AIME 2024. So I was just wondering if some people here ran it locally or have found a service hosting it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeMurmurer"&gt; /u/CodeMurmurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7yea</id>
    <title>Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!</title>
    <updated>2025-02-15T18:36:22+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt; &lt;img alt="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" src="https://external-preview.redd.it/yabl__4Ab0fX56Bb4wbP-vpdHxIRx-xXjgB7Jvk4-sU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d1e1f2fe159585b0d0e6b897ae3b1557ad44856" title="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/taylorwilsdon/llm-context-limits"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T18:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipy2fg</id>
    <title>Microsoft drops OmniParser V2 - Agent that controls Windows and Browser</title>
    <updated>2025-02-15T09:45:40+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released an open source tool that acts as an Agent that controls Windows and Browser to complete tasks given through prompts.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/"&gt;https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0"&gt;https://huggingface.co/microsoft/OmniParser-v2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/microsoft/OmniParser/tree/master/omnitool"&gt;https://github.com/microsoft/OmniParser/tree/master/omnitool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0og"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv2b2</id>
    <title>Need advice on where to start: A few questions</title>
    <updated>2025-02-16T15:50:08+00:00</updated>
    <author>
      <name>/u/Gold_Hornet_923</name>
      <uri>https://old.reddit.com/user/Gold_Hornet_923</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just recently got into offline LLMs, I usually just used chat GPT to help me with everything previously. Currently I am using LM Studio with uncensored Dolphin 3 running as my model. I'm clearly a noob when it comes to this. I'm specifically looking for a model that can replace GPT for me, I want it to be able to not only help me when it comes to piracy, I want to be able to ask it questions and it give me true reliable answers, but I also want it to be able to help me with my essays and school work, and just be an overall chat companion. I understand it is a big ask and I might have to use multiple models in order to make this a reality.&lt;/p&gt; &lt;p&gt;My other question: should I be using LM Studio or a different frontend? I've heard of things like SillyTavern and was wondering what is the best, most seamless frontend. I'd really prefer all of these to be as easy as possible, but I can get my hands dirty if need be. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gold_Hornet_923"&gt; /u/Gold_Hornet_923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv2b2/need_advice_on_where_to_start_a_few_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv2b2/need_advice_on_where_to_start_a_few_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv2b2/need_advice_on_where_to_start_a_few_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvxhe</id>
    <title>Aren´t people curious about the effects of hardware on token generation speed?</title>
    <updated>2025-02-16T16:27:57+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have many people who report tokens per second (T/s) here on reddit, mostly only naming their graphics card. There can be other factors influencing T/s, that we have no overview over.&lt;/p&gt; &lt;p&gt;I tried to start an initiative to get more visibility into the influence of different kinds of hardware on token generation speed. &lt;/p&gt; &lt;p&gt;And based on very limited feedback, there is an indication that GPU's may run slower in conjnction with AMD CPU's than with their Intel couterpart.&lt;/p&gt; &lt;p&gt;Aren´t people curious about this? Wouldn't they like to know if this is the case?&lt;/p&gt; &lt;p&gt;I structured the test so I may be able to observe various other correlations, from very limited feedback. I think this could help the community a lot.&lt;/p&gt; &lt;p&gt;Please help me out here, spend 5 minutes running a small test on your system with Ollama, as specified here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvxhe/arent_people_curious_about_the_effects_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvxhe/arent_people_curious_about_the_effects_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvxhe/arent_people_curious_about_the_effects_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T16:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtyw6</id>
    <title>Langchain and Langgraph tool calling support for DeepSeek-R1</title>
    <updated>2025-02-16T14:59:34+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While working on a side project, I needed to use tool calling with DeepSeek-R1, however LangChain and LangGraph haven't supported tool calling for DeepSeek-R1 yet. So I decided to manually write some custom code to do this.&lt;/p&gt; &lt;p&gt;Posting it here to help anyone who needs it. This package also works with any newly released model available on Langchain's ChatOpenAI library (and by extension, any newly released model available on OpenAI's library) which may not have tool calling support yet by LangChain and LangGraph. Also even though DeepSeek-R1 haven't been fine-tuned for tool calling, I am observing the JSON parser method that I had employed still produces quite stable results (close to 100% accuracy) with tool calling (likely because DeepSeek-R1 is a reasoning model).&lt;/p&gt; &lt;p&gt;Please give my Github repo a star if you find this helpful and interesting. Thanks for your support!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtyw6/langchain_and_langgraph_tool_calling_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipz13t</id>
    <title>Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now</title>
    <updated>2025-02-15T10:58:27+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt; &lt;img alt="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" src="https://preview.redd.it/lz0e93q9aaje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaf4142f69cd28ee8e23da316f638a807cbb3526" title="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lz0e93q9aaje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtd15</id>
    <title>How to use Deepkseek r1 locally but with Internet for it to use?</title>
    <updated>2025-02-16T14:29:58+00:00</updated>
    <author>
      <name>/u/AssistantVisible3889</name>
      <uri>https://old.reddit.com/user/AssistantVisible3889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm writing a script for a video and i use web version of deepthink r1 to get links to facts and research data &lt;/p&gt; &lt;p&gt;I'm installing deepseek r1 using LMstudio following youtube video for a guide &lt;/p&gt; &lt;p&gt;But i failed to find any guide or video to know how to connect it to the internet &lt;/p&gt; &lt;p&gt;I have no background in coding python anything I'm just downloading it locally bcoz website takes lot of time &lt;/p&gt; &lt;p&gt;Can you please suggest me a way to connect it to internet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AssistantVisible3889"&gt; /u/AssistantVisible3889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtd15/how_to_use_deepkseek_r1_locally_but_with_internet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6ite</id>
    <title>GPT-4o reportedly just dropped on lmarena</title>
    <updated>2025-02-15T17:33:40+00:00</updated>
    <author>
      <name>/u/Worldly_Expression43</name>
      <uri>https://old.reddit.com/user/Worldly_Expression43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt; &lt;img alt="GPT-4o reportedly just dropped on lmarena" src="https://preview.redd.it/cjz352y89cje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9b527fb493206e2b7fe73cec4a70245655f39c" title="GPT-4o reportedly just dropped on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Expression43"&gt; /u/Worldly_Expression43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cjz352y89cje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T17:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvmba</id>
    <title>The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig</title>
    <updated>2025-02-16T16:14:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt; &lt;img alt="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" src="https://b.thumbs.redditmedia.com/gFgeC1rJT-bzG1Ahait3F528S102s2yDDa6EevYG4iI.jpg" title="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve wanted to build a quad 3090 server for llama.cpp/Open WebUI for a while now, but massive shrouds really hampered those efforts. There are very few blower style RTX 3090 out there. They typically cost more than RTX 4090. Experimentation with DeepSeek makes the thought of loading all those weights via x1 risers a nightmare. Already suffering with native x1 on CMP 100-210 trying to offload DeepSeek weights to 6 GPUs.&lt;/p&gt; &lt;p&gt;Also thinking with some systems with 7-8 x16 lane support, upto 32gpu on x4 is entirely possible. DeepSeek fp8 fully GPU powered on a ~$30k retail mostly build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iqvmba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T16:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqf4v2</id>
    <title>Created a gui for llama.cpp and other apis - all contained in a single html</title>
    <updated>2025-02-15T23:56:24+00:00</updated>
    <author>
      <name>/u/tar_alex</name>
      <uri>https://old.reddit.com/user/tar_alex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt; &lt;img alt="Created a gui for llama.cpp and other apis - all contained in a single html" src="https://external-preview.redd.it/Nnk1dGhwajI1ZWplMX73soshjT9KG-paaQOq0mm21JJPvLVNQkyV4_Cd4e00.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d972894ad4c00fad5f2bde2993e3727538a2c84d" title="Created a gui for llama.cpp and other apis - all contained in a single html" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tar_alex"&gt; /u/tar_alex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8g1dqnj25eje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipxszq</id>
    <title>Ridiculous</title>
    <updated>2025-02-15T09:25:02+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt; &lt;img alt="Ridiculous" src="https://preview.redd.it/95cr17p3u9je1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6000872e6551351c948ff99297bb4130600cc27d" title="Ridiculous" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95cr17p3u9je1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqmwsl</id>
    <title>I pay for chatGPT (20 USD), I specifically use the 4o model as a writing editor. For this kind of task, am I better off using a local model instead?</title>
    <updated>2025-02-16T07:21:41+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't use chatGPT for anything else beyond editing my stories, as mentioned in the title, I only use the 4o model, and I tell it to edit my writing (stories) for grammar, and help me figure out better pacing, better approaches to explain a scene. It's like having a personal editor 24/7.&lt;/p&gt; &lt;p&gt;Am I better off using a local model for this kind of task? If so which one? I've got a 8GB RTX 3070 and 32 GB of RAM.&lt;/p&gt; &lt;p&gt;I'm asking since I don't use chatGPT for anything else. I used to use it for coding and used a better model, but I recently quit programming and only need a writer editor :) &lt;/p&gt; &lt;p&gt;Any model suggestions or system prompts are more than welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T07:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh3k1</id>
    <title>Meta's Brain-to-Text AI</title>
    <updated>2025-02-16T01:35:00+00:00</updated>
    <author>
      <name>/u/Particular-Sea2005</name>
      <uri>https://old.reddit.com/user/Particular-Sea2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta's groundbreaking research, conducted in collaboration with the Basque Center on Cognition, Brain and Language, marks a significant advancement in non-invasive brain-to-text communication. The study involved 35 healthy volunteers at BCBL, using both magnetoencephalography (MEG) and electroencephalography (EEG) to record brain activity while participants typed sentences[1][2]. Researchers then trained an AI model to reconstruct these sentences solely from the recorded brain signals, achieving up to 80% accuracy in decoding characters from MEG recordings - at least twice the performance of traditional EEG systems[2].&lt;/p&gt; &lt;p&gt;This research builds upon Meta's previous work in decoding image and speech perception from brain activity, now extending to sentence production[1]. The study's success opens new possibilities for non-invasive brain-computer interfaces, potentially aiding in restoring communication for individuals who have lost the ability to speak[2]. However, challenges remain, including the need for further improvements in decoding performance and addressing the practical limitations of MEG technology, which requires subjects to remain still in a magnetically shielded room[1].&lt;/p&gt; &lt;p&gt;Sources [1] Meta announces technology that uses AI and non-invasive magnetic ... &lt;a href="https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/"&gt;https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/&lt;/a&gt; [2] Using AI to decode language from the brain and advance our ... &lt;a href="https://ai.meta.com/blog/brain-ai-research-human-communication/"&gt;https://ai.meta.com/blog/brain-ai-research-human-communication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Sea2005"&gt; /u/Particular-Sea2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T01:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqugti</id>
    <title>Kernel refinement via LLM</title>
    <updated>2025-02-16T15:22:39+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt; &lt;img alt="Kernel refinement via LLM" src="https://external-preview.redd.it/Ud40tCnkvgrTgsbDjMTILGOg7G9SqNJ-hrdg_SDxvWo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b1994afccbdb6a19230f6779589edce8ca823a5" title="Kernel refinement via LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly haven't seen this mentioned as of yet. Has left me quite speechless tbh. &lt;/p&gt; &lt;p&gt;&amp;quot;This closed-loop approach makes the code generation process better by guiding it in a different way each time. The team found that by letting this process continue for 15 minutes resulted in an improved attention kernel. &lt;/p&gt; &lt;p&gt;A bar chart showing averaged attention kernel speedup on Hopper GPU, compares the speedup of different attention kernel types between two approaches: 'PyTorch API (Flex Attention)' in orange and 'NVIDIA Workflow with DeepSeek-R1' in green. The PyTorch API maintains a baseline of 1x for all kernels, while the NVIDIA Workflow with DeepSeek-R1 achieves speedups of 1.1x for Causal Mask and Document Mask, 1.5x for Relative Positional, 1.6x for Alibi Bias and Full Mask, and 2.1x for Softcap. Figure 3. Performance of automatically generated optimized attention kernels with flex attention This workflow produced numerically correct kernels for 100% of Level-1 problems and 96% of Level-2 problems, as tested by Stanford’s KernelBench benchmark. ‌&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1gh</id>
    <title>Why we don't use RXs 7600 XT?</title>
    <updated>2025-02-16T09:57:52+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This GPU has probably cheapest VRAM out there. $330 for 16gb is crazy value, but most people use RTXs 3090 which cost ~$700 on a used market and draw significantly more power. I know that RTXs are better for other tasks, but as far as I know, only important thing in running LLMs is VRAM, especially capacity. Or there's something I don't know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp2dd</id>
    <title>I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B</title>
    <updated>2025-02-16T09:59:45+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt; &lt;img alt="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" src="https://external-preview.redd.it/dXNvamJ5ZjQ0aGplMYWu7AlJhgav8Lym1d_sC2ZIykYlrs6Ptnxf7yQvwsov.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8bdc25d398763f3bc1abe33281d773d4310de4" title="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ro798yf44hje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv5s0</id>
    <title>Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC.</title>
    <updated>2025-02-16T15:54:42+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt; &lt;img alt="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." src="https://preview.redd.it/asmx7nh0wije1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05a245ed92468ec7ad3869e7776b9c2d8b8e5f63" title="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asmx7nh0wije1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtfy9</id>
    <title>Just a bunch of H100s required</title>
    <updated>2025-02-16T14:33:57+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt; &lt;img alt="Just a bunch of H100s required" src="https://b.thumbs.redditmedia.com/CMhWZRD3a6zl90Wagyddf6mqUWPT3h6kxFjzeLVrOCc.jpg" title="Just a bunch of H100s required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6"&gt;https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpzpk</id>
    <title>8x RTX 3090 open rig</title>
    <updated>2025-02-16T11:04:58+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt; &lt;img alt="8x RTX 3090 open rig" src="https://preview.redd.it/sx3t2omvghje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8156846c180a3c1bdf1f4c1dceba69bdbf7a6a6" title="8x RTX 3090 open rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The whole length is about 65 cm. Two PSUs 1600W and 2000W 8x RTX 3090, all repasted with copper pads Amd epyc 7th gen 512 gb ram Supermicro mobo&lt;/p&gt; &lt;p&gt;Had to design and 3D print a few things. To raise the GPUs so they wouldn't touch the heatsink of the cpu or PSU. It's not a bug, it's a feature, the airflow is better! Temperatures are maximum at 80C when full load and the fans don't even run full speed.&lt;/p&gt; &lt;p&gt;4 cards connected with risers and 4 with oculink. So far the oculink connection is better, but I am not sure if it's optimal. Only pcie 4x connection to each. &lt;/p&gt; &lt;p&gt;Maybe SlimSAS for all of them would be better? &lt;/p&gt; &lt;p&gt;It runs 70B models very fast. Training is very slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sx3t2omvghje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T11:04:58+00:00</published>
  </entry>
</feed>
