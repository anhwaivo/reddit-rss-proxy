<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-30T07:24:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lnu0o0</id>
    <title>Kimi-Dev-72B - Minimum specs needed to run on a high end PC</title>
    <updated>2025-06-30T00:28:19+00:00</updated>
    <author>
      <name>/u/texrock100</name>
      <uri>https://old.reddit.com/user/texrock100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just recently watched Julian Goldie's facebook post on Kimi-dev-72b. He seemed to be saying he was running this on a PC, but the AI models are saying it takes a high end server, that costs substantially more money, to run it. Anyone have any experience or helpful input on this? &lt;/p&gt; &lt;p&gt;Thanks,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texrock100"&gt; /u/texrock100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu0o0/kimidev72b_minimum_specs_needed_to_run_on_a_high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu0o0/kimidev72b_minimum_specs_needed_to_run_on_a_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu0o0/kimidev72b_minimum_specs_needed_to_run_on_a_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln93o3</id>
    <title>Is anyone here using Llama to code websites and apps? From my experience, it sucks</title>
    <updated>2025-06-29T07:48:53+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at &lt;a href="https://www.designarena.ai/models/llama-4-maverick"&gt;some examples from Llama 4&lt;/a&gt;, it seems absolutely horrific at any kind of UI/UX. Also on this &lt;a href="https://www.designarena.ai/leaderboard"&gt;benchmark for UI/UX&lt;/a&gt;, Llama 4 Maverick and Llama 4 Scout sit in the bottom 25% when compared to toher models such as GPT, Claude, Grok, etc. &lt;/p&gt; &lt;p&gt;What would you say are Llama's strengths are there if it's not coding interfaces and design? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmz4kf</id>
    <title>Transformer ASIC 500k tokens/s</title>
    <updated>2025-06-28T22:26:25+00:00</updated>
    <author>
      <name>/u/tvmaly</name>
      <uri>https://old.reddit.com/user/tvmaly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this company in a post where they are claiming 500k tokens/s on Llama 70B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.etched.com/blog-posts/oasis"&gt;https://www.etched.com/blog-posts/oasis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Impressive if true&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tvmaly"&gt; /u/tvmaly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T22:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnf00q</id>
    <title>GUI for Writing Long Stories with LLMs?</title>
    <updated>2025-06-29T13:42:45+00:00</updated>
    <author>
      <name>/u/BlacksmithRadiant322</name>
      <uri>https://old.reddit.com/user/BlacksmithRadiant322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a GUI that can assist in writing long stories, similar to Perchance's story generator. Perchance allows you to write what happens next, generates the subsequent passage, let's you edit what it generates and automatically makes summaries of previous passages to keep everything within the context window.&lt;/p&gt; &lt;p&gt;I'm wondering if there are any similar programs with a user interface that can be connected to Ollama or another LLM to help write long, coherent stories. Any recommendations or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;The only resource about this topic that I've found is the awesome story generation github page. I haven't even been able to find a Discord server for writing enthusiasts that try using AI to help with their writing. At this pace book to movie is going to arrive before AI is capable of writing a lengthy story of any substance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlacksmithRadiant322"&gt; /u/BlacksmithRadiant322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnsgvy</id>
    <title>Build a PC or not?</title>
    <updated>2025-06-29T23:13:14+00:00</updated>
    <author>
      <name>/u/InternetBest7599</name>
      <uri>https://old.reddit.com/user/InternetBest7599</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m planning to get started with machine learning. Right now, I have an M1 Mac Mini (16GB RAM, 50GB storage left). Will it be enough? &lt;/p&gt; &lt;p&gt;Appreciate any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternetBest7599"&gt; /u/InternetBest7599 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T23:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnv75q</id>
    <title>GPU Learning and Optimization on Macbook</title>
    <updated>2025-06-30T01:28:24+00:00</updated>
    <author>
      <name>/u/Electronic-Guess-878</name>
      <uri>https://old.reddit.com/user/Electronic-Guess-878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my doubt is very simple. I wish to buy a macbook and would like to locally build and train my VLM and LLM models (mini ones).&lt;br /&gt; What are my options of frameworks etc to learn and utilise to squeeze out the compute juice for this in macOS GPU cores. Any alternative to cuda? Does JAX work alright? What are my options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Guess-878"&gt; /u/Electronic-Guess-878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnv75q/gpu_learning_and_optimization_on_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnv75q/gpu_learning_and_optimization_on_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnv75q/gpu_learning_and_optimization_on_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T01:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo0rk8</id>
    <title>Accelerated LLM Inference on AMD Instinct™ GPUs with vLLM 0.9.x and ROCm</title>
    <updated>2025-06-30T06:48:55+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo0rk8/accelerated_llm_inference_on_amd_instinct_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T06:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxml5</id>
    <title>Best Model For Text-To-Audio &amp; Voice Assistant?</title>
    <updated>2025-06-30T03:38:03+00:00</updated>
    <author>
      <name>/u/ExcogitationMG</name>
      <uri>https://old.reddit.com/user/ExcogitationMG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize if this has been asked before, or asked often but i personally couldn't find anything solid through self-research or scrolling through this reddit feed. Maybe I just don't know what i'm looking for, idk. &lt;strong&gt;Are there any GOOD local AI text to voice models that can work independently/and with a local SLM/LLM?&lt;/strong&gt; I'm really trying to give my home assistant a voice/have web articles, pdfs, and ebooks read to me. &lt;strong&gt;MUST&lt;/strong&gt; be able to run &lt;strong&gt;LOCALLY&lt;/strong&gt;. Preferably free or non-subscription payment. Thank you all in advance and I hope you all are having a good day/night.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcogitationMG"&gt; /u/ExcogitationMG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln7rll</id>
    <title>I made a writing assistant Chrome extension. Completely free with Gemini Nano.</title>
    <updated>2025-06-29T06:20:00+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt; &lt;img alt="I made a writing assistant Chrome extension. Completely free with Gemini Nano." src="https://external-preview.redd.it/aTR3azl2YzY3dDlmMRg_TmPcBoSM13pUYzKlWo7qhuAMWmP4IKxV8h55ZV-h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=966e826f016a8a652196742e287bca65c09c3a8d" title="I made a writing assistant Chrome extension. Completely free with Gemini Nano." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2f6200d67t9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T06:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lny5qy</id>
    <title>What subscription to buy?</title>
    <updated>2025-06-30T04:07:58+00:00</updated>
    <author>
      <name>/u/ConsistentStruggle82</name>
      <uri>https://old.reddit.com/user/ConsistentStruggle82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a beginner and I want to start learning about LLMs and finetuning.&lt;br /&gt; I have an old laptop with just 4 gigabytes of VRAM (RTX 2050). I can't invest in new hardware. What is currently the best rental service available for getting a decent GPU/TPU that can handle finetuning and RL for small models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsistentStruggle82"&gt; /u/ConsistentStruggle82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lny5qy/what_subscription_to_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lny5qy/what_subscription_to_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lny5qy/what_subscription_to_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T04:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnrda7</id>
    <title>GitHub - khimaros/enc: `cc`, but for english</title>
    <updated>2025-06-29T22:23:14+00:00</updated>
    <author>
      <name>/u/xhimaros</name>
      <uri>https://old.reddit.com/user/xhimaros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrda7/github_khimarosenc_cc_but_for_english/"&gt; &lt;img alt="GitHub - khimaros/enc: `cc`, but for english" src="https://external-preview.redd.it/jKqSSxi8Sioq0zv6YFa1yIx8ylabQGBwCeebx6g7SYA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ac2eba450547855e6b39f0a7ae64dba7d1197eb" title="GitHub - khimaros/enc: `cc`, but for english" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this tool &amp;quot;compiles&amp;quot; (more accurately, transpiles) english language files to any other programming language. for example &lt;code&gt;enc hello.en -o hello.py&lt;/code&gt;. there is more documentation and many examples in the repo. it is compatible (and has been tested with) llama.cpp/server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xhimaros"&gt; /u/xhimaros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/khimaros/enc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrda7/github_khimarosenc_cc_but_for_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrda7/github_khimarosenc_cc_but_for_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnin1x</id>
    <title>AI coding agents...what am I doing wrong?</title>
    <updated>2025-06-29T16:19:12+00:00</updated>
    <author>
      <name>/u/furyfuryfury</name>
      <uri>https://old.reddit.com/user/furyfuryfury</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are other people having such good luck with ai coding agents and I can't even get mine to write a simple comment block at the top of a 400 line file?&lt;/p&gt; &lt;p&gt;The common refrain is it's like having a junior engineer to pass a coding task off to...well, I've never had a junior engineer scroll 1/3rd of the way through a file and then decide it's too big for it to work with. It frequently just gets stuck in a loop reading through the file looking for where it's supposed to edit and then giving up part way through and saying it's reached a token limit. How many tokens do I need for a 300-500 line C/C++ file? Most of mine are about this big, I try to split them up if they get much bigger because even my own brain can't fathom my old 20k line files very well anymore...&lt;/p&gt; &lt;p&gt;Tell me what I'm doing wrong?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LM Studio on a Mac M4 max with 128 gigglebytes of RAM&lt;/li&gt; &lt;li&gt;Qwen3 30b A3B, supports up to 40k tokens&lt;/li&gt; &lt;li&gt;VS Code with Continue extension pointed to the local LM Studio instance (I've also tried through OpenWebUI's OpenAI endpoint in case API differences were the culprit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do I need a beefier model? Something with more tokens? Different extension? More gigglebytes? Why can't I just give it 10 million tokens if I otherwise have enough RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/furyfuryfury"&gt; /u/furyfuryfury &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T16:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnejb6</id>
    <title>What is the best open source TTS model with multi language support?</title>
    <updated>2025-06-29T13:20:44+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently developing an addon for Anki (an open source flashcard software). One part of my plan is to integrate an option to generate audio samples based on the preexisting content of the flashcards (for language learning). The point of it is using a local TTS model that doesn't require any paid services or APIs. To my knowledge the addons that are currently available for this have no option for a free version that still generate quite good audio.&lt;/p&gt; &lt;p&gt;I've looked a lot on HF but I struggle a bit to find out which models are actually suitable and versatile enough to support enough languages. My current bet would be XTTS2 due to the broad language support and its evaluation on leaderboards, but I find it to be a little &amp;quot;glitchy&amp;quot; at times.&lt;/p&gt; &lt;p&gt;I don't know if it's a good pick because it's mostly focussed on voice cloning. Could that be an issue? Do I have to think about some sort of legal concerns when using such a model? Which voice samples am I allowed to distribute to people so they can be used for voice cloning? I guess it wouldn't be user friendly to ask them to find their own 10s voice samples for generating audio.&lt;/p&gt; &lt;p&gt;So my question to my beloved local model nerds is:&lt;br /&gt; Which models have you tested and which ones would you say are the most consistent and reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnzj5e</id>
    <title>So whatever happened to d(iffuser)LLMs?</title>
    <updated>2025-06-30T05:29:09+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning, I got an E-Mail from the team behind the Mercury Coder LLM, Inception (&lt;a href="https://www.inceptionlabs.ai/"&gt;https://www.inceptionlabs.ai/&lt;/a&gt;) that basically announced a chat-focused model. Pretty neat, sent along an API example with cURL also. Simple and nice.&lt;/p&gt; &lt;p&gt;But this reminded me of dLLMs in general - they haven't really been talked a lot about lately. So I wanted to ask into the broad space: What's up? I like the idea of dLLMs being a different approach and perhaps easier to run compared to transformers. But I also understand the tech is relatively new - that is, diffusers for text rather than images.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnzj5e/so_whatever_happened_to_diffuserllms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T05:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnjw6m</id>
    <title>Prompt Smells, Just Like Code</title>
    <updated>2025-06-29T17:10:07+00:00</updated>
    <author>
      <name>/u/thesmallstar</name>
      <uri>https://old.reddit.com/user/thesmallstar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnjw6m/prompt_smells_just_like_code/"&gt; &lt;img alt="Prompt Smells, Just Like Code" src="https://external-preview.redd.it/CM_MCPik5clqILb0zxA6bpU1V0O-DrGVq9FOihM0CAc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=945d5f5ba53e3e3648dd0c5b46af70c88c067214" title="Prompt Smells, Just Like Code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know about code smells. When your code works, but it’s messy and you just know it’s going to cause pain later.&lt;/p&gt; &lt;p&gt;The same thing happens with prompts. I didn’t really think about it until I saw our LLM app getting harder and harder to tweak… and the root cause? Messy, overcomplicated prompts, complex workflows.&lt;/p&gt; &lt;p&gt;Some examples, Prompt Smell when they: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try to do five different things at once&lt;/li&gt; &lt;li&gt;Are copied all over the place with slight tweaks&lt;/li&gt; &lt;li&gt;Ask the LLM to do basic stuff your code should have handled&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s basically tech debt, just hiding in your prompts instead of your code. And without proper tests or evals, changing them feels like walking on eggshells.&lt;/p&gt; &lt;p&gt;I wrote a blog post about this. I’m calling it &lt;strong&gt;prompt smells&lt;/strong&gt; and sharing how I think we can avoid them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href="https://blog.surkar.in/prompt-smells-just-like-code"&gt;Full post here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's your take on this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thesmallstar"&gt; /u/thesmallstar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.surkar.in/prompt-smells-just-like-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnjw6m/prompt_smells_just_like_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnjw6m/prompt_smells_just_like_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T17:10:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnsax9</id>
    <title>Please convince me not to get a GPU I don't need. Can any local LLM compare with cloud models?</title>
    <updated>2025-06-29T23:05:31+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren't as strong or even as cheap as the cloud models.&lt;/p&gt; &lt;p&gt;I'm trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won't use much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T23:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnf7eo</id>
    <title>Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model</title>
    <updated>2025-06-29T13:52:29+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"&gt; &lt;img alt="Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model" src="https://preview.redd.it/cutzsrmpfv9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80c376025a4d94f4078234f25a798b979d501fb" title="Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a huge fan of Yann Lecun and follow all his work very closely, especially the world model concept which I love. And I just finished reading &lt;strong&gt;“Whole-Body Conditioned Egocentric Video Prediction” -&lt;/strong&gt; the new FAIR/Berkeley paper with Yann LeCun listed as lead author. The whole pipeline looks like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Frame codec:&lt;/strong&gt; Every past RGB frame (224 × 224) is shoved through a &lt;strong&gt;frozen Stable-Diffusion VAE&lt;/strong&gt; -&amp;gt; 32 × 32 × 4 latent grid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamics model:&lt;/strong&gt; A &lt;strong&gt;Conditional Diffusion Transformer (CDiT)&lt;/strong&gt; autoregressively predicts the &lt;em&gt;next&lt;/em&gt; latent, conditioned on a full 3-D body-pose trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visualisation:&lt;/strong&gt; The predicted latents are pushed back through the frozen VAE decoder so we can actually &lt;em&gt;see&lt;/em&gt; the roll-outs and compute LPIPS / FID.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That’s… exactly the sort of “predict the next frame” setup Yann spends entire keynotes dunking on:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;So I’m stuck with a big &lt;strong&gt;???&lt;/strong&gt; right now.&lt;/p&gt; &lt;h1&gt;Here’s why it feels contradictory&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frozen VAE or not, you’re still using a VAE.&lt;/strong&gt; If VAEs allegedly learn lousy representations, why lean on them at all -even as a codec - when V-JEPA exists? Why not learn a proper decoder on your great JEPA models?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The model&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;is&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;autoregressive.&lt;/strong&gt; Sure, the loss is ε-prediction in latent space, but at inference time you unroll it exactly like the next-token models he calls a dead end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JEPA latents are absent.&lt;/strong&gt; If V-JEPA is so much better, why not swap it in - even without a public decoder - ignite the debate, and skip the “bad” VAE entirely?&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Or am I missing something?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Does freezing the VAE magically sidesteps the “bad representation” critique?&lt;/li&gt; &lt;li&gt;Is this just an engineering placeholder until JEPA ships with decoder?&lt;/li&gt; &lt;li&gt;Is predicting latents via diffusion fundamentally different enough from next-pixel CE that it aligns with his worldview after all?&lt;/li&gt; &lt;li&gt;Or… is Yann quietly conceding that you still need a pixel-space codec (VAE, JPEG, whatever) for any practical world-model demo?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Honestly I don’t know whether this is a change in philosophy or just pragmatic glue code to get a body-conditioned world model out the door before NeurIPS deadlines. What do you all think?&lt;/p&gt; &lt;p&gt;Has anyone from FAIR hinted at a JEPA-codec drop?&lt;br /&gt; Is there a principled reason we should stop worrying about the “no VAE, no autoregression” mantra in this context?&lt;/p&gt; &lt;p&gt;I’d love to hear takes from people who’ve played with JEPA, latent diffusion, or any large-scale world-model work. Am I missing something and totally wrong, or does this paper actually mark a shift in Yann’s stance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cutzsrmpfv9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnfl21</id>
    <title>KoboldCpp v1.95 with Flux Kontext support</title>
    <updated>2025-06-29T14:09:23+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt; &lt;img alt="KoboldCpp v1.95 with Flux Kontext support" src="https://b.thumbs.redditmedia.com/Acro72oIt0wqz90aniNWfVPCfmpg8vp4szhxO44ZpMU.jpg" title="KoboldCpp v1.95 with Flux Kontext support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flux Kontext is a relatively new open weights model based on Flux that can &lt;strong&gt;edit images using natural language&lt;/strong&gt;. Easily replace backgrounds, edit text, or add extra items into your images.&lt;/p&gt; &lt;p&gt;With the release of KoboldCpp v1.95, Flux Kontext support has been added to KoboldCpp! No need for any installation or complicated workflows, just download one executable and launch with &lt;a href="https://huggingface.co/koboldcpp/kcppt/resolve/main/Flux-Kontext.kcppt"&gt;&lt;strong&gt;a ready-to-use kcppt template&lt;/strong&gt;&lt;/a&gt; (recommended at least 12gb VRAM), and you're ready to go, the necessary models will be fetched and loaded.&lt;/p&gt; &lt;p&gt;Then you can open a browser window to &lt;a href="http://localhost:5001/sdui"&gt;http://localhost:5001/sdui&lt;/a&gt;, a simple A1111 like UI.&lt;/p&gt; &lt;p&gt;Supports using up to 4 reference images. Also supports the usual inpainting, img2img, sampler settings etc. You can also load the component models individually (e.g. you can reuse the VAE or T5-XXL for Chroma, which koboldcpp also supports).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff"&gt;https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff&lt;/a&gt;&lt;/p&gt; &lt;p&gt;KoboldCpp also emulates the A1111/Forge and ComfyUI APIs so third party tools can use it as a drop in replacement.&lt;/p&gt; &lt;p&gt;This is possible thanks to the hard work of stable-diffusion.cpp contributors leejet and stduhpf.&lt;/p&gt; &lt;p&gt;P.s. Also, gemma 3n support is included in this release too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T14:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnmp98</id>
    <title>hunyuan-a13b: any news? GGUF? MLX?</title>
    <updated>2025-06-29T19:04:56+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many I’m excited about this model. We had a big thread on it, then crickets. Any news?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnmp98/hunyuana13b_any_news_gguf_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T19:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnx8js</id>
    <title>Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4</title>
    <updated>2025-06-30T03:16:29+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt; &lt;img alt="Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4" src="https://b.thumbs.redditmedia.com/SDjfig6sg4eLnygQ_8EUyGIhQK35Ih7ixM3P3g-oh_M.jpg" title="Week 2: Building a Small Language Model from Scratch(Positional Embeddings, RoPE, and Model Distillation) - June 30 - July 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h0h0kuq6fz9f1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adff81fdbb4ff3138a504a84b18e4440a6185b98"&gt;https://preview.redd.it/h0h0kuq6fz9f1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adff81fdbb4ff3138a504a84b18e4440a6185b98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m currently working on a hands-on series where I’m building a small language model from scratch. Last week was all about tokenization, embedding layers, and transformer fundamentals. This week, I’m shifting focus to something crucial but often overlooked: how transformers understand &lt;em&gt;order&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Here’s the breakdown for June 30 – July 4:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;June 30&lt;/strong&gt; – What are Positional Embeddings and why do they matter&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 1&lt;/strong&gt; – Coding sinusoidal positional embeddings from scratch&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 2&lt;/strong&gt; – A deep dive into Rotary Positional Embeddings (RoPE) and how DeepSeek uses them&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 3&lt;/strong&gt; – Implementing RoPE in code and testing it on token sequences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;July 4&lt;/strong&gt; – Bonus: Intro to model distillation, compressing large models into smaller, faster ones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each day, I’ll be sharing learnings, visuals, and code walkthroughs. The goal is to understand the concepts &lt;em&gt;and&lt;/em&gt; implement them in practice.&lt;/p&gt; &lt;p&gt;If you'd like to follow along more closely, I’m posting regular updates on LinkedIn. Feel free to connect with me there &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/"&gt;https://www.linkedin.com/in/prashant-lakhera-696119b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, questions, or suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnx8js/week_2_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnl6we</id>
    <title>According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM</title>
    <updated>2025-06-29T18:03:15+00:00</updated>
    <author>
      <name>/u/BringerOfNuance</name>
      <uri>https://old.reddit.com/user/BringerOfNuance</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"&gt; &lt;img alt="According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM" src="https://external-preview.redd.it/nvS9CwLfNaU7BwSp_zJYNRF4C9Wqsv0EQEs557kjO8Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b293bc24e9202fbe689a8234c25aeb41c48c3d15" title="According to rumors NVIDIA is planning a RTX 5070 Ti SUPER with 24GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BringerOfNuance"&gt; /u/BringerOfNuance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-also-planning-geforce-rtx-5070-ti-super-with-24gb-gddr7-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnl6we/according_to_rumors_nvidia_is_planning_a_rtx_5070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxo8y</id>
    <title>Major AI platforms will eventually have ads</title>
    <updated>2025-06-30T03:40:35+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this as a huge reason to continue advancement of local LLMs. OpenAI, Google, Microsoft, Anthropic, etc. all the big players have investors to answer to, and will eventually need to stop burning money. They will get pressured into a sustainable business model. I think Google has already lost a lot of traffic to AI search that they will try to win back. Right now, they are giving LLM access in exchange for data to train on. Eventually they will have enough that it won’t be worth it anymore. &lt;/p&gt; &lt;p&gt;Anyone else see this coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnrd1t</id>
    <title>You can just RL a model to beat any "AI detectors"</title>
    <updated>2025-06-29T22:22:55+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt; &lt;img alt="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d"&gt;https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd"&gt;https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baseline&lt;br /&gt; • Model: Llama-3.1 8B-Instruct&lt;br /&gt; • Prompt: plain &amp;quot;Write an essay about X&amp;quot;&lt;br /&gt; • Detector: ZeroGPT&lt;br /&gt; Result: 100 % AI-written&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d"&gt;https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Data&lt;br /&gt; • Synthetic dataset of 150 school-style prompts (history, literature, tech). Nothing fancy, just json lines + system prompt &amp;quot;You are a human essay writer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e"&gt;https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First training run&lt;br /&gt; After ~30 GRPO steps on a single A100:&lt;br /&gt; • ZeroGPT score drops from 100 → 42 %&lt;br /&gt; The model learned:&lt;br /&gt; Write a coherent intro&lt;br /&gt; Stuff one line of high-entropy junk&lt;br /&gt; Finish normally&lt;br /&gt; Average &amp;quot;human-ness&amp;quot; skyrockets because detector averages per-sentence scores&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75"&gt;https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch #1&lt;br /&gt; Added a gibberish classifier (tiny DistilRoBERTa) and multiplied reward by its minimum &amp;quot;clean&amp;quot; score. Junk lines now tank reward → behaviour disappears. GRPO’s beta ≈ how harshly to penalize incoherence. Set β = 0.4 and reward curve stabilized; no more oscillation between genius &amp;amp; garbage. Removed reasoning (memory constraints).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252"&gt;https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny models crush it&lt;br /&gt; Swapped in Qwen 0.5B LoRA rank 8, upped num_generations → 64.&lt;br /&gt; Result after 7 steps: best sample already at 28 % &amp;quot;human&amp;quot;. Smaller vocab seems to help leak less LM &amp;quot;signature&amp;quot; (the model learned to use lots of proper nouns to trick the detector).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2"&gt;https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;Detector bug?&lt;br /&gt; ZeroGPT sometimes marks the first half AI, second half human for the same paragraph. The RL agent locks onto that gradient and exploits it. Classifier clearly over-fits surface patterns rather than semantics&lt;/p&gt; &lt;p&gt;Single scalar feedback is enough for LMs to reverse-engineer public detectors &lt;/p&gt; &lt;p&gt;Add even a tiny auxiliary reward (gibberish, length) to stop obvious failure modes &lt;/p&gt; &lt;p&gt;Public &amp;quot;AI/Not-AI&amp;quot; classifiers are security-through-obscurity&lt;/p&gt; &lt;p&gt;Reward function: &lt;a href="https://codefile.io/f/R4O9IdGEhg"&gt;https://codefile.io/f/R4O9IdGEhg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnlxp1</id>
    <title>4x 4090 48GB inference box (I may have overdone it)</title>
    <updated>2025-06-29T18:33:40+00:00</updated>
    <author>
      <name>/u/101m4n</name>
      <uri>https://old.reddit.com/user/101m4n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt; &lt;img alt="4x 4090 48GB inference box (I may have overdone it)" src="https://external-preview.redd.it/o67J1SHcLKrQAlXicnfT20w0glJr7s4wb4-c1GOwiA8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=936572d5a67f4298cbb8ecc135d737e991ade403" title="4x 4090 48GB inference box (I may have overdone it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I discovered that 48GB 4090s were starting to show up on the western market in large numbers. I didn't think much of it at the time, but then I got my payout from the mt.gox bankruptcy filing (which has been ongoing for over 10 years now), and decided to blow a chunk of it on an inference box for local machine learning experiments.&lt;/p&gt; &lt;p&gt;After a delay receiving some of the parts (and admittedly some procrastination on my end), I've finally found the time to put the whole machine together!&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock romed8-2t motherboard (SP3)&lt;/li&gt; &lt;li&gt;32 core epyc&lt;/li&gt; &lt;li&gt;256GB 2666V memory&lt;/li&gt; &lt;li&gt;4x &amp;quot;tronizm&amp;quot; rtx 4090D 48GB modded GPUs from china&lt;/li&gt; &lt;li&gt;2x 1tb nvme (striped) for OS and local model storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The cards are very well built. I have no doubts as to their quality whatsoever. They were heavy, the heatsinks made contact with all the board level components and the shrouds were all-metal and very solid. It was almost a shame to take them apart! They were however incredibly loud. At idle, the fan sits at 30%, and at that level they are already as loud as the loudest blower cards for gaming. At full load, they are truly deafening and definitely not something you want to share space with. Hence the water-cooling.&lt;/p&gt; &lt;p&gt;There are however no full-cover waterblocks for these GPUs (they use a custom PCB), so to cool them I had to get a little creative. Corsair makes a (kinda) &lt;a href="https://www.corsair.com/uk/en/p/custom-liquid-cooling/cx-9025001-ww/icue-link-xg3-rgb-hybrid-gpu-water-block-4090-4080-cx-9025001-ww?srsltid=AfmBOopBdweqKN5Wpj6wHKLSR9SEYZmNpOpOyaFZTLLdld7hLBrg1iCg"&gt;generic block&lt;/a&gt; called the xg3. The product itself is a bit rubbish, requiring corsairs proprietary i-cue system to run the fan which is supposed to cool the components not covered by the coldplate. It's also overpriced. However these are more or less the only option here. As a side note, these &amp;quot;generic&amp;quot; blocks only work work because the mounting hole and memory layout around the core is actually standardized to some extent, something I learned during my research.&lt;/p&gt; &lt;p&gt;The cold-plate on these blocks turned out to foul one of the components near the core, so I had to modify them a bit. I also couldn't run the aforementioned fan without corsairs i-cue link nonsense and the fan and shroud were too thick anyway and would have blocked the next GPU anyway. So I removed the plastic shroud and fabricated a frame + heatsink arrangement to add some support and cooling for the VRMs and other non-core components.&lt;/p&gt; &lt;p&gt;As another side note, the marketing material for the xg3 claims that the block contains a built-in temperature sensor. However I saw no indication of a sensor anywhere when disassembling the thing. Go figure.&lt;/p&gt; &lt;p&gt;Lastly there's the case. I couldn't find a case that I liked the look of that would support three 480mm radiators, so I built something out of pine furniture board. Not the easiest or most time efficient approach, but it was fun and it does the job (fire hazard notwithstanding).&lt;/p&gt; &lt;p&gt;As for what I'll be using it for, I'll be hosting an LLM for local day-to-day usage, but I also have some more unique project ideas, some of which may show up here in time. Now that such projects won't take up resources on my regular desktop, I can afford to do a lot of things I previously couldn't!&lt;/p&gt; &lt;p&gt;P.S. If anyone has any questions or wants to replicate any of what I did here, feel free to DM me with any questions, I'm glad to help any way I can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/101m4n"&gt; /u/101m4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnlxp1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnu4zl</id>
    <title>Baidu releases ERNIE 4.5 models on huggingface</title>
    <updated>2025-06-30T00:34:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt; &lt;img alt="Baidu releases ERNIE 4.5 models on huggingface" src="https://external-preview.redd.it/Wyzo5BvQjbbXvCrrpLypEcj3XicuXWigLyl_Acs2b5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33f879948e7c84df63582ea3398eb078c0298c8e" title="Baidu releases ERNIE 4.5 models on huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp support for ERNIE 4.5 0.3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;https://github.com/ggml-org/llama.cpp/pull/14408&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vllm Ernie4.5 and Ernie4.5MoE Model Support&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20220"&gt;https://github.com/vllm-project/vllm/pull/20220&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:34:16+00:00</published>
  </entry>
</feed>
