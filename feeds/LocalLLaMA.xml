<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T15:49:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mzkfwo</id>
    <title>Do you work in this field or it's your hobby?</title>
    <updated>2025-08-25T07:57:54+00:00</updated>
    <author>
      <name>/u/Lxxtsch</name>
      <uri>https://old.reddit.com/user/Lxxtsch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I wonder how many people are interested in LLMs and similar AI stuff purely as a hobby.&lt;/p&gt; &lt;p&gt;I find myself messing with llms only because I think it's cool and don't want to be behind other people who know how and why use these tools.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1mzkfwo"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lxxtsch"&gt; /u/Lxxtsch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzkfwo/do_you_work_in_this_field_or_its_your_hobby/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz6f1</id>
    <title>Fast CUDA DFloat11 decoding kernel</title>
    <updated>2025-08-24T15:51:15+00:00</updated>
    <author>
      <name>/u/No_Dimension41</name>
      <uri>https://old.reddit.com/user/No_Dimension41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago, I came across the amazing work on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DFloat11&lt;/a&gt;, which achieves lossless output while shrinking models to 70% of their original size by compressing the exponent bits of BF16. It is a great work. However, I found a problem: it decompresses an entire tensor into VRAM, and then perform computations separately, which severely impacts the model's decoding speed. According to some &lt;a href="https://github.com/LeanModels/DFloat11/issues/7"&gt;issues&lt;/a&gt; on GitHub, it only reaches about 1/3 of the native BF16 speed. Furthermore, the author hasn't released the code for encoding the models, and the decoding kernel is provided in a nearly unreadable PTX format.&lt;/p&gt; &lt;p&gt;So, I decided to write my own implementation. I used the Huffman coding and LUT-based decoding algorithms described in their &lt;a href="https://arxiv.org/abs/2504.11651"&gt;paper&lt;/a&gt;, but I &lt;strong&gt;fused the Huffman decoding process and the GEMV operation into a single kernel&lt;/strong&gt;. This avoids unnecessary memory bandwidth overhead and dramatically speeds up decoding.&lt;/p&gt; &lt;p&gt;With a batch size of 1, my implementation can now reach about &lt;strong&gt;90% of native BF16 speed&lt;/strong&gt; on regular GPUs. On some VRAM bandwidth-constrained GPUs, like the RTX 4060 Ti, it can even &lt;strong&gt;surpass native BF16 speed&lt;/strong&gt; because the compressed weights reduce the demand on VRAM bandwidth.&lt;/p&gt; &lt;p&gt;Here's a simple benchmark for generating 256 tokens:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Raw BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Compressed BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Raw / Compressed Size&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 7B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;14.98s&lt;/td&gt; &lt;td align="left"&gt;13.02s&lt;/td&gt; &lt;td align="left"&gt;14.19 / 10.99 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;6.66s&lt;/td&gt; &lt;td align="left"&gt;7.23s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;14.11s&lt;/td&gt; &lt;td align="left"&gt;15.26 / 11.52 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;7.75s&lt;/td&gt; &lt;td align="left"&gt;8.24s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of course, there are still areas for improvement. Due to the extra padding required by the CUDA kernel's layout, the current compression rate is slightly lower than the original DFloat11, achieving around 75%-80%. Additionally, support for uncommon tensor shapes and batch sizes greater than 1 is currently limited.&lt;/p&gt; &lt;p&gt;For more information, please visit my GitHub repository: &lt;a href="https://github.com/lszxb/bf16_huffman_infer"&gt;https://github.com/lszxb/bf16_huffman_infer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Dimension41"&gt; /u/No_Dimension41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzl9tp</id>
    <title>I built Husk, a native, private, and open-source iOS client for your local models</title>
    <updated>2025-08-25T08:53:17+00:00</updated>
    <author>
      <name>/u/nathan12581</name>
      <uri>https://old.reddit.com/user/nathan12581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama a lot and wanted a really clean, polished, and native way to interact with my privately hosted models on my iPhone. While there are some great options out there, I wanted something that felt like a first-party Apple app—fast, private, and simple.&lt;/p&gt; &lt;p&gt;Husk is an open-source, Ollama-compatible app for iOS. The whole idea is to provide a beautiful and seamless experience for chatting with your models without your data ever leaving your control.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline &amp;amp; Private:&lt;/strong&gt; It's a native Ollama client. Your conversations stay on your devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optional iCloud Sync:&lt;/strong&gt; If you want, you can sync your chat history across your devices using Apple's end-to-end encryption (macOS support coming soon!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attachments:&lt;/strong&gt; You can attach text-based files to your chats (image support for multimodal models is on the roadmap!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highly Customisable:&lt;/strong&gt; You can set custom names, system prompts, and other parameters for your models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; The entire project is open-source under the MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help support me, I've put Husk on the App Store with a small fee. If you buy it, thank you so much! It directly funds continued development.&lt;/p&gt; &lt;p&gt;However, since it's fully open-source, you are more than welcome to build and install yourself from the GitHub repo. The instructions are all in the README.&lt;/p&gt; &lt;p&gt;I'm also planning to add macOS support and integrations for other model providers soon.&lt;/p&gt; &lt;p&gt;I'd love to hear what you all think! Any feedback, feature requests, or bug reports are super welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I made a native, private, open-source iOS app for Ollama. It's a paid app on the App Store to support development, but you can also build it yourself for free from the Github Repo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nathan12581"&gt; /u/nathan12581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T08:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqqy2</id>
    <title>What is the best vision Model for a consumer GPU (24GB VRAM)?</title>
    <updated>2025-08-25T13:38:35+00:00</updated>
    <author>
      <name>/u/1GewinnerTwitch</name>
      <uri>https://old.reddit.com/user/1GewinnerTwitch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best Vision Model I can run on my 4090 and what Model is a great mix between speed and quality? With all the new Models I am not really sure, I was using Qwen 2.5 VL 7b but I am unsure if there a better solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1GewinnerTwitch"&gt; /u/1GewinnerTwitch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqqy2/what_is_the_best_vision_model_for_a_consumer_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqqy2/what_is_the_best_vision_model_for_a_consumer_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqqy2/what_is_the_best_vision_model_for_a_consumer_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk2dg</id>
    <title>What in your experience is the best model with the smallest size in GB?</title>
    <updated>2025-08-25T07:32:42+00:00</updated>
    <author>
      <name>/u/Brilliant-Piece1490</name>
      <uri>https://old.reddit.com/user/Brilliant-Piece1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 4060 8gb and I am having a lot of fun testing 7b models and so on. But what is the best one in reasoning and code and so on in your experiance?(Doesn't have to be under 8gb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Piece1490"&gt; /u/Brilliant-Piece1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk2dg/what_in_your_experience_is_the_best_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzd0ik</id>
    <title>PSA: Filling those empty DIMM slots will slow down inference if you don’t have enough memory channels</title>
    <updated>2025-08-25T01:04:16+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 7900x on a x670e Pro RS mobo with 2x32GB &lt;a href="mailto:DDR5@5200"&gt;DDR5@5200&lt;/a&gt;. I really wanted to run GPT-OSS 120B with CPU moe but it wasn’t fully able to load. I obtained another pair of the same RAM (different batch, but same model/specs) and was able to run 120B, but only at 15 tk/s. I noticed that other models were slower as well. Then I realized that my RAM was running at 3600MTS as opposed to the 4800 it was at before. After digging into this issue it appears to be the grim reality with AMD AM5 boards that there isn’t much support for full throttle with DDR5 at 4 DIMMs. One would need an Intel build to get there apparently. In my case I think I’ll try to exchange for 2x48GB and sell my old RAM. &lt;/p&gt; &lt;p&gt;Does anyone know any way to use 4 slots at decent speeds and stability without buying a TR/EPYC?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T01:04:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzllf3</id>
    <title>Hardware to run Qwen3-235B-A22B-Instruct</title>
    <updated>2025-08-25T09:13:48+00:00</updated>
    <author>
      <name>/u/Sea-Replacement7541</name>
      <uri>https://old.reddit.com/user/Sea-Replacement7541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone experimented with above model and can shed some light on what the minimum hardware reqs are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Replacement7541"&gt; /u/Sea-Replacement7541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzllf3/hardware_to_run_qwen3235ba22binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza0wy</id>
    <title>Made Chatterbox TTS a bit faster again on CUDA (155it/s on 3090)</title>
    <updated>2025-08-24T22:49:20+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/rsxdalv/chatterbox/tree/faster"&gt;https://github.com/rsxdalv/chatterbox/tree/faster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous version discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/&lt;/a&gt; (hopefully most of the old questions will become obsolete)&lt;/p&gt; &lt;p&gt;Disclaimer - for batched generation in dedicated deployments Chatterbox-VLLM should be the better choice.&lt;/p&gt; &lt;p&gt;I have mostly exhausted the options for speeding up almost vanilla HF Transformers' Llama with torch. Inductor, Triton, Max Autotune, different cache sizes etc, and they are available in the codebase. In the end, manually capturing cuda-graphs was the fastest. The model should be able to run around 230 it/s with fused kernels and better code. (I was unable to remedy the kv_cache code to enable cuda graph capture with torch.compile's max autotune.) Besides the speed, the main benefit is that setting a small cache size is no longer necessary, neither are max_new_tokens important. I plan to make it compile by default to facilitate drop-in use in other projects. Since the main effort is exhausted, I will keep on updating incrementally - for example, speeding up the s3gen (which is now a bottleneck).&lt;/p&gt; &lt;h1&gt;Results for 1500 cache size with BFloat16&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:02&amp;lt;00:04, 159.15it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 2.05 seconds 156.29 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:01&amp;lt;00:03, 170.52it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 1.88 seconds 170.87 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([2, 339, 1024]) Sampling: 62%|██████▏ | 620/1000 [00:04&amp;lt;00:02, 154.58it/s] Stopping at 621 because EOS token was generated Generated 621 tokens in 4.01 seconds 154.69 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([2, 46, 1024]) Sampling: 4%|▍ | 40/1000 [00:00&amp;lt;00:05, 182.08it/s] Stopping at 41 because EOS token was generated Generated 41 tokens in 0.22 seconds 184.94 it/s &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Disabling classifier free guidance (cfg_weight=0)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 169.38it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.89 seconds 158.95 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 194.04it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.55 seconds 193.66 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([1, 338, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 182.28it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.65 seconds 182.22 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([1, 45, 1024]) Sampling: 20%|██ | 60/300 [00:00&amp;lt;00:01, 208.54it/s] Stopping at 61 because EOS token was generated Generated 61 tokens in 0.29 seconds 210.54 it/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current code example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: ChatterboxTTS, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) torch.cuda.empty_cache() return model # Most new GPUs would work the fastest with this, but not all. t3_to(model, torch.bfloat16) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, warmup&amp;quot;) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, full speed&amp;quot;) # Extra options: audio = model.generate( text, t3_params={ # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;eager&amp;quot;, # slower - default # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # speeds up set up # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-manual&amp;quot;, # fastest - default # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;eager&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor-strided&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-strided&amp;quot;, # &amp;quot;stride_length&amp;quot;: 4, # &amp;quot;strided&amp;quot; options compile &amp;lt;1-2-3-4&amp;gt; iteration steps together, which improves performance by reducing memory copying issues in torch.compile # &amp;quot;skip_when_1&amp;quot;: True, # skips Top P when it's set to 1.0 # &amp;quot;benchmark_t3&amp;quot;: True, # Synchronizes CUDA to get the real it/s } ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T22:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mztnjn</id>
    <title>Explaining the Real Reason I Started My AI Chatbot Project</title>
    <updated>2025-08-25T15:30:14+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Since I’ve been sharing my progress here for a while, I realized I never actually explained why I decided to build my own chatbot platform in the first place. So I wanted to share the story behind it — and hear your thoughts. &lt;/p&gt; &lt;p&gt;I’ve been a SillyTavern user for over a year. It’s an amazing project — powerful, flexible, and full of features. But when I tried to get some of my friends (non-devs) into it… it was a disaster. And that experience is what pushed me to start building something new. &lt;/p&gt; &lt;p&gt;Here’s what happened: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Installation&lt;br /&gt; For people without a tech background, even the first step was too much.&lt;br /&gt; “Why do I need Node.js?” “Why isn’t this working?”&lt;br /&gt; Most didn’t even make it past setup. I had to handhold every step, including setting up a local LLM. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Interface&lt;br /&gt; Once they finally got it running, they were overwhelmed. The UI is super dense, menus and sliders everywhere, with no clear explanations. Questions I got: &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;“What does this slider even do?” &lt;/p&gt; &lt;p&gt;“How do I actually start chatting with a character?” &lt;/p&gt; &lt;p&gt;“Why does the chat keep resetting?” &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Characters, models, prompts&lt;br /&gt; Total confusion. Where to find characters? How to write prompts? Which models to pick, how to run them, whether their hardware could handle it?&lt;br /&gt; One of my friends literally asked if they needed to learn Python just to talk to a chatbot. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extensions and advanced features&lt;br /&gt; Most didn’t even know extensions or agents existed. And even if they did, all the info is scattered across Discord threads. Documentation is spotty at best, and half the knowledge is just “tribal.” &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So here’s where my project comes in&lt;br /&gt; That frustration gave me an idea: what if there was a dead-simple LLM chatbot platform? Something that just runs in the browser — no GitHub setup, no config hell, no Discord archaeology. &lt;/p&gt; &lt;p&gt;You’d just: &lt;/p&gt; &lt;p&gt;Pick a model &lt;/p&gt; &lt;p&gt;Load a character &lt;/p&gt; &lt;p&gt;Maybe tweak some behavior &lt;/p&gt; &lt;p&gt;And it just works. &lt;/p&gt; &lt;p&gt;Right now, it’s just me building this solo. I’ve been sharing my development journey here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, and I’ll keep posting progress updates, demos, and breakdowns as I go. &lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts on this problem - do you see the same barriers for newcomers?&lt;br /&gt; And if anyone here wants to help test my platform (currently with unlimited tokens), just DM me and I’ll send you an invite.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz6som</id>
    <title>Almost done with the dashboard for local llama.cpp agents</title>
    <updated>2025-08-24T20:38:43+00:00</updated>
    <author>
      <name>/u/PayBetter</name>
      <uri>https://old.reddit.com/user/PayBetter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt; &lt;img alt="Almost done with the dashboard for local llama.cpp agents" src="https://b.thumbs.redditmedia.com/7LaV7Jli4Sm51VyrQaQKuYWfN3-w_vEMntHaCP24k1w.jpg" title="Almost done with the dashboard for local llama.cpp agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This won't be for sale and will be released as open source with a non commercial license. No code will be released until after the hackathon I've entered is over next month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PayBetter"&gt; /u/PayBetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz6som"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk0jr</id>
    <title>Efficiently detecting spam e-mails: can super small LLMs like Gemma 3 270M do it?</title>
    <updated>2025-08-25T07:29:32+00:00</updated>
    <author>
      <name>/u/s101c</name>
      <uri>https://old.reddit.com/user/s101c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been reiterated many times that the 270M Gemma has been created to be finetuned for specific narrow tasks and that it works wells as a classifier.&lt;/p&gt; &lt;p&gt;So here's a use-case: a website with a contact form receives human-written messages, all the conventional spam filters work, but plenty of the irrelevant messages still get through because they are copy-pasted and written by actual people.&lt;/p&gt; &lt;p&gt;Does Gemma 270M and other similar sized models effectively classify those messages as spam? Is there a reason to use bigger models for this kind of tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s101c"&gt; /u/s101c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mznzt6</id>
    <title>Testers for Seed-OSS tool calling wanted!</title>
    <updated>2025-08-25T11:33:19+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the adoption of the model architecture itself, I've added a pull request to llama.cpp to support Seed-OSS native toolcalls and reasoning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15552"&gt;https://github.com/ggml-org/llama.cpp/pull/15552&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This one has been somewhat annoying because Seed has its own toolcalling format, very similar to the infamous Qwen-Coder, so I would be grateful if someone being able to run the model at a higher quant than Q2_K_S could test it send report on any potential problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T11:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzpi3o</id>
    <title>Biased comparison of frontends</title>
    <updated>2025-08-25T12:46:08+00:00</updated>
    <author>
      <name>/u/moritzchow</name>
      <uri>https://old.reddit.com/user/moritzchow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since day 1 of my journey on using local LLMs (I jumped right in without actually trying the ChatGPT that kind of providers) I’ve been using Open-WebUI that is kind of vanilla when it comes to an Unraid server setup (Ollama + Open WebUI).&lt;/p&gt; &lt;p&gt;After going deeper into this I switched hardwares, backends, frontends, and become a little bit frustrated in the recent development of OWUI.&lt;/p&gt; &lt;p&gt;Let’s cut short (not short tbh):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open WebUI: Pros: &lt;/li&gt; &lt;li&gt;easy to use and setup on docker&lt;/li&gt; &lt;li&gt;integrated web search&lt;/li&gt; &lt;li&gt;customisation including parameters, TTS&lt;/li&gt; &lt;li&gt;WebUI to serve LLM across devices&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No native support on MCP servers (a dealbreaker for me since recent MCP development) - separate backend is required&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LM Studio: Pros:&lt;/li&gt; &lt;li&gt;one-stop solution for downloading and running local LLM on different hardwares including Apple Silicon&lt;/li&gt; &lt;li&gt;native MCP server support&lt;/li&gt; &lt;li&gt;easy to setup and run (can’t be easier tbh)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - no web search (it can be done via MCP tool tho) - no WebUI for serving LLM across devices (sad it’s almost perfect) - no plug-ins (the registration on beta channel did not work for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AnythingLLM: Pros:&lt;/li&gt; &lt;li&gt;Support Serving LLM on docker&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;AI Agent setup made easy&lt;/li&gt; &lt;li&gt;Sophisticated RAG setup&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No Serving LLM across devices if running desktop version - No customisation on using different external TTS endpoints - Agent has to be called out in each chat&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LibreChat: Pros:&lt;/li&gt; &lt;li&gt;Native support on MCP servers&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Pain in the bud in setting up&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SillyTavern Pros:&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;Sophisticated RP setting (some find it useful)&lt;/li&gt; &lt;li&gt;Extension available at ease on supporting MCP servers&lt;/li&gt; &lt;li&gt;customisable TTS setup&lt;/li&gt; &lt;li&gt;once it’s up and running you can get things out of it that no other frontends can give you&lt;/li&gt; &lt;li&gt;WebUI serving across devices is available&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Setting up docker is not the most easiest thing - setting up the rest through UI is a daunting task before things can be up and running - Seriously SillyTavern? How can it be named like that while having such full features available? I can’t even tell people I learn things through it&lt;/p&gt; &lt;p&gt;Verdict: I’m using ST now while it’s not the perfect solution and the damn silly name.&lt;/p&gt; &lt;p&gt;All the frontends tested here are quite good actually, it’s just that ST seems to offer more while meaning it’s another rabbit hole.&lt;/p&gt; &lt;p&gt;LM Studio is my go to backend + frontend for its support on different architectures including Apple Silicon (I switched to Apple from ROCm). If ever they can offer same interfaces via webUI it will be a killer.&lt;/p&gt; &lt;p&gt;Not tested much on LibreChat cuz it’s a painful setup and maintenance&lt;/p&gt; &lt;p&gt;Open WebUI started to becoming a No No for me since it’s MCPO model of supporting MCP servers&lt;/p&gt; &lt;p&gt;AnythingLLM - I’m not a big RAG user but it’s quite nice on that plus the nice interface. I just hated that I need to call the agent every new chat.&lt;/p&gt; &lt;p&gt;So to wrap up - give them a try yourself if you’re looking for different frontends. Plz let me know if you have some UI recommendations as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moritzchow"&gt; /u/moritzchow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T12:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfh73</id>
    <title>Intel Granite Rapids CPU on sale at Newegg up to 65% off MSRP</title>
    <updated>2025-08-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very good news for people who want to run the huge MoE models nowadays.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;MSRP&lt;/th&gt; &lt;th align="left"&gt;newegg&lt;/th&gt; &lt;th align="left"&gt;% off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P&lt;/td&gt; &lt;td align="left"&gt;$17800&lt;/td&gt; &lt;td align="left"&gt;$6179&lt;/td&gt; &lt;td align="left"&gt;65.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6972P&lt;/td&gt; &lt;td align="left"&gt;$14600&lt;/td&gt; &lt;td align="left"&gt;$5433.2&lt;/td&gt; &lt;td align="left"&gt;62.79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6944P&lt;/td&gt; &lt;td align="left"&gt;$6850&lt;/td&gt; &lt;td align="left"&gt;$4208&lt;/td&gt; &lt;td align="left"&gt;38.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6781P&lt;/td&gt; &lt;td align="left"&gt;$8960&lt;/td&gt; &lt;td align="left"&gt;$7590&lt;/td&gt; &lt;td align="left"&gt;15.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6761P&lt;/td&gt; &lt;td align="left"&gt;$6570&lt;/td&gt; &lt;td align="left"&gt;$6001&lt;/td&gt; &lt;td align="left"&gt;8.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6741P&lt;/td&gt; &lt;td align="left"&gt;$4421&lt;/td&gt; &lt;td align="left"&gt;$3900&lt;/td&gt; &lt;td align="left"&gt;11.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6731P&lt;/td&gt; &lt;td align="left"&gt;$2700&lt;/td&gt; &lt;td align="left"&gt;$2260.1&lt;/td&gt; &lt;td align="left"&gt;16,29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6521P&lt;/td&gt; &lt;td align="left"&gt;$1250&lt;/td&gt; &lt;td align="left"&gt;$1208.2&lt;/td&gt; &lt;td align="left"&gt;3.34%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsg6v</id>
    <title>DeepSeek V3.1 - Getting token " extreme" / "极" / "極" out of nowhere</title>
    <updated>2025-08-25T14:46:02+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with DeepSeek V3.1, and found that somehow the model likes to generate the token:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot; extreme&amp;quot; (id:15075)&lt;/li&gt; &lt;li&gt;&amp;quot;极&amp;quot; (id:2577, extreme in Simplified Chinese)&lt;/li&gt; &lt;li&gt;&amp;quot;極&amp;quot; (id:16411, extreme in Traditional Chinese)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in totally unexpected places.&lt;/p&gt; &lt;p&gt;At first I thought it was due to the extreme IQ1_S quantization that I did or some edge case with imatrix calibration dataset, but then the same issue also happened with the FP8 full precision model from Fireworks.&lt;/p&gt; &lt;p&gt;Case 1 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.Se极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.3718461990356445 }, { &amp;quot;id&amp;quot;: 1511, &amp;quot;token&amp;quot;: &amp;quot;cond&amp;quot;, &amp;quot;bytes&amp;quot;: [99,111,110,100], &amp;quot;logprob&amp;quot;: -1.5412302017211914 }, { &amp;quot;id&amp;quot;: 1957, &amp;quot;token&amp;quot;: &amp;quot; second&amp;quot;, &amp;quot;bytes&amp;quot;: [32,115,101,99,111,110,100], &amp;quot;logprob&amp;quot;: -1.9008493423461914 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 2 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.Se extreme&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 15075, &amp;quot;token&amp;quot;: &amp;quot; extreme&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109,101], &amp;quot;logprob&amp;quot;: -1.0279325246810913 }, { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.077283263206482 }, { &amp;quot;id&amp;quot;: 9189, &amp;quot;token&amp;quot;: &amp;quot; extrem&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109], &amp;quot;logprob&amp;quot;: -1.8691496849060059 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 3 (fireworks, top_k=1, temperature=1):&lt;br /&gt; Expected: V1&lt;br /&gt; Generated: V极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;logprob&amp;quot;: -0.27936283, &amp;quot;token_id&amp;quot;: 2577, &amp;quot;bytes&amp;quot;: [230,158,129] }, { &amp;quot;token&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;logprob&amp;quot;: -1.90436232, &amp;quot;token_id&amp;quot;: 19, &amp;quot;bytes&amp;quot;: [49] }, { &amp;quot;token&amp;quot;: &amp;quot;極&amp;quot;, &amp;quot;logprob&amp;quot;: -2.40436196, &amp;quot;token_id&amp;quot;: 16411, &amp;quot;bytes&amp;quot;: [230,165,181] } ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Worse still, other than these 3 cases where an extreme token was the top choice in greedy decoding, these extreme tokens are also constantly lurking as the 2nd or 3rd choice in other unexpected places as well.&lt;/p&gt; &lt;p&gt;I have done this exact eval for all the popular coding models, and this is the first time I am seeing this kind of issue. Has anyone experienced this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm5dk</id>
    <title>support interns1-mini has been merged into llama.cpp</title>
    <updated>2025-08-25T09:49:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support interns1-mini has been merged into llama.cpp" src="https://external-preview.redd.it/C4PZMcjKvXogRwaLothTEm2AuNm9c8ehdTTP3nuiquQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=136a713391edcd4645ecfc6fd874eb5f837f3b30" title="support interns1-mini has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;Intern-S1-mini&lt;/strong&gt;, a lightweight open-source multimodal reasoning model based on the same techniques as &lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;&lt;strong&gt;Intern-S1&lt;/strong&gt;&lt;/a&gt;. Built upon an 8B dense language model (Qwen3) and a 0.3B Vision encoder (InternViT), Intern-S1-mini has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1-mini to be a capable research assistant for real-world scientific applications.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini#features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt; &lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt; &lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas and protein sequences.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15412"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm677</id>
    <title>u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments.</title>
    <updated>2025-08-25T09:51:02+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt; &lt;img alt="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." src="https://external-preview.redd.it/Nm9qN2ppZGIwNWxmMYB8gfxVUG7ntLAy6UFGKU3bfv7xh4HVFM-UizvnZAOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82b54df6d326f2b562855d3069b5bdeddfccffd" title="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fork: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/RSXLV"&gt;u/RSXLV&lt;/a&gt; again, huge shoutout to you, my guy. This fork is so fast now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9txv4idb05lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a training approach I’m calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both “good” and “bad” completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on “conflict tokens.”&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn’t wreck formatting.&lt;/li&gt; &lt;li&gt;I didn’t need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious what others think, especially folks who’ve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you’d like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
