<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-27T14:06:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kwfftk</id>
    <title>AgentKit - Drop-in plugin system for AI agents and MCP servers</title>
    <updated>2025-05-27T05:28:10+00:00</updated>
    <author>
      <name>/u/atrfx</name>
      <uri>https://old.reddit.com/user/atrfx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"&gt; &lt;img alt="AgentKit - Drop-in plugin system for AI agents and MCP servers" src="https://external-preview.redd.it/zd-ZiUz7OOd-ZVTfTYwqHZdD3FQ-Zlrik2BdtQdhqMY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d2e9c474ad2b76e2777114e47d96183c9dfe273" title="AgentKit - Drop-in plugin system for AI agents and MCP servers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of rebuilding the same tools every time I started a new project, or ripping out server/agent implementation to switch solutions, so I built a lightweight plugin system that lets you drop Python files into a folder and generate requirements.txt for them, create a .env with all the relevant items, and dynamically load them into an MCP/Agent solution. It also has a CLI to check compatibility and conflicts.&lt;/p&gt; &lt;p&gt;Hope it's useful to someone else - feedback would be greatly appreciated.&lt;/p&gt; &lt;p&gt;I also converted some of my older tools into this format like a glossary lookup engine and a tool I use to send myself MacOS notifications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/batteryshark/agentkit_plugins"&gt;https://github.com/batteryshark/agentkit_plugins&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atrfx"&gt; /u/atrfx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/batteryshark/agentkit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfftk/agentkit_dropin_plugin_system_for_ai_agents_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvzkb5</id>
    <title>350k samples to match distilled R1 on *all* benchmark</title>
    <updated>2025-05-26T17:02:43+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"&gt; &lt;img alt="350k samples to match distilled R1 on *all* benchmark" src="https://preview.redd.it/fblf9e21q53f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c79dbfdfebcffa0c87fa3cb2dbcdee441fc3ade" title="350k samples to match distilled R1 on *all* benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;dataset: &lt;a href="https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts"&gt;https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts&lt;/a&gt;&lt;br /&gt; Cool project from our post training team at Hugging Face, hope you will like it! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fblf9e21q53f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvzkb5/350k_samples_to_match_distilled_r1_on_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T17:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvpwq3</id>
    <title>Deepseek v3 0526?</title>
    <updated>2025-05-26T09:09:20+00:00</updated>
    <author>
      <name>/u/Stock_Swimming_6015</name>
      <uri>https://old.reddit.com/user/Stock_Swimming_6015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt; &lt;img alt="Deepseek v3 0526?" src="https://external-preview.redd.it/fxYCW6fqdbJ5RWjh_x1fsIyj0ZtZFx8MOAvXVxIw2PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e74df95b54af72feafa558281ef5e11bc4e8a7c" title="Deepseek v3 0526?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock_Swimming_6015"&gt; /u/Stock_Swimming_6015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.unsloth.ai/basics/deepseek-v3-0526-how-to-run-locally"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvpwq3/deepseek_v3_0526/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T09:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvxn13</id>
    <title>üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2</title>
    <updated>2025-05-26T15:45:47+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt; &lt;img alt="üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" src="https://external-preview.redd.it/YRkD_4f9GG3JjS7U-VyOMhD6UqAgTs9g61YUbxvrlqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b248daf592d1e451e027b35573c081cecc63696" title="üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! üëã&lt;/p&gt; &lt;p&gt;I recently built a fully local speech-to-text system using &lt;strong&gt;NVIDIA‚Äôs Parakeet-TDT 0.6B v2&lt;/strong&gt; ‚Äî a 600M parameter ASR model capable of transcribing real-world audio &lt;strong&gt;entirely offline with GPU acceleration&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; Most ASR tools rely on cloud APIs and miss crucial formatting like punctuation or timestamps. This setup works offline, includes segment-level timestamps, and handles a range of real-world audio inputs ‚Äî like news, lyrics, and conversations.&lt;/p&gt; &lt;p&gt;üìΩÔ∏è &lt;strong&gt;Demo Video:&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Shows transcription of 3 samples ‚Äî financial news, a song, and a conversation between Jensen Huang &amp;amp; Satya Nadella.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kvxn13/video/1ho0mrnrc53f1/player"&gt;A full walkthrough of the local ASR system built with Parakeet-TDT 0.6B. Includes architecture overview and transcription demos for financial news, song lyrics, and a tech dialogue.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üß™ &lt;strong&gt;Tested On:&lt;/strong&gt;&lt;br /&gt; ‚úÖ Stock market commentary with spoken numbers&lt;br /&gt; ‚úÖ Song lyrics with punctuation and rhyme&lt;br /&gt; ‚úÖ Multi-speaker tech conversation on AI and silicon innovation&lt;/p&gt; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA Parakeet-TDT 0.6B v2 (ASR model)&lt;/li&gt; &lt;li&gt;NVIDIA NeMo Toolkit&lt;/li&gt; &lt;li&gt;PyTorch + CUDA 11.8&lt;/li&gt; &lt;li&gt;Streamlit (for local UI)&lt;/li&gt; &lt;li&gt;FFmpeg + Pydub (preprocessing)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/82jw99tvc53f1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f142584ca7752c796c8efcefa006dd7692500d9b"&gt;Flow diagram showing Local ASR using NVIDIA Parakeet-TDT with Streamlit UI, audio preprocessing, and model inference pipeline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs 100% offline (no cloud APIs required)&lt;/li&gt; &lt;li&gt;Accurate punctuation + capitalization&lt;/li&gt; &lt;li&gt;Word + segment-level timestamp support&lt;/li&gt; &lt;li&gt;Works on my local RTX 3050 Laptop GPU with CUDA 11.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìå &lt;strong&gt;Full blog + code + architecture + demo screenshots:&lt;/strong&gt;&lt;br /&gt; üîó &lt;a href="https://medium.com/towards-artificial-intelligence/%EF%B8%8F-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c"&gt;https://medium.com/towards-artificial-intelligence/Ô∏è-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SridharSampath/parakeet-asr-demo"&gt;https://github.com/SridharSampath/parakeet-asr-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üñ•Ô∏è &lt;strong&gt;Tested locally on:&lt;/strong&gt;&lt;br /&gt; NVIDIA RTX 3050 Laptop GPU + CUDA 11.8 + PyTorch&lt;/p&gt; &lt;p&gt;Would love to hear your feedback! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvxn13/offline_speechtotext_with_nvidia_parakeettdt_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T15:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhg3t</id>
    <title>What are the best vision models at the moment ?</title>
    <updated>2025-05-27T07:42:01+00:00</updated>
    <author>
      <name>/u/Wintlink-</name>
      <uri>https://old.reddit.com/user/Wintlink-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to create an app that extract data from scanned documents and photos, and I was using InterVL2.5-4b running with ollama, but I was wondering if there are better models out there ?&lt;br /&gt; What are your recommendation ?&lt;br /&gt; I wanted to try the 8b version of intervl but there is no GGUF available at the moment.&lt;br /&gt; Thank you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wintlink-"&gt; /u/Wintlink- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhg3t/what_are_the_best_vision_models_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T07:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw310h</id>
    <title>I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories</title>
    <updated>2025-05-26T19:21:25+00:00</updated>
    <author>
      <name>/u/DanielAPO</name>
      <uri>https://old.reddit.com/user/DanielAPO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"&gt; &lt;img alt="I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories" src="https://external-preview.redd.it/ZXJzMW1rY2RmNjNmMdaMStUEb5oAuu0jCl0Xw3e5m5dlVJowjoJYmTy8vqCj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae8d75bb795c12292f2f9e29616605f17169afc" title="I fine-tuned Qwen2.5-VL 7B to re-identify objects across frames and generate grounded stories" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielAPO"&gt; /u/DanielAPO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0yb58acdf63f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw310h/i_finetuned_qwen25vl_7b_to_reidentify_objects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T19:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwl974</id>
    <title>Are there any good small MoE models? Something like 8B or 6B or 4B with active 2B</title>
    <updated>2025-05-27T11:50:41+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwl974/are_there_any_good_small_moe_models_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwl974/are_there_any_good_small_moe_models_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwl974/are_there_any_good_small_moe_models_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T11:50:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwn27n</id>
    <title>FairyR1 32B / 14B</title>
    <updated>2025-05-27T13:19:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"&gt; &lt;img alt="FairyR1 32B / 14B" src="https://external-preview.redd.it/W-qV0BV1voPJhiTsOQdsGmcAlL-lVFIkzu14DCr59cA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b257239b7a4b96c72e2b478eb3665269afe6ea4" title="FairyR1 32B / 14B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/PKU-DS-LAB/fairy-r1-6834014fe8fd45bc211c6dd7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwdpey</id>
    <title>Best settings for running Qwen3-30B-A3B with llama.cpp (16GB VRAM and 64GB RAM)</title>
    <updated>2025-05-27T03:44:47+00:00</updated>
    <author>
      <name>/u/gamesntech</name>
      <uri>https://old.reddit.com/user/gamesntech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past I used to mostly configure gpu layers to fit as closely as possible on the 16GB RAM. But lately there seem to be much better options to optimize for VRAM/RAM split. Especially with MoE models? I'm currently running Q4_K_M version (about 18.1 GB in size) with 38 layers and 8k context size because I was focusing on fitting as much of the model as possible on VRAM. That runs fairly well but I want to know if there is a much better way to optimize for my configuration.&lt;/p&gt; &lt;p&gt;I would really like to see if I can run the Q8_0 (32 GB obviously) version in a way to utilize my VRAM and RAM as effectively possible and still be usable? I would also love to at least use the full 40K context if possible in this setting.&lt;/p&gt; &lt;p&gt;Lastly, for anyone experimenting with the A22B version as well, I assume it's usable with 128GB RAM? In this scenario, I'm not sure how much the 16GB VRAM can actually help.&lt;/p&gt; &lt;p&gt;Thanks for any advice in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamesntech"&gt; /u/gamesntech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwdpey/best_settings_for_running_qwen330ba3b_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T03:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwm5z0</id>
    <title>Any good way to use LM Studio API as a chat backend with anything besides OpenWebUI? Tired of ChatGPT model switching and want all local with damn web search.</title>
    <updated>2025-05-27T12:37:11+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried for hours with OpenWebUI and it doesn't see a single model I have with Lmstudio even with it loaded I lowkey just want a local web UI with web search I can use qwen 30b with and stop dealing with ChatGPT's awful model switching which just gives me wrong answers to basic questions unless I manually switch it to o4-mini for EVERY query.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwm5z0/any_good_way_to_use_lm_studio_api_as_a_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwm5z0/any_good_way_to_use_lm_studio_api_as_a_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwm5z0/any_good_way_to_use_lm_studio_api_as_a_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwnv4o</id>
    <title>Switched from a PC to Mac for LLM dev - One week Later</title>
    <updated>2025-05-27T13:54:23+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;Broke down and bought a Mac Mini - my processes run 5x faster : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exactly a week ago I tromped to the Apple Store and bought a Mac Mini M4 Pro with 24gb memory - the model they usually stock in store. I really *didn't* want to move from Windows because I've used Windows since 3.0 and while it has its annoyances, I know the platform and didn't want to stall my development to go down a rabbit hole of new platform hassles - and I'm not a Windows, Mac or Linux 'fan' - they're tools to me - I've used them all - but always thought the MacOS was the least enjoyable to use. &lt;/p&gt; &lt;p&gt;Despite my reservations I bought the thing - and a week later - I'm glad I did - it's a keeper. &lt;/p&gt; &lt;p&gt;It took about 2 hours to set up my simple-as-possible free stack. Anaconda, Ollama, VScode. Download models, build model files, and maybe an hour of cursing to adjust the code for the Mac and I was up and running. I have a few python libraries that complain a bit but still run fine - no issues there. &lt;/p&gt; &lt;p&gt;The unified memory is a game-changer. It's not like having a gamer box with multiple slots having Nvidia cards, but it fits my use-case perfectly - I need to be able to travel with it in a backpack. I run a 13b model 5x faster than my CPU-constrained MiniPC did with an 8b model. I do need to use a free Mac utility to speed my fans up to full blast when running so I don't melt my circuit boards and void my warranty - but this box is the sweet-spot for me. &lt;/p&gt; &lt;p&gt;Still not a big lover of the MacOS but it works - and the hardware and unified memory architecture jams a lot into a small package. &lt;/p&gt; &lt;p&gt;I was hesitant to make the switch because I thought it would be a hassle - but it wasn't all that bad. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwd7tg</id>
    <title>I forked llama-swap to add an ollama compatible api, so it can be a drop in replacement</title>
    <updated>2025-05-27T03:17:12+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone else who has been annoyed with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ollama&lt;/li&gt; &lt;li&gt;client programs that only support ollama for local models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I present you with &lt;a href="https://github.com/kooshi/llama-swappo"&gt;llama-swappo&lt;/a&gt;, a bastardization of the simplicity of llama-swap which adds an ollama compatible api to it.&lt;/p&gt; &lt;p&gt;This was mostly a quick hack I added for my own interests, so I don't intend to support it long term. All credit and support should go towards the original, but I'll probably set up a github action at some point to try to auto-rebase this code on top of his.&lt;/p&gt; &lt;p&gt;I offered to merge it, but he, correctly, declined based on concerns of complexity and maintenance. So, if anyone's interested, it's available, and if not, well at least it scratched my itch for the day. (Turns out Qwen3 isn't all that competent at driving the Github Copilot Agent, it gave it a good shot though)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwd7tg/i_forked_llamaswap_to_add_an_ollama_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T03:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwndsy</id>
    <title>Made app for LLM/MCP/Agent experimenation</title>
    <updated>2025-05-27T13:33:30+00:00</updated>
    <author>
      <name>/u/Gold_Ad_2201</name>
      <uri>https://old.reddit.com/user/Gold_Ad_2201</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwndsy/made_app_for_llmmcpagent_experimenation/"&gt; &lt;img alt="Made app for LLM/MCP/Agent experimenation" src="https://external-preview.redd.it/SvhIAAf7rr58Ch8QyfpquKGkkCLO_l6uEg7_aB6MIk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6399a2ecd25506ae1cf1af135c7fae535457b787" title="Made app for LLM/MCP/Agent experimenation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is app for experimenting with different AI models and MCP servers. It supports anything OpenAI-compatible - OpenAI, Google, Mistral, LM Studio, Ollama, llama.cpp.&lt;/p&gt; &lt;p&gt;It's an open-source desktop app in Go &lt;a href="https://github.com/unra73d/agent-smith"&gt;https://github.com/unra73d/agent-smith&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can select any combination of AI model/tool/agent role and experiment for your PoC/demo or maybe that would be your daily assistant.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chat with LLM model. You can change model, role, tools mid-converstaion which allows pretty neat scenarios&lt;/li&gt; &lt;li&gt;Create customized agent roles via system prompts&lt;/li&gt; &lt;li&gt;Use tools from MCP servers (both SSE and stdio)&lt;/li&gt; &lt;li&gt;Builtin tool - Lua code execution when you need model to calculate something precisely&lt;/li&gt; &lt;li&gt;Multiple chats in parallel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There is bunch of predefined roles but obviously you can configure them as you like. For example explain-to-me-like-I'm-5 agent:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/njt76bb1tb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a522284551092d8142866fbf19969e3b89e3ce4e"&gt;https://preview.redd.it/njt76bb1tb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a522284551092d8142866fbf19969e3b89e3ce4e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And agent with the role of teacher would answer completely differently - it will see that app has built in Lua interpreter, will write an actual code to calculate stuff and answer you like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u5s9fzigtb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2345a5bc320144e99ee206af43ca8edb9095871e"&gt;https://preview.redd.it/u5s9fzigtb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2345a5bc320144e99ee206af43ca8edb9095871e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Different models behave differently, and it is exactly one of the reasons I built this - to have a playground where I can freely combine different models, prompts and tools:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ukfi7evtb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5d1139fb858a452d31b1bc857937701d51231cc"&gt;https://preview.redd.it/5ukfi7evtb3f1.png?width=1668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5d1139fb858a452d31b1bc857937701d51231cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since this is a simple Go project, it is quite easy to run it:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/unra73d/agent-smith"&gt;&lt;code&gt;https://github.com/unra73d/agent-smith&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd agent-smith&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then you can either run it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;go run main.go&lt;/code&gt;&lt;/p&gt; &lt;p&gt;or build an app that you can just double-click&lt;/p&gt; &lt;p&gt;&lt;code&gt;go build main.go&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gold_Ad_2201"&gt; /u/Gold_Ad_2201 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwndsy/made_app_for_llmmcpagent_experimenation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwndsy/made_app_for_llmmcpagent_experimenation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwndsy/made_app_for_llmmcpagent_experimenation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw6akn</id>
    <title>CRAZY voice quality for uncensored roleplay, I wish it's local.</title>
    <updated>2025-05-26T21:37:18+00:00</updated>
    <author>
      <name>/u/ExplanationEqual2539</name>
      <uri>https://old.reddit.com/user/ExplanationEqual2539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Fcq85N0grk4"&gt;https://www.youtube.com/watch?v=Fcq85N0grk4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplanationEqual2539"&gt; /u/ExplanationEqual2539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw6akn/crazy_voice_quality_for_uncensored_roleplay_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T21:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwmlos</id>
    <title>mtmd : support Qwen 2.5 Omni (input audio+vision, no audio output) by ngxson ¬∑ Pull Request #13784 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-05-27T12:58:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13784"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw7n6w</id>
    <title>DIA 1B Podcast Generator - With Consistent Voices and Script Generation</title>
    <updated>2025-05-26T22:36:14+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt; &lt;img alt="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" src="https://external-preview.redd.it/NG1pdDduNDFlNzNmMUcfJmyGLBoX3HGWzWW7GBEQ5TlU9sPw-Gkkjhi-K8NK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07c6ebc8b3055e1fcfc4b4a856d4bdb99beffb3f" title="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pleased to share üêê GOATBookLM üêê... &lt;/p&gt; &lt;p&gt;A dual voice Open Source podcast generator powered by &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23narilabs&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#NariLabs&lt;/a&gt; &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23dia&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#Dia&lt;/a&gt; 1B audio model (with a little sprinkling of &lt;a href="https://www.linkedin.com/company/googledeepmind/"&gt;Google DeepMind&lt;/a&gt;'s Gemini Flash 2.5 and &lt;a href="https://www.linkedin.com/company/anthropicresearch/"&gt;Anthropic&lt;/a&gt; Sonnet 4) &lt;/p&gt; &lt;p&gt;What started as an evening playing around with a new open source audio model on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt; ended up as a week building an open source podcast generator.&lt;/p&gt; &lt;p&gt;Out of the box Dia 1B, the model powering the audio, is a rather unpredictable model, with random voices spinning up for every audio generation.&lt;/p&gt; &lt;p&gt;With a little exploration and testing I was able to fix this, and optimize the speaker dialogue format for pretty strong results.&lt;/p&gt; &lt;p&gt;Running entirely in Google colab üêê GOATBookLM üêê includes:&lt;/p&gt; &lt;p&gt;üîä Dual voice/ speaker podcast script creation from any text input file&lt;/p&gt; &lt;p&gt;üîä Full consistency in Dia 1B voices using a selection of demo cloned voices&lt;/p&gt; &lt;p&gt;üîä Full preview and regeneration of audio files (for quick corrections)&lt;/p&gt; &lt;p&gt;üîä Full final output in .wav or .mp3&lt;/p&gt; &lt;p&gt;Link to the Notebook: &lt;a href="https://github.com/smartaces/dia_podcast_generator"&gt;https://github.com/smartaces/dia_podcast_generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4ym9al41e73f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T22:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvz322</id>
    <title>Qwen 3 30B A3B is a beast for MCP/ tool use &amp; Tiny Agents + MCP @ Hugging Face! üî•</title>
    <updated>2025-05-26T16:44:22+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heya everyone, I'm VB from Hugging Face, we've been experimenting with MCP (Model Context Protocol) quite a bit recently. In our (vibe) tests, Qwen 3 30B A3B gives the best performance overall wrt size and tool calls! Seriously underrated.&lt;/p&gt; &lt;p&gt;The most recent &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12379"&gt;streamable tool calling support&lt;/a&gt; in llama.cpp makes it even more easier to use it locally for MCP. Here's how you can try it out too:&lt;/p&gt; &lt;p&gt;Step 1: Start the llama.cpp server `llama-server --jinja -fa -hf unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M -c 16384`&lt;/p&gt; &lt;p&gt;Step 2: Define an `agent.json` file w/ MCP server/s&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M&amp;quot;, &amp;quot;endpointUrl&amp;quot;: &amp;quot;http://localhost:8080/v1&amp;quot;, &amp;quot;servers&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;sse&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse&amp;quot; } } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Step 3: Run it &lt;/p&gt; &lt;pre&gt;&lt;code&gt;npx @huggingface/tiny-agents run ./local-image-gen &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details here: &lt;a href="https://github.com/Vaibhavs10/experiments-with-mcp"&gt;https://github.com/Vaibhavs10/experiments-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To make it easier for tinkerers like you, we've been experimenting around tooling for MCP and registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MCP Registry - you can now host spaces as MCP server on Hugging Face (with just one line of code): &lt;a href="https://huggingface.co/spaces?filter=mcp-server"&gt;https://huggingface.co/spaces?filter=mcp-server&lt;/a&gt; (all the spaces that are MCP compatible)&lt;/li&gt; &lt;li&gt;MCP Clients - we've created &lt;a href="https://github.com/huggingface/huggingface.js/tree/main/packages/tiny-agents"&gt;TypeScript&lt;/a&gt; and &lt;a href="https://huggingface.co/blog/python-tiny-agents"&gt;Python interfaces&lt;/a&gt; for you to experiment local and deployed models directly w/ MCP&lt;/li&gt; &lt;li&gt;MCP Course - learn more about MCP in an applied manner directly here: &lt;a href="https://huggingface.co/learn/mcp-course/en/unit0/introduction"&gt;https://huggingface.co/learn/mcp-course/en/unit0/introduction&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We're experimenting a lot more with open models, local + remote workflows for MCP, do let us know what you'd like to see. Moore so keen to hear your feedback on all!&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;VB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T16:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhr56</id>
    <title>Why LLM Agents Still Hallucinate (Even with Tool Use and Prompt Chains)</title>
    <updated>2025-05-27T08:04:06+00:00</updated>
    <author>
      <name>/u/Mountain-Insect-2153</name>
      <uri>https://old.reddit.com/user/Mountain-Insect-2153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You‚Äôd think calling external tools would ‚Äúfix‚Äù hallucinations in LLM agents, but even with tools integrated (LangChain, ReAct, etc.), the bots still confidently invent or misuse tool outputs.&lt;/p&gt; &lt;p&gt;Part of the problem is that most pipelines treat the LLM like a black box between prompt ‚Üí tool ‚Üí response. There's no consistent &lt;em&gt;reasoning checkpoint&lt;/em&gt; before the final output. So even if the tool gives the right data, the model might still mess up interpreting it or worse, hallucinate extra ‚Äúcontext‚Äù to justify a bad answer.&lt;/p&gt; &lt;p&gt;What‚Äôs missing is a self-check step before the response is finalized. Like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Did this answer follow the intended logic?&lt;/li&gt; &lt;li&gt;Did the tool result get used properly?&lt;/li&gt; &lt;li&gt;Are we sticking to domain constraints?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Without that, you're just crossing your fingers and hoping the model doesn't go rogue. This matters a ton in customer support, healthcare, or anything regulated.&lt;/p&gt; &lt;p&gt;Also, tool use is only as good as your control over &lt;em&gt;when and how&lt;/em&gt; tools are triggered. I‚Äôve seen bots misfire APIs just because the prompt hinted at it vaguely. Unless you gate tool calls with precise logic, you get weird or premature tool usage that ruins the UX.&lt;/p&gt; &lt;p&gt;Curious what others are doing to get more reliable LLM behavior around tools + reasoning. Are you layering on more verification? Custom wrappers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mountain-Insect-2153"&gt; /u/Mountain-Insect-2153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwile2</id>
    <title>Engineers who work in companies that have embraced AI coding, how has your worklife changed?</title>
    <updated>2025-05-27T09:03:54+00:00</updated>
    <author>
      <name>/u/thezachlandes</name>
      <uri>https://old.reddit.com/user/thezachlandes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on my own since just before GPT 4, so I never experienced AI in the workplace. How has the job changed? How are sprints run? Is more of your time spent reviewing pull requests? Has the pace of releases increased? Do things break more often?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thezachlandes"&gt; /u/thezachlandes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T09:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwlxvb</id>
    <title>Run qwen 30b-a3b on Android local with Alibaba MNN Chat</title>
    <updated>2025-05-27T12:26:03+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt; &lt;img alt="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" src="https://external-preview.redd.it/aGZnZW1ma2hpYjNmMebcV0-OYASONSRSOZTsoevngxFFIFBRatfx4SVyyBoC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5360d08748a08af161ba7604536a366b958cba1" title="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aafvzgkhib3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwer9z</id>
    <title>Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</title>
    <updated>2025-05-27T04:46:15+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt; &lt;img alt="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" src="https://external-preview.redd.it/Mslr5FmgDa5Wl6TVAGHIe-yyfpC8KB7GpupP6mmM8Ko.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aefa740a7c49432c821d22fe05c260150bb95bc" title="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.&lt;br /&gt; Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Haoz0206/Omni-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T04:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhw20</id>
    <title>Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.</title>
    <updated>2025-05-27T08:13:51+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easiest Setup: No python, no docker, no endless dev packages.&lt;/strong&gt; Just download it from &lt;a href="https://chromewebstore.google.com/detail/pphjdjdoclkedgiaahmiahladgcpohca?utm_source=item-share-cb"&gt;Chrome&lt;/a&gt; or my &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick"&gt;Github&lt;/a&gt; (Same with the store, just the latest release). You don't need an exe.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No privacy issue: you can check the code yourself.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless AI Integration:&lt;/strong&gt; Connect to a wide array of powerful AI models: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; Ollama, LM Studio, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud Services: several&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Connections:&lt;/strong&gt; all OpenAI compatible endpoints.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Content Interaction:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instant Summaries:&lt;/strong&gt; Get the gist of any webpage in seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextual Q&amp;amp;A:&lt;/strong&gt; Ask questions about the current page, PDFs, selected text in the notes or you can simply send the urls directly to the bot, the scrapper will give the bot context to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Web Search with scrapper:&lt;/strong&gt; Conduct context-aware searches using Google, DuckDuckGo, and Wikipedia, with the ability to fetch and analyze content from search results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Personas (system prompts):&lt;/strong&gt; Choose from 7 pre-built AI personalities (Researcher, Strategist, etc.) or create your own.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Speech (TTS):&lt;/strong&gt; Hear AI responses read aloud (supports browser TTS and integration with external services like Piper).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; You can search it (also planed to be used in RAG).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't know how to post image here, tried links, markdown links or directly upload, all failed to display. Screenshots gifs links below: &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwfp8v</id>
    <title>Used A100 80 GB Prices Don't Make Sense</title>
    <updated>2025-05-27T05:44:37+00:00</updated>
    <author>
      <name>/u/fakebizholdings</name>
      <uri>https://old.reddit.com/user/fakebizholdings</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone explain what I'm missing? The median price of the A100 80GB PCIe on eBay is $18,502 RTX 6000 Pro Blackwell cards can be purchased new for $8500. &lt;/p&gt; &lt;p&gt;What am I missing here? Is there something about the A100s that justifies the price difference? The only thing I can think of is 200w less power consumption and NVlink.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakebizholdings"&gt; /u/fakebizholdings &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwj2p2</id>
    <title>The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet</title>
    <updated>2025-05-27T09:37:08+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt; &lt;img alt="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" src="https://preview.redd.it/ls92grf5oa3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89933d9870d06458186daafb142b31f9c95830f" title="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ls92grf5oa3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T09:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwk1jm</id>
    <title>Wife isn‚Äôt home, that means H200 in the living room ;D</title>
    <updated>2025-05-27T10:40:11+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt; &lt;img alt="Wife isn‚Äôt home, that means H200 in the living room ;D" src="https://a.thumbs.redditmedia.com/CHdnIbD-SLsvZOKpoU7Rs4hqE0GREYpW_lt-IICeGd0.jpg" title="Wife isn‚Äôt home, that means H200 in the living room ;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got our H200 System, until it‚Äôs going in the datacenter next week that means localLLaMa with some extra power :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kwk1jm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T10:40:11+00:00</published>
  </entry>
</feed>
