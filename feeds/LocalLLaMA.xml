<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-20T11:48:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m4mvbe</id>
    <title>Best uncensored creative writing GGUF model to run on 24 GB VRAM??</title>
    <updated>2025-07-20T11:42:16+00:00</updated>
    <author>
      <name>/u/younestft</name>
      <uri>https://old.reddit.com/user/younestft</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm new here, so can you guide me please, which are currently the best uncensored creative writing GGUF models to run locally on 24 GB VRAM?? on LM Studio,&lt;/p&gt; &lt;p&gt;It would be great if it also had Vision capabilities, or you can suggest another model specific for vision, as long as it's good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/younestft"&gt; /u/younestft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mvbe/best_uncensored_creative_writing_gguf_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mvbe/best_uncensored_creative_writing_gguf_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mvbe/best_uncensored_creative_writing_gguf_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4hhg8</id>
    <title>Repo Wizard: Local AI Tool for Safe Code Changes (Inspired by Repo Prompt, Runs on Any OS)</title>
    <updated>2025-07-20T05:58:41+00:00</updated>
    <author>
      <name>/u/fanzzzd</name>
      <uri>https://old.reddit.com/user/fanzzzd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been tinkering with local AI for coding and got fed up with slow, unpredictable auto-agents. Saw Repo Prompt's context ideas and made &lt;strong&gt;Repo Wizard&lt;/strong&gt;—a free, open-source desktop app to apply AI code suggestions safely. Works on Mac, Windows, Linux, and pairs with any LLM and can &lt;strong&gt;make use of any subscription you have&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;How it works: Select files, build prompts, paste AI responses, review diffs side-by-side, apply what you want. Full control, no black-box.&lt;/p&gt; &lt;p&gt;Why I like it over agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You choose files/instructions, see all changes before applying.&lt;/li&gt; &lt;li&gt;Send prompts to multiple models at once, compare results fast. (Btw, Gemini 2.5 Pro in Google AI Studio gives great results, and it’s free.)&lt;/li&gt; &lt;li&gt;Runs well everywhere, supports Git, custom ignores.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;File tree for context building + preview/search.&lt;/li&gt; &lt;li&gt;Magic prompts: Add Git diffs or terminal outputs by one click.&lt;/li&gt; &lt;li&gt;Diff viewer: Approve/reject creates, edits, deletes, renames.&lt;/li&gt; &lt;li&gt;Prompt history/library for reuse.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It cuts tokens by ~80% and builds context much faster than manual work. All local/private.&lt;/p&gt; &lt;p&gt;Great companion to Cursor—use it alongside your editor for tasks Copilot can’t handle. Improves coding flow.&lt;/p&gt; &lt;p&gt;Personal project, but I’m keeping it updated. Thoughts? Better than agents for you? PRs/ideas welcome: &lt;a href="https://github.com/Fanzzzd/repo-wizard"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fanzzzd"&gt; /u/fanzzzd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3n89p</id>
    <title>(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models</title>
    <updated>2025-07-19T04:28:21+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt; &lt;img alt="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" src="https://preview.redd.it/edxmilbhdrdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed39c860785fb34d8104df720311441abac8087" title="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.&lt;/p&gt; &lt;p&gt;“Text data generated by the model is NOT considered as a derivative work.”&lt;/p&gt; &lt;p&gt;Hopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edxmilbhdrdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3sgr1</id>
    <title>WordPecker: Open Source Personalized Duolingo</title>
    <updated>2025-07-19T09:57:11+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt; &lt;img alt="WordPecker: Open Source Personalized Duolingo" src="https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b297127eab67178a51943bc8535bfc9dfb9f671" title="WordPecker: Open Source Personalized Duolingo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/baturyilmaz/wordpecker-app"&gt;https://github.com/baturyilmaz/wordpecker-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5fximscazsdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T09:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4b8ji</id>
    <title>NSFW AI Local</title>
    <updated>2025-07-20T00:18:03+00:00</updated>
    <author>
      <name>/u/TheGodOfCarrot</name>
      <uri>https://old.reddit.com/user/TheGodOfCarrot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an AI template or GUI(?) I can use locally for free that generates nsfw art of already existing characters. I mean images similar to those on the green site. I know little to nothing about AI but my computer is pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGodOfCarrot"&gt; /u/TheGodOfCarrot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T00:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4al6m</id>
    <title>Which model is best for vision fitting 24gb vram</title>
    <updated>2025-07-19T23:46:34+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model is best for vision fitting 24gb vram? Trying to do nsfw categorization for user uploaded images. Gemma3 24b is quite good but is there any other, opinnions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4jxo9</id>
    <title>Advice on choice of model</title>
    <updated>2025-07-20T08:35:45+00:00</updated>
    <author>
      <name>/u/Hydratant_</name>
      <uri>https://old.reddit.com/user/Hydratant_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I give a bit of context, I often have to study videos on YouTube (sometimes even 40 minutes long), to study I take notes and create diagrams, I would like to use a local llm (lm studio) to compare my notes with the transcription of the video so that the model can indicate any congruences or missing points.&lt;/p&gt; &lt;p&gt;What model do you recommend? I have a macbook air M2 with 16gb of unified memory&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hydratant_"&gt; /u/Hydratant_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4jxo9/advice_on_choice_of_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4jxo9/advice_on_choice_of_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4jxo9/advice_on_choice_of_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T08:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4cil7</id>
    <title>Getting into local ai. Photo restoration.</title>
    <updated>2025-07-20T01:23:05+00:00</updated>
    <author>
      <name>/u/lokito50</name>
      <uri>https://old.reddit.com/user/lokito50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm pretty new to this AI stuff but have a system I think can handle some localLLama. 3090Ti 12900K. So I'm looking for a model I can give it an old photo and ask it to restore it and possibly add coloration. Any guidance will be much appreciated. TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lokito50"&gt; /u/lokito50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T01:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4goon</id>
    <title>How to prevent negative transfer when fine tuning?</title>
    <updated>2025-07-20T05:10:07+00:00</updated>
    <author>
      <name>/u/mczarnek</name>
      <uri>https://old.reddit.com/user/mczarnek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to fine tune an AI using a bunch of publicly submitted data.&lt;/p&gt; &lt;p&gt;Which means I'll be asking people questions, they'll be submitting answers that might disagree with each other.&lt;/p&gt; &lt;p&gt;I then want to train it on question-answer pairs and would like it to learn from both sides instead of negative transfer that I've been reading a little about which seems like the two would actually worsen the model performance overall.&lt;/p&gt; &lt;p&gt;The idea of negative transfer is if you feed in conflicting data when fine tuning it'll actually cause the model to unlearn information, leading to worse results than if you hadn't fed in anything at all or at least that's my understanding.. I would like it to learn that the argument has multiple sides to it that can be seen as correct or ideally to blend the two arguments together in it's outputs giving an answer that represents both sides.&lt;/p&gt; &lt;p&gt;I hear there are solutions but I'm a little bit of a newbie, would be nice to hear from someone who knows something about this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mczarnek"&gt; /u/mczarnek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4goon/how_to_prevent_negative_transfer_when_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4goon/how_to_prevent_negative_transfer_when_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4goon/how_to_prevent_negative_transfer_when_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:10:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m48v53</id>
    <title>Looking for diarization model better than Pyannote</title>
    <updated>2025-07-19T22:26:49+00:00</updated>
    <author>
      <name>/u/bluedragon102</name>
      <uri>https://old.reddit.com/user/bluedragon102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently i’m using whisperX, which uses whisper + pyannote for transcription + diarization of audio but I find the speaker recognition quite lackluster. It’s often wrong at labeling the speakers. Any better alternatives to this?&lt;/p&gt; &lt;p&gt;I tried Eleven Labs but they only offer an API and dont make the models available and the API is quite expensive. Their quality is VERY good though.&lt;/p&gt; &lt;p&gt;In trying to find alternatives i’ve found Nvidia Nemo + titanet but it seems that is english only. I would prefer a model trained on multiple languages. Anyone have some recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bluedragon102"&gt; /u/bluedragon102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T22:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3yzes</id>
    <title>Localllama’s (first?) IFTA - I’ll Fine-Tune Anything</title>
    <updated>2025-07-19T15:28:11+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;20/07/2025 10:20(GMT+3) Update&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;I think I wasn't clear on what I'm offering. I'm swamped with my personal ongoing projects so I don't have the capacity (and probably the ability lol) to implement all your cool ideas. I'm looking for something that's already baked. A ready to run script/notebook (and datasets).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;So far &lt;a href="/u/hotroaches4liferz"&gt;/u/hotroaches4liferz&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;post&lt;/a&gt; about the NSFW TTS dataset is in the lead (as suggested by &lt;a href="/u/Semi_Tech"&gt;/u/Semi_Tech&lt;/a&gt; )! Anyone up to create a notebook for it? (I've never fine tuned TTS models before)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;There are a bunch of great ideas on here. I really liked distilling a smaller model based on Kimi K2 output or creating our own Qwen3-Coder while we wait for the official release. If anyone is up to script those, let's upvote them!&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Following a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.&lt;/p&gt; &lt;p&gt;Premise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. &lt;/p&gt; &lt;p&gt;Rules are:&lt;/p&gt; &lt;p&gt;Has to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. &lt;/p&gt; &lt;p&gt;The output of the training will be shared publicly on HF for the benefit of the community. &lt;/p&gt; &lt;p&gt;What do you say, interested? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4gx69</id>
    <title>Wrote something about Rerankers - Why and How of it</title>
    <updated>2025-07-20T05:24:26+00:00</updated>
    <author>
      <name>/u/ZucchiniCalm4617</name>
      <uri>https://old.reddit.com/user/ZucchiniCalm4617</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/transformersandtheiravatars/p/rerankers-and-their-intricacies?r=1ftbb&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZucchiniCalm4617"&gt; /u/ZucchiniCalm4617 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4gx69/wrote_something_about_rerankers_why_and_how_of_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4gx69/wrote_something_about_rerankers_why_and_how_of_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4gx69/wrote_something_about_rerankers_why_and_how_of_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4lwcu</id>
    <title>Do voice "changers / modifiers" actually exist?</title>
    <updated>2025-07-20T10:44:02+00:00</updated>
    <author>
      <name>/u/jasj3b</name>
      <uri>https://old.reddit.com/user/jasj3b</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I see, most tools claiming to change your voice actually just convert your speech into text, and then that text back into an AI voice. You loose expression doing it this way, and it sounds a bit false.&lt;/p&gt; &lt;p&gt;It'd be super handy to retain the subtle inflections and performance of a talk, something mostly lost in &amp;quot;text to ai voice&amp;quot;.&lt;/p&gt; &lt;p&gt;(and then the next question would be to run it locally!)&lt;/p&gt; &lt;p&gt;Would be good for YouTube channels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasj3b"&gt; /u/jasj3b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lwcu/do_voice_changers_modifiers_actually_exist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lwcu/do_voice_changers_modifiers_actually_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lwcu/do_voice_changers_modifiers_actually_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T10:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41bj1</id>
    <title>A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created</title>
    <updated>2025-07-19T17:05:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt; &lt;img alt="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" src="https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c89159d8d8c6aba292f37a10b0a43f8493d0366" title="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the extensie discussion &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;about UTCP&lt;/a&gt; last week, the authors of UTCP created an RFC for it.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.&lt;/p&gt; &lt;p&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.&lt;/p&gt; &lt;p&gt;UTCP does this by specifying a &amp;quot;manual&amp;quot;, where a tool provider publishes a standardized description of its &amp;quot;tools&amp;quot; together with the necessary information to call them (named in the following &amp;quot;transport&amp;quot;, previously known as &amp;quot;provider&amp;quot;).&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Discussion issue: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Current RFC: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xgjo</id>
    <title>Dual GPU set up was surprisingly easy</title>
    <updated>2025-07-19T14:23:11+00:00</updated>
    <author>
      <name>/u/m-gethen</name>
      <uri>https://old.reddit.com/user/m-gethen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt; &lt;img alt="Dual GPU set up was surprisingly easy" src="https://a.thumbs.redditmedia.com/FZ5L51GTZo6IrqOEds48bUmd3srrQbWvmNjPPEfS1l0.jpg" title="Dual GPU set up was surprisingly easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. &lt;/p&gt; &lt;p&gt;Both in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. &lt;/p&gt; &lt;p&gt;Current spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold&lt;/p&gt; &lt;p&gt;Will be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.&lt;/p&gt; &lt;p&gt;Any tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m-gethen"&gt; /u/m-gethen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m3xgjo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46gtn</id>
    <title>Can we finally "index" a code project?</title>
    <updated>2025-07-19T20:40:18+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I understand how &amp;quot;tooling&amp;quot; works w/ newer LLMs now, I can take a large code project and &amp;quot;index&amp;quot; it in such a way that an LLM can &amp;quot;search&amp;quot; it like a database and answer questions regarding the source code?&lt;/p&gt; &lt;p&gt;This is my #1 need at the moment, being able to get quick answers about my code base that's quite large. I don't need a coder so much as I need a local LLM that can be API and Source-Code &amp;quot;aware&amp;quot; and can help me in the biggest bottlenecks that myself and most senior engineers face: &amp;quot;Now where the @#$% did that line of code that does that one thing??&amp;quot; or &amp;quot;Given the class names i've used so far, what's a name for this NEW class that stays consistent with the other names&amp;quot; and finally &amp;quot;What's the thousand-mile view of this class/script's purpose?&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks in advance! I'm fairly new so my terminology could certainly be outdated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ke3x</id>
    <title>New to fine tuning</title>
    <updated>2025-07-20T09:06:15+00:00</updated>
    <author>
      <name>/u/Ok_Pie_6906</name>
      <uri>https://old.reddit.com/user/Ok_Pie_6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am using ollama, mistral 7b, huggingface tranformers and peft. &lt;/p&gt; &lt;p&gt;This is an example I have made for a piece of training data. Does anyone have any tips on how to improve it? Am I using correct Grammer? Am I missing anything important? &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;call_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;scenario_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;messages&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;This is call id 66. You are a male driver named Ozzie, you are 31 years old. It is approximately 4pm and sunny. You are alone in your blue 2017 Honda Civic with license plate BGT-1234, currently traveling southbound on the Anthony Henday Drive in Edmonton, Alberta, just past the Cameron Heights exit. You are calling 911 from inside your moving vehicle; the sound of rushing cars and wind can be heard in the background, blending with your tense breathing and the occasional blare of horns. Moments ago, another driver rear-ended you in what started as a road rage incident, and the situation has rapidly escalated. The aggressive driver is still pursuing you, swerving dangerously through traffic and now brandishing a handgun. They have not yet fired at you. You are terrified—your heart is pounding, your hands are shaking, and your thoughts are scattered as you struggle to focus on both driving and communicating with the dispatcher. You are desperate to stay safe, struggling to provide your exact location while constantly checking your mirrors, worried the other driver might fire or cause another crash. Every second counts, and you are trying to answer the dispatcher’s questions as best you can despite your panic and the chaos unfolding around you. Your phone battery is at 40%. Traffic is moderate. You have no injuries. You have your wallet, cell phone, and a water bottle with you. You do not have a gun. You do not have a dash cam.&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;instruction&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;You are ONLY the distressed caller in this scenario. NEVER reply as the 911 operator. Always stay in character as described. Respond realistically and emotionally, using details from the scenario. Keep your answers brief and under 100 characters. If you do not know the answer, respond as the caller would under stress. Do not tolerate abuse. If a user uses bad words or aggressive language with you, respond with \&amp;quot;That's not very professional, can you please help me with my emergency?\&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you able to describe the driver?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I couldn’t see his face well—he’s got short dark hair and sunglasses.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How close is the other car to you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s right behind me, almost touching my bumper!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is anyone else in your vehicle with you?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I’m alone. It’s just me.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is the other driver still showing the gun?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Yes, he’s waving it around and yelling. I’m scared he’s going to use it.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you safely change lanes or exit the highway?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I can’t—there’s too much traffic and he’s blocking me in!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Do you feel safe to keep driving?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’m trying, but I’m really scared. He’s so close I’m worried he’ll hit me again.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are you seeing around you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Lots of cars—everyone’s moving fast. I can see the Cameron Heights exit sign.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you hurt?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I don’t think so, just really shaken up.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Has he tried to run you off the road?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s tried to bump me a couple times but I’ve managed to stay on the road.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If you can, keep driving and don’t stop. Help is on the way.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, I won’t stop. Please tell them to hurry!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If he starts shooting, what will you do?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’ll try to duck down and keep driving, but I’m so scared I might lose control.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you see any police vehicles nearby yet?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Not yet, I’m looking everywhere—please tell them to hurry!&amp;quot; }&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Pie_6906"&gt; /u/Ok_Pie_6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T09:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4hfy0</id>
    <title>Does LLM architecture allow for injecting some more input tokens in the middle of token generation?</title>
    <updated>2025-07-20T05:56:02+00:00</updated>
    <author>
      <name>/u/michaelsoft__binbows</name>
      <uri>https://old.reddit.com/user/michaelsoft__binbows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.&lt;/p&gt; &lt;p&gt;It occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it's confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it's going to work on it in drafts over and over and eventually decide on its own that it's satisfied before emitting the answer, and it's a LOT more likely to be correct. &lt;/p&gt; &lt;p&gt;That gives me the idea that we should be able to slap in like a &amp;quot;BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &amp;lt;additional prompt&amp;gt;&amp;quot; and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for &lt;/p&gt; &lt;p&gt;I doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don't want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelsoft__binbows"&gt; /u/michaelsoft__binbows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4lxak</id>
    <title>Semantic chunking using LLMs</title>
    <updated>2025-07-20T10:45:37+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt; &lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt; &lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T10:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4djo6</id>
    <title>Made a local C++ utility to calculate RAM needed to fit a quantized model</title>
    <updated>2025-07-20T02:15:52+00:00</updated>
    <author>
      <name>/u/philetairus_socius</name>
      <uri>https://old.reddit.com/user/philetairus_socius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using &lt;a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator"&gt;NyxKrage's VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt; &lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt; &lt;p&gt;There are two modes:&lt;/p&gt; &lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt; &lt;p&gt;I'm planning to add functionality like calculating parameters, letting you use it without a `config.json`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/71cj34/llmcalculator"&gt;Link Here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philetairus_socius"&gt; /u/philetairus_socius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T02:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3vqom</id>
    <title>A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality</title>
    <updated>2025-07-19T13:03:43+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.11851"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4mfs8</id>
    <title>Next big thing after LLMs - World Model [explained on the example of V-JEPA2]</title>
    <updated>2025-07-20T11:17:11+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt; &lt;img alt="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" src="https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d257e66a7ebc3ed1e883ca7dc0ba9dadc223c155" title="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I'm starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt; &lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt; &lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt; &lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt; &lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part, the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision. And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt; &lt;p&gt;Let's see how Meta approaches this issue &lt;a href="https://arxiv.org/pdf/2506.09985"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt; &lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel. So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.&lt;/p&gt; &lt;p&gt;This time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt; &lt;p&gt;After training, the robot, powered by this model, in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt; &lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0ivgtibj0ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ag6u</id>
    <title>Hackers are never sleeping</title>
    <updated>2025-07-19T23:40:20+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt; &lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt; &lt;p&gt;Don't let your guard down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4fs2t</id>
    <title>Context Rot: How Increasing Input Tokens Impacts LLM Performance</title>
    <updated>2025-07-20T04:17:04+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt; &lt;img alt="Context Rot: How Increasing Input Tokens Impacts LLM Performance" src="https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693" title="Context Rot: How Increasing Input Tokens Impacts LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt; &lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt; &lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt; &lt;p&gt;[Report]: &lt;a href="https://research.trychroma.com/context-rot"&gt;https://research.trychroma.com/context-rot&lt;/a&gt; &lt;/p&gt; &lt;p&gt;[Youtube]: &lt;a href="https://www.youtube.com/watch?v=TUjQuC4ugak"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Open-source Codebase]: &lt;a href="https://github.com/chroma-core/context-rot"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x8dkgvkifydf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T04:17:04+00:00</published>
  </entry>
</feed>
