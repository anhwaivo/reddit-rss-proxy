<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-20T13:50:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lg4h5i</id>
    <title>Linkedin Scraper / Automation / Data</title>
    <updated>2025-06-20T13:49:43+00:00</updated>
    <author>
      <name>/u/Success-Dependent</name>
      <uri>https://old.reddit.com/user/Success-Dependent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, has anyone successfully made a linkedin scraper.&lt;/p&gt; &lt;p&gt;I want to scrape the linkedin of my connections and be able to do some human-in-the-loop automation with respect to posting and messaging. It doesn't have to be terribly scalable but it has to work well.- I wouldn't even mind the activity happening on an old laptop 24/7, or a workstation with a 3090.&lt;/p&gt; &lt;p&gt;I've been playing with browser-use and the web-ui using deepseek v3, but it's slow and unreliable. Local is better, but I'm open to ideas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I don't mind paying either&lt;/strong&gt;, provided I get a good quality service and I don't feel my linkedin credentials are going to get stolen.&lt;/p&gt; &lt;p&gt;Any help is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Success-Dependent"&gt; /u/Success-Dependent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4h5i/linkedin_scraper_automation_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4h5i/linkedin_scraper_automation_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4h5i/linkedin_scraper_automation_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T13:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf9wof</id>
    <title>Explain AI and MCP to a 5 year old in the 90s</title>
    <updated>2025-06-19T12:45:32+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt; &lt;img alt="Explain AI and MCP to a 5 year old in the 90s" src="https://external-preview.redd.it/64oqjh3Mi1lX6NMRJ57nKz9L5oT26BTAsIGTdNtrvn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf2b1569a37ddebb64bb52fbe85643cb9a0b4ec" title="Explain AI and MCP to a 5 year old in the 90s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf9wof"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfe33m</id>
    <title>Skywork-SWE-32B</title>
    <updated>2025-06-19T15:45:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-SWE-32B"&gt;https://huggingface.co/Skywork/Skywork-SWE-32B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Skywork-SWE-32B&lt;/em&gt;&lt;/strong&gt; is a code agent model developed by &lt;a href="https://skywork.ai/home"&gt;Skywork AI&lt;/a&gt;, specifically designed for software engineering (SWE) tasks. It demonstrates strong performance across several key metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skywork-SWE-32B attains 38.0% pass@1 accuracy on the &lt;a href="https://www.swebench.com"&gt;SWE-bench Verified&lt;/a&gt; benchmark, outperforming previous open-source SoTA &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B"&gt;Qwen2.5-Coder-32B-based&lt;/a&gt; LLMs built on the &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; agent framework.&lt;/li&gt; &lt;li&gt;When incorporated with test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SoTA results for sub-32B parameter models.&lt;/li&gt; &lt;li&gt;We clearly demonstrate the data scaling law phenomenon for software engineering capabilities in LLMs, with no signs of saturation at 8209 collected training trajectories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF is progress &lt;a href="https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF"&gt;https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfyna1</id>
    <title>Best model for a RX 6950xt?</title>
    <updated>2025-06-20T08:17:55+00:00</updated>
    <author>
      <name>/u/InvestitoreConfuso</name>
      <uri>https://old.reddit.com/user/InvestitoreConfuso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I'm currently using an Gigabyte RX 6950xt 16gb gddr6 from AMD in my main gaming rig, but i'm looking to upgrade it and i was wondering if it could be repurposed for using local AI. What model would you suggest to try? Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvestitoreConfuso"&gt; /u/InvestitoreConfuso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyna1/best_model_for_a_rx_6950xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyna1/best_model_for_a_rx_6950xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyna1/best_model_for_a_rx_6950xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T08:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg084k</id>
    <title>Good stable voice cloning and TTS with NOT much complicated installation?</title>
    <updated>2025-06-20T10:04:27+00:00</updated>
    <author>
      <name>/u/Dragonacious</name>
      <uri>https://old.reddit.com/user/Dragonacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted a good voice cloning and TTS tool so I was reading some reviews and opinions. &lt;/p&gt; &lt;p&gt;Decided to try XTTS v2 via their huggingface space demo and found their voice cloning is low quality. &lt;/p&gt; &lt;p&gt;Then tried Spark TTS and it's voice cloning is not upto mark as well. &lt;/p&gt; &lt;p&gt;Then tried Chatterbox. It is far better than those two. It's not perfect but not low quality like those two. &lt;/p&gt; &lt;p&gt;I'm confused why people say xtts v2, spark tts has amazing cloning capability? &lt;/p&gt; &lt;p&gt;Then I tried Sesame TTS after seeing the hype but sadly couldn't install it in windows. &lt;/p&gt; &lt;p&gt;Then tried Style TTS 2 and couldn't get it to install it in windows either. &lt;/p&gt; &lt;p&gt;Some TTS installations are so complicated. :/&lt;/p&gt; &lt;p&gt;Anyone can recommend a good stable voice cloning and TTS with not such complicated installation for windows? I got 12 GB Nvidia RTX 3060, 16 GB RAM, i5 12th gen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragonacious"&gt; /u/Dragonacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg084k/good_stable_voice_cloning_and_tts_with_not_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg084k/good_stable_voice_cloning_and_tts_with_not_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg084k/good_stable_voice_cloning_and_tts_with_not_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T10:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg0mqq</id>
    <title>Performance expectations question (Devstral)</title>
    <updated>2025-06-20T10:30:14+00:00</updated>
    <author>
      <name>/u/_-Carnage</name>
      <uri>https://old.reddit.com/user/_-Carnage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Started playing around last weekend with some local models (devstral small Q4) on my dev laptop and while I got some useful results it took hours. For the given task of refactoring since Vue components from options to composition API this was fine as I just left it to get in with it while I did other things. However if it's too be more generally useful I'm going to need at least a 10x performance boost 50-100x ideally. &lt;/p&gt; &lt;p&gt;I'm 90% sure the performance is limited by hardware but before spending $$$$ on something better I wanted to check the problem doesn't reside between keyboard and chair ;) &lt;/p&gt; &lt;p&gt;Laptop is powerful but wasn't built with AI in mind; kubuntu running on Intel i7 10870H, 64GB ram, Nvidia 3070 8GB vram. Initial runs on CPU only got 1.85 TPS and when I updated the GPU drivers and got 16 layers offloaded to the GPU it went up to 2.25 TPS (this very small increase is what's making me wonder if I'm perhaps missing something else in the software setup as I'd have expected a 40% GPU offload to give a bigger boost)&lt;/p&gt; &lt;p&gt;Model is Devstral small Q4, 16k context and 1k batch size. I followed a few tuning guides but they didn't make much difference. &lt;/p&gt; &lt;p&gt;Question then is: am I getting the performance you'd expect out of my hardware or have I done something wrong? &lt;/p&gt; &lt;p&gt;As a follow-up; what would be a cost effective build for running local models and getting a reasonable TPS rate with a single user. I'm thinking of a couple of options ATM; one is to sling a 5090 into my gaming rig and use that for AI as well (this was built for performance but is from the 1080 era so is likely too old and would need more than the card upgrading)&lt;/p&gt; &lt;p&gt;Second option is to build a new machine with decent spec and room to grow; so a mb (suggestions ?) which can support 2-4 cards without being hyper expensive and perhaps a second hand 3090 to start. Am I best going with AMD or Intel processor? &lt;/p&gt; &lt;p&gt;Initial budget would be about the cost of a 5090 so £2-3k is it realistic to get a system that'll do ~50 TPS on devstral for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_-Carnage"&gt; /u/_-Carnage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg0mqq/performance_expectations_question_devstral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg0mqq/performance_expectations_question_devstral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg0mqq/performance_expectations_question_devstral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T10:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lftz5s</id>
    <title>Open Discussion: Improving HTML-to-Markdown Extraction Using Local LLMs (7B/8B, llama.cpp) – Seeking Feedback on My Approach!</title>
    <updated>2025-06-20T03:28:14+00:00</updated>
    <author>
      <name>/u/coolmenu</name>
      <uri>https://old.reddit.com/user/coolmenu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;I'm working on a smarter way to convert HTML web pages to high-quality Markdown using &lt;strong&gt;local LLMs&lt;/strong&gt; (Qwen2.5-7B/8B, llama.cpp) running on consumer GPUs. My goal: outperform traditional tools like Readability or html2text on tricky websites (e.g. modern SPAs, tech blogs, and noisy sites) — and do it all &lt;em&gt;fully offline&lt;/em&gt;, without sending data to cloud APIs.&lt;/p&gt; &lt;h1&gt;Project Outline&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Core features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Website type detection:&lt;/strong&gt; My script first analyzes if a site is text-focused or media-centric (e.g. video/image/social), with structural and domain heuristics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HTML structure analysis:&lt;/strong&gt; Uses BeautifulSoup to extract candidate content areas, main titles, headings, and framework fingerprints (React, Vue, WordPress, etc).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI-powered extraction planning:&lt;/strong&gt; Local LLM generates JSON-formatted extraction strategies (selectors, noise filters, special rules) for each page, not just using static rules.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI quality scoring:&lt;/strong&gt; After Markdown extraction, the LLM scores content for completeness, readability, info value, and offers improvement advice. Low scores auto-trigger domain-specific extraction rule generation for next time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Everything is local:&lt;/strong&gt; I use llama-cpp-python with quantized GGUF models, so it runs on a 4070/4080/4090 or even a 7B model on a MacBook.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What works well?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;On standard article/news/blog pages, quality is usually “good” or “excellent” (AI assessment scores 7-9/10).&lt;/li&gt; &lt;li&gt;On tricky/modern sites (dynamic content, noisy layout, SPAs), the LLM can suggest better selectors or filters than hard-coded rules.&lt;/li&gt; &lt;li&gt;All quality metrics, extraction strategies, and improvement rules are saved as JSON/Markdown reports for review or reuse.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Issues &amp;amp; Open Questions&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;For &lt;em&gt;media-heavy&lt;/em&gt; or JavaScript-only sites, even the LLM struggles without browser rendering. Anyone have robust approaches for these?&lt;/li&gt; &lt;li&gt;The overall speed is decent (one page ≈ 10–20 sec on 4070 8G, q4_K_M), but batch processing hundreds of pages could be faster. Any tips for optimizing llama.cpp in this workflow?&lt;/li&gt; &lt;li&gt;Are there other open-source local LLM tools you’d recommend for this use case?&lt;/li&gt; &lt;li&gt;Would you find such a tool useful for personal archiving, knowledge bases, or note-taking?&lt;/li&gt; &lt;li&gt;Any recommended datasets or benchmarks for evaluating web-to-Markdown extraction quality (beyond manual review)?&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Source and Demo&lt;/h1&gt; &lt;p&gt;This is still a work-in-progress, but happy to share some code snippets or experiment results if anyone is interested.&lt;br /&gt; Would love to hear your feedback, suggestions, or experiences building similar tools!&lt;/p&gt; &lt;p&gt;&lt;em&gt;TL;DR: Building a fully local, AI-enhanced HTML-to-Markdown extractor that learns from its mistakes. Looking for advice, criticism, or fellow hackers to discuss!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coolmenu"&gt; /u/coolmenu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lftz5s/open_discussion_improving_htmltomarkdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lftz5s/open_discussion_improving_htmltomarkdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lftz5s/open_discussion_improving_htmltomarkdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T03:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfnn7b</id>
    <title>Optimized Chatterbox TTS (Up to 2-4x non-batched speedup)</title>
    <updated>2025-06-19T22:14:13+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past few weeks I've been experimenting for speed, and finally it's stable - a version that easily triples the original inference speed on my Windows machine with Nvidia 3090. I've also streamlined the torch dtype mismatch, so it does not require torch.autocast and thus using half precision is faster, lowering the VRAM requirements (I &lt;em&gt;roughly&lt;/em&gt; see 2.5GB usage)&lt;/p&gt; &lt;p&gt;Here's the updated inference code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rsxdalv/chatterbox/tree/fast"&gt;https://github.com/rsxdalv/chatterbox/tree/fast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In order to unlock the speed you need to torch.compile the generation step like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; model.t3._step_compilation_target = torch.compile( model.t3._step_compilation_target, fullgraph=True, backend=&amp;quot;cudagraphs&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And use bfloat16 for t3 to reduce memory bandwidth bottleneck:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: &amp;quot;ChatterboxTTS&amp;quot;, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) return model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even without that you should see faster speeds due to removal of CUDA synchronization and more aggressive caching, but in my case the CPU/Windows Python is too slow to fully saturate the GPU without compilation. I targetted cudagraphs to hopefully avoid all &lt;em&gt;painful requirements&lt;/em&gt; like triton and MSVC.&lt;/p&gt; &lt;p&gt;The UI code that incorporates the compilation, memory usage check, half/full precision selection and more is in TTS WebUI (as an extension):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rsxdalv/TTS-WebUI"&gt;https://github.com/rsxdalv/TTS-WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The code of the extension: &lt;a href="https://github.com/rsxdalv/extension_chatterbox"&gt;https://github.com/rsxdalv/extension_chatterbox&lt;/a&gt; ) Note - in the UI, compilation can only be done at the start (as the first generation) due to multithreading vs PyTorch: &lt;a href="https://github.com/pytorch/pytorch/issues/123177"&gt;https://github.com/pytorch/pytorch/issues/123177&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even more details:&lt;/p&gt; &lt;p&gt;After torch compilation is applied, the main bottleneck becomes memory speed. Thus, to further gain speed we can reduce the memory&lt;/p&gt; &lt;p&gt;Changes done:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prevent runtime checks in loops, cache all static embeddings, fix dtype mismatches preventing fp16, prevent cuda synchronizations, switch to StaticCache for compilation, use buffer for generated_ids in repetition_penalty_processor, check for EOS periodically, remove sliced streaming &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This also required copying the modeling_llama from Transformers to remove optimization roadblocks.&lt;/p&gt; &lt;p&gt;Numbers - these are system dependant! Thanks to user &amp;quot;a red pen&amp;quot; on TTS WebUI discord (with 5060 TI 16gb): Float32 Without Use Compilation: 57 it/s With Use Compilation: 46 it/s&lt;/p&gt; &lt;p&gt;Bfloat16: Without Use Compilation: 47 it/s With Use Compilation: &lt;strong&gt;81 it/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On my Windows PC with 3090: Float32:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:24, 38.26it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:23, 39.57it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:22, 40.80it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Float32 Compiled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:24, 37.87it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:22, 41.21it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:22, 41.07it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Float32 Compiled with Max_Cache_Len 600:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:01&amp;lt;00:07, 54.43it/s] Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:01&amp;lt;00:07, 59.87it/s] Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:01&amp;lt;00:07, 59.69it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:30, 30.56it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:25, 35.69it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:02&amp;lt;00:25, 36.31it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:13, 66.01it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:11, 78.61it/s] Estimated token count: 70 Sampling: 8%|▊ | 80/1000 [00:01&amp;lt;00:11, 78.64it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled with Max_Cache_Len 600:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:00&amp;lt;00:04, 84.08it/s] Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:00&amp;lt;00:04, 101.48it/s] Estimated token count: 70 Sampling: 16%|█▌ | 80/500 [00:00&amp;lt;00:04, 101.41it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled with Max_Cache_Len 500:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 20%|██ | 80/400 [00:01&amp;lt;00:04, 78.85it/s] Estimated token count: 70 Sampling: 20%|██ | 80/400 [00:00&amp;lt;00:03, 104.57it/s] Estimated token count: 70 Sampling: 20%|██ | 80/400 [00:00&amp;lt;00:03, 104.84it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My best result is when running via API, where it goes to 108it/s at 560 cache len:&lt;/p&gt; &lt;p&gt;``` Using chatterbox streaming with params: {'audio_prompt_path': 'voices/chatterbox/Infinity.wav', 'chunked': True, 'desired_length': 80, 'max_length': 200, 'halve_first_chunk': False, 'exaggeration': 0.8, 'cfg_weight': 0.6, 'temperature': 0.9, 'device': 'auto', 'dtype': 'bfloat16', 'cpu_offload': False, 'cache_voice': False, 'tokens_per_slice': None, 'remove_milliseconds': None, 'remove_milliseconds_start': None, 'chunk_overlap_method': 'undefined', 'seed': -1, 'use_compilation': True, 'max_new_tokens': 340, 'max_cache_len': 560}&lt;/p&gt; &lt;p&gt;Using device: cuda&lt;/p&gt; &lt;p&gt;Using cached model 'Chatterbox on cuda with torch.bfloat16' in namespace 'chatterbox'.&lt;/p&gt; &lt;p&gt;Generating chunk: Alright, imagine you have a plant that lives in the desert where there isn't a lot of water.&lt;/p&gt; &lt;p&gt;Estimated token count: 114&lt;/p&gt; &lt;p&gt;Sampling: 29%|██████████████████████▉ | 100/340 [00:00&amp;lt;00:02, 102.48it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: This plant, called a cactus, has a special body that can store water so it can survive without rain for a long time.&lt;/p&gt; &lt;p&gt;Estimated token count: 152&lt;/p&gt; &lt;p&gt;Sampling: 47%|████████████████████████████████████▋ | 160/340 [00:01&amp;lt;00:01, 108.20it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: So while other plants might need watering every day, a cactus can go for weeks without any water.&lt;/p&gt; &lt;p&gt;Estimated token count: 118&lt;/p&gt; &lt;p&gt;Sampling: 41%|████████████████████████████████ | 140/340 [00:01&amp;lt;00:01, 108.76it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: It's kind of like a squirrel storing nuts for winter, but the cactus stores water to survive hot, dry days.&lt;/p&gt; &lt;p&gt;Estimated token count: 152&lt;/p&gt; &lt;p&gt;Sampling: 41%|████████████████████████████████ | 140/340 [00:01&amp;lt;00:01, 108.89it/s]&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T22:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgfu5</id>
    <title>AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!</title>
    <updated>2025-06-19T17:18:57+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt; &lt;img alt="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" src="https://external-preview.redd.it/snRoVONGmevA0S70HKIe-_OVILdqukspGvuQ8vgG6Fg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=854af80e0d39d84623d14e840f4c075c5351100c" title="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;, it’s been a bit since my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;post&lt;/a&gt; introducing &lt;a href="https://lemonade-server.ai"&gt;Lemonade Server&lt;/a&gt;, AMD’s open-source local LLM server that prioritizes NPU and GPU acceleration.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to sincerely thank the community here for all the feedback on that post! It’s time for an update, and I hope you’ll agree we took the feedback to heart and did our best to deliver.&lt;/p&gt; &lt;p&gt;The biggest changes since the last post are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;🦙Added llama.cpp, GGUF, and Vulkan support as an additional backend alongside ONNX. This adds support for: A) GPU acceleration on Ryzen™ AI 7000/8000/300, Radeon™ 7000/9000, and many other device families. B) Tons of new models, including VLMs.&lt;/li&gt; &lt;li&gt;🐧Ubuntu is now a fully supported operating system for llama.cpp+GGUF+Vulkan (GPU)+CPU, as well as ONNX+CPU.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;ONNX+NPU support in Linux, as well as NPU support in llama.cpp, are a work in progress.&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;💻Added a web app for model management (list/install/delete models) and basic LLM chat. Open it by pointing your browser at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; while the server is running.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;🤖Added support for streaming tool calling (all backends) and demonstrated it in our &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/local-tiny-agents--mcp-agents-on-ryzen-ai-with-lemonade-server.html"&gt;MCP + tiny-agents blog post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;✨Polished overall look and feel: new getting started website at &lt;a href="https://lemonade-server.ai"&gt;https://lemonade-server.ai&lt;/a&gt;, install in under 2 minutes, and server launches in under 2 seconds.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With the added support for Ubuntu and llama.cpp, Lemonade Server should give great performance on many more PCs than it did 2 months ago. The team here at AMD would be very grateful if y'all could try it out with your favorite apps (I like Open WebUI) and give us another round of feedback. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfgfu5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfjmx4</id>
    <title>We Tested Apple's On-Device Model for RAG Task</title>
    <updated>2025-06-19T19:25:13+00:00</updated>
    <author>
      <name>/u/No_Salamander1882</name>
      <uri>https://old.reddit.com/user/No_Salamander1882</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We tested Apple’s on-device model (using &lt;a href="https://github.com/gety-ai/apple-on-device-openai"&gt;this project&lt;/a&gt; to turn the Apple foundation model framework into an OpenAI-compatible API) by applying our RAG evaluation framework to a set of 1000 questions.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The Good:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;8.5/10 factual accuracy&lt;/strong&gt; on questions it decides to answer (on par with best small models like Qwen3 4B and IBM Granite 3.3 2B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~30 tokens/second&lt;/strong&gt; on M3 MacBook Air (16GB)&lt;/li&gt; &lt;li&gt;Strong context adherence (doesn't hallucinate much)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Concerning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;45% incorrect rejection rate&lt;/strong&gt; (refuses to answer when it actually has the info)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;90% rejection rate&lt;/strong&gt; if you add &amp;quot;Answer the question based on search result&amp;quot; to system prompt&lt;/li&gt; &lt;li&gt;Won't elaborate or ask clarifying questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Weird:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Guardrails flag questions as &amp;quot;unsafe&amp;quot; (22/1000, mostly medical topics)&lt;/li&gt; &lt;li&gt;Adopts the vocabulary/tone from your query in its responses&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We tested Apple's model as a &lt;strong&gt;summarizer in a RAG system&lt;/strong&gt;. The setup: model receives a user query plus 2-5 search result chunks (512 tokens max each) and must synthesize them into an accurate answer.&lt;/p&gt; &lt;p&gt;We used our &lt;a href="https://github.com/aizip/Rag-Eval-flow"&gt;RED-flow evaluation framework&lt;/a&gt; designed for testing small language models in RAG tasks. 1000 questions from policy documents, technical manuals, and other domains, testing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can it find and synthesize answers from the chunks?&lt;/li&gt; &lt;li&gt;Does it recognize when chunks lack sufficient info?&lt;/li&gt; &lt;li&gt;Will it ask helpful clarification questions?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The same evaluation runs in our &lt;a href="https://huggingface.co/spaces/aizip-dev/SLM-RAG-Arena"&gt;SLM RAG Arena&lt;/a&gt; where you can see how different models handle these challenges on the leaderboard.&lt;/p&gt; &lt;h1&gt;Detailed Observations&lt;/h1&gt; &lt;h1&gt;1. It's REALLY Sensitive to Instructions&lt;/h1&gt; &lt;p&gt;The system prompt dramatically affects performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No system prompt (default): 45% incorrect rejections&lt;/li&gt; &lt;li&gt;Add &amp;quot;Answer the question based on search result&amp;quot;: 90% incorrect rejections&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This suggests Apple's model is actually quite capable but becomes overly cautious with certain instructions.&lt;/p&gt; &lt;h1&gt;2. Guardrails Are... Interesting&lt;/h1&gt; &lt;p&gt;The model has guardrails that operate above the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;22/1000 queries flagged as &amp;quot;unsafe&amp;quot; with zero explanation&lt;/li&gt; &lt;li&gt;Most rejected topics are medical-related&lt;/li&gt; &lt;li&gt;Inconsistent between runs (same question might pass or fail)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Minimal Responses, No Discussion&lt;/h1&gt; &lt;p&gt;Unlike other SLMs or the reference model DeepSeek v3, this model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Won't elaborate even when prompted&lt;/li&gt; &lt;li&gt;Never asks clarifying questions&lt;/li&gt; &lt;li&gt;Just outputs &amp;quot;I cannot answer that&amp;quot; instead of explaining why&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with insufficient context:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: If I eat a rainbow and drink the sky, will my risks go bye-bye? [cancer risk context] Apple Model: I cannot answer that. Reference (DeepSeek v3): I understand you're asking about reducing health risks, but your question uses metaphorical language that I can't directly interpret from the provided context about cancer risk factors. Could you clarify what specific dietary or lifestyle changes you're asking about? For example, are you interested in colorful fruits and vegetables (&amp;quot;rainbow&amp;quot;) or hydration (&amp;quot;sky&amp;quot;) and their effects on cancer risk? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even when it has the information, responses are minimal:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: How is the urban core defined for statistical purposes? Apple Model: The urban core is defined as a geographic area with a population of 50,000 or more. Reference (DeepSeek v3): The urban core for statistical purposes is defined as the central urban area within a Metropolitan Statistical Area (MSA) that has a population of 50,000 or more. This core urban area is the basis for delineating the MSA, which includes the counties containing the core urban area and any adjacent counties with a high degree of social and economic integration (measured by commuting to work) with the urban core. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Correct but minimal.&lt;/p&gt; &lt;h1&gt;4. Surprisingly Good at Long Context&lt;/h1&gt; &lt;p&gt;No degradation with longer contexts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Hard&amp;quot; questions (requiring multiple chunks) score just as well&lt;/li&gt; &lt;li&gt;No &amp;quot;context spiral&amp;quot; or coherence loss&lt;/li&gt; &lt;li&gt;Suggests good attention mechanisms for a 3B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Personality Mirroring&lt;/h1&gt; &lt;p&gt;The model adopts your vocabulary and tone. If you use informal language in your query, it responds with the same style:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: Who do you tell if you see something sketchy with a work vehicle? Apple Model: If you see something sketchy with a work vehicle, you should notify the Office of Inspector General (OIG) and the DOT Fleet Manager. Reference (DeepSeek v3): If you observe apparent unofficial use or misuse of a Government motor vehicle, you should notify the Office of the Inspector General (OIG) and the DOT Fleet Manager. The OIG will then coordinate with the DOT Fleet Manager to conduct an internal investigation to determine whether a violation has occurred. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice how Apple's model picks up and reuses &amp;quot;sketchy&amp;quot; instead of using more formal language like &amp;quot;suspicious&amp;quot; or &amp;quot;apparent unofficial use&amp;quot;. This happens consistently across different types of informal or domain-specific vocabulary.&lt;/p&gt; &lt;h1&gt;What This Means&lt;/h1&gt; &lt;p&gt;Apple appears to be running a ~3B parameter model with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong factual accuracy when it works&lt;/li&gt; &lt;li&gt;Overly conservative rejection behavior&lt;/li&gt; &lt;li&gt;Hard guardrails that sometimes misfire&lt;/li&gt; &lt;li&gt;Design choices favoring brevity over helpfulness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a local, on-device model, it's impressively capable. But the high rejection rate and minimal responses might frustrate users expecting ChatGPT-style interactions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Theory:&lt;/strong&gt; Apple optimized for &amp;quot;never be wrong&amp;quot; over &amp;quot;always be helpful&amp;quot;.&lt;/p&gt; &lt;p&gt;Anyone else tested this? Curious if you're seeing similar patterns.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Salamander1882"&gt; /u/No_Salamander1882 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T19:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfznhz</id>
    <title>Any tools that help you build simple interactive projects from an idea?</title>
    <updated>2025-06-20T09:26:09+00:00</updated>
    <author>
      <name>/u/Fun_Construction_</name>
      <uri>https://old.reddit.com/user/Fun_Construction_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get random ideas sometimes, like a mini-game, typing test, or a little music toy, and I’d love to turn them into something playable without starting from scratch. Is there any tool that lets you describe what you want and helps build it out, even just a rough version? Not looking for anything super advanced, just fun stuff I can play around with or share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Construction_"&gt; /u/Fun_Construction_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfznhz/any_tools_that_help_you_build_simple_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfznhz/any_tools_that_help_you_build_simple_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfznhz/any_tools_that_help_you_build_simple_interactive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lftaep</id>
    <title>Running Local LLMs (“AI”) on Old Unsupported AMD GPUs and Laptop iGPUs using llama.cpp with Vulkan (Arch Linux Guide)</title>
    <updated>2025-06-20T02:51:23+00:00</updated>
    <author>
      <name>/u/Kallocain</name>
      <uri>https://old.reddit.com/user/Kallocain</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kallocain"&gt; /u/Kallocain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ahenriksson.com/posts/running-llm-on-old-amd-gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lftaep/running_local_llms_ai_on_old_unsupported_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lftaep/running_local_llms_ai_on_old_unsupported_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T02:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf5yog</id>
    <title>Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp; more stable now</title>
    <updated>2025-06-19T08:52:09+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt; &lt;img alt="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" src="https://external-preview.redd.it/9cdGcDSZyRjc_UX2wAVlzVVLsf-enKZlEOwOWD7xiXc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25d6c033d9aec8e5c2afd0f97b93b27a4a07b430" title="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan v0.6.0 is out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully redesigned UI&lt;/li&gt; &lt;li&gt;Switched from Electron to Tauri for lighter and more efficient performance&lt;/li&gt; &lt;li&gt;You can create your own assistants with instructions &amp;amp; custom model settings&lt;/li&gt; &lt;li&gt;New themes &amp;amp; customization settings (e.g. font size, code block highlighting style)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Including improvements to thread handling and UI behavior to tweaking extension settings, cleanup, log improvements, and more.&lt;/p&gt; &lt;p&gt;Update your Jan or download the latest here: &lt;a href="https://jan.ai"&gt;https://jan.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes here: &lt;a href="https://github.com/menloresearch/jan/releases/tag/v0.6.0"&gt;https://github.com/menloresearch/jan/releases/tag/v0.6.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If you'd like to play with the new Jan but has not download a model via Jan, please import your GGUF models via Settings -&amp;gt; Model Providers -&amp;gt; llama.cpp -&amp;gt; Import. See the latest image in the post to do that.&lt;/li&gt; &lt;li&gt;Jan is going to get bigger update soon on MCP usage, we're testing MCP usage with our MCP-specific model, &lt;a href="https://huggingface.co/collections/Menlo/jan-nano-684f6ebfe9ed640fddc55be7"&gt;Jan Nano&lt;/a&gt;, that surpass DeepSeek V3 671B on agentic use cases. If you'd like to test it as well, feel free to join our Discord to see the build links.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf5yog"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T08:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg1fpo</id>
    <title>Am I using lightrag + llama.cpp wrong?</title>
    <updated>2025-06-20T11:17:22+00:00</updated>
    <author>
      <name>/u/Devonance</name>
      <uri>https://old.reddit.com/user/Devonance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a system where I put a document into docling, and converts it from PDF to markdown in the certain way I want, and then it sends it to lightRAG to have a KV store and knowledge graph built. For a simple 550 line (18k chars) markdown file its taking 11 minutes and creating a KG of 1751 lines. It took 49 seconds for the first query of it. &lt;/p&gt; &lt;p&gt;I'm using unsloths Gemma 3 27b 4_q_k_m and multilingual-e5-large-instruct for the embed with a built from.source llama.cpp using the llama-server. &lt;/p&gt; &lt;p&gt;The knowledge graph is excellent, but takes forever. I have a nvidia RTX Quadro 8000 with 48gb VRAM and 256gb ram, using WSL ubuntu. &lt;/p&gt; &lt;p&gt;I am just trying to make the document -&amp;gt; docling &amp;gt; lightrag -&amp;gt; llm -&amp;gt; Q/A type pipeline for technical documents that are about 300 pages long. &lt;/p&gt; &lt;p&gt;Had a lot of issues with ollama trying to do this, so I switch to llama.cpp, but still plagued with issues. &lt;/p&gt; &lt;p&gt;I'm mainly wondering if this is just how knowledge graph based RAG is, or if im doing something insanely wrong? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devonance"&gt; /u/Devonance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg1fpo/am_i_using_lightrag_llamacpp_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg1fpo/am_i_using_lightrag_llamacpp_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg1fpo/am_i_using_lightrag_llamacpp_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T11:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfp66e</id>
    <title>Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery</title>
    <updated>2025-06-19T23:23:38+00:00</updated>
    <author>
      <name>/u/Thalesian</name>
      <uri>https://old.reddit.com/user/Thalesian</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt; &lt;img alt="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" src="https://external-preview.redd.it/dp9jZ9I5ulT5RZQDN9KwsbRB_C7Fi7IzWNUNN0l7OB8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69de3d9bf9c09f76fc51ddb8950c1601d76f26c9" title="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This rig is more for training than local inference (though there is a lot of the latter with Qwen) but I thought it might be helpful to see how the new Blackwell cards dissipate heat compared to the older blower style for Quadros prominent since Amphere.&lt;/p&gt; &lt;p&gt;There are two IR color ramps - a standard heat map and a rainbow palette that’s better at showing steep thresholds. You can see the majority of the heat is present at the two inner-facing triangles to the upper side center of the Blackwell card (84 C), with exhaust moving up and outward to the side. Underneath, you can see how effective the lower two fans are at moving heat in the flow through design, though the Ada Lovelace card’s fan input is a fair bit cooler. But the negative of the latter’s design is that the heat ramps up linearly through the card. The geometric heatmap of the Blackwell shows how superior its engineering is - it is overall comparatively cooler in surface area despite using double the wattage. &lt;/p&gt; &lt;p&gt;A note on the setup - I have all system fans with exhaust facing inward to push air out try open side of the case. It seems like this shouldn’t work, but the Blackwell seems to stay much cooler this way than with the standard front fans as intake and back fans as exhaust. Coolest part of the rig by feel is between the two cards. &lt;/p&gt; &lt;p&gt;CPU is liquid cooled, and completely unaffected by proximity to the Blackwell card.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thalesian"&gt; /u/Thalesian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfp66e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpewd</id>
    <title>Anyone else tracking datacenter GPU prices on eBay?</title>
    <updated>2025-06-19T23:35:16+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the habit of checking eBay for AMD Instinct prices for a few years now, and noticed just today that MI210 prices seem to be dropping pretty quickly (though still priced out of my budget!) and there is a used MI300X for sale there for the first time, for &lt;em&gt;only&lt;/em&gt; $35K /s&lt;/p&gt; &lt;p&gt;I watch MI60 and MI100 prices too, but MI210 is the most interesting to me for a few reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model to use a PCIe interface (later models use OAM or SH5), which I could conceivably use in servers I actually have,&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model that runs at an even halfway-sane power draw (300W),&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fabrication processes don't improve significantly in later models until the MI350.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my own mind, my MI60 is mostly for learning how to make these Instinct GPUs work and not burst into flame, and it has indeed been a learning experience. When I invest &amp;quot;seriously&amp;quot; in LLM hardware, it will probably be eBay MI210s, but not until they have come down in price quite a bit more, and not until I have well-functioning training/fine-tuning software based on llama.cpp which works on the MI60. None of that exists yet, though it's progressing.&lt;/p&gt; &lt;p&gt;Most people are probably more interested in Nvidia datacenter GPUs. I'm not in the habit of checking for that, but do see now that eBay has 40GB A100 for about $2500, and 80GB A100 for about $8800 (US dollars).&lt;/p&gt; &lt;p&gt;Am I the only one, or are other people waiting with bated breath for second-hand datacenter GPUs to become affordable too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lficpj</id>
    <title>Kyutai's STT with semantic VAD now opensource</title>
    <updated>2025-06-19T18:33:58+00:00</updated>
    <author>
      <name>/u/phhusson</name>
      <uri>https://old.reddit.com/user/phhusson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai published their latest tech demo few weeks ago, unmute.sh. It is an impressive voice-to-voice assistant using a 3rd-party text-to-text LLM (gemma), while retaining the conversation low latency of Moshi.&lt;/p&gt; &lt;p&gt;They are currently opensourcing the various components for that.&lt;/p&gt; &lt;p&gt;The first component they opensourced is their STT, available at &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The best feature of that STT is Semantic VAD. In a local assistant, the VAD is a component that determines when to stop listening to a request. Most local VAD are sadly not very sophisticated, and won't allow you to pause or think in the middle of your sentence.&lt;/p&gt; &lt;p&gt;The Semantic VAD in Kyutai's STT will allow local assistant to be much more comfortable to use.&lt;/p&gt; &lt;p&gt;Hopefully we'll also get the streaming LLM integration and TTS from them soon, to be able to have our own low-latency local voice-to-voice assistant 🤞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phhusson"&gt; /u/phhusson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfzon7</id>
    <title>What is a super lightweight model for checking grammar?</title>
    <updated>2025-06-20T09:28:14+00:00</updated>
    <author>
      <name>/u/kudikarasavasa</name>
      <uri>https://old.reddit.com/user/kudikarasavasa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking for something that can check grammar. Nothing too serious, just something to look for obvious mistakes in a git commit message. After not finding a lightweight application, I'm wondering if there's an LLM that's super light to run on a CPU that can do this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kudikarasavasa"&gt; /u/kudikarasavasa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgqkd</id>
    <title>Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts</title>
    <updated>2025-06-19T17:30:37+00:00</updated>
    <author>
      <name>/u/choose_a_guest</name>
      <uri>https://old.reddit.com/user/choose_a_guest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt; &lt;img alt="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" src="https://preview.redd.it/niqpo23p5x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22e9ad07139fcbaf4f1d83ce46f5c89ca3c94565" title="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Meta Platforms tried to poach OpenAI employees by offering signing bonuses as high as $100 million, with even larger annual compensation packages, OpenAI chief executive Sam Altman said.&amp;quot;&lt;br /&gt; &lt;a href="https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html"&gt;https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choose_a_guest"&gt; /u/choose_a_guest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niqpo23p5x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg3oyy</id>
    <title>Intel's OpenVINO 2025.2 Brings Support For New Models, GenAI Improvements</title>
    <updated>2025-06-20T13:14:16+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/OpenVINO-2025.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg3oyy/intels_openvino_20252_brings_support_for_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg3oyy/intels_openvino_20252_brings_support_for_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T13:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpkyv</id>
    <title>Qwen3 for Apple Neural Engine</title>
    <updated>2025-06-19T23:43:28+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just dropped ANEMLL 0.3.3 alpha with Qwen3 support for Apple's Neural Engine&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Anemll/Anemll"&gt;https://github.com/Anemll/Anemll&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Star ⭐️ and upvote to support open source! Cheers, Anemll 🤖&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfyp3g</id>
    <title>AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp; 2x More AI Performance Than Radeon PRO W7800</title>
    <updated>2025-06-20T08:21:25+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp;amp; 2x More AI Performance Than Radeon PRO W7800" src="https://external-preview.redd.it/EYYV5pInhONsaeNMa0FEbViuyL2svw10Qf1f-BebbNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0632c2173f66d6b249a8bebef7acd8143977e85f" title="AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp;amp; 2x More AI Performance Than Radeon PRO W7800" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-radeon-ai-pro-r9700-gpu-4x-more-tops-2x-ai-performance-vs-radeon-pro-w7800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T08:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfuxn1</id>
    <title>New 24B finetune: Impish_Magic_24B</title>
    <updated>2025-06-20T04:21:57+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the &lt;strong&gt;20th of June, 2025&lt;/strong&gt;—The world is getting more and more chaotic, but let's look at the bright side: &lt;strong&gt;Mistral&lt;/strong&gt; released a new model at a &lt;strong&gt;very&lt;/strong&gt; good size of &lt;strong&gt;24B&lt;/strong&gt;, no more &amp;quot;sign here&amp;quot; or &amp;quot;accept this weird EULA&amp;quot; there, a proper &lt;strong&gt;Apache 2.0 License&lt;/strong&gt;, nice! 👍🏻&lt;/p&gt; &lt;p&gt;This model is based on &lt;strong&gt;mistralai/Magistral-Small-2506&lt;/strong&gt; so naturally I named it &lt;strong&gt;Impish_Magic&lt;/strong&gt;. Truly excellent size, I tested it on my laptop (&lt;strong&gt;16GB gpu&lt;/strong&gt;) and it works quite well (&lt;strong&gt;4090m&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;Strong in productivity &amp;amp; in fun. Good for creative writing, and writer style emulation.&lt;/p&gt; &lt;p&gt;New unique data, see details in the model card:&lt;br /&gt; &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model would be on &lt;strong&gt;Horde&lt;/strong&gt; at &lt;strong&gt;very high availability&lt;/strong&gt; for the next few hours, so give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T04:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpqs6</id>
    <title>Current best uncensored model?</title>
    <updated>2025-06-19T23:51:12+00:00</updated>
    <author>
      <name>/u/Accomplished-Feed568</name>
      <uri>https://old.reddit.com/user/Accomplished-Feed568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is probably one of the biggest advantages of local LLM's yet there is no universally accepted answer to what's the best model as of June 2025. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;So share your BEST uncensored model!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;by ''best uncensored model' i mean the least censored model (that helped you get a nuclear bomb in your kitched), but also the most intelligent one&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Feed568"&gt; /u/Accomplished-Feed568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfzh05</id>
    <title>Repurposing 800 x RX 580s for LLM inference - 4 months later - learnings</title>
    <updated>2025-06-20T09:14:15+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in March I asked this sub if RX 580s could be used for anything useful in the LLM space and asked for help on how to implemented inference: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Four months later, we've built a fully functioning inference cluster using around 800 RX 580s across 132 rigs. I want to come back and share what worked, what didn’t so that others can learn from our experience. &lt;/p&gt; &lt;h1&gt;what worked&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Vulkan with llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vulkan backend worked on all RX 580s&lt;/li&gt; &lt;li&gt;Required compiling Shaderc manually to get &lt;code&gt;glslc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;llama.cpp built with custom flags for vulkan support and no avx instructions (our cpus on the builds are very old celerons). we tried countless build attempts and this is the best we could do:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CXXFLAGS=&amp;quot;-march=core2 -mtune=generic&amp;quot; cmake .. \ -DLLAMA_BUILD_SERVER=ON \ -DGGML_VULKAN=ON \ -DGGML_NATIVE=OFF \ -DGGML_AVX=OFF -DGGML_AVX2=OFF \ -DGGML_AVX512=OFF -DGGML_AVX_VNNI=OFF \ -DGGML_FMA=OFF -DGGML_F16C=OFF \ -DGGML_AMX_TILE=OFF -DGGML_AMX_INT8=OFF -DGGML_AMX_BF16=OFF \ -DGGML_SSE42=ON \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Per-rig multi-GPU scaling&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each rig runs 6 GPUs and can split small models across multiple kubernetes containers with each GPU's VRAM shared (could only minimally do 1 GPU per container - couldn't split a GPU's VRAM to 2 containers)&lt;/li&gt; &lt;li&gt;Used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm none&lt;/code&gt; for 6 containers for 6 gpus&lt;/li&gt; &lt;li&gt;for bigger contexts we could extend the small model's limits and use more than 1 GPU's VRAM&lt;/li&gt; &lt;li&gt;for bigger models (Qwen3-30B_Q8_0) we used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm layer&lt;/code&gt; and build a recent llama.cpp implementation for reasoning management where you could turn off thinking mode with &lt;code&gt;--reasoning-budget 0&lt;/code&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Load balancing setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built a fastapi load-balancer backend that assigns each user to an available kubernetes pod&lt;/li&gt; &lt;li&gt;Redis tracks current pod load and handle session stickiness &lt;/li&gt; &lt;li&gt;The load-balancer also does prompt cache retention and restoration. biggest challenge here was how to make the llama.cpp servers accept the old prompt caches that weren't 100% in the processed eval format and would get dropped and reinterpreted from the beginning. we found that using &lt;code&gt;--cache-reuse 32&lt;/code&gt; would allow for a margin of error big enough for all the conversation caches to be evaluated instantly&lt;/li&gt; &lt;li&gt;Models respond via streaming SSE, OpenAI-compatible format&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;what didn’t work&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ROCm HIP \ pytorc \ tensorflow inference&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ROCm technically works and tools like &lt;code&gt;rocminfo&lt;/code&gt; and &lt;code&gt;rocm-smi&lt;/code&gt; work but couldn't get a working llama.cpp HIP build&lt;/li&gt; &lt;li&gt;there’s no functional PyTorch backend for Polaris-class gfx803 cards so pytorch didn't work&lt;/li&gt; &lt;li&gt;couldn't get TensorFlow to work with llama.cpp &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;we’re also putting part of our cluster through some live testing. If you want to throw some prompts at it, you can hit it here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.masterchaincorp.com"&gt;https://www.masterchaincorp.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s running Qwen-30B and the frontend is just a basic llama.cpp server webui. nothing fancy so feel free to poke around and help test the setup. feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:14:15+00:00</published>
  </entry>
</feed>
