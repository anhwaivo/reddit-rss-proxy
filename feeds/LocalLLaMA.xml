<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-03T13:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ifyzvv</id>
    <title>Mistral Small 3 24b is the first model under 70b I’ve seen pass the “apple” test (even using Q4).</title>
    <updated>2025-02-02T14:59:50+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put all the Deepseek-R1 distills through the “apple” benchmark last week and only 70b passed the “Write 10 sentences that end with the word “apple” “ test, getting all 10 out of10 sentences correct.&lt;/p&gt; &lt;p&gt;I tested a slew of other newer open source models (all the major ones, Qwen, Phi-, Llama, Gemma, Command-R, etc) as well, but no model under 70b has ever managed to succeed in getting all 10 right….until Mistral Small 3 24b came along. It is the first and only model under 70b parameters that I’ve found that could pass this test. Congrats Mistral Team!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifsb1m</id>
    <title>Is the UK about to ban running LLMs locally?</title>
    <updated>2025-02-02T07:48:06+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The UK government is targetting the use of AI to generate illegal imagery, which of course is a good thing, but the wording seems like any kind of AI tool run locally can be considered illegal, as it has the *potential* of generating questionable content. Here's a quote from the news:&lt;/p&gt; &lt;p&gt;&amp;quot;The Home Office says that, to better protect children, the UK will be the first country in the world to make it illegal to possess, create or distribute AI tools designed to create child sexual abuse material (CSAM), with a punishment of up to five years in prison.&amp;quot; They also mention something about manuals that teach others how to use AI for these purposes.&lt;/p&gt; &lt;p&gt;It seems to me that any uncensored LLM run locally can be used to generate illegal content, whether the user wants to or not, and therefore could be prosecuted under this law. Or am I reading this incorrectly?&lt;/p&gt; &lt;p&gt;And is this a blueprint for how other countries, and big tech, can force people to use (and pay for) the big online AI services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T07:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1igoy14</id>
    <title>5080 16gb vs 4090 24gb</title>
    <updated>2025-02-03T13:22:52+00:00</updated>
    <author>
      <name>/u/HeyDontSkipLegDay</name>
      <uri>https://old.reddit.com/user/HeyDontSkipLegDay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For running local DeepSeek models, the RTX 4090 (24GB VRAM) is likely a better choice than the upcoming RTX 5080 (16GB VRAM)—even if the 5080 has better overall architecture improvements. Here’s why:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;VRAM Capacity Matters Most for Large Models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Deep learning models require a lot of VRAM for inference, and VRAM size is the main bottleneck when running larger models.&lt;/p&gt; &lt;p&gt;Model VRAM Required (FP16) VRAM Required (8-bit) VRAM Required (4-bit) 7B ~14GB ~8GB ~5GB 14B ~28GB ~16GB ~10GB 32B ~64GB ~32GB ~20GB 67B ~128GB ~64GB ~40GB&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• A 4090 (24GB VRAM) can comfortably run 7B and 14B models, possibly 32B at 4-bit quantization. • A 5080 (16GB VRAM) might struggle with 14B models and would be limited to smaller models or aggressive quantization. &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;VRAM Bandwidth and Performance Differences • RTX 4090: Has 384-bit memory bus → high bandwidth, which improves large model inference speeds. • RTX 5080 (rumored): Expected to have 256-bit memory bus, which is significantly lower.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even if the 5080 has a faster GPU core, its 16GB VRAM and smaller memory bus will cripple performance for larger models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tensor and Compute Performance • The 5080 will likely have DLSS 3.5, better power efficiency, and higher raw TFLOPS, but inference relies more on VRAM capacity and bandwidth than raw GPU power. • The 4090 already has Tensor Cores optimized for AI and performs well in inference tasks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Verdict: Get the 4090 for DeepSeek Models • 4090 (24GB VRAM) → Better for DeepSeek models, capable of running 7B, 14B, and even 32B (4-bit). • 5080 (16GB VRAM) → Limited to 7B models or highly quantized versions of 14B. • If you’re serious about running bigger models locally, more VRAM always wins.&lt;/p&gt; &lt;p&gt;If you’re planning to run anything above 14B, you might want to consider a 4090 or even a 4090 Ti / 5000-series Titan (if it has more VRAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeyDontSkipLegDay"&gt; /u/HeyDontSkipLegDay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igoy14/5080_16gb_vs_4090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igoy14/5080_16gb_vs_4090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igoy14/5080_16gb_vs_4090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:22:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1igp4ro</id>
    <title>u/AutoModerator delete message</title>
    <updated>2025-02-03T13:32:23+00:00</updated>
    <author>
      <name>/u/rx22230</name>
      <uri>https://old.reddit.com/user/rx22230</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="/u/AutoModerator"&gt;u/AutoModerator&lt;/a&gt; delete message but I cannot read the reason..&lt;/p&gt; &lt;p&gt;difficut to improve myself...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rx22230"&gt; /u/rx22230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igp4ro/uautomoderator_delete_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igp4ro/uautomoderator_delete_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igp4ro/uautomoderator_delete_message/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T13:32:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifw36r</id>
    <title>R1 is cool, but Mistral 3 Small is the boring workhorse I’m actually excited to fine-tune and deploy</title>
    <updated>2025-02-02T12:21:07+00:00</updated>
    <author>
      <name>/u/logan-diamond</name>
      <uri>https://old.reddit.com/user/logan-diamond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As soon as you use it, you realize it's not meant to be fun. It's a masterfully designed bland base model with very thoughtful trade-offs, especially for one-offs. Unless qwen replies soon, I think it might frequently replace both qwen 14b &amp;amp; 32b. &lt;/p&gt; &lt;p&gt;In 2024 I don't know how many times I read &amp;quot;... is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of...&amp;quot;. &lt;/p&gt; &lt;p&gt;Those times are back ☺️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logan-diamond"&gt; /u/logan-diamond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T12:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7fwk</id>
    <title>What’s are the best GUIs for chat?</title>
    <updated>2025-02-02T20:55:35+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to completely move on from using ChatGPTs plus plan or Anthropic to using my own api keys or self hosting everything. &lt;/p&gt; &lt;p&gt;What are the best GUIs you know for this? Hopefully with all the features like web, interpreter, projects, artifacts or even the GPTs. &lt;/p&gt; &lt;p&gt;TIA! This can help a lot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1igmrm8</id>
    <title>[Research] Using Adaptive Classification to Automatically Optimize LLM Temperature Settings</title>
    <updated>2025-02-03T11:13:45+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an approach to automatically optimize LLM configurations (particularly temperature) based on query characteristics. The idea is simple: different types of prompts need different temperature settings for optimal results, and we can learn these patterns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM behavior varies significantly with temperature settings (0.0 to 2.0)&lt;/li&gt; &lt;li&gt;Manual configuration is time-consuming and error-prone&lt;/li&gt; &lt;li&gt;Most people default to temperature=0.7 for everything&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Approach:&lt;/strong&gt; We trained an adaptive classifier that categorizes queries into five temperature ranges:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DETERMINISTIC (0.0-0.1): For factual, precise responses&lt;/li&gt; &lt;li&gt;FOCUSED (0.2-0.5): For technical, structured content&lt;/li&gt; &lt;li&gt;BALANCED (0.6-1.0): For conversational responses&lt;/li&gt; &lt;li&gt;CREATIVE (1.1-1.5): For varied, imaginative outputs&lt;/li&gt; &lt;li&gt;EXPERIMENTAL (1.6-2.0): For maximum variability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results (tested on 500 diverse queries):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;69.8% success rate in finding optimal configurations&lt;/li&gt; &lt;li&gt;Average similarity score of 0.64 (using RTC evaluation)&lt;/li&gt; &lt;li&gt;Most interesting finding: BALANCED and CREATIVE temps consistently performed best (scores: 0.649 and 0.645)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Distribution of optimal settings:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;FOCUSED: 26.4%&lt;br /&gt; BALANCED: 23.5%&lt;br /&gt; DETERMINISTIC: 18.6%&lt;br /&gt; CREATIVE: 17.8%&lt;br /&gt; EXPERIMENTAL: 13.8%&lt;/p&gt; &lt;p&gt;This suggests that while the default temp=0.7 (BALANCED) works well, it's only optimal for about a quarter of queries. Many queries benefit from either more precise or more creative settings.&lt;/p&gt; &lt;p&gt;The code and pre-trained models are available on GitHub: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;. Would love to hear your thoughts, especially if you've experimented with temperature optimization before.&lt;/p&gt; &lt;p&gt;EDIT: Since people are asking - evaluation was done using Round-Trip Consistency testing, measuring how well the model maintains response consistency across similar queries at each temperature setting.&lt;/p&gt; &lt;p&gt;^(Disclaimer: This is a research project, and while the results are promising, your mileage may vary depending on your specific use case and model.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igmrm8/research_using_adaptive_classification_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igmrm8/research_using_adaptive_classification_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igmrm8/research_using_adaptive_classification_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T11:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1igiufi</id>
    <title>Morning Radio - Locally Generated Personal Morning Broadcast</title>
    <updated>2025-02-03T06:25:14+00:00</updated>
    <author>
      <name>/u/wuduzodemu</name>
      <uri>https://old.reddit.com/user/wuduzodemu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"&gt; &lt;img alt="Morning Radio - Locally Generated Personal Morning Broadcast" src="https://external-preview.redd.it/nm0MbB12ySzfO4ubcdzXCPER80VcIu9Qwn1HsmfJdUo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86dfe57958b48ede90588fe692559b9bc0184778" title="Morning Radio - Locally Generated Personal Morning Broadcast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuduzodemu"&gt; /u/wuduzodemu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/smy20011/MorningRadio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1igmuba</id>
    <title>gsh with gemma2 can predict 50% of my shell commands! Full benchmark comparing different local models included.</title>
    <updated>2025-02-03T11:18:48+00:00</updated>
    <author>
      <name>/u/atinylittleshell</name>
      <uri>https://old.reddit.com/user/atinylittleshell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igmuba/gsh_with_gemma2_can_predict_50_of_my_shell/"&gt; &lt;img alt="gsh with gemma2 can predict 50% of my shell commands! Full benchmark comparing different local models included." src="https://external-preview.redd.it/PAtbLYnBtJ3ArHfwFmRjw8Ux15ZBrIBJqJZllgU90TU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=341367896bd6be16bf4be49dd9f30b4581fab4b2" title="gsh with gemma2 can predict 50% of my shell commands! Full benchmark comparing different local models included." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been building &lt;a href="https://github.com/atinylittleshell/gsh"&gt;https://github.com/atinylittleshell/gsh&lt;/a&gt; which can use local LLM to auto complete and explain shell commands, like this -&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/swrqluodpwge1.png?width=636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d37508a63ad065a4590fec87092dd84ac28459"&gt;gsh's predicts the next command I want to run&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To better understand which model performs the best for me, I built an evaluation system in gsh that can &lt;strong&gt;use my command history as an evaluation dataset&lt;/strong&gt; to test different LLMs and see how well they could predict my commands (retroactively), like this -&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7j7vuiaspwge1.png?width=675&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=413400ad84191a926665824008c0b0bd8a2b9933"&gt;gsh now has a built-in evaluation system&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result really surprised me! &lt;/p&gt; &lt;p&gt;I tested almost every popular open source model between 1b-14b (excluded deepseek R1 and distills as reasoning models are not suited for low latency generation which we need here), and it turns out Google's gemma2:9b did the best with almost 30% exact matches, and overall 50% similarity score. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/en67gou6qwge1.png?width=991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3bf955d1c24975fceed3316ed6b33964b7bbeb21"&gt;Model benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was done with a M4 Mac Mini.&lt;/p&gt; &lt;p&gt;Some other observations -&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;qwen2.5 3b is somehow better at this than its 7b and 14b variant.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;qwen2.5-coder scales well linearly with more parameters.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;mistral and llama3.2 aren't very good at this.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm pretty impressed by gemma2 - would not have thought they were a good choice but here I am looking at hard data. I'll likely use gemma2 as a base to fine-tune even better predictors. Just thought this was interesting to share! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atinylittleshell"&gt; /u/atinylittleshell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igmuba/gsh_with_gemma2_can_predict_50_of_my_shell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igmuba/gsh_with_gemma2_can_predict_50_of_my_shell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igmuba/gsh_with_gemma2_can_predict_50_of_my_shell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T11:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ignwxh</id>
    <title>Cursor now supports deepseek v3 and r1 models</title>
    <updated>2025-02-03T12:27:03+00:00</updated>
    <author>
      <name>/u/Available-Stress8598</name>
      <uri>https://old.reddit.com/user/Available-Stress8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt; &lt;img alt="Cursor now supports deepseek v3 and r1 models" src="https://b.thumbs.redditmedia.com/Ugm-o5reKn35RsdVj7A8-79UAJyFEluwtKhDUSaoGas.jpg" title="Cursor now supports deepseek v3 and r1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1t2mirr83xge1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6ea465bb381d350dac2fe65c04c009d8a98a7ef"&gt;https://preview.redd.it/1t2mirr83xge1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6ea465bb381d350dac2fe65c04c009d8a98a7ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since Deepseek is open source, you don't need Cursor Pro to use it, we don't need to worry about rate limits anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Stress8598"&gt; /u/Available-Stress8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ignwxh/cursor_now_supports_deepseek_v3_and_r1_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T12:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1igiyxw</id>
    <title>Underthinking of o1-like LLMs</title>
    <updated>2025-02-03T06:34:03+00:00</updated>
    <author>
      <name>/u/Xiwei</name>
      <uri>https://old.reddit.com/user/Xiwei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Points: * Frequent Switching: DeepSeek-R1 often switches between different reasoning approaches without fully exploring them, particularly on difficult maths problems. * Inefficient Reasoning: Incorrect answers often involve more tokens, but these don't lead to better results. * Early Abandonment of Correct Thoughts: Surprisingly, models often start with a correct idea, only to abandon it.This 'underthinking' shows that simply scaling up models isn't enough; we need to enhance how they explore and deepen their reasoning. It's about quality, not just quantity of thought!&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.18585"&gt;https://arxiv.org/pdf/2501.18585&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiwei"&gt; /u/Xiwei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcuub</id>
    <title>... All I wrote is test!</title>
    <updated>2025-02-03T00:58:58+00:00</updated>
    <author>
      <name>/u/internetpillows</name>
      <uri>https://old.reddit.com/user/internetpillows</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt; &lt;img alt="... All I wrote is test!" src="https://preview.redd.it/g90sz09motge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c34ddea7f426493d998245465a45f6cce9c8c3a" title="... All I wrote is test!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internetpillows"&gt; /u/internetpillows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g90sz09motge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7ruy</id>
    <title>A bunch of LLMs scheduled to come at end of January were cancelled / delayed</title>
    <updated>2025-02-02T21:09:06+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all ix-nixed by deepseek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/patience_cave/status/1886122517359886745"&gt;https://x.com/patience_cave/status/1886122517359886745&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;gemini 2 - don’t even think about it&lt;/li&gt; &lt;li&gt;grok 3 - welcome to elon timelines&lt;/li&gt; &lt;li&gt;o3 - an unforeseen problem occurred&lt;/li&gt; &lt;li&gt;opus 3.5 - long gone&lt;/li&gt; &lt;li&gt;llama 4 - won’t be sota (sorry zuck)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Update: Rumor for end of January: &lt;a href="https://x.com/iruletheworldmo/status/1877391558305001747"&gt;https://x.com/iruletheworldmo/status/1877391558305001747&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig8ve3</id>
    <title>Americans can distill models too</title>
    <updated>2025-02-02T21:55:59+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, I'm a TTS model trainer and a US citizen. Last month, I put out a &lt;a href="https://huggingface.co/posts/hexgrad/418806998707773"&gt;call for synthetic training data&lt;/a&gt;, that call was answered with well over a hundred hours of audio in various languages, and the resulting model &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; has since been upgraded/delivered. Happy customers all around.&lt;/p&gt; &lt;p&gt;The current model mostly excels at &lt;em&gt;reading long texts&lt;/em&gt; and has some glaring limitations, especially on short texts. It's also been described as relatively flat and emotionless. Nevertheless, it is currently the most-liked &lt;a href="https://huggingface.co/models?pipeline_tag=text-to-speech&amp;amp;sort=likes"&gt;TTS model&lt;/a&gt; and &lt;a href="https://huggingface.co/spaces?sort=likes&amp;amp;search=tts"&gt;TTS space&lt;/a&gt; on Hugging Face thanks to people smashing that like button.&lt;/p&gt; &lt;p&gt;Now, I'm considering making another call for crowdsourced data, except this time with a focus on only ChatGPT Advanced Voice Mode text/audio pairs, likely just in English, spanning whatever emotions people can prompt out of it. If successful, it could result in a substantially better &lt;em&gt;conversational&lt;/em&gt; model within the same size class, albeit more limited on voices and languages.&lt;/p&gt; &lt;p&gt;There are many things to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top priority would be given to paying ChatGPT subscribers, $20 and $200, but in practice free AVM audio would likely be admitted as well. This is because the paying subscribers would be least likely to be using a quantized and/or distilled AVM product.&lt;/li&gt; &lt;li&gt;Ideally I could maximally open source any voicepack derived from the AVM data, which means the people contributing audio would have to do it for ideological reasons, and couldn't be compensated with an &amp;quot;exclusive voicepack&amp;quot;. Also, any sponsorships I receive are directed at GPU compute, and both on principle + potential legal liability, I cannot financially compensate people who give me synthetic data.&lt;/li&gt; &lt;li&gt;As far as ToS goes, this distillation strategy rests on the fact that I am not the one obtaining the data, others are. Obviously, I do not agree with the OpenAI ToS or feel bound by it because I don't use any of their products. Feel free to comment on how dumb this strategy is.&lt;/li&gt; &lt;li&gt;I have skimmed Part 2 of the US Copyright Office's Report on AI. I still see no copyright protection on synthetic data of this nature, but any lawyers (real or wannabe) can chime in here with the default prefix of IANL.&lt;/li&gt; &lt;li&gt;I do not wish to be sued, and I'm also deeply allergic to .50 caliber bullets. Jokes aside, I think OpenAI likely has bigger whales to fry, than some guy training 82M param speech models.&lt;/li&gt; &lt;li&gt;Why do it: these small TTS models are (relatively) cheap to train, especially compared to LLMs, and the total utility they offer might exceed their cost, at least for now, until Zucc drops Llama 4 multimodal or DeepSeek puts up a good audio model, etc.&lt;/li&gt; &lt;li&gt;The scale of data I am looking for is at least 10 hours per voice/emotion, but label quality also matters. Each audio file would have to be fished out one-by-one, since there are no API calls for AVM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand this is LocalLLaMA and people here are likely very pro-open-weights, pro-open-source, and therefore anti-OpenAI. But putting aside any feelings you might have about various sides of history, (A) how do we generally feel about building a model this way and (B) do we think enough people would answer the call?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1igo6c9</id>
    <title>Don't forget to optimize your hardware! (Windows)</title>
    <updated>2025-02-03T12:41:48+00:00</updated>
    <author>
      <name>/u/rpwoerk</name>
      <uri>https://old.reddit.com/user/rpwoerk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt; &lt;img alt="Don't forget to optimize your hardware! (Windows)" src="https://b.thumbs.redditmedia.com/mq5j1Xjh-aMGbb1yZh8muOtGDY8qezCGISvJPZlb6kM.jpg" title="Don't forget to optimize your hardware! (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rpwoerk"&gt; /u/rpwoerk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1igo6c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igo6c9/dont_forget_to_optimize_your_hardware_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T12:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1igdnx2</id>
    <title>Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]</title>
    <updated>2025-02-03T01:38:56+00:00</updated>
    <author>
      <name>/u/teddybear082</name>
      <uri>https://old.reddit.com/user/teddybear082</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt; &lt;img alt="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" src="https://preview.redd.it/evlscivtvtge1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8e17dd7562de9086f58eb97b4363b79f94ad14a3" title="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teddybear082"&gt; /u/teddybear082 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evlscivtvtge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcpwz</id>
    <title>Kokoro TTS 1.0</title>
    <updated>2025-02-03T00:52:05+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt; &lt;img alt="Kokoro TTS 1.0" src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Kokoro TTS 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1igf1vi</id>
    <title>Phi 4 is so underrated</title>
    <updated>2025-02-03T02:49:57+00:00</updated>
    <author>
      <name>/u/jeremyckahn</name>
      <uri>https://old.reddit.com/user/jeremyckahn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a GPU poor pleb with but a humble M4 Mac mini (24 GB RAM), my local LLM options are limited. As such, I've found Phi 4 (&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Q8, Unsloth variant&lt;/a&gt;) to be an extremely capable model for my hardware. My use cases are general knowledge questions and coding prompts. It's at least as good as GPT 3.5 in my experience and sets me on the right direction more often then not. I can't speak to benchmarks because I don't really understand (or frankly care about) any of them. It's just a good model for the things I need a model for.&lt;/p&gt; &lt;p&gt;And no, Microsoft isn't paying me. I'm just a fan. 🙂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremyckahn"&gt; /u/jeremyckahn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T02:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ign0lz</id>
    <title>DeepSeek-R1 never ever relaxes...</title>
    <updated>2025-02-03T11:30:28+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt; &lt;img alt="DeepSeek-R1 never ever relaxes..." src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 never ever relaxes..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was testing DeepSeek-R1 with a math problem I found in a textbook for 9-year-olds &lt;strong&gt;(yes, really)&lt;/strong&gt;, and the model managed to crack it.&lt;/p&gt; &lt;p&gt;The problem was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Find two 3-digit palindromic numbers that add up to a 4-digit palindromic number. Note: the first digit of any of these numbers can't be 0.&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ml5hnng3rwge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1456610eeff8d8b9a122d86fbb44967f84f682d9"&gt;R1 starts thinking...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, here’s where it gets interesting. R1 thought for a bit, found the correct answer in its &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block, then went ahead to output it—but made a mistake.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/77bke6q1swge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d6eac07677fe576be9e699776a2134cba1d15c62"&gt;R1 makes a mistake...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before even finishing its response, it caught its own error, backtracked, and corrected itself on the fly outside of the&lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; block.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yc3zjamsswge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=903d42998593e95a68ff32006b7bac6335df9f1e"&gt;R1 corrects itself...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j8vgvxn3twge1.jpg?width=1800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b189fce4a099ed9182b315c2164a1071a4a32104"&gt;R1's final answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/0Ayv77LN"&gt;DeepSeek-R1 complete answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regarding the problem, &lt;strong&gt;no other LLM solved it, except for&lt;/strong&gt; &lt;a href="https://pastebin.com/YCRR521W"&gt;&lt;strong&gt;OpenAI o1&lt;/strong&gt;&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;So now I’m wondering—&lt;strong&gt;what's holding them back?&lt;/strong&gt; Is it the tokenizer's weaknesses? The sampling parameters (even when all where at the recommended settings they failed)? Or maybe, just maybe, non-thinking LLMs are really that bad at math? &lt;/p&gt; &lt;p&gt;Would love to hear thoughts on this.&lt;/p&gt; &lt;p&gt;Unsuccessful attemps by other models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pastebin.com/r8VKHrcA"&gt;chatgpt-4o-latest-20241120&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/tXc7wGVz"&gt;claude-3-5-sonnet-20241022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/zGzQJ8B5"&gt;phi-4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/vt54UFBe"&gt;amazon-nova-pro-v1.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/eSN4y6E0"&gt;gemini-exp-1206&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/jVj1KcMF"&gt;llama-3.1-405b-instruct-bf16&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://pastebin.com/ZRLfhEfU"&gt;qwen-max-2025-01-25&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ign0lz/deepseekr1_never_ever_relaxes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T11:30:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggetv</id>
    <title>Make your Mistral Small 3 24B Think like R1-distilled models</title>
    <updated>2025-02-03T04:01:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt; &lt;img alt="Make your Mistral Small 3 24B Think like R1-distilled models" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="Make your Mistral Small 3 24B Think like R1-distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing a lot of posts about the Mistral Small 3 24B model, and I remember having this CoT system prompt in my collection. I might as well try it out on this new model. I haven't used it for a long time since I switched to R1-distilled-32b. I'm not the original writer of this prompt; I've rewritten some parts of it, and I can't remember where I got it from.&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/sVMrgZBp"&gt;https://pastebin.com/sVMrgZBp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment. I doubt it will actually make your model smarter in a noticeable way, this is not a replacement of Mistral's furture reasoning models&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/d1geatbckuge1.gif"&gt;https://i.redd.it/d1geatbckuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hyrryecnkuge1.gif"&gt;https://i.redd.it/hyrryecnkuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggwff</id>
    <title>Mistral, Qwen, Deepseek</title>
    <updated>2025-02-03T04:28:53+00:00</updated>
    <author>
      <name>/u/Stargazer-8989</name>
      <uri>https://old.reddit.com/user/Stargazer-8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren't you noticing a pattern? Companies outside the USA are releasing models like Mistral AI, Qwen, and DeepSeek - reliable models that are made accessible, smaller and open-source, compared to most US-based companies &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stargazer-8989"&gt; /u/Stargazer-8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcvol</id>
    <title>I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!</title>
    <updated>2025-02-03T01:00:09+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt; &lt;img alt="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" src="https://external-preview.redd.it/bnIwMGoyaXludGdlMVL1KlPwXSM4mwFtLRlx6KM67CArRsK705RfUy_x1msn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b064a65e7251b4b07e096a39fc4d698d7f457b36" title="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh90m1iyntge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1igc6r0</id>
    <title>20 yrs in jail or $1 million for downloading Chinese models proposed at congress</title>
    <updated>2025-02-03T00:26:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seriously stop giving your money to these anti open companies and encourage everyone and anyone you know to do the same, don't let your company use their products. Anthrophic and OpenAI are the worse. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:26:00+00:00</published>
  </entry>
</feed>
