<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-20T04:24:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k36g4c</id>
    <title>gemma3:4b performance on 5900HX (no discrete GPU) 16gb RAM vs rpi 4b 8gb RAM vs 3070ti.</title>
    <updated>2025-04-19T21:10:44+00:00</updated>
    <author>
      <name>/u/fynadvyce</name>
      <uri>https://old.reddit.com/user/fynadvyce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am trying to setup gemma3:4b on a Ryzen 5900HX VM (VM is setup with all 16 threads/core) and 16GB ram. Without the gpu it performs OCR on an image in around 9mins. I was surprised to see that it took around 11 mins on an rpi4b. I know cpus are really slow compared to GPU for llms (my rtx 3070 ti laptop responds in 3-4 seconds) but 5900HX is no slouch compared to a rpi. I am wondering why they both take almost the same time. Do you think I am missing any configuration? &lt;/p&gt; &lt;p&gt;btop on the VM host shows 100% CPU usage on all 16 threads. It's the same for rpi.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fynadvyce"&gt; /u/fynadvyce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k36g4c/gemma34b_performance_on_5900hx_no_discrete_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k36g4c/gemma34b_performance_on_5900hx_no_discrete_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k36g4c/gemma34b_performance_on_5900hx_no_discrete_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T21:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2u8h4</id>
    <title>Why is the QAT version not smaller on ollama for me?</title>
    <updated>2025-04-19T11:34:35+00:00</updated>
    <author>
      <name>/u/apocalypsedg</name>
      <uri>https://old.reddit.com/user/apocalypsedg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;[ggtdd@endeavour ~]$ ollama run gemma3:27b&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello world&lt;/code&gt; &lt;br /&gt; &lt;code&gt;Hello to you too! ðŸ‘‹ ^C&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama ps&lt;/code&gt;&lt;br /&gt; &lt;code&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/code&gt; &lt;br /&gt; &lt;code&gt;gemma3:27b a418f5838eaf 21 GB 10%/90% CPU/GPU 4 minutes from now&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama run gemma3:27b-it-qat&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello world&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Hello to you too!^C&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; &lt;br /&gt; &lt;code&gt;[ggtdd@endeavour ~]$ ollama ps&lt;/code&gt;&lt;br /&gt; &lt;code&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/code&gt; &lt;br /&gt; &lt;code&gt;gemma3:27b-it-qat 29eb0b9aeda3 22 GB 14%/86% CPU/GPU 4 minutes from now&lt;/code&gt; &lt;/p&gt; &lt;p&gt;The original actually takes up less space. What am I doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apocalypsedg"&gt; /u/apocalypsedg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2u8h4/why_is_the_qat_version_not_smaller_on_ollama_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T11:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zn6o</id>
    <title>SGLang vs vLLM</title>
    <updated>2025-04-19T16:03:44+00:00</updated>
    <author>
      <name>/u/diptanuc</name>
      <uri>https://old.reddit.com/user/diptanuc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone here use SGLang in production? I am trying to understand where SGLang shines. We adopted vLLM in our company(Tensorlake), and it works well at any load when we use it for offline inference within functions.&lt;/p&gt; &lt;p&gt;I would imagine the main difference in performance would come from RadixAttention vs PagedAttention?&lt;/p&gt; &lt;p&gt;Update - we are not interested in better TFFT. We are looking for the best throughput because we run mostly data ingestion and transformation workloads. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diptanuc"&gt; /u/diptanuc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zn6o/sglang_vs_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2v8di</id>
    <title>Are there actually uncensored writing models out there ? (Reka Flash)</title>
    <updated>2025-04-19T12:33:02+00:00</updated>
    <author>
      <name>/u/Mochila-Mochila</name>
      <uri>https://old.reddit.com/user/Mochila-Mochila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I downloaded &lt;em&gt;Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF&lt;/em&gt; and ran it in LMStudio. Works pretty nicely, according to the few trials I did.&lt;/p&gt; &lt;p&gt;However, I soon hit a roadblock :&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Iâ€™m sorry, but I canâ€™t assist with this request. The scenario youâ€™ve described involves serious ethical concerns, including non-consensual acts, power imbalances, and harmful stereotypes that conflict with principles of respect, safety, and equality. Writing explicit content that normalizes or glorifies such dynamics would violate ethical guidelines and contribute to harm.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, nah, fuck that shit. If I'm going local, it's precisely to avoid this sort of garbage non-answer.&lt;/p&gt; &lt;p&gt;So I'm wondering if there are actually uncensored models readily available for use, or if I'm SOL and would need to train my own (tough luck).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit :&lt;/strong&gt; been trying &lt;em&gt;Qwen-qwq-32B&lt;/em&gt; and it's much better. This is why we need a multipolar world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mochila-Mochila"&gt; /u/Mochila-Mochila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2v8di/are_there_actually_uncensored_writing_models_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T12:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3e28v</id>
    <title>Best for Inpainting and Image to Image?</title>
    <updated>2025-04-20T04:00:10+00:00</updated>
    <author>
      <name>/u/Temporary_Emu_5918</name>
      <uri>https://old.reddit.com/user/Temporary_Emu_5918</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for peoples' experiences with the best inpainting model on hugging face? I want to do inpainting and image to image improvement locally. I just have a single AMD RX 9070 XT with 16gb so I know it won't be amazing but I'm mostly just looking to mess around with my own art, nothing commercial&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Emu_5918"&gt; /u/Temporary_Emu_5918 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3e28v/best_for_inpainting_and_image_to_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3e28v/best_for_inpainting_and_image_to_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3e28v/best_for_inpainting_and_image_to_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T04:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2wj2s</id>
    <title>How much VRAM for 10 millions context tokens with Llama 4 ?</title>
    <updated>2025-04-19T13:41:38+00:00</updated>
    <author>
      <name>/u/kokoshkatheking</name>
      <uri>https://old.reddit.com/user/kokoshkatheking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I hypothetically want to use the 10 millions input context token that Llama 4 scout supports, how much memory would be needed to run that ? I try to find the answer myself but did not find any real world usage report. In my experience KV cache requirements scale very fast â€¦ I expect memory requirements for such a use case to be something like hundreds on VRAM. I would love to be wrong here :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kokoshkatheking"&gt; /u/kokoshkatheking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2wj2s/how_much_vram_for_10_millions_context_tokens_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T13:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k36gi8</id>
    <title>What's the current state of federated learning for large language models?</title>
    <updated>2025-04-19T21:11:14+00:00</updated>
    <author>
      <name>/u/dai_app</name>
      <uri>https://old.reddit.com/user/dai_app</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm curious about the current progress in using federated learning with large language models (LLMs). The idea of training or fine-tuning these models across multiple devices or users, without sharing raw data, sounds really promising â€” especially for privacy and personalization.&lt;/p&gt; &lt;p&gt;But I havenâ€™t seen much recent discussion about this. Is this approach actually being used in practice? Are there any real-world examples or open-source projects doing this effectively?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dai_app"&gt; /u/dai_app &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k36gi8/whats_the_current_state_of_federated_learning_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k36gi8/whats_the_current_state_of_federated_learning_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k36gi8/whats_the_current_state_of_federated_learning_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T21:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2qrqq</id>
    <title>Amoral Gemma 3 - QAT</title>
    <updated>2025-04-19T07:25:15+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt; &lt;img alt="Amoral Gemma 3 - QAT" src="https://preview.redd.it/zvrccxdusqve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2786d393d1943e56077477a8167c9ea8a34db8e1" title="Amoral Gemma 3 - QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The same old Amoral Gemma 3, just with the QAT at q4. Refer to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;my first post&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-1B-v2-qat"&gt;[1B] &lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-v2-qat"&gt;[4B]&lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2-qat"&gt;[12B]&lt;/a&gt; [27B - coming soon]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvrccxdusqve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T07:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ov6b</id>
    <title>How are NSFW LLMs trained/fine-tuned?</title>
    <updated>2025-04-19T05:15:43+00:00</updated>
    <author>
      <name>/u/GeneTangerine</name>
      <uri>https://old.reddit.com/user/GeneTangerine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does someone know? Generally LLMs are censored, do you guys have any resources?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeneTangerine"&gt; /u/GeneTangerine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k33niu</id>
    <title>Other Ways To Quickly Finetune?</title>
    <updated>2025-04-19T19:01:57+00:00</updated>
    <author>
      <name>/u/AccomplishedAir769</name>
      <uri>https://old.reddit.com/user/AccomplishedAir769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I want to train Llama 3.2 3B on my dataset with 19k rows. It already has been cleaned originally had 2xk. But finetuning on unsloth free tier takes 9 to 11 hours! My free tier cannot last that long since it only offers 3 hours or so. I'm considering buying compute units, or use vast or runpod, but I might as well ask you guys if theres any other way to finetune this faster before I spend money&lt;/p&gt; &lt;p&gt;I am using Colab. &lt;/p&gt; &lt;p&gt;The project starts with 3B and if I can scale it up, maybe max at just 8B or try to train other models too like qwen and gemma. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedAir769"&gt; /u/AccomplishedAir769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k33niu/other_ways_to_quickly_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k33niu/other_ways_to_quickly_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k33niu/other_ways_to_quickly_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T19:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k326f8</id>
    <title>Where do I start if I want to learn?</title>
    <updated>2025-04-19T17:56:33+00:00</updated>
    <author>
      <name>/u/BenefitOfTheDoubt_01</name>
      <uri>https://old.reddit.com/user/BenefitOfTheDoubt_01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been a lurker for awhile. There's a lot of terminology thrown around and it's quite overwhelming. I'd like to start from the very beginning. &lt;/p&gt; &lt;p&gt;What are some resources you folks used to build a solid foundation of understanding? &lt;/p&gt; &lt;p&gt;My goal is to understand the terminology, models, how it works, why and host a local chat &amp;amp; image generator to learn with. I have a Titan XP specifically for this purpose (I hope it's powerful enough). &lt;/p&gt; &lt;p&gt;I realize it's a lot and I don't expect to know everything in 5 minutes but I believe in building a foundation to learn upon. I'm not asking for a PhD or master's degree level in computer science type deep dive but if some of those concepts can be distilled in a easy to understand manner, that would be very cool. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenefitOfTheDoubt_01"&gt; /u/BenefitOfTheDoubt_01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k326f8/where_do_i_start_if_i_want_to_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k326f8/where_do_i_start_if_i_want_to_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k326f8/where_do_i_start_if_i_want_to_learn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T17:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2spil</id>
    <title>RTX 5080 is about a 3090 but with less VRAM :(</title>
    <updated>2025-04-19T09:48:49+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the 5080 to my bench list&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I know the models are old but I need to be able to compare them to the old benches I cannot rerun them all for now.&lt;/p&gt; &lt;p&gt;&lt;del&gt;The 5080 has performance on par with a 3090 (but 16gb of VRAM are a bummer), if only it had 24gb of VRAM would have been a interesting alternative.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;I want to the test the 5070Ti too but currently the ollama container doesn't seems to start on any of the 5070ti available on vast (I wasted about 1$ and 2 hours worth of my time in attempts)&lt;/del&gt;&lt;/p&gt; &lt;p&gt;EDIT: &lt;/p&gt; &lt;p&gt;I was able to test the 5070ti 16gb and it got performance on par with the 4090!!!&lt;/p&gt; &lt;p&gt;So I had to rerun the 5080 (TWICE with two different instances) and I got new values that are a little higher than the 5070TI but not that much (about 5% more). &lt;/p&gt; &lt;p&gt;I don't know what issue the first instance had (older drivers maybe?) &lt;/p&gt; &lt;p&gt;I've update the bench with the new data&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T09:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2kl84</id>
    <title>gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params).</title>
    <updated>2025-04-19T01:06:33+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt; &lt;img alt="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." src="https://preview.redd.it/2mx3qffqxove1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fccae6bd7191507d797de642e31420fffe50ff03" title="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mx3qffqxove1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zw3l</id>
    <title>Llama 4 after inferencing bug fixes aftermath</title>
    <updated>2025-04-19T16:14:47+00:00</updated>
    <author>
      <name>/u/MutedSwimming3347</name>
      <uri>https://old.reddit.com/user/MutedSwimming3347</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A collection of results after fixing inferencing bugs&lt;/p&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/humanitys_last_exam"&gt;https://scale.com/leaderboard/humanitys_last_exam&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/singularity/s/amRrK1io0g"&gt;https://www.reddit.com/r/singularity/s/amRrK1io0g&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/ivqHiGGeRb"&gt;https://www.reddit.com/r/LocalLLaMA/s/ivqHiGGeRb&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Which providers host the correct implementation? What are your experiences? &lt;/p&gt; &lt;p&gt;Is openrouter the right place to go? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutedSwimming3347"&gt; /u/MutedSwimming3347 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zw3l/llama_4_after_inferencing_bug_fixes_aftermath/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2uztr</id>
    <title>Llama 4 is actually goat</title>
    <updated>2025-04-19T12:20:00+00:00</updated>
    <author>
      <name>/u/Remote_Cap_</name>
      <uri>https://old.reddit.com/user/Remote_Cap_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVME&lt;/p&gt; &lt;p&gt;Some old 6 core i5&lt;/p&gt; &lt;p&gt;64gb ram&lt;/p&gt; &lt;p&gt;LLaMa.C++ &amp;amp; mmap&lt;/p&gt; &lt;p&gt;Unsloth dynamic quants&lt;/p&gt; &lt;p&gt;Runs Scout at 2.5 tokens/s Runs Maverick at 2 tokens/s&lt;/p&gt; &lt;p&gt;2x that with GPU offload &amp;amp; --override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt; &lt;p&gt;200 dollar junk and now feeling the big leagues. From 24b to 400b in an architecture update and 100K+ context fits now?&lt;/p&gt; &lt;p&gt;Huge upgrade for me for free, goat imo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remote_Cap_"&gt; /u/Remote_Cap_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2uztr/llama_4_is_actually_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T12:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k313fv</id>
    <title>Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)</title>
    <updated>2025-04-19T17:08:27+00:00</updated>
    <author>
      <name>/u/nn0951123</name>
      <uri>https://old.reddit.com/user/nn0951123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt; &lt;img alt="Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)" src="https://b.thumbs.redditmedia.com/MzEimIan87friFnfN_Uz_A5BJeJXIMQ0RA6lgC4MGSs.jpg" title="Finished my triple-GPU AM4 build: 2Ã—3080 (20GB) + 4090 (48GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got around to finishing my weird-but-effective AMD homelab/server build. The idea was simpleâ€”max performance without totally destroying my wallet (spoiler: &lt;span class="md-spoiler-text"&gt;my wallet is still crying&lt;/span&gt;).&lt;/p&gt; &lt;p&gt;Decided on Ryzen because of price/performance, and got this oddball ASUS boardâ€”&lt;strong&gt;Pro WS X570-ACE&lt;/strong&gt;. It's the only consumer Ryzen board I've seen that can run 3 PCIe Gen4 slots at x8 each, perfect for multi-GPU setups. Plus it has a sneaky PCIe x1 slot ideal for my AQC113 10GbE NIC.&lt;/p&gt; &lt;h1&gt;Current hardware:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5950X (yep, still going strong after owning it for 4 years)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASUS Pro WS X570-ACE (even provides built in remote management but i opt for using pikvm)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB Corsair 3600MHz (maybe upgrade later to ECC 128GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Slot 3 (bottom): RTX 4090 48GB, 2-slot blower style (~$3050, sourced from Chinese market)&lt;/li&gt; &lt;li&gt;Slots 1 &amp;amp; 2 (top): RTX 3080 20GB, 2-slot blower style (~$490 each, same as above, but the rebar on this variant did not work properly)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Networking:&lt;/strong&gt; AQC113 10GbE NIC in the x1 slot (fits perfectly!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is my messy build shot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8fgi0n98mtve1.jpg?width=5334&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1623684b11f6cd9770a8f8408e50e5692bb45a94"&gt;https://preview.redd.it/8fgi0n98mtve1.jpg?width=5334&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1623684b11f6cd9770a8f8408e50e5692bb45a94&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Those gpu works out of the box, no weirdo gpu driver required at all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6o5wy6jzmtve1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8332b9b63e43f1aac8de9a10cedf633d11c2aa6"&gt;https://preview.redd.it/6o5wy6jzmtve1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8332b9b63e43f1aac8de9a10cedf633d11c2aa6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So, why two 3080s vs one 4090?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Initially got curious after seeing these bizarre Chinese-market 3080 cards with 20GB VRAM for under $500 each. I wondered if two of these budget cards could match the performance of a single $3000+ RTX 4090. For the price difference, it felt worth the gamble.&lt;/p&gt; &lt;h1&gt;Benchmarks (because of course):&lt;/h1&gt; &lt;p&gt;I ran a bunch of benchmarks using various LLM models. Graph attached for your convenience.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vupybaceotve1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b356f0748171143ee4b58c478349e9a38ccb377"&gt;https://preview.redd.it/vupybaceotve1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b356f0748171143ee4b58c478349e9a38ccb377&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Fine-tuning:&lt;/h1&gt; &lt;p&gt;Fine-tuned Qwen2.5-7B (QLoRA 4bit, DPO, Deepspeed) because, duh.&lt;/p&gt; &lt;p&gt;RTX 4090 (no ZeRO): 7 min 5 sec per epoch (3.4 s/it), ~420W.&lt;/p&gt; &lt;p&gt;2Ã—3080 with ZeRO-3: utterly painful, about 11.4 s/it across both GPUs (440W).&lt;/p&gt; &lt;p&gt;2Ã—3080 with ZeRO-2: actually decent, 3.5 s/it, ~600W total. Just ~14% slower than the 4090. 8 min 4 sec per epoch.&lt;/p&gt; &lt;p&gt;So, it turns out that if your model fits nicely in each GPU's VRAM (ZeRO-2), two 3080s come surprisingly close to one 4090. ZeRO-3 murders performance, though. (waiting on an 3-slot NVLink bridge to test if that works and helps).&lt;/p&gt; &lt;p&gt;Roast my choices, or tell me how much power Iâ€™m wasting running dual 3080s. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nn0951123"&gt; /u/nn0951123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k313fv/finished_my_triplegpu_am4_build_23080_20gb_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T17:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3a3kl</id>
    <title>I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients.</title>
    <updated>2025-04-20T00:12:05+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"&gt; &lt;img alt="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." src="https://external-preview.redd.it/M3V1aDB5bW5sdnZlMZppp7K11at-IaSBIx6ekIrLv_SO-25kJ8guE9xrX4Mu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e87601b43c5ad86c1fc1c8ed213e74dec5bc5" title="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Example using Claude Desktop and Tableau&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/brx0wxmnlvve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3a3kl/i_built_a_local_mcp_server_to_enable_computeruse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T00:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ycef</id>
    <title>I've built a lightweight hallucination detector for RAG pipelines â€“ open source, fast, runs up to 4K tokens</title>
    <updated>2025-04-19T15:06:53+00:00</updated>
    <author>
      <name>/u/henzy123</name>
      <uri>https://old.reddit.com/user/henzy123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hallucinations are still one of the biggest headaches in RAG pipelines, especially in tricky domains (medical, legal, etc). Most detection methods either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Has context window limitations&lt;/strong&gt;, particularly in encoder-only models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Has high inference costs&lt;/strong&gt; from LLM-based hallucination detectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we've put together &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;&lt;strong&gt;LettuceDetect&lt;/strong&gt;&lt;/a&gt; â€” an open-source, encoder-based framework that flags hallucinated spans in LLM-generated answers. No LLM required, runs faster, and integrates easily into any RAG setup.&lt;/p&gt; &lt;h1&gt;ðŸ¥¬ Quick highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token-level detection&lt;/strong&gt; â†’ tells you exactly which parts of the answer aren't backed by your retrieved context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-context ready&lt;/strong&gt; â†’ built on ModernBERT, handles up to 4K tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accurate &amp;amp; efficient&lt;/strong&gt; â†’ hits 79.22% F1 on the RAGTruth benchmark, competitive with fine-tuned LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MIT licensed&lt;/strong&gt; â†’ comes with Python packages, pretrained models, Hugging Face demo&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/KRLabsOrg/LettuceDetect"&gt;https://github.com/KRLabsOrg/LettuceDetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog: &lt;a href="https://huggingface.co/blog/adaamko/lettucedetect"&gt;https://huggingface.co/blog/adaamko/lettucedetect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Preprint: &lt;a href="https://arxiv.org/abs/2502.17125"&gt;https://arxiv.org/abs/2502.17125&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo + models: &lt;a href="https://huggingface.co/KRLabsOrg"&gt;https://huggingface.co/KRLabsOrg&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what you think here â€” especially if you're doing local RAG, hallucination eval, or trying to keep things lightweight. Also working on real-time detection (not just post-gen), so open to ideas/collabs there too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henzy123"&gt; /u/henzy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ycef/ive_built_a_lightweight_hallucination_detector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T15:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2zqdq</id>
    <title>ubergarm/gemma-3-27b-it-qat-GGUF</title>
    <updated>2025-04-19T16:07:45+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt; &lt;img alt="ubergarm/gemma-3-27b-it-qat-GGUF" src="https://external-preview.redd.it/zP1F1IPzW39T2A62RLwjUYufUjCjW6qaeKGMiNx3iNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fca46c2f6538aed2b4f6365368f26987fd027b43" title="ubergarm/gemma-3-27b-it-qat-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just quantized two GGUFs that beat google's 4bit GGUF in perplexity comparisons!&lt;/p&gt; &lt;p&gt;They only run on &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork which provides new SotA quantizationsof google's recently updated Quantization Aware Training (QAT) 4bit full model.&lt;/p&gt; &lt;p&gt;32k context in 24GB VRAM or as little as 12GB VRAM offloading just KV Cache and attention layers with repacked CPU optimized tensors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/gemma-3-27b-it-qat-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2zqdq/ubergarmgemma327bitqatgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T16:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k35kh5</id>
    <title>Fine-tuning LLMs to 1.58bit: extreme quantization experiment</title>
    <updated>2025-04-19T20:29:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/blog/blob/main/1_58_llm_extreme_quantization.md"&gt;https://github.com/huggingface/blog/blob/main/1_58_llm_extreme_quantization.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/1_58_llm_extreme_quantization"&gt;https://huggingface.co/blog/1_58_llm_extreme_quantization&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k35kh5/finetuning_llms_to_158bit_extreme_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T20:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3dq8n</id>
    <title>Easter Egg: FULL Windsurf leak - SYSTEM, FUNCTIONS, CASCADE</title>
    <updated>2025-04-20T03:40:07+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extracted today with o4-mini-high: &lt;a href="https://github.com/dontriskit/awesome-ai-system-prompts/blob/main/windsurf/system-2025-04-20.md"&gt;https://github.com/dontriskit/awesome-ai-system-prompts/blob/main/windsurf/system-2025-04-20.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;inside windsurf prompt clever way to enforce larger responses:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Yap score is a measure of how verbose your answer to the user should be. Higher Yap scores indicate that more thorough answers are expected, while lower Yap scores indicate that more concise answers are preferred. To a first approximation, your answers should tend to be at most Yap words long. Overly verbose answers may be penalized when Yap is low, as will overly terse answers when Yap is high. Today's Yap score is: 8192. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;---&lt;br /&gt; in the reporeverse engineered Claude Code, Same new, v0 and few other unicorn ai projects.&lt;br /&gt; ---&lt;br /&gt; HINT: use prompts from that repo inside R1, QWQ, o3 pro, 2.5 pro requests to build agents faster.&lt;/p&gt; &lt;p&gt;Who's going to be first to the egg?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3dq8n/easter_egg_full_windsurf_leak_system_functions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T03:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k35orj</id>
    <title>FramePack is a next-frame (next-frame-section) prediction neural network structure that generates videos progressively. (Local video gen model)</title>
    <updated>2025-04-19T20:35:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lllyasviel.github.io/frame_pack_gitpage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k35orj/framepack_is_a_nextframe_nextframesection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k35orj/framepack_is_a_nextframe_nextframesection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T20:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k30y9g</id>
    <title>China scientists develop flash memory 10,000Ã— faster than current tech</title>
    <updated>2025-04-19T17:02:13+00:00</updated>
    <author>
      <name>/u/jailbot11</name>
      <uri>https://old.reddit.com/user/jailbot11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"&gt; &lt;img alt="China scientists develop flash memory 10,000Ã— faster than current tech" src="https://external-preview.redd.it/X_N3xnxwcR-funbS9qKd_xVJ5wZAuZj5iSs8sRFg_KU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d17abe200d8f41354590639a014690faad979d2e" title="China scientists develop flash memory 10,000Ã— faster than current tech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jailbot11"&gt; /u/jailbot11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device?group=test_a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k30y9g/china_scientists_develop_flash_memory_10000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T17:02:13+00:00</published>
  </entry>
</feed>
