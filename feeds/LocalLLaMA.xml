<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-12T12:26:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j9iq4w</id>
    <title>Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?</title>
    <updated>2025-03-12T12:19:07+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt; &lt;img alt="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" src="https://b.thumbs.redditmedia.com/4ay_KKrgtBYQvDUAcyo7ae-1n_dMK9Xm5bS78dAQ9jY.jpg" title="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j9iq4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gthl</id>
    <title>FYI, ollama run gemma3 uses gemma3-4b-q4_K_M.</title>
    <updated>2025-03-12T10:18:07+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama seems to use small 4b gemma3. If you want to use a bigger model, you need to specify &lt;code&gt;ollama run gemma3:27b&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Also only q4_K_m and fp16 are available at the moment, but hopefully more quants are coming. I can't quantize it with the latest Ollama 0.6.0. I get:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% ollama create gemma3:27-it-q8_0 --quantize q8_0 -f gemma3.modelfile gathering model components quantizing F16 model to Q8_0 Error: llama_model_quantize: 1 llama_model_quantize: failed to quantize: unknown model architecture: 'gemma3' &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gthl/fyi_ollama_run_gemma3_uses_gemma34bq4_k_m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gthl/fyi_ollama_run_gemma3_uses_gemma34bq4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gthl/fyi_ollama_run_gemma3_uses_gemma34bq4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T10:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j91e71</id>
    <title>Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!</title>
    <updated>2025-03-11T20:55:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"&gt; &lt;img alt="Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!" src="https://external-preview.redd.it/ddQTqHwEP1NhKvcKA2BhPTcEYYfofNwH1aagxf91uVw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39197fb9b8aef7cafa4c8c1c0bddc77b3d5e3f62" title="Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemmasutra-Small-4B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9df59</id>
    <title>AMD new open source Vision Language model: Instella-VL-1B</title>
    <updated>2025-03-12T06:31:32+00:00</updated>
    <author>
      <name>/u/v1an1</name>
      <uri>https://old.reddit.com/user/v1an1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v1an1"&gt; /u/v1an1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/Instella-BL-1B-VLM/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9df59/amd_new_open_source_vision_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9df59/amd_new_open_source_vision_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:31:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j91zx4</id>
    <title>7B reasoning model outperforming Claude-3.7 Sonnet on IOI</title>
    <updated>2025-03-11T21:20:30+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"&gt; &lt;img alt="7B reasoning model outperforming Claude-3.7 Sonnet on IOI" src="https://preview.redd.it/rzu5zsd0n4oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=733bdf42006187dcf40def927d8ea4bf5171e755" title="7B reasoning model outperforming Claude-3.7 Sonnet on IOI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzu5zsd0n4oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T21:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8uvg0</id>
    <title>New Reasoning model (Reka Flash 3 - 21B)</title>
    <updated>2025-03-11T16:29:11+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt; &lt;img alt="New Reasoning model (Reka Flash 3 - 21B)" src="https://preview.redd.it/fgldu1ml73oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f085204b9fc6819966a9114f4e794afbed28a54f" title="New Reasoning model (Reka Flash 3 - 21B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fgldu1ml73oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:29:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9fgyd</id>
    <title>smOllama – A tiny, no-Bloat chat interface for Ollama</title>
    <updated>2025-03-12T08:35:46+00:00</updated>
    <author>
      <name>/u/GUNNM_VR</name>
      <uri>https://old.reddit.com/user/GUNNM_VR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt; &lt;img alt="smOllama – A tiny, no-Bloat chat interface for Ollama" src="https://a.thumbs.redditmedia.com/HgzECGrbkfEPXlBWKXdlsBaI_EFQkyJkyVUIPQYlew0.jpg" title="smOllama – A tiny, no-Bloat chat interface for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I created &lt;strong&gt;smOllama&lt;/strong&gt;, a lightweight web interface for Ollama models. It’s just &lt;strong&gt;24KB&lt;/strong&gt;, a single &lt;strong&gt;HTML file&lt;/strong&gt;, and runs with &lt;strong&gt;zero dependencies&lt;/strong&gt; - pure HTML, CSS, and JavaScript.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use it?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No setup - just open in a browser&lt;/li&gt; &lt;li&gt;Fast and minimalist&lt;/li&gt; &lt;li&gt;Markdown &amp;amp; LaTeX support&lt;/li&gt; &lt;li&gt;Works on any device&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s simple but does the job. If you’re interested, check it out: &lt;a href="https://github.com/GUNNM-VR/smOllama"&gt;GitHub&lt;/a&gt;. Feedback is welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b5uddu0xk8oe1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e11d6eb48ffa1ab71e1774355131a168e712771"&gt;https://preview.redd.it/b5uddu0xk8oe1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e11d6eb48ffa1ab71e1774355131a168e712771&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GUNNM_VR"&gt; /u/GUNNM_VR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T08:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9erfw</id>
    <title>Try Gemma 3 with our new Gemma Python library!</title>
    <updated>2025-03-12T07:43:09+00:00</updated>
    <author>
      <name>/u/ResponsibleSolid8404</name>
      <uri>https://old.reddit.com/user/ResponsibleSolid8404</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleSolid8404"&gt; /u/ResponsibleSolid8404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gemma-llm.readthedocs.io/en/latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T07:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gz1i</id>
    <title>I call it Daddy LLM</title>
    <updated>2025-03-12T10:29:03+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt; &lt;img alt="I call it Daddy LLM" src="https://preview.redd.it/0kbolzlck8oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0acfd5a5ea43b09dd3077e24c0ebd0fa9b2f4238" title="I call it Daddy LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4x 3090 on an Asus rampage V extreme motherboard. Using LM studio it can do 15 tokens/s on 70b models, but I think 2 3090 are enough for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0kbolzlck8oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T10:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9hqni</id>
    <title>GRPO on a diffusion model - Unsloth?</title>
    <updated>2025-03-12T11:19:28+00:00</updated>
    <author>
      <name>/u/heisenbork4</name>
      <uri>https://old.reddit.com/user/heisenbork4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know if unsloth can load diffusion LLMs? I don't think I see any in the list of supported models...&lt;/p&gt; &lt;p&gt;I wondered if it might be possible to try training a reasoning model following their GRPO tutorial (&lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"&gt;https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo&lt;/a&gt;), but using the dLLM because it generates faster. I have a very cool application in mind, and maybe even some half decent training data I can line up for it.&lt;/p&gt; &lt;p&gt;There's probably more to it, like getting LoRA support working for dLLMs, but I'd love to give this a go if anyone has any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heisenbork4"&gt; /u/heisenbork4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:19:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8tfh5</id>
    <title>Reka Flash 3, New Open Source 21B Model</title>
    <updated>2025-03-11T15:29:02+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tweet: &lt;a href="https://x.com/RekaAILabs/status/1899481289495031825"&gt;https://x.com/RekaAILabs/status/1899481289495031825&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;https://huggingface.co/RekaAI/reka-flash-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.reka.ai/news/introducing-reka-flash"&gt;https://www.reka.ai/news/introducing-reka-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T15:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r2nr</id>
    <title>M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)</title>
    <updated>2025-03-11T13:44:15+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt; &lt;img alt="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" src="https://external-preview.redd.it/Z3KKrFryWMuFPZGHYHDmgzf48KaEB5A-Ze6pFibC3lk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992e2d56bcc2473a9ea6913ceadc30c7eb46bb1f" title="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=J4qwuCXyAcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9iazd</id>
    <title>Gemma3 technical report detailed analysis 💎</title>
    <updated>2025-03-12T11:54:57+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt; &lt;img alt="Gemma3 technical report detailed analysis 💎" src="https://preview.redd.it/a5jgz1vlz8oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd67d4f1cca341b88b273756d22a450cb91848ec" title="Gemma3 technical report detailed analysis 💎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a5jgz1vlz8oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j95fjo</id>
    <title>Gemma 3 is confirmed to be coming soon</title>
    <updated>2025-03-11T23:49:44+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt; &lt;img alt="Gemma 3 is confirmed to be coming soon" src="https://preview.redd.it/0iudkfrrd5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5436382ec43a49f4f586d49c5ecdf024a9a21612" title="Gemma 3 is confirmed to be coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0iudkfrrd5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T23:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8u90g</id>
    <title>New Gemma models on 12th of March</title>
    <updated>2025-03-11T16:03:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt; &lt;img alt="New Gemma models on 12th of March" src="https://preview.redd.it/8qfnwj7433oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4ac1bb57e9292b5685c7637a5bd9e4ac889d7c" title="New Gemma models on 12th of March" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X pos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qfnwj7433oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9drfk</id>
    <title>Gemma 3: Technical Report</title>
    <updated>2025-03-12T06:49:36+00:00</updated>
    <author>
      <name>/u/David-Kunz</name>
      <uri>https://old.reddit.com/user/David-Kunz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/David-Kunz"&gt; /u/David-Kunz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j90u4u</id>
    <title>What happened to the promised open source o3-mini ?</title>
    <updated>2025-03-11T20:32:37+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does everybody forget that this was once promised ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9hsfc</id>
    <title>Gemma 3 - GGUFs + recommended settings</title>
    <updated>2025-03-12T11:22:36+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We uploaded GGUFs and 16-bit versions of Gemma 3 to Hugging Face! Gemma 3 is Google's new multimodal models that come in 1B, 4B, 12B and 27B sizes. We also made a step-by-step guide on How to run Gemma 3 correctly: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training Gemma 3 with Unsloth does work (yet), but there's currently bugs with training in 4-bit QLoRA (not on Unsloth's side) so 4-bit dynamic and QLoRA training with our notebooks will be released tomorrow!&lt;/p&gt; &lt;p&gt;Gemma 3 GGUF uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-GGUF"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-GGUF"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-GGUF"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Gemma 3 Instruct 16-bit uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;See the rest of our models &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;in our docs&lt;/a&gt;. Remember to pull the &lt;strong&gt;LATEST llama.cpp&lt;/strong&gt; for stuff to work!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;According to the Gemma team&lt;/strong&gt;, the recommended settings for inference are (I auto made a params file for example in &lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params"&gt;https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params&lt;/a&gt; which can help if you use Ollama ie like &lt;code&gt;ollama run&lt;/code&gt; &lt;a href="http://hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M"&gt;&lt;code&gt;hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 1.0 top_k = 64 top_p = 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the chat template is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user\nHello!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\nHey there!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;user\nWhat is 1+1?&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;WARNING: Do not add a &amp;lt;bos&amp;gt; to llama.cpp&lt;/strong&gt; or other inference engines, or else you will get &lt;strong&gt;DOUBLE &amp;lt;BOS&amp;gt; tokens&lt;/strong&gt;! llama.cpp auto adds the token for you!&lt;/p&gt; &lt;p&gt;More spaced out chat template (newlines rendered):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user Hello!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model Hey there!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;user What is 1+1?&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Read more in our docs on how to run Gemma 3 effectively: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gafp</id>
    <title>EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s</title>
    <updated>2025-03-12T09:40:39+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt; &lt;img alt="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" src="https://external-preview.redd.it/TzhJISgYeqmJ66gJa8ISrsZbvEzELCLxSu1XvFxOBXk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c3d163b23ac0c0c9e4c6af40dd5ca4af8494305" title="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1899735281781411907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T09:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dt8l</id>
    <title>Gemma 3 on Huggingface</title>
    <updated>2025-03-12T06:52:16+00:00</updated>
    <author>
      <name>/u/DataCraftsman</name>
      <uri>https://old.reddit.com/user/DataCraftsman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Gemma 3! Comes in 1B, 4B, 12B, 27B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-1b-it"&gt;https://huggingface.co/google/gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;https://huggingface.co/google/gemma-3-12b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-it"&gt;https://huggingface.co/google/gemma-3-27b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text string, such as a question, a prompt, or a document to be summarized&lt;/li&gt; &lt;li&gt;Images, normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context of 8192 tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update: They have added it to Ollama already!&lt;/p&gt; &lt;p&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apparently it has an ELO of 1338 on Chatbot Arena, better than DeepSeek V3 671B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataCraftsman"&gt; /u/DataCraftsman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dmny</id>
    <title>Gemma 3 27B</title>
    <updated>2025-03-12T06:42:38+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt; &lt;img alt="Gemma 3 27B" src="https://preview.redd.it/foonq7ewf7oe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30b8fa38fe5d18d26cf0aca72f47b346cd9ad56" title="Gemma 3 27B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/foonq7ewf7oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j981ci</id>
    <title>This is the first response from an LLM that has made me cry laughing</title>
    <updated>2025-03-12T01:50:28+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt; &lt;img alt="This is the first response from an LLM that has made me cry laughing" src="https://preview.redd.it/kw96telpz5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7223e09ee41672180f06db34a031ef87fae195a" title="This is the first response from an LLM that has made me cry laughing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kw96telpz5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T01:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96j3g</id>
    <title>I hacked Unsloth's GRPO code to support agentic tool use. In 1 hour of training on my RTX 4090, Llama-8B taught itself to take baby steps towards deep research! (23%→53% accuracy)</title>
    <updated>2025-03-12T00:40:21+00:00</updated>
    <author>
      <name>/u/diegocaples</name>
      <uri>https://old.reddit.com/user/diegocaples</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I've been experimenting with getting &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;Llama-8B to bootstrap its own research skills through self-play.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified Unsloth's GRPO implementation (❤️ Unsloth!) to support function calling and agentic feedback loops.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama generates its own questions about documents (you can have it learn from any documents, but I chose the Apollo 13 mission report)&lt;/li&gt; &lt;li&gt;It learns to search for answers in the corpus using a search tool&lt;/li&gt; &lt;li&gt;It evaluates its own success/failure using llama-as-a-judge&lt;/li&gt; &lt;li&gt;Finally, it trains itself through RL to get better at research&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The model starts out hallucinating and making all kinds of mistakes, but after an hour of training on my 4090, it quickly improves. It goes from getting 23% of answers correct to 53%!&lt;/p&gt; &lt;p&gt;Here is the full &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;code and instructions&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diegocaples"&gt; /u/diegocaples &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T00:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9bvll</id>
    <title>Gemma 3 27b now available on Google AI Studio</title>
    <updated>2025-03-12T05:13:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt; &lt;img alt="Gemma 3 27b now available on Google AI Studio" src="https://external-preview.redd.it/4sjcMoBy8c8hywZZD7DFEQHtY85E3eDlhYRBqIdn2eQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f55bb78cef85467f757df883df24bca99ee8925" title="Gemma 3 27b now available on Google AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://aistudio.google.com/"&gt;https://aistudio.google.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context length 128k&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output length 8k&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/2WvMTPS"&gt;&lt;strong&gt;https://imgur.com/a/2WvMTPS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd"&gt;https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dkvh</id>
    <title>Gemma 3 Release - a google Collection</title>
    <updated>2025-03-12T06:39:59+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt; &lt;img alt="Gemma 3 Release - a google Collection" src="https://external-preview.redd.it/XbF6RBBvzvCU6XDYyRoYk_HGSNjj77rcnuXfCRK9sgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d72653de324cc030e9dad7f7ea4df6ef94e0688" title="Gemma 3 Release - a google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:39:59+00:00</published>
  </entry>
</feed>
