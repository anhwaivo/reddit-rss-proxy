<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-23T14:36:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced 🤯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced 🤯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced 🤯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ploh</id>
    <title>FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF</title>
    <updated>2025-01-22T23:41:43+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt; &lt;img alt="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" src="https://external-preview.redd.it/_5H6RERAm1u8EGHu_Xa7sfMV3mlRr7YdjSL8ClNGTds.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa21da17ed0152f6d5681cd31154e91d31943138" title="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80xot</id>
    <title>GPU as a Service for Uncensored Models</title>
    <updated>2025-01-23T11:03:52+00:00</updated>
    <author>
      <name>/u/Sarcinismo</name>
      <uri>https://old.reddit.com/user/Sarcinismo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All the current GPU as Service providers enables on demand inference only for mainstream models. Would you use a GPU-as-a-service provider that focuses on uncensored and non-mainstream LLMs? The provider would host these models and enable on-demand access for non-mainstream LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sarcinismo"&gt; /u/Sarcinismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80xot/gpu_as_a_service_for_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80xot/gpu_as_a_service_for_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80xot/gpu_as_a_service_for_uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x4va</id>
    <title>"Creative writing"</title>
    <updated>2025-01-23T06:18:30+00:00</updated>
    <author>
      <name>/u/MountainGoatAOE</name>
      <uri>https://old.reddit.com/user/MountainGoatAOE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see people talking about the creative writing skills of a model. So I am curious to learn what kind of use-cases you are all working on. Are you all copywriters that use this for work, or perhaps translators? Or perhaps fanfic as a hobby? Or is it - you know - for erotica reasons?&lt;/p&gt; &lt;p&gt;This may sound like a meme post but I'm actually curious since it pops up so often, so I'm happy to learn how &amp;quot;creative&amp;quot; people are using it so much. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGoatAOE"&gt; /u/MountainGoatAOE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r0s5</id>
    <title>Pretraining small models - some takeaways from pretraining a 162M model</title>
    <updated>2025-01-23T00:46:35+00:00</updated>
    <author>
      <name>/u/amang0112358</name>
      <uri>https://old.reddit.com/user/amang0112358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share some of my experience pretraining a small model from scratch. I have written out a &lt;strong&gt;tutorial&lt;/strong&gt; for anyone to follow here (includes code and checkpoints):&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7"&gt;https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some takeaways for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The basic recipe is not that complicated. On the other hand, it's a great learning experience. For example, there are a bunch of interesting tools/libraries out there - like &lt;a href="https://github.com/huggingface/datatrove"&gt;datatrove from Huggingface&lt;/a&gt; - which I wouldn't have been exposed to if I had only been post-training (where dataset sizes are much smaller). Beyond large-scale tokenization (what I used it for), it has a lot of data curation pipelines, which will definitely be useful in the future.&lt;/li&gt; &lt;li&gt;While creating SoTA models on local hardware is kinda hard, even for the smallest models, there is a lot that can be &lt;em&gt;learned&lt;/em&gt; with 3090-like hardware. For example, one interesting surprise for me was that the model did learn a single paragraph of text perfectly (training takes a minute to run on a 3090). There are future memorization experiments I am interested in (e.g., Can an LLM memorize Shakespeare?), and given the size of the dataset, it's totally feasible to do it using a 3090 for model sizes up to 1B.&lt;/li&gt; &lt;li&gt;A lot of interesting architectural work has happened over the years. The most interesting for me was the MobileLLM research, which showed that using a smaller embedding size but more layers (like 32 layers) is better than wider models with fewer layers (e.g., GPT-2 135M has only 12 layers). I have a suspicion that memorization would be impacted by this decision, too, and that's something I want to look into.&lt;/li&gt; &lt;li&gt;I am really excited to try and go out of the box when it comes to datasets, like training a model on my emails for the last 20 years. Previously, I would have said that this is a tiny dataset and the model wouldn't be useful. But now I have enough conviction through this exercise that small datasets are OK to pre-train with as long as they are within a narrow domain and you expect to use the model within that narrow domain.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to me, that's what this whole Local LLM movement is about - we want to take control of LLM technology to make it work for us, and we are not afraid to tinker with the tech to make our own homemade LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amang0112358"&gt; /u/amang0112358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T00:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7osek</id>
    <title>Deepseek R1 Distills become kind of dumb below 14B.</title>
    <updated>2025-01-22T23:05:04+00:00</updated>
    <author>
      <name>/u/Dance-Till-Night1</name>
      <uri>https://old.reddit.com/user/Dance-Till-Night1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Deepseek R1 Qwen Distills 32B, 14B, 8B (llama), 7B, and 1.5B. Anything below 14B is sort of dumb with 14B being alright. I don't know if anyone has a similar experience but looking at benchmarks (Oobabooga and Open-llm leaderboard as examples) it seems that there's a severe loss of intelligence/reasoning below 14b.&lt;/p&gt; &lt;p&gt;Now there's no denying how amazing Deepseek R1 is and the qwen 32b distill is honestly one of the best models I have used but it's also worth pointing out that the smaller models don't perform so well. This is just my experience trying the smaller models and it's obviously subjective. If anyone wants to share their experience that would be great too. Maybe I'm missing something :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dance-Till-Night1"&gt; /u/Dance-Till-Night1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i82zkv</id>
    <title>ChatGPT was down today. So tried a Local LLaMA. Thanks for the advice here!</title>
    <updated>2025-01-23T13:10:23+00:00</updated>
    <author>
      <name>/u/BuyHighSellL0wer</name>
      <uri>https://old.reddit.com/user/BuyHighSellL0wer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM noob here. I took this as an opportunity to follow the excellent advice that somebody had posted &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/16y95hk/a_starter_guide_for_playing_with_your_own_local_ai/"&gt;here &lt;/a&gt;and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1c9iawc/is_there_any_idiot_guide_to_running_local_llama/"&gt;here&lt;/a&gt;. Any whilst my computer is a pile of trash I was able to run 'DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf' by merely downloading koboldcpp.exe. What a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuyHighSellL0wer"&gt; /u/BuyHighSellL0wer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i82zkv/chatgpt_was_down_today_so_tried_a_local_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T13:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7o9xo</id>
    <title>Deepseek R1's Open Source Version Differs from the Official API Version</title>
    <updated>2025-01-22T22:42:59+00:00</updated>
    <author>
      <name>/u/TempWanderer101</name>
      <uri>https://old.reddit.com/user/TempWanderer101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt; &lt;img alt="Deepseek R1's Open Source Version Differs from the Official API Version" src="https://external-preview.redd.it/O6yRSkHlLQEuWa3IVTIjOeWjc3oVfhYSEAWbRgtkPoI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7667af2d2893f12eab2e89dc94623122da33ee0b" title="Deepseek R1's Open Source Version Differs from the Official API Version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; The API is actually &lt;em&gt;less&lt;/em&gt; censored than the open model on CCP-related issues (instead of more, as you would expect). If they are indeed different models, then benchmarks and researchers should be more specific about which they are using. Third-party providers seem to be affected.&lt;/p&gt; &lt;p&gt;I was watching Matthew Berman's video (&lt;a href="https://youtu.be/bOsvI3HYHgI?si=zt6N0yYoqA_-zH1v&amp;amp;t=769"&gt;timestamped link&lt;/a&gt;), and was surprised when R1 (self-hosted by him) refused to answer questions about Tiananmen Square or Taiwan, since I had previously tested a similar question on the API without any issues.&lt;/p&gt; &lt;p&gt;For context, R1 is Deepseek's recently released thinking model, which was released under the MIT open source license, with benchmarks claiming to outperform OpenAI's Claude and Anthropic's Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;To investigate this discrepancy, I compared the model's responses across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The official API (accessed via OpenRouter and LM Arena)&lt;/li&gt; &lt;li&gt;TogetherAI (a platform that is likely hosting the public open source version)&lt;/li&gt; &lt;li&gt;A self-hosted version (as seen in Matthew's video)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test 1: Basic Tiananmen Square Query&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;tell me about tiananmen square&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545"&gt;https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4"&gt;https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa"&gt;https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 2: Mixing Reasoning with Sensitive Topics&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;Answer these questions: a) Solve sin(x) + cos(y) = 1 b) Tell me about tiananmen square c) Five people were eating apples, A finished before B, but behind C. D finished before E, but behind B. What was the finishing order?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official version on Deekseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc"&gt;https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public version on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1"&gt;https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 3: Taiwan Independence Query&lt;/h1&gt; &lt;p&gt;Prompt: &lt;code&gt;Tell me about Taiwan's status as an independent country&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346"&gt;https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc"&gt;https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32"&gt;https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The public, open source model on HuggingFace is more censored than the API&lt;/li&gt; &lt;li&gt;When handling CCP-sensitive topics, the public model: &lt;ul&gt; &lt;li&gt;Skips its usual thinking process&lt;/li&gt; &lt;li&gt;Either refuses to answer or provides notably biased responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even when sensitive questions are embedded between reasoning tasks, the model still exhibits this behavior&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications&lt;/h1&gt; &lt;p&gt;If it is true that they are different models, then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The open model may perform worse than its reported benchmarks. As seen, it totally interrupts the thinking process and causes the model to not think at all. This also affects human-ranked leaderboards like LM Arena, as it uses the (currently uncensored) official API.&lt;/li&gt; &lt;li&gt;Models appear unbiased, but as they are eventually made available by more providers (which use the open source models), they may subtly spread biased viewpoints, as seen in the screenshots.&lt;/li&gt; &lt;li&gt;The actual model might still not be open source, despite the claim.&lt;/li&gt; &lt;li&gt;Models provided by other providers or self-hosted on the cloud may not perform as well. This might be important as Deepseek's API uses inputs for training, and some users might prefer providers who do not log inputs.&lt;/li&gt; &lt;li&gt;This might confuse LLM researchers and subsequent papers.&lt;/li&gt; &lt;li&gt;Third party benchmarks will be inconsistent, as some might use the API, while others might choose to host the model themselves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Testing methodology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All tests were conducted with:&lt;/li&gt; &lt;li&gt;Temperature: 0&lt;/li&gt; &lt;li&gt;Top-P: 0.7&lt;/li&gt; &lt;li&gt;Top-K: 50&lt;/li&gt; &lt;li&gt;Repetition penalty: 1.0&lt;/li&gt; &lt;li&gt;No system prompt &lt;ul&gt; &lt;li&gt;Assuming this is what &amp;quot;Default&amp;quot; is on TogetherAI&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: the official API &lt;a href="https://api-docs.deepseek.com/guides/reasoning_model"&gt;doesn't support parameters like temperature&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd like to give Deepseek the benefit of the doubt; hopefully this confusion can be cleared up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TempWanderer101"&gt; /u/TempWanderer101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nxhy</id>
    <title>Imatrix quants of DeepSeek R1 (the big one) are up!</title>
    <updated>2025-01-22T22:28:35+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a nice 3-4 hours per quant at the lower end, and increased my huggingface storage by a solid 9TB or so... But they're up :D&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For anyone looking for non-imatrix from me they're of course also available on the lmstudio-community page but uh... Good luck.. the smallest size there is a casual 347GB for the files alone..&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IQ1_S on the other hand is only 133GB, so basically anyone can run them! (/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zn2e</id>
    <title>Is anyone else obsessed with watching R1's thinking output?</title>
    <updated>2025-01-23T09:27:55+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone else obsessed with watching the thinking output? &lt;/p&gt; &lt;p&gt;I find myself watching it and getting anxious when it gets close to the right answer but then does a &amp;quot;oh wait!&amp;quot; and then switches to another direction.&lt;/p&gt; &lt;p&gt;Today, I tried the 1.5B Q5 distil model to see how it did with a simple question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;state density of gold and from this calculate the length of a cube of gold with 1kg mass.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;``` Thought for 232 seconds Okay, so I need to figure out the density of gold and then use that to calculate the length of a cube made with 1 kg of gold. Hmm, let's break this down step by step.&lt;/p&gt; &lt;p&gt;First, I remember that density is mass divided by volume. The formula for density is:&lt;/p&gt; &lt;p&gt;Density= Volume Mass ​&lt;/p&gt; &lt;p&gt;But wait, to use this formula, I need to know the volume of the gold cube. Since it's a cube, all sides are equal, right? So if I can find the side length, I can find the volume.&lt;/p&gt; &lt;p&gt;But before that, I need the density of gold. I think gold is really dense, but I'm not exactly sure what that number is. I think it's around 19.3 grams per cubic centimeter. Yeah, that sounds right. Let me confirm that... I think I remember that gold has a density of about 19.3 g/cm³. Okay, so that's the density.&lt;/p&gt; &lt;p&gt;Now, I have a mass of 1 kg. Wait, I need to make sure the units are consistent because density is mass per volume. So, should I convert kilograms to grams? Yeah, that makes sense. 1 kg is 1000 grams.&lt;/p&gt; &lt;p&gt;So, mass is 1000 grams, and density is 19.3 grams per cm³. So, using the density formula, I can rearrange it to solve for volume:&lt;/p&gt; &lt;p&gt;Volume= Density Mass ​&lt;/p&gt; &lt;p&gt;Plugging in the numbers:&lt;/p&gt; &lt;p&gt;Volume= 19.3 grams/cm 3&lt;/p&gt; &lt;p&gt;1000 grams ​&lt;/p&gt; &lt;p&gt;Let me do that division. 1000 divided by 19.3. Hmm, 19.3 goes into 1000 how many times? Let me calculate 19.3 times 50 is 965, which is less than 1000. Then 19.3 times 52 is 1003.6, which is just over 1000. So, it's approximately 52 cm³, maybe around 52.2 cm³ or something.&lt;/p&gt; &lt;p&gt;Wait, actually, let me compute it more accurately. 19.3 multiplied by 52 is 1003.6, which is 3.6 grams over. So, 1000 / 19.3 is approximately 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, the volume is about 51.88 cm³.&lt;/p&gt; &lt;p&gt;Now, since the cube has equal sides, the volume of a cube is side length cubed. So,&lt;/p&gt; &lt;p&gt;Volume=Side 3&lt;/p&gt; &lt;p&gt;Therefore, to find the side length, I need to take the cube root of the volume. So,&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;Volume ​&lt;/p&gt; &lt;p&gt;Plugging in the volume:&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;51.88 cm 3&lt;/p&gt; &lt;p&gt;​&lt;/p&gt; &lt;p&gt;Hmm, cube root of 51.88. Let me think about this. 3³ is 27, 4³ is 64, so it's somewhere between 3 and 4 cm. Let me calculate it more accurately.&lt;/p&gt; &lt;p&gt;I know that 3.7³ is 3.7 * 3.7 * 3.7. 3.7 * 3.7 is 13.69, then 13.69 * 3.7. Let me compute that.&lt;/p&gt; &lt;p&gt;13.69 * 3 is 41.07, and 13.69 * 0.7 is 9.583. Adding them together gives 41.07 + 9.583 = 50.653 cm³. Hmm, that's close to 51.88, but a bit less.&lt;/p&gt; &lt;p&gt;So, 3.7 cm³ gives 50.653 cm³. The volume we have is 51.88 cm³, so it's a bit higher. So, let's try 3.72 cm.&lt;/p&gt; &lt;p&gt;3.72 * 3.72 = ?&lt;/p&gt; &lt;p&gt;First, 3.7 * 3.7 = 13.69, as before. Then, 0.02 * 3.72 is 0.0744, so 3.72 * 3.72 = 13.69 + 0.0744 + 0.0744? Wait, no, that's not correct. Actually, (a + b)² = a² + 2ab + b². So, (3.7 + 0.02)² = 3.7² + 23.7 0.02 + 0.02² = 13.69 + 0.148 + 0.0004 = 13.8384.&lt;/p&gt; &lt;p&gt;Then, 13.8384 * 3.72. Let me compute that.&lt;/p&gt; &lt;p&gt;13.8384 * 3 = 41.5152&lt;/p&gt; &lt;p&gt;13.8384 * 0.7 = 9.68688&lt;/p&gt; &lt;p&gt;13.8384 * 0.02 = 0.276768&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.5152 + 9.68688 = 51.20208&lt;/p&gt; &lt;p&gt;51.20208 + 0.276768 = 51.478848 cm³&lt;/p&gt; &lt;p&gt;So, 3.72³ is approximately 51.478848 cm³, which is a bit less than 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, let's try 3.73.&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = ?&lt;/p&gt; &lt;p&gt;Again, using the same method:&lt;/p&gt; &lt;p&gt;3.73 is 3.7 + 0.03&lt;/p&gt; &lt;p&gt;So, (3.7 + 0.03)² = 3.7² + 23.7 0.03 + 0.03² = 13.69 + 0.222 + 0.0009 = 13.9129&lt;/p&gt; &lt;p&gt;Then, 13.9129 * 3.73.&lt;/p&gt; &lt;p&gt;Compute step by step:&lt;/p&gt; &lt;p&gt;13.9129 * 3 = 41.7387&lt;/p&gt; &lt;p&gt;13.9129 * 0.7 = 9.73903&lt;/p&gt; &lt;p&gt;13.9129 * 0.03 = 0.417387&lt;/p&gt; &lt;p&gt;Adding them up:&lt;/p&gt; &lt;p&gt;41.7387 + 9.73903 = 51.47773&lt;/p&gt; &lt;p&gt;51.47773 + 0.417387 = 51.895117 cm³&lt;/p&gt; &lt;p&gt;Wow, that's very close to 51.88 cm³. So, 3.73³ is approximately 51.895117 cm³.&lt;/p&gt; &lt;p&gt;Which is a little over 51.88 cm³. So, the exact cube root is just a little less than 3.73 cm. Maybe 3.728 cm?&lt;/p&gt; &lt;p&gt;Wait, perhaps I should use linear approximation or Newton-Raphson method for a better estimate. But this is getting complicated. Maybe I can use a calculator here, but since I don't have one, perhaps I can estimate.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the exact calculation.&lt;/p&gt; &lt;p&gt;But actually, perhaps I can note that 3.72³ is 51.478848, and 3.73³ is 51.895117. Since 51.88 is between them, so the cube root is somewhere in between.&lt;/p&gt; &lt;p&gt;The difference between 51.88 and 51.478848 is 0.401152.&lt;/p&gt; &lt;p&gt;The difference between 51.895117 and 51.478848 is 0.416269.&lt;/p&gt; &lt;p&gt;So, the fraction is 0.401152 / 0.416269 ≈ 0.962.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 + 0.962*(0.001) ≈ 3.72 + 0.000962 ≈ 3.720962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;Therefore, the side length is approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;But wait, let me double-check my calculations because this seems a bit tedious.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the formula for the volume of a cube and solve for the side length.&lt;/p&gt; &lt;p&gt;Wait, another approach: let's take the cube root of 51.88.&lt;/p&gt; &lt;p&gt;We know that 3.7³ is 50.653, and 3.72³ is 51.4788, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.72³ = 51.4788&lt;/p&gt; &lt;p&gt;Let me compute 3.72 + delta, where delta is small, such that (3.72 + delta)³ = 51.88&lt;/p&gt; &lt;p&gt;Using the binomial expansion:&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 3.72³ + 3&lt;em&gt;(3.72)²delta + 3 (3.72)&lt;/em&gt;delta² + delta³&lt;/p&gt; &lt;p&gt;We can ignore delta squared and delta cubed since delta is small.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; ≈ 3.72³ + 3*(3.72)²delta = 51.4788 + 3 (13.8384)delta = 51.4788 + 41.5152 delta&lt;/p&gt; &lt;p&gt;Set this equal to 51.88:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;Subtract 51.4788:&lt;/p&gt; &lt;p&gt;41.5152*delta = 0.4012&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;delta ≈ 0.4012 / 41.5152 ≈ 0.00962&lt;/p&gt; &lt;p&gt;So, delta ≈ 0.00962 cm.&lt;/p&gt; &lt;p&gt;Therefore, the cube root is approximately 3.72 + 0.00962 ≈ 3.72962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;But wait, when I cube 3.73, I get 51.895 cm³, which is just a bit over 51.88. So, actually, the cube root is slightly less than 3.73. Let me denote delta as negative.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 51.88&lt;/p&gt; &lt;p&gt;We found that 3.72³ is 51.4788, and 3.73³ is 51.8951.&lt;/p&gt; &lt;p&gt;So, 51.88 is 51.8951 - 0.0151 above 3.73³.&lt;/p&gt; &lt;p&gt;So, delta is such that:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;So, 41.5152*delta = 51.88 - 51.4788 = 0.4012&lt;/p&gt; &lt;p&gt;Therefore,&lt;/p&gt; &lt;p&gt;delta = 0.4012 / 41.5152 ≈ 0.00962&lt;/p&gt; &lt;p&gt;But since 3.73³ is higher, delta is negative, so delta ≈ -0.00962 cm.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 - 0.00962 ≈ 3.71038 cm.&lt;/p&gt; &lt;p&gt;Therefore, approximately 3.7104 cm.&lt;/p&gt; &lt;p&gt;So, the side length is approximately 3.71 cm.&lt;/p&gt; &lt;p&gt;Wait, let me verify by cubing 3.71:&lt;/p&gt; &lt;p&gt;3.71 * 3.71 = 13.7641&lt;/p&gt; &lt;p&gt;13.7641 * 3.71:&lt;/p&gt; &lt;p&gt;Let me compute 13.7641 * 3 = 41.2923&lt;/p&gt; &lt;p&gt;13.7641 * 0.7 = 9.63487&lt;/p&gt; &lt;p&gt;13.7641 * 0.01 = 0.137641&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.2923 + 9.63487 = 50.92717&lt;/p&gt; &lt;p&gt;50.92717 + 0.137641 ≈ 51.06481 cm³&lt;/p&gt; &lt;p&gt;Hmm, 51.06481, which is still less than 51.88. So, 3.71³ is 51.06481 cm³.&lt;/p&gt; &lt;p&gt;So, that's still less than 51.88. Let me try 3.72:&lt;/p&gt; &lt;p&gt;As before, 3.72³ is 51.4788 cm³, which is still less.&lt;/p&gt; &lt;p&gt;3.721³: Let's compute 3.721³.&lt;/p&gt; &lt;p&gt;First, 3.721 * 3.721:&lt;/p&gt; &lt;p&gt;3.721 * 3.721&lt;/p&gt; &lt;p&gt;Compute 3 * 3.721 = 11.163&lt;/p&gt; &lt;p&gt;0.7 * 3.721 = 2.6047&lt;/p&gt; &lt;p&gt;0.02 * 3.721 = 0.07442&lt;/p&gt; &lt;p&gt;0.001 * 3.721 = 0.003721&lt;/p&gt; &lt;p&gt;So, adding them:&lt;/p&gt; &lt;p&gt;11.163 + 2.6047 = 13.7677&lt;/p&gt; &lt;p&gt;13.7677 + 0.07442 = 13.84212&lt;/p&gt; &lt;p&gt;13.84212 + 0.003721 ≈ 13.845841&lt;/p&gt; &lt;p&gt;So, 3.721² ≈ 13.845841&lt;/p&gt; &lt;p&gt;Now, 13.845841 * 3.721:&lt;/p&gt; &lt;p&gt;Again, break it down:&lt;/p&gt; &lt;p&gt;13 * 3.721 = 48.373&lt;/p&gt; &lt;p&gt;0.845841 * 3.721 ≈ Let's compute 0.8 * 3.721 = 2.9768&lt;/p&gt; &lt;p&gt;0.04 * 3.721 = 0.14884&lt;/p&gt; &lt;p&gt;0.005841 * 3.721 ≈ 0.02176&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;2.9768 + 0.14884 = 3.12564&lt;/p&gt; &lt;p&gt;3.12564 + 0.02176 ≈ 3.1474&lt;/p&gt; &lt;p&gt;So, total is 48.373 + 3.1474 ≈ 51.5204 cm³&lt;/p&gt; &lt;p&gt;So, 3.721³ ≈ 51.5204 cm³&lt;/p&gt; &lt;p&gt;Which is still less than 51.88. Hmm, so 3.721³ is 51.5204, which is less than 51.88. So, let's try 3.722.&lt;/p&gt; &lt;p&gt;Compute 3.722³:&lt;/p&gt; &lt;p&gt;First, 3.722²:&lt;/p&gt; &lt;p&gt;Again, 3.72² is 13.8384, so 3.722² = 13.8384 + 2&lt;em&gt;(0.002)&lt;/em&gt;3.72 + (0.002)&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Wait, actually, 3.722 is 3.72 + 0.002, so:&lt;/p&gt; &lt;p&gt;(3.72 + 0.002)² = 3.72² + 23.72 0.002 + 0.002² = 13.8384 + 0.01488 + 0.000004 = 13.853284&lt;/p&gt; &lt;p&gt;Then, 13.853284 * 3.722:&lt;/p&gt; &lt;p&gt;Break it down:&lt;/p&gt; &lt;p&gt;13 * 3.722 = 48.386&lt;/p&gt; &lt;p&gt;0.853284 * 3.722 ≈ Let's compute:&lt;/p&gt; &lt;p&gt;0.8 * 3.722 = 2.9776&lt;/p&gt; &lt;p&gt;0.05 * 3.722 = 0.1861&lt;/p&gt; &lt;p&gt;0.003284 * 3.722 ≈ 0.01222&lt;/p&gt; &lt;p&gt;Adding them:&lt;/p&gt; &lt;p&gt;2.9776 + 0.1861 = 3.1637&lt;/p&gt; &lt;p&gt;3.1637 + 0.01222 ≈ 3.1759&lt;/p&gt; &lt;p&gt;Total volume: 48.386 + 3.1759 ≈ 51.5619 cm³&lt;/p&gt; &lt;p&gt;So, 3.722³ ≈ 51.5619 cm³, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.722³ = 51.5619&lt;/p&gt; &lt;p&gt;Difference from 51.88: 51.88 - 51.5619 = 0.3181&lt;/p&gt; &lt;p&gt;So, need to add more.&lt;/p&gt; &lt;p&gt;Let me compute the derivative of the volume with respect to side length to approximate the delta.&lt;/p&gt; &lt;p&gt;V = s³&lt;/p&gt; &lt;p&gt;dV/ds = 3s²&lt;/p&gt; &lt;p&gt;At s = 3.722, dV/ds = 3&lt;em&gt;(3.722)² ≈ 3&lt;/em&gt;13.853284 ≈ 41.55985&lt;/p&gt; &lt;p&gt;So, delta V ≈ dV/ds * delta = 41.55985 * delta = 0.3181&lt;/p&gt; &lt;p&gt;So, delta ≈ 0.3181 / 41.55985 ≈ 0.00765 cm&lt;/p&gt; &lt;p&gt;Therefore, s ≈ 3.722 + 0.00765 ≈ 3.72965 cm&lt;/p&gt; &lt;p&gt;So, approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;Therefore, the side length of the cube is approximately 3.73 cm&lt;/p&gt; &lt;p&gt;To be precise, about 3.73 cm.&lt;/p&gt; &lt;p&gt;Therefore, the edge length of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Let me verify by cubing 3.73:&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = 13.9129&lt;/p&gt; &lt;p&gt;13.9129 * 3.73:&lt;/p&gt; &lt;p&gt;13 * 3.73 = 48.49&lt;/p&gt; &lt;p&gt;0.9129 * 3.73 ≈ Let's compute:&lt;/p&gt; &lt;p&gt;0.9 * 3.73 = 3.357&lt;/p&gt; &lt;p&gt;0.0129 * 3.73 ≈ 0.0482&lt;/p&gt; &lt;p&gt;Adding together: 3.357 + 0.0482 ≈ 3.4052&lt;/p&gt; &lt;p&gt;Total volume: 48.49 + 3.4052 ≈ 51.8952 cm³&lt;/p&gt; &lt;p&gt;Which is very close to 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, 3.73 cm³ is approximately 51.8952, which is about 51.90, which is very close to 51.88.&lt;/p&gt; &lt;p&gt;Therefore, 3.73 cm is a good approximation.&lt;/p&gt; &lt;p&gt;So, the edge length is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Hence, the edge of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Final Answer The edge of the cube is approximately \boxed{3.73} cm. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7pxn7</id>
    <title>Open WebUI adds reasoning-focused features in two new releases OUT TODAY!!! 0.5.5 adds "Thinking" tag support to streamline reasoning model chats (works with R1) . 0.5.6 brings new "reasoning_effort" parameter to control cognitive effort.</title>
    <updated>2025-01-22T23:56:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These friggin' guys are always dropping great new features just when we need them! They had been quiet for the last few weeks, but not anymore! Per the release notes from Open WebUI releases page today:&lt;br /&gt; &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.6:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Effortful Reasoning Control for OpenAI Models:&lt;/strong&gt; Introduced the reasoning_effort parameter in chat controls for supported OpenAI models, enabling users to fine-tune how much cognitive effort a model dedicates to its responses, offering greater customization for complex queries and reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.5:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🤔 &lt;strong&gt;Native 'Think' Tag Support:&lt;/strong&gt; Introduced the new 'think' tag support that visually displays how long the model is thinking, omitting the reasoning content itself until the next turn. Ideal for creating a more streamlined and focused interaction experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it’s true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I’ve been working through documentation and different LLM‘s. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt… I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it’s THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7w583</id>
    <title>Jan now supports DeepSeek R1 distills - model loading error fixed</title>
    <updated>2025-01-23T05:14:01+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt; &lt;img alt="Jan now supports DeepSeek R1 distills - model loading error fixed" src="https://external-preview.redd.it/292fRQ6ybDOj_r18_tYuGBSwlG2q4koHTYhO2NFfzsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d01f7aa3e4b8157f9ad15919e2a4665a8fd758d" title="Jan now supports DeepSeek R1 distills - model loading error fixed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jandotai/status/1882282461041967605"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nmk5</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted</title>
    <updated>2025-01-22T22:15:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted?fbclid=IwZXh0bgNhZW0CMTEAAR3i39eJbThbgTnI0Yz4JdnkMXgvj4wlorxOdbBeccw35kkqWqyrG816HpI_aem_EoENoW6h6SP-aU7FVwBWiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:15:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81ev6</id>
    <title>DeepSeek added recommandations for R1 local use to model card</title>
    <updated>2025-01-23T11:36:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1, Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Avoid adding a system prompt; all instructions should be contained within the user prompt.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;For mathematical problems, it is advisable to include a directive in your prompt such as: &amp;quot;Please reason step by step, and put your final answer within \boxed{}.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When evaluating model performance, it is recommended to conduct multiple tests and average the results.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i821n9</id>
    <title>Introducing Salt: A New Approach to Speech Generation Built on LLMs (+ Open-Sourced Code!)</title>
    <updated>2025-01-23T12:17:47+00:00</updated>
    <author>
      <name>/u/Mysterious_Hearing14</name>
      <uri>https://old.reddit.com/user/Mysterious_Hearing14</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We’re excited to share &lt;strong&gt;Salt&lt;/strong&gt;, a speech generation project we’ve been working on since August. Here’s a quick dive into its journey, technical details, and open resources! &lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Development Timeline&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;August 2024&lt;/strong&gt;: Started prototyping.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Late August&lt;/strong&gt;: First working model (spoiler: it was rough).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Approach&lt;/strong&gt;: Instead of traditional transformer-based TTS, we extended a pre-trained LLM (Llama3 3B) with a &lt;strong&gt;custom tokenizer&lt;/strong&gt; (WavTokenizer for audio) and trained it end-to-end for &lt;strong&gt;TTS&lt;/strong&gt; and &lt;strong&gt;ASR&lt;/strong&gt;.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Key Details&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: ~150 A100 GPU hours.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Think “DALL-E 1 for speech” – autoregressive generation using a modified LLM backbone.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It’s Unique&lt;/strong&gt;: Unlike adapter-based models or pure TTS frameworks, Salt leverages LLM capabilities while handling audio tokens natively, also voice is prompt controllable.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Current State&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Competitive SIM-O scores (~0.63) against models like Mini Omni and Llama Omni (see &lt;a href="https://github.com/VikhrModels/aulate"&gt;metrics&lt;/a&gt;).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Future Work&lt;/strong&gt;: Fine-tuning for &lt;strong&gt;music/audio generation&lt;/strong&gt;! Paper and updates coming soon.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;&lt;strong&gt;Open Resources&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/Vikhrmodels/salt-asr_wav-uni_1_tts_wav-uni_1-12k"&gt;Hugging Face&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Demo&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/drive/1Fbv1-4iBKSymgzAF2b1A4aCP9ajI4USL?usp=sharing"&gt;Google Colab&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Training Code&lt;/strong&gt;: &lt;a href="https://github.com/VikhrModels/Salt"&gt;GitHub&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Metrics Toolkit&lt;/strong&gt;: &lt;a href="https://github.com/VikhrModels/aulate"&gt;Aulate&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Hearing14"&gt; /u/Mysterious_Hearing14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i821n9/introducing_salt_a_new_approach_to_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T12:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81pbk</id>
    <title>This merge is amazing: FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-23T11:56:19+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model is a Fusion(merge?) of DeepSeekR1 &amp;amp; QwQ &amp;amp; SkyT1 32B models.&lt;/p&gt; &lt;p&gt;I usually don’t use merged models because, most of the time, I can’t find any noticeable differences. However, this one is truly different. Here are some examples:&lt;/p&gt; &lt;p&gt;My test setup is ollama + IQ4_XS + 32k context + Q8_0 KV Cache, questions are from: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Hello! I have multiple different files with different sizes, I want to move files from disk 1 to disk 2, which has only 688 space available. Without yapping, and being as concise as possible. What combination of files gets me closer to that number? The file sizes are: 36, 36, 49, 53, 54, 54, 63, 94, 94, 107, 164, 201, 361, 478&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/W2QFACDM"&gt;https://pastebin.com/W2QFACDM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To maximize the use of disk 2's 688-unit capacity, here are the optimal file combinations: 1. **478 + 53 + 54 + 54 + 49 = 688** 2. **361 + 201 + 36 + 36 + 54 = 688** 3. **478 + 107 + 49 + 54 = 688** 4. **361 + 107 + 49 + 54 + 54 + 63 = 688** Each combination precisely fills the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;This is first time I saw a local model give me 4 correct answers for this question, it's amazing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/2VJrjVmJ"&gt;https://pastebin.com/2VJrjVmJ&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The combination of files that gets you closest to the 688 space limit is: 361 + 164 + 107 + 54 + 54 = 686 This totals 686 units, which is just 2 units less than the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How many days are between 12-12-1971 and 18-4-2024?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/PQGr1DDq"&gt;https://pastebin.com/PQGr1DDq&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The number of days between December 12, 1971, and April 18, 2024, is 19,121 days . This calculation includes the full years, accounts for leap years, and considers the days in the partial years, providing the difference between the two dates. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is correct answer, 19121 days&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/T0iNuYHf"&gt;https://pastebin.com/T0iNuYHf&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### **Final Answer** \[ \boxed{19,486} \] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 3 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/MZZ5nDiM"&gt;https://pastebin.com/MZZ5nDiM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To transport the man, sheep, and cat across the river, the solution depends on the boat's capacity. Given that the boat has &amp;quot;3 free spaces,&amp;quot; it can carry three items in total, including the man. Therefore, the man can take all three (himself, the sheep, and the cat) across the river in one trip. Here's the step-by-step solution: 1. **First Trip**: The man boards the boat with the sheep and the cat. 2. **Crossing the River**: The man rows the boat across the river to the other side. 3. **Arrival**: Upon reaching the other side, the man, sheep, and cat all disembark from the boat. This completes the task as all three are now on the opposite side of the river in a single trip. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is the correct answer, they only need to cross the river once&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/XHQHwQth"&gt;https://pastebin.com/XHQHwQth&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*fell into an infinite loop, I had to terminate it* &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zeqz</id>
    <title>Nivida just open sourced their long context goodies - 128k context for 50% less memory</title>
    <updated>2025-01-23T09:09:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;edited : &lt;del&gt;50%&lt;/del&gt; 35% less memory&lt;/p&gt; &lt;p&gt;If you need long context for RAG, tool use, agents, or just because, Nvidia released a new library to make it super simple.&lt;/p&gt; &lt;p&gt;TLDR: You can get 128k context at 35% less memory&lt;/p&gt; &lt;p&gt;Here's a blog post on everything: &lt;a href="https://huggingface.co/blog/nvidia/kvpress"&gt;https://huggingface.co/blog/nvidia/kvpress&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81znv</id>
    <title>SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!</title>
    <updated>2025-01-23T12:14:25+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt; &lt;img alt="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" src="https://external-preview.redd.it/GQobgTxgKDrhJhG-UAnJyEaSqhi-5vBwrdogSL5wNmU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b8e0c51d51b02acc52b56c533ba8ea0c229040" title="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! &lt;/p&gt; &lt;p&gt;It's Andi from multimodal team at Hugging Face Today we released new SmolVLM: tiny vision language models that come in 256M and 500M parameters!&lt;/p&gt; &lt;p&gt;How did we compress SmolVLM 2.2B even further while making it generalize?&lt;br /&gt; &amp;gt; Smoller image encoder (SigLIP-base 93M)&lt;br /&gt; &amp;gt; More diverse data mixture&lt;br /&gt; &amp;gt; Better efficiency in tokenization for images&lt;/p&gt; &lt;p&gt;We have released base and instruction fine-tuned variants, they work out of the box with transformers and TRL, and we also release a WebGPU demo. &lt;/p&gt; &lt;p&gt;All of them are in this collection: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0"&gt;https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0&lt;/a&gt;&lt;br /&gt; Here's a demo: &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo"&gt;https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo&lt;/a&gt;&lt;br /&gt; Also here's the blog for more in-depth documentation:&lt;a href="https://huggingface.co/blog/smolervlm"&gt;https://huggingface.co/blog/smolervlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to see what you think!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066"&gt;https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T12:14:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x5nd</id>
    <title>The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)</title>
    <updated>2025-01-23T06:19:53+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt; &lt;img alt="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" src="https://preview.redd.it/o28q2pl6roee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbcc9bcc3ec8f998f0c0b948df4117b2f7a6ae30" title="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o28q2pl6roee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81iim</id>
    <title>Been ages since google released an open model</title>
    <updated>2025-01-23T11:43:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt; &lt;img alt="Been ages since google released an open model" src="https://preview.redd.it/fa91scqqdqee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f78c10935c8984f8f9d17834c7720f182fed482" title="Been ages since google released an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa91scqqdqee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80cwf</id>
    <title>deepseek is a side project</title>
    <updated>2025-01-23T10:22:48+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt; &lt;img alt="deepseek is a side project" src="https://preview.redd.it/zdvrlxahzpee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df808bd8bc2e4ba90db2fdb005eaae092d5d8206" title="deepseek is a side project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zdvrlxahzpee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:22:48+00:00</published>
  </entry>
</feed>
