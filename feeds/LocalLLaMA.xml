<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-17T16:40:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jctquk</id>
    <title>Introducing Mochi, a finetuned version of Moshi.</title>
    <updated>2025-03-16T19:38:29+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidBrowne17/Muchi"&gt;https://huggingface.co/DavidBrowne17/Muchi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finetuned a version of Moshi, using a modified version of this repo &lt;a href="https://github.com/yangdongchao/RSTnet"&gt;https://github.com/yangdongchao/RSTnet&lt;/a&gt; it still has some of the issues with intelligence but it seems better to me. Using that repo we can also finetune new moshi style models using other smarter LLMs than the helium model that moshi is based on. There is no moat.&lt;/p&gt; &lt;p&gt;Edit: Renamed to Muchi as there is already an AI named Mochi&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgkdt</id>
    <title>How to measure the true utilization of a GPU?</title>
    <updated>2025-03-17T16:17:03+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed Ollama to run against a NVidia A5000 24GB card. When I ask Gemma something, it reports 24 tokens per second speed. And that it's operating at close to &lt;a href="https://imgur.com/raf2nvG"&gt;100% utilization&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;However, if I hit it with multiple questions (one from OpenWebUI and one from command line) - it's still just as snappy at 24 tps. So obviously, 100% figure was incorrect (unless I misunderstand how GPUs operate). &lt;/p&gt; &lt;p&gt;So I am trying to understand 2 things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How is the GPU operating at the same speed whether it's 1 or 2 simultaneous questions.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;I am trying to gauge how many simultaneous queries I can let loose on this instance of Ollama.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgkdt/how_to_measure_the_true_utilization_of_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgkdt/how_to_measure_the_true_utilization_of_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgkdt/how_to_measure_the_true_utilization_of_a_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd0p8a</id>
    <title>Taking prompt suggestions for a new version of EQ-Bench creative writing benchmark</title>
    <updated>2025-03-17T00:57:20+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, creator of EQ-Bench here.&lt;/p&gt; &lt;p&gt;Many people have criticised the prompts in the current creative writing eval as, variously, &amp;quot;garbage&amp;quot; and &amp;quot;complete slop&amp;quot;. This is fair, and honestly I used chatgpt to make most of those prompts.&lt;/p&gt; &lt;p&gt;This time around there will be less of that. Give me your suggestions for prompts which:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;separate good writers from bad writers&lt;/li&gt; &lt;li&gt;you'd actually like to read for manual vibe checking&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Two slightly different questions because I may include prompts that are useful to humans but not include them in scoring.&lt;/p&gt; &lt;p&gt;The prototype is already much more discriminative between the top models (which is the reason I'm making a new version -- it was saturating).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T00:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7jll</id>
    <title>Gemma3 recommended settings on Ollama v0.6.1 (Open WebUI)</title>
    <updated>2025-03-17T08:03:38+00:00</updated>
    <author>
      <name>/u/Tx3hc78</name>
      <uri>https://old.reddit.com/user/Tx3hc78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So for Gemma3, it is recommended to use the following settings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 1.0 top_k = 64 top_p = 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But for ollama it was recommended to keep using only&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With new version of ollama &lt;strong&gt;v0.6.1&lt;/strong&gt; they improved handling of &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_k&lt;/code&gt; so what are we supposed to change back to general recommended values now?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Improved sampling parameters such as &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_k&lt;/code&gt; to behave similar to other implementations&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There is no mention for &lt;code&gt;top_p&lt;/code&gt; so should we set that to &lt;code&gt;0.95&lt;/code&gt; as well?&lt;/p&gt; &lt;p&gt;On the Gemma3 model page from &lt;a href="https://ollama.com/library/gemma3"&gt;ollama's website&lt;/a&gt; the &lt;code&gt;temperature&lt;/code&gt; parameter is still set to &lt;code&gt;0.1&lt;/code&gt;.&lt;br /&gt; Also do you set the &lt;code&gt;stop&lt;/code&gt; (Stop Sequence) parameter to &lt;code&gt;&amp;quot;&amp;lt;end_of_turn&amp;gt;&amp;quot;&lt;/code&gt; as well? Like it says from ollama website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tx3hc78"&gt; /u/Tx3hc78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jct1lk</id>
    <title>PR for native Windows support was just submitted to vLLM</title>
    <updated>2025-03-16T19:08:31+00:00</updated>
    <author>
      <name>/u/Nextil</name>
      <uri>https://old.reddit.com/user/Nextil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;User SystemPanic just &lt;a href="https://github.com/vllm-project/vllm/pull/14891"&gt;submitted a PR&lt;/a&gt; to the vLLM repo adding native Windows support. Before now it was only possible to run on Linux/WSL. This should make it significantly easier to run new models (especially VLMs) on Windows. No builds that I can see but it includes build instructions. The patched repo is &lt;a href="https://github.com/SystemPanic/vllm-windows/tree/vllm-windows"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The PR mentions submitting a FlashInfer PR adding Windows support, but that doesn't appear to have been done as of writing so it might not be possible to build just yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nextil"&gt; /u/Nextil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7u63</id>
    <title>[Open Source] Deploy and run voice AI models with one click on MacOS</title>
    <updated>2025-03-17T08:26:54+00:00</updated>
    <author>
      <name>/u/Heybud221</name>
      <uri>https://old.reddit.com/user/Heybud221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Ilikepizza2/localspeech-AI"&gt;LocalSpeech&lt;/a&gt; is an open source project that I created to make it easy to run and deploy Voice AI models on MacOS in an openai compliant api server along with an API playground. Currently it supports Zonos, Spark, Whisper and Kokoro. Had been away for the weekend so I am still working on adding support for Sesame CSM. &lt;/p&gt; &lt;p&gt;Currently learning MLOps to make it reliable for prod. I don't have a good GPU machine for linux, so I am not able to test but I want this to be compatible with linux too. If you have one and are willing to assist, PRs would be welcome :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heybud221"&gt; /u/Heybud221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd8cw3</id>
    <title>Is there a better service than Grok for NSFW 'Creative Writing'?</title>
    <updated>2025-03-17T09:08:28+00:00</updated>
    <author>
      <name>/u/PangurBanTheCat</name>
      <uri>https://old.reddit.com/user/PangurBanTheCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've found it to be largely unfiltered, while also very capable. However every so often it just outright refuses requests, or the service will just... start writing really poorly? It's also a bit spendy at $30/month. &lt;/p&gt; &lt;p&gt;I want something that's intelligent and capable of handling maybe a short-novels worth of context. I enjoy writing things that are maybe a few chapters worth. &lt;/p&gt; &lt;p&gt;So far the only service I've encountered that can handle my needs is Grok. It's fairly intelligent, it writes well as long as it's prompted to do so, it has good understanding of what or who characters are and how they should respond in certain situations, and it can handle a pretty decent amount of context. &lt;/p&gt; &lt;p&gt;Just wondering if there's anything better. Or at least cheaper while offering the same. Or even just more unfiltered as the occasional refusals do get annoying.&lt;/p&gt; &lt;p&gt;Edit: Oh, also, if you push Grok too much to try to get past it's refusals, it will literally just delete the entire session. So. Yeah, fuck that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangurBanTheCat"&gt; /u/PangurBanTheCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T09:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9ttx</id>
    <title>PSA: c4ai-command-a-03-2025 seems to be trained for reasoning / "thinking"</title>
    <updated>2025-03-17T10:53:07+00:00</updated>
    <author>
      <name>/u/CheatCodesOfLife</name>
      <uri>https://old.reddit.com/user/CheatCodesOfLife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tested &lt;a href="https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF"&gt;c4ai-command-a-03-2025-GGUF&lt;/a&gt; Q4_K with this simple prompt (very crude, I'm sure there's a lot of room for improvement) system prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Think about your response within &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags before responding to the user. There's no need for structure or formatting, take as long as you need. When you're ready, write the final response outside the thinking tags. The user will only see the final response.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It even did the QwQ/R1-style reasoning with &amp;quot;wait...&amp;quot; within the tags, and it managed to solve a problem that no other local model I've tried could solve.&lt;/p&gt; &lt;p&gt;Without the system prompt, it just gave me the usual incorrect response that other models like Mistral-Large and QwQ provide.&lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheatCodesOfLife"&gt; /u/CheatCodesOfLife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T10:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdf5ag</id>
    <title>New Paper by Yann LeCun (META) - Transformers without Normalization</title>
    <updated>2025-03-17T15:19:34+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://arxiv.org/abs/2503.10622"&gt;Transformers without Normalization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new AI paper by Yann LeCun (@ylecun), one of the fathers of Deep Learning, has been released, and it could bring a radical shift in the architecture of deep neural networks and LLMs.&lt;/p&gt; &lt;p&gt;The paper is called &lt;em&gt;&amp;quot;Transformers without Normalization&amp;quot;&lt;/em&gt; and introduces a surprisingly simple technique called Dynamic Tanh (DyT), which replaces traditional normalization layers (Layer Norm or RMSNorm) with a single operation:&lt;br /&gt; DyT(x) = tanh(αx)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdf5ag/new_paper_by_yann_lecun_meta_transformers_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcqy6c</id>
    <title>We have Deep Research at home</title>
    <updated>2025-03-16T17:39:04+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt; &lt;img alt="We have Deep Research at home" src="https://external-preview.redd.it/NA_JTAjwBAYLbzLjIgJ3Q_k4TmFsR5MWHCoiYKiIQJ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9bd51d7c05f78cbad725a12ad69bc6ff6fe2ec" title="We have Deep Research at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/atineiatte/deep-research-at-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd0zpf</id>
    <title>Token Explorer - A simple interface for quickly exploring and modifying the token generation process!</title>
    <updated>2025-03-17T01:12:19+00:00</updated>
    <author>
      <name>/u/CountBayesie</name>
      <uri>https://old.reddit.com/user/CountBayesie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend &lt;em&gt;a lot&lt;/em&gt; of my time working on the logit end of LLMs and have long wanted a way to more quickly and interactively understand what LLMs are doing during the token generation process and how that might help us improve prompting and better understand these models!&lt;/p&gt; &lt;p&gt;So to scratch that itch I put together &lt;a href="https://github.com/willkurt/token-explorer"&gt;Token Explorer&lt;/a&gt;. It's an open source Python tool with a simple interface that allows you to visually step through the token generation process.&lt;/p&gt; &lt;p&gt;Features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simple keyboard interface (WASD + arrow keys).&lt;/li&gt; &lt;li&gt;Ability to select which token is chosen at each step.&lt;/li&gt; &lt;li&gt;Likewise, the ability to backtrack and try a new path.&lt;/li&gt; &lt;li&gt;Fork prompts and iterate them to explore and compare alternative sampling possibilities.&lt;/li&gt; &lt;li&gt;Visualization layers allow you to see the &lt;em&gt;probability&lt;/em&gt; of each token at time generation and the &lt;em&gt;entropy&lt;/em&gt; of tokens in the prompt/generation so far.&lt;/li&gt; &lt;li&gt;Load prompts from a plain text file.&lt;/li&gt; &lt;li&gt;Defaults to &lt;code&gt;Qwen/Qwen2.5-0.5B&lt;/code&gt; so can be run on most hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The caveat, of course, is that this &lt;em&gt;is&lt;/em&gt; just a quick weekend project so it's a bit rough around the edges. The current setup is absolutely not built for performance so trying long prompts and large models might cause some issues.&lt;/p&gt; &lt;p&gt;Nonethless, I thought people might appreciate the ability to experiment with the internal sampling process of LLMs. I've already had a lot of fun testing out whether or not the LLM can still get the correct answer to math questions if you intentionally make it choose low probability tokens! It's also interesting to look at prompts and see where the model is the most uncertain and how changing that can impact downstream success!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CountBayesie"&gt; /u/CountBayesie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T01:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdawqj</id>
    <title>What is the difference between an AI agent and a background job calling LLM API?</title>
    <updated>2025-03-17T11:58:44+00:00</updated>
    <author>
      <name>/u/superloser48</name>
      <uri>https://old.reddit.com/user/superloser48</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - I am a programmer and I use LLMs extensively for work. For coding and for data cleaning - I have found LLMs INSANELY helpful.&lt;/p&gt; &lt;p&gt;But I am struggling to understand the &lt;strong&gt;difference between using an AI agent vs calling the LLMs' API in a background job&lt;/strong&gt; (cron). My code currently runs in cron jobs and passes PDFs to LLMs' API to OCR for dirty PDFs. (eg. we have a lot of PDF submissions on our website).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is not a loaded question or a diss on AI agents.&lt;/strong&gt; Would love it if someone could point what can be done differently in a AI agent vs a background job. I am curious if I can reduce my codebase size for data cleaning.&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superloser48"&gt; /u/superloser48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jddh2e</id>
    <title>Do any of you have a "hidden gem" LLM that you use daily?</title>
    <updated>2025-03-17T14:07:31+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was common back in the Llama2 days when fine-tunes often out-performed the popular models. I don't see it quite as often, so I figured I'd ask.&lt;/p&gt; &lt;p&gt;For every major model (Mistral, Llama, Qwen, etc..) I'll try and download one community version of it to test out. Sometimes they're about &lt;em&gt;as&lt;/em&gt; good, sometimes they're slightly worse. Rarely are they better.&lt;/p&gt; &lt;p&gt;I'd say the &amp;quot;oddest&amp;quot; one I have is IBM-Granite-3.2-2B . Not exactly a community/small-time model, but it's managed to replace Llama 3B in certain use-cases for me. It performs exactly as well but is a fair bit smaller.&lt;/p&gt; &lt;p&gt;Are you using anything that you'd consider un/less common?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jddh2e/do_any_of_you_have_a_hidden_gem_llm_that_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T14:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdfdqz</id>
    <title>open source coding agent refact</title>
    <updated>2025-03-17T15:29:13+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"&gt; &lt;img alt="open source coding agent refact" src="https://preview.redd.it/vn8fcj32q9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f198892766072d65d46bc5c22c6a0a240669fddf" title="open source coding agent refact" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn8fcj32q9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfdqz/open_source_coding_agent_refact/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdfgx1</id>
    <title>QwQ 32B appears on LMSYS Arena Leaderboard</title>
    <updated>2025-03-17T15:32:48+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"&gt; &lt;img alt="QwQ 32B appears on LMSYS Arena Leaderboard" src="https://preview.redd.it/5zj3vxe1r9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9766a91861bd2ecad6af7013d5f14d8d3610d132" title="QwQ 32B appears on LMSYS Arena Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5zj3vxe1r9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdfgx1/qwq_32b_appears_on_lmsys_arena_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd616a</id>
    <title>Why are audio (tts/stt) models so much smaller in size than general llms?</title>
    <updated>2025-03-17T06:07:10+00:00</updated>
    <author>
      <name>/u/Heybud221</name>
      <uri>https://old.reddit.com/user/Heybud221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs have possible outputs comprising of words(text) but speech models require words as well as phenomes. Shouldn't they be larger?&lt;/p&gt; &lt;p&gt;From what I think, it is because they don't have the understanding (technically, llms also don't &amp;quot;understand&amp;quot; words) as much as LLMs. Is that correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heybud221"&gt; /u/Heybud221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T06:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd87wv</id>
    <title>underwhelming MCP Vs hype</title>
    <updated>2025-03-17T08:58:04+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My early thoughts on MCPs :&lt;/p&gt; &lt;p&gt;As I see the current state of hype, the experience is underwhelming:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Confusing targeting — developers and non devs both.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For devs — it’s straightforward coding agent basically just llm.txt , so why would I use MCP isn’t clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For non devs — It’s like tools that can be published by anyone and some setup to add config etc. But the same stuff has been tried by ChatGPT GPTs as well last year where anyone can publish their tools as GPTs, which in my experience didn’t work well.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;There’s isn’t a good client so far and the clients UIs not being open source makes the experience limited as in our case, no client natively support video upload and playback.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Installing MCPs on local machines can have setup issues later with larger MCPs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I feel the hype isn’t organic and fuelled by Anthropic. I was expecting MCP ( being a protocol ) to have deeper developer value for agentic workflows and communication standards then just a wrapper over docker and config files.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let’s imagine a world with lots of MCPs — how would I choose which one to install and why, how would it rank similar servers? Are they imagining it like a ecosystem like App store where my main client doesn’t change but I am able to achieve any tasks that I do with a SaaS product.&lt;/p&gt; &lt;p&gt;We tried a simple task — &lt;code&gt;&amp;quot;take the latest video on Gdrive and give me a summary&amp;quot;&lt;/code&gt; For this the steps were not easy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Go through Gdrive MCP and setup documentation — Gdrive MCP has 11 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;VideoDB MCP has 1 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall 12, 13 step to do a basic task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdc0hq</id>
    <title>Mathematics for Machine Learning: 417 page pdf ebook</title>
    <updated>2025-03-17T12:58:19+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mml-book.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T12:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx69i</id>
    <title>Text an LLM at +61493035885</title>
    <updated>2025-03-16T22:10:18+00:00</updated>
    <author>
      <name>/u/benkaiser</name>
      <uri>https://old.reddit.com/user/benkaiser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a basic service running on an old Android phone + cheap prepaid SIM card to allow people to send a text and receive a response from Llama 3.1 8B. I felt the need when we recently lost internet access during a tropical cyclone but SMS was still working.&lt;/p&gt; &lt;p&gt;Full details in the blog post: &lt;a href="https://benkaiser.dev/text-an-llm/"&gt;https://benkaiser.dev/text-an-llm/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benkaiser"&gt; /u/benkaiser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnh4</id>
    <title>Mistral Small 3.1 (24B)</title>
    <updated>2025-03-17T16:20:23+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt; &lt;img alt="Mistral Small 3.1 (24B)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Small 3.1 (24B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgqcj</id>
    <title>NEW MISTRAL JUST DROPPED</title>
    <updated>2025-03-17T16:23:29+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt; &lt;img alt="NEW MISTRAL JUST DROPPED" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="NEW MISTRAL JUST DROPPED" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Outperforms&lt;/strong&gt; GPT-4o Mini, Claude-3.5 Haiku, and others in text, vision, and multilingual tasks.&lt;br /&gt; &lt;strong&gt;128k context window&lt;/strong&gt;, blazing &lt;strong&gt;150 tokens/sec speed&lt;/strong&gt;, and runs on a &lt;strong&gt;single RTX 4090&lt;/strong&gt; or &lt;strong&gt;Mac (32GB RAM)&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;—free to use, fine-tune, and deploy. Handles chatbots, docs, images, and coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;https://mistral.ai/fr/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"&gt;https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdasng</id>
    <title>Heads up if you're using Gemma 3 vision</title>
    <updated>2025-03-17T11:52:09+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt; &lt;img alt="Heads up if you're using Gemma 3 vision" src="https://external-preview.redd.it/YXiABCYSLmR9qQ-LeZnYrdVC2XA7zhm-nWxrhAxjA3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f558a293cfea6675b1b3428038710d15adc358d8" title="Heads up if you're using Gemma 3 vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a quick heads up for anyone using Gemma 3 in &lt;strong&gt;LM Studio&lt;/strong&gt; or &lt;strong&gt;Koboldcpp&lt;/strong&gt;, its vision capabilities aren't fully functional within those interfaces, resulting in degraded quality. (I do not know about Open WebUI as I'm not using it).&lt;/p&gt; &lt;p&gt;I believe a lot of users potentially have used vision without realizing it has been more or less crippled, not showcasing Gemma 3's full potential. However, when you do &lt;strong&gt;not&lt;/strong&gt; use vision for details or texts, the degraded accuracy is often not noticeable and works quite good, for example with general artwork and landscapes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Koboldcpp&lt;/strong&gt; resizes images before being processed by Gemma 3, which particularly distorts details, perhaps most noticeable with smaller text. While Koboldcpp &lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.81.1"&gt;version 1.81&lt;/a&gt; (released January 7th) expanded supported resolutions and aspect ratios, the resizing still affects vision quality negatively, resulting in degraded accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; is behaving more odd, initial image input sent to Gemma 3 is relatively accurate (but still somewhat crippled, probably because it's doing re-scaling here as well), but subsequent regenerations using the same image or starting new chats with new images results in &lt;em&gt;significantly&lt;/em&gt; degraded output, most noticeable images with finer details such as characters in far distance or text.&lt;/p&gt; &lt;p&gt;When I send images to Gemma 3 directly (not through these UIs), its accuracy becomes much better, especially for details and texts.&lt;/p&gt; &lt;p&gt;Below is a collage (I can't upload multiple images on Reddit) demonstrating how vision quality degrades even more when doing a regeneration or starting a new chat in LM Studio.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb"&gt;https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9xjj</id>
    <title>Gemma 3 is now available for free on HuggingChat!</title>
    <updated>2025-03-17T10:59:36+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt; &lt;img alt="Gemma 3 is now available for free on HuggingChat!" src="https://external-preview.redd.it/fWy5OWPAOAEc-6PWhgSCmMIjuAf3liLshl-njJSXolI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc784c22ed0ab0a891d88c523a62c0663cf6cf" title="Gemma 3 is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/google/gemma-3-27b-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T10:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnw5</id>
    <title>Mistrall Small 3.1 released</title>
    <updated>2025-03-17T16:20:51+00:00</updated>
    <author>
      <name>/u/Dirky_</name>
      <uri>https://old.reddit.com/user/Dirky_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt; &lt;img alt="Mistrall Small 3.1 released" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistrall Small 3.1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dirky_"&gt; /u/Dirky_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdaq7x</id>
    <title>3x RTX 5090 watercooled in one desktop</title>
    <updated>2025-03-17T11:48:07+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt; &lt;img alt="3x RTX 5090 watercooled in one desktop" src="https://preview.redd.it/zsu6kw5pm8pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313ede5ac5797a563ccfc2f875620a34ab784cbe" title="3x RTX 5090 watercooled in one desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zsu6kw5pm8pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:48:07+00:00</published>
  </entry>
</feed>
