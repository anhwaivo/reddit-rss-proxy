<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T23:24:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n05h62</id>
    <title>Navigating LLM Inference on AMD 9070 XT: A Debian-based Journey</title>
    <updated>2025-08-25T22:58:17+00:00</updated>
    <author>
      <name>/u/Lazy-Routine-Handler</name>
      <uri>https://old.reddit.com/user/Lazy-Routine-Handler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throughout the past week, I've been delving into LLM model inference to better understand the hardware requirements. Running an AMD 9070xt on Debian posed its challenges, primarily due to dependency complications. While Ubuntu comes with support for ROCm, integrating ROCm on Debian systems was initially a complex task.&lt;/p&gt; &lt;p&gt;I found success by installing Debian 12, which supported the necessary drivers and packages, and then upgrading to Debian 13 to cover the remaining requirements. This strategic upgrade allowed both the integrated GPU (iGPU) and the discrete GPU (dGPU) to be recognized and utilized effectively.&lt;/p&gt; &lt;p&gt;Upon ensuring my hardware was ready, I conducted inference tests using PyTorch. After extensive testing, my peak throughput with PyTorch ROCm was 20-30 tokens per second (Tps). Dissatisfied with this performance, I explored llama.cpp for potential improvements.&lt;/p&gt; &lt;p&gt;Here's a quick comparison of my results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Operating System&lt;/th&gt; &lt;th align="left"&gt;Vulkan (Pre)compiled&lt;/th&gt; &lt;th align="left"&gt;ROCm Compile&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Debian 13&lt;/td&gt; &lt;td align="left"&gt;Prompt: ~200-600Tps \ Ouput: ~60Tps&lt;/td&gt; &lt;td align="left"&gt;Failed to Compile (linker crash on bfloat16)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Windows 11&lt;/td&gt; &lt;td align="left"&gt;Prompt: ~200-400Tps \ Ouput: ~60Tps&lt;/td&gt; &lt;td align="left"&gt;Requires WSL2&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;On the client side, using llama.cpp, the application reports completion times akin to around ~54Tps, with response times ranging from 0.3 to 1.5 seconds.&lt;/p&gt; &lt;p&gt;Given this testing, I believe a 9070xt configuration would be ample for the typical user aiming to run a local LLM, with an approximate cost of ~700USD.&lt;/p&gt; &lt;p&gt;**Additional Testing Insights (Using PyTorch):**&lt;/p&gt; &lt;p&gt;On Debian 13, I tested the following hardware configurations:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Hardware&lt;/th&gt; &lt;th align="left"&gt;PyTorch-CPU Throughput&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;R7-7800X3D &amp;amp; 32GB DDR5&lt;/td&gt; &lt;td align="left"&gt;~10Tps&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;E5-2699 &amp;amp; 256GB DDR4&lt;/td&gt; &lt;td align="left"&gt;~1Tps&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*Note: This post has been polished using the Microsoft/phi-3-mini-4k-instruct-gguf model and a final re-read.*&lt;br /&gt; *Note: When processing this post for reddit Prompt processing spiked to 1k Tps, so obviously my above chart is more along the lines of more moderate prompt size of a couple sentences.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy-Routine-Handler"&gt; /u/Lazy-Routine-Handler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n05h62/navigating_llm_inference_on_amd_9070_xt_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n05h62/navigating_llm_inference_on_amd_9070_xt_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n05h62/navigating_llm_inference_on_amd_9070_xt_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:58:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n05v0a</id>
    <title>Thoughts on E2E testing for MCP</title>
    <updated>2025-08-25T23:14:30+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n05v0a/thoughts_on_e2e_testing_for_mcp/"&gt; &lt;img alt="Thoughts on E2E testing for MCP" src="https://preview.redd.it/he9djhz509lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c34340a69d5ec6a14f5c01e8319d5748b502bd7e" title="Thoughts on E2E testing for MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is End to End (E2E) testing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;End to end testing (E2E) is a testing method that simulates a real user flow to validate the correctness. For example, if you're building a sign up page, you'd set up your E2E test to fill out the form inputs, click submit, and assert that a user account was created. E2E testing is the purest form of testing: it ensures that the system works from and end user's environment.&lt;/p&gt; &lt;p&gt;There's an &lt;a href="https://kentcdodds.com/blog/write-tests"&gt;awesome article&lt;/a&gt; by Kent Dodds comparing unit tests, integration tests, and E2E tests and explaining the pyramid of tests. I highly recommend giving that a read. In regards to E2E testing, it is the highest confidence form of testing. If your E2E tests work, you can ensure that it'll work for your end users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;E2E testing for MCP servers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;E2E testing for API servers is typical practice, where the E2E tests are testing a chain of API calls that simulate a real user flow. The same testing is needed for MCP servers where we set up an environment simulating an end user's environment and test popular user flows.&lt;/p&gt; &lt;p&gt;Whereas APIs are consumed by other APIs / web clients, MCP servers are consumed by LLMs and agents. End users are using MCP servers in MCP clients like Claude Desktop and Cursor. We need to simulate these environments in MCP E2E testing. This is where testing with Agents come in. We configure the agent to simulate an end user's environment. To build an E2E test for MCP servers, we connect the server to an agent and have the agent interact with the server. We have the agent run queries that real users would ask in chat and confirm whether or not the user flow ran correctly.&lt;/p&gt; &lt;p&gt;An example of running an E2E test for PayPal MCP:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Connect the PayPal MCP server to testing agent. To simulate Claude Desktop, we can configure the agent to use a Claude model with a default system prompt.&lt;/li&gt; &lt;li&gt;Query the agent to run a typical user query like &amp;quot;Create a refund for order ID 412&amp;quot;&lt;/li&gt; &lt;li&gt;Let the testing agent run the query.&lt;/li&gt; &lt;li&gt;Check the testing agents' tracing, make sure that it called the tool &lt;code&gt;create_refund&lt;/code&gt; and successfully created a refund.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For step 4, we can have an LLM as a judge analyzing the testing agent's trace and check if the query was a success.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we're building E2E tests at MCPJam&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're building MCPJam, an alternative to the MCP inspector - an open source testing and debugging tool for MCP servers. We started building E2E testing in the project and we're set to have a beta out for people to try sometime tomorrow. We're going to take the principles in this article to build the beta. We'd love to have the community test it out, critique our approach, and contribute!&lt;/p&gt; &lt;p&gt;If you like projects like this, please check out our repo and consider giving it a star! ⭐&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MCPJam/inspector"&gt;https://github.com/MCPJam/inspector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also discussing our E2E testing approach on Discord&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/JEnDtz8X6z"&gt;https://discord.com/invite/JEnDtz8X6z&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/he9djhz509lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n05v0a/thoughts_on_e2e_testing_for_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n05v0a/thoughts_on_e2e_testing_for_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mztnjn</id>
    <title>Explaining the Real Reason I Started My AI Chatbot Project</title>
    <updated>2025-08-25T15:30:14+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Since I’ve been sharing my progress here for a while, I realized I never actually explained why I decided to build my own chatbot platform in the first place. So I wanted to share the story behind it — and hear your thoughts. &lt;/p&gt; &lt;p&gt;I’ve been a SillyTavern user for over a year. It’s an amazing project — powerful, flexible, and full of features. But when I tried to get some of my friends (non-devs) into it… it was a disaster. And that experience is what pushed me to start building something new. &lt;/p&gt; &lt;p&gt;Here’s what happened: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Installation&lt;br /&gt; For people without a tech background, even the first step was too much.&lt;br /&gt; “Why do I need Node.js?” “Why isn’t this working?”&lt;br /&gt; Most didn’t even make it past setup. I had to handhold every step, including setting up a local LLM. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Interface&lt;br /&gt; Once they finally got it running, they were overwhelmed. The UI is super dense, menus and sliders everywhere, with no clear explanations. Questions I got: &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;“What does this slider even do?” &lt;/p&gt; &lt;p&gt;“How do I actually start chatting with a character?” &lt;/p&gt; &lt;p&gt;“Why does the chat keep resetting?” &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Characters, models, prompts&lt;br /&gt; Total confusion. Where to find characters? How to write prompts? Which models to pick, how to run them, whether their hardware could handle it?&lt;br /&gt; One of my friends literally asked if they needed to learn Python just to talk to a chatbot. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extensions and advanced features&lt;br /&gt; Most didn’t even know extensions or agents existed. And even if they did, all the info is scattered across Discord threads. Documentation is spotty at best, and half the knowledge is just “tribal.” &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So here’s where my project comes in&lt;br /&gt; That frustration gave me an idea: what if there was a dead-simple LLM chatbot platform? Something that just runs in the browser — no GitHub setup, no config hell, no Discord archaeology. &lt;/p&gt; &lt;p&gt;You’d just: &lt;/p&gt; &lt;p&gt;Pick a model &lt;/p&gt; &lt;p&gt;Load a character &lt;/p&gt; &lt;p&gt;Maybe tweak some behavior &lt;/p&gt; &lt;p&gt;And it just works. &lt;/p&gt; &lt;p&gt;Right now, it’s just me building this solo. I’ve been sharing my development journey here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, and I’ll keep posting progress updates, demos, and breakdowns as I go. &lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts on this problem - do you see the same barriers for newcomers?&lt;br /&gt; And if anyone here wants to help test my platform (currently with unlimited tokens), just DM me and I’ll send you an invite.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzpi3o</id>
    <title>Biased comparison of frontends</title>
    <updated>2025-08-25T12:46:08+00:00</updated>
    <author>
      <name>/u/moritzchow</name>
      <uri>https://old.reddit.com/user/moritzchow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since day 1 of my journey on using local LLMs (I jumped right in without actually trying the ChatGPT that kind of providers) I’ve been using Open-WebUI that is kind of vanilla when it comes to an Unraid server setup (Ollama + Open WebUI).&lt;/p&gt; &lt;p&gt;After going deeper into this I switched hardwares, backends, frontends, and become a little bit frustrated in the recent development of OWUI.&lt;/p&gt; &lt;p&gt;Let’s cut short (not short tbh):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open WebUI: Pros: &lt;/li&gt; &lt;li&gt;easy to use and setup on docker&lt;/li&gt; &lt;li&gt;integrated web search&lt;/li&gt; &lt;li&gt;customisation including parameters, TTS&lt;/li&gt; &lt;li&gt;WebUI to serve LLM across devices&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No native support on MCP servers (a dealbreaker for me since recent MCP development) - separate backend is required&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LM Studio: Pros:&lt;/li&gt; &lt;li&gt;one-stop solution for downloading and running local LLM on different hardwares including Apple Silicon&lt;/li&gt; &lt;li&gt;native MCP server support&lt;/li&gt; &lt;li&gt;easy to setup and run (can’t be easier tbh)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - no web search (it can be done via MCP tool tho) - no WebUI for serving LLM across devices (sad it’s almost perfect) - no plug-ins (the registration on beta channel did not work for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AnythingLLM: Pros:&lt;/li&gt; &lt;li&gt;Support Serving LLM on docker&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;AI Agent setup made easy&lt;/li&gt; &lt;li&gt;Sophisticated RAG setup&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No Serving LLM across devices if running desktop version - No customisation on using different external TTS endpoints - Agent has to be called out in each chat&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LibreChat: Pros:&lt;/li&gt; &lt;li&gt;Native support on MCP servers&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Pain in the bud in setting up&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SillyTavern Pros:&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;Sophisticated RP setting (some find it useful)&lt;/li&gt; &lt;li&gt;Extension available at ease on supporting MCP servers&lt;/li&gt; &lt;li&gt;customisable TTS setup&lt;/li&gt; &lt;li&gt;once it’s up and running you can get things out of it that no other frontends can give you&lt;/li&gt; &lt;li&gt;WebUI serving across devices is available&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Setting up docker is not the most easiest thing - setting up the rest through UI is a daunting task before things can be up and running - Seriously SillyTavern? How can it be named like that while having such full features available? I can’t even tell people I learn things through it&lt;/p&gt; &lt;p&gt;Verdict: I’m using ST now while it’s not the perfect solution and the damn silly name.&lt;/p&gt; &lt;p&gt;All the frontends tested here are quite good actually, it’s just that ST seems to offer more while meaning it’s another rabbit hole.&lt;/p&gt; &lt;p&gt;LM Studio is my go to backend + frontend for its support on different architectures including Apple Silicon (I switched to Apple from ROCm). If ever they can offer same interfaces via webUI it will be a killer.&lt;/p&gt; &lt;p&gt;Not tested much on LibreChat cuz it’s a painful setup and maintenance&lt;/p&gt; &lt;p&gt;Open WebUI started to becoming a No No for me since it’s MCPO model of supporting MCP servers&lt;/p&gt; &lt;p&gt;AnythingLLM - I’m not a big RAG user but it’s quite nice on that plus the nice interface. I just hated that I need to call the agent every new chat.&lt;/p&gt; &lt;p&gt;So to wrap up - give them a try yourself if you’re looking for different frontends. Plz let me know if you have some UI recommendations as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moritzchow"&gt; /u/moritzchow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T12:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwglm</id>
    <title>Dynamic summoning spell</title>
    <updated>2025-08-25T17:12:40+00:00</updated>
    <author>
      <name>/u/formicidfighter</name>
      <uri>https://old.reddit.com/user/formicidfighter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"&gt; &lt;img alt="Dynamic summoning spell" src="https://external-preview.redd.it/OWZtdjNjcTU3N2xmMezH27CblZKQ26GYAL2SlFoMJw3SGjfOit0CGUqdjNXv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d11eb5b1d07df4a8ce3c4eb16555f9851ed354af" title="Dynamic summoning spell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Exploring AI-powered game mechanics and built this demo to showcase what local embedding models and language models are capable of. They're already quite capable and I think are a promising way to make game experiences more dynamic. &lt;/p&gt; &lt;p&gt;Here’s a link to a &lt;a href="https://www.youtube.com/watch?v=DU06YYVKnNk"&gt;longer tutorial&lt;/a&gt; on how I built this. If you’re interested in playing around with it, check out &lt;a href="https://assetstore.unity.com/packages/tools/generative-ai/aviad-ai-llms-slms-for-unity-325891"&gt;our Unity package&lt;/a&gt; (free and open-source) that has some pre-built versions of these mechanics!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/formicidfighter"&gt; /u/formicidfighter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ws61ccq577lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwglm/dynamic_summoning_spell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzxvot</id>
    <title>GEPA: Reflective Prompt Evolution beats RL with 35× fewer rollouts</title>
    <updated>2025-08-25T18:04:17+00:00</updated>
    <author>
      <name>/u/No_Marionberry_5366</name>
      <uri>https://old.reddit.com/user/No_Marionberry_5366</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new preprint (Agrawal et al., 2025) introduces &lt;strong&gt;GEPA (Genetic-Pareto Prompt Evolution)&lt;/strong&gt;, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.&lt;/p&gt; &lt;p&gt;The results are striking:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GEPA outperforms GRPO by up to &lt;strong&gt;19%&lt;/strong&gt; while using &lt;strong&gt;35× fewer rollouts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.&lt;/li&gt; &lt;li&gt;In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as &lt;em&gt;textual artifacts&lt;/em&gt; that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.&lt;/p&gt; &lt;p&gt;What’s interesting is the infra angle: GEPA’s success in multi-hop QA hinges on generating better second-hop queries. &lt;strong&gt;That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself&lt;/strong&gt;. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently. &lt;strong&gt;Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This work suggests that the real frontier may not be reinforcement learning at scale, but &lt;strong&gt;language-native optimization loops&lt;/strong&gt; where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Marionberry_5366"&gt; /u/No_Marionberry_5366 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxvot/gepa_reflective_prompt_evolution_beats_rl_with_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm5dk</id>
    <title>support interns1-mini has been merged into llama.cpp</title>
    <updated>2025-08-25T09:49:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support interns1-mini has been merged into llama.cpp" src="https://external-preview.redd.it/C4PZMcjKvXogRwaLothTEm2AuNm9c8ehdTTP3nuiquQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=136a713391edcd4645ecfc6fd874eb5f837f3b30" title="support interns1-mini has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;Intern-S1-mini&lt;/strong&gt;, a lightweight open-source multimodal reasoning model based on the same techniques as &lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;&lt;strong&gt;Intern-S1&lt;/strong&gt;&lt;/a&gt;. Built upon an 8B dense language model (Qwen3) and a 0.3B Vision encoder (InternViT), Intern-S1-mini has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1-mini to be a capable research assistant for real-world scientific applications.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini#features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt; &lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt; &lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas and protein sequences.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15412"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzyg24</id>
    <title>NVIDIA Jetson AGX Thor seems to be available for preorder</title>
    <updated>2025-08-25T18:25:31+00:00</updated>
    <author>
      <name>/u/disillusioned_okapi</name>
      <uri>https://old.reddit.com/user/disillusioned_okapi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/introducing-nvidia-jetson-thor-the-ultimate-platform-for-physical-ai/"&gt;Announcement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a pre-order page on seeedstudio.&lt;/p&gt; &lt;p&gt;for LLMs this might be very similar to the framework desktop, but possibly with faster prompt-processing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/disillusioned_okapi"&gt; /u/disillusioned_okapi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzyg24/nvidia_jetson_agx_thor_seems_to_be_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsg6v</id>
    <title>DeepSeek V3.1 - Getting token " extreme" / "极" / "極" out of nowhere</title>
    <updated>2025-08-25T14:46:02+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with DeepSeek V3.1, and found that somehow the model likes to generate the token:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot; extreme&amp;quot; (id:15075)&lt;/li&gt; &lt;li&gt;&amp;quot;极&amp;quot; (id:2577, extreme in Simplified Chinese)&lt;/li&gt; &lt;li&gt;&amp;quot;極&amp;quot; (id:16411, extreme in Traditional Chinese)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in totally unexpected places.&lt;/p&gt; &lt;p&gt;At first I thought it was due to the extreme IQ1_S quantization that I did or some edge case with imatrix calibration dataset, but then the same issue also happened with the FP8 full precision model from Fireworks.&lt;/p&gt; &lt;p&gt;Case 1 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.Se极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.3718461990356445 }, { &amp;quot;id&amp;quot;: 1511, &amp;quot;token&amp;quot;: &amp;quot;cond&amp;quot;, &amp;quot;bytes&amp;quot;: [99,111,110,100], &amp;quot;logprob&amp;quot;: -1.5412302017211914 }, { &amp;quot;id&amp;quot;: 1957, &amp;quot;token&amp;quot;: &amp;quot; second&amp;quot;, &amp;quot;bytes&amp;quot;: [32,115,101,99,111,110,100], &amp;quot;logprob&amp;quot;: -1.9008493423461914 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 2 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: &lt;a href="http://time.Se"&gt;time.Se&lt;/a&gt; extreme&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 15075, &amp;quot;token&amp;quot;: &amp;quot; extreme&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109,101], &amp;quot;logprob&amp;quot;: -1.0279325246810913 }, { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.077283263206482 }, { &amp;quot;id&amp;quot;: 9189, &amp;quot;token&amp;quot;: &amp;quot; extrem&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109], &amp;quot;logprob&amp;quot;: -1.8691496849060059 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 3 (fireworks, top_k=1, temperature=1):&lt;br /&gt; Expected: V1&lt;br /&gt; Generated: V极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;logprob&amp;quot;: -0.27936283, &amp;quot;token_id&amp;quot;: 2577, &amp;quot;bytes&amp;quot;: [230,158,129] }, { &amp;quot;token&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;logprob&amp;quot;: -1.90436232, &amp;quot;token_id&amp;quot;: 19, &amp;quot;bytes&amp;quot;: [49] }, { &amp;quot;token&amp;quot;: &amp;quot;極&amp;quot;, &amp;quot;logprob&amp;quot;: -2.40436196, &amp;quot;token_id&amp;quot;: 16411, &amp;quot;bytes&amp;quot;: [230,165,181] } ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Worse still, other than these 3 cases where an extreme token was the top choice in greedy decoding, these extreme tokens are also constantly lurking as the 2nd or 3rd choice in other unexpected places as well.&lt;/p&gt; &lt;p&gt;I have done this exact eval for all the popular coding models, and this is the first time I am seeing this kind of issue. Has anyone experienced this?&lt;/p&gt; &lt;p&gt;EDIT: Seeing the same issue with Novita as well, so it is quite unlikely to be an issue with the inference stack.&lt;/p&gt; &lt;p&gt;EDIT 2: Current suspect is that the issue might be masked by MTP, and becomes a lot more apparent when the inference stack doesn't support MTP.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm677</id>
    <title>u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments.</title>
    <updated>2025-08-25T09:51:02+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt; &lt;img alt="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." src="https://external-preview.redd.it/Nm9qN2ppZGIwNWxmMYB8gfxVUG7ntLAy6UFGKU3bfv7xh4HVFM-UizvnZAOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82b54df6d326f2b562855d3069b5bdeddfccffd" title="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fork: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/RSXLV"&gt;u/RSXLV&lt;/a&gt; again, huge shoutout to you, my guy. This fork is so fast now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9txv4idb05lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzxmdg</id>
    <title>Local LLM-powered retro game builder (Lemonade Arcade)</title>
    <updated>2025-08-25T17:55:02+00:00</updated>
    <author>
      <name>/u/vgodsoe-amd</name>
      <uri>https://old.reddit.com/user/vgodsoe-amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt; &lt;img alt="Local LLM-powered retro game builder (Lemonade Arcade)" src="https://external-preview.redd.it/dm11dnNlbXplN2xmMfWDxdpPNldKtz87A5VyOKcGjTTlEA-df_2Lj-7xBpjN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eb379b4a343e9b82de0ce6d68492f198e8fbc96" title="Local LLM-powered retro game builder (Lemonade Arcade)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I'd share something that came out of a side project from one of my teammates. I’ve been playing around with the app, Lemonade Arcade, that uses a local LLM (Qwen3-Coder-30B) to generate PyGames. In a couple of minutes, it builds retro-style games like Snake, Pong, Asteroids, Pac-Man, etc., from your inputted prompt. I also remixed some games to see what else I could get.&lt;/p&gt; &lt;p&gt;I was using a Ryzen AI 395 (Strix Halo) PC, but any machine with 32 GB RAM or 16 GB VRAM should be fine. It works on both Windows and Linux.&lt;/p&gt; &lt;p&gt;Curious what kinds of games people would come up with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vgodsoe-amd"&gt; /u/vgodsoe-amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6sy4a6nze7lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzxmdg/local_llmpowered_retro_game_builder_lemonade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvk44</id>
    <title>Codebase to Knowledge Graph generator</title>
    <updated>2025-08-25T16:39:59+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/aXlnMWRvdXExN2xmMW6IHesd2IpIEgbCcYmw7k3fEr5nk2vPdZm2_jU5G_lC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf22eb683182f2e28b2652a8c7fac245c1add93" title="Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-based chatbot. It runs entirely client-side in the browser, making it privacy-focused. I’m using &lt;strong&gt;tree-sitter.wasm&lt;/strong&gt; to parse code inside the browser and logic to use the generated AST to map out all relations. Now trying to optimize it through parallel processing with Web Workers, worker pool. For the in-memory graph database, I’m using &lt;strong&gt;KuzuDB&lt;/strong&gt;, which also runs through WebAssembly (&lt;strong&gt;kuzu.wasm&lt;/strong&gt;). Graph RAG chatbot uses langchains ReAct agent, generating cypher queries to get information.&lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Need advice from anyone who has experience in graph rag agents, will this be better than rag based grep features which is popular in all AI IDEs.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gix425uq17lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n02e7s</id>
    <title>How does huggingface make money?</title>
    <updated>2025-08-25T20:55:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I sure download from it a lot. What’s their way to bring profitably safe from shenanigans? Will it be stuff like GitHub? What’s the backup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T20:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzykeu</id>
    <title>DeepSeek 3.1 Update is Awesome!</title>
    <updated>2025-08-25T18:29:57+00:00</updated>
    <author>
      <name>/u/lovetootiesteele</name>
      <uri>https://old.reddit.com/user/lovetootiesteele</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who countless conversations interrupted due to length limits, the ability to re-visit those chats and pick-up where we left off has been a dream come true. Even though we would try to continue our projects in new chats, the foundation had been set in another. This update is awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lovetootiesteele"&gt; /u/lovetootiesteele &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a training approach I’m calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both “good” and “bad” completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on “conflict tokens.”&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn’t wreck formatting.&lt;/li&gt; &lt;li&gt;I didn’t need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious what others think, especially folks who’ve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you’d like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
