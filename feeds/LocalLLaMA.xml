<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-20T15:06:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ite7vw</id>
    <title>Large Language Diffusion Models</title>
    <updated>2025-02-19T19:29:31+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ite7vw/large_language_diffusion_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ite7vw/large_language_diffusion_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1itfg77</id>
    <title>[TEST] Prompt Processing VS Inferense Speed VS GPU layers</title>
    <updated>2025-02-19T20:18:48+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"&gt; &lt;img alt="[TEST] Prompt Processing VS Inferense Speed VS GPU layers" src="https://preview.redd.it/acrxsw0el5ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d242ada08454709d5206ff1f83db21e0f9241413" title="[TEST] Prompt Processing VS Inferense Speed VS GPU layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/acrxsw0el5ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T20:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1itdy0k</id>
    <title>No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity</title>
    <updated>2025-02-19T19:18:54+00:00</updated>
    <author>
      <name>/u/Aikodex3D</name>
      <uri>https://old.reddit.com/user/Aikodex3D</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"&gt; &lt;img alt="No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity" src="https://external-preview.redd.it/bnFnMDByNG5iNWtlMW12blNfQLF_g4OMKxQvzt-tAZaceY72pEZjInNM7LQL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=110e6a9d041b8ce5ff38c14d097b0dc020a3dc4f" title="No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aikodex3D"&gt; /u/Aikodex3D &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/glxnes4nb5ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1itc4hp</id>
    <title>Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot</title>
    <updated>2025-02-19T18:07:18+00:00</updated>
    <author>
      <name>/u/PataFunction</name>
      <uri>https://old.reddit.com/user/PataFunction</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"&gt; &lt;img alt="Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot" src="https://external-preview.redd.it/8rOw0WWweYgxygKbe5fxUQ-d38ZXq4mjYXqeNVPHsPs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e804fb26cbbe5364fe905c12bdf9215a979580c1" title="Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PataFunction"&gt; /u/PataFunction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danieljeffries.substack.com/p/defending-open-source-ai-against"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1isu4un</id>
    <title>o3-mini won the poll! We did it guys!</title>
    <updated>2025-02-19T02:06:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt; &lt;img alt="o3-mini won the poll! We did it guys!" src="https://preview.redd.it/ogpvvrth70ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=441716ec57e99b756f365455cea717ed23f4f00b" title="o3-mini won the poll! We did it guys!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a lot here yesterday to vote for the o3-mini. Thank you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogpvvrth70ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T02:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1itw7e5</id>
    <title>[Open Source] JSONL Training Data Editor - A Visual Tool for AI Training Dataset Preparation</title>
    <updated>2025-02-20T11:17:33+00:00</updated>
    <author>
      <name>/u/PsychologicalCry9387</name>
      <uri>https://old.reddit.com/user/PsychologicalCry9387</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI enthusiasts! 👋&lt;/p&gt; &lt;p&gt;We've just released a free, open-source tool that makes preparing AI jsonl training datasets much easier: &lt;a href="https://finetune.psy.tech"&gt;https://finetune.psy.tech&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/treehole-hk/openai-trainingset-editor"&gt;https://github.com/treehole-hk/openai-trainingset-editor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a fork of this Github project &lt;a href="https://github.com/baryhuang/openai-trainingset-editor?tab=readme-ov-file"&gt;https://github.com/baryhuang/openai-trainingset-editor?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Visual editor for JSONL training data (OpenAI fine-tuning format)with drag-and-drop interface&lt;/p&gt; &lt;p&gt;- Built specifically for conversation datasets and DPO (Direct Preference Optimization) preparation&lt;/p&gt; &lt;p&gt;- Handles system messages for fine-tuning&lt;/p&gt; &lt;p&gt;- Real-time validation and error checking&lt;/p&gt; &lt;p&gt;- 100% client-side processing (your data never leaves your browser)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perfect for:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- OpenAI fine-tuning projects&lt;/p&gt; &lt;p&gt;- DPO training data preparation&lt;/p&gt; &lt;p&gt;- Managing conversation datasets&lt;/p&gt; &lt;p&gt;- Cleaning and structuring training data&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Mark conversations as chosen/rejected for DPO&lt;/p&gt; &lt;p&gt;- Export in both JSONL and CSV formats&lt;/p&gt; &lt;p&gt;- Drag-and-drop message reordering&lt;/p&gt; &lt;p&gt;- System prompt management&lt;/p&gt; &lt;p&gt;- Clean, modern interface with syntax highlighting&lt;/p&gt; &lt;p&gt;This started as an internal tool for our &lt;a href="https://mindforest.ai/"&gt;AI coaching project&lt;/a&gt;. It's MIT licensed, so feel free to use it for any purpose.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalCry9387"&gt; /u/PsychologicalCry9387 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itw7e5/open_source_jsonl_training_data_editor_a_visual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itw7e5/open_source_jsonl_training_data_editor_a_visual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itw7e5/open_source_jsonl_training_data_editor_a_visual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T11:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1itjbcm</id>
    <title>LM Studio - Hugging Face Model Manager</title>
    <updated>2025-02-19T22:58:34+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My personal gift and sign of love for &lt;a href="https://x.com/huggingface"&gt;u/huggingface&lt;/a&gt; and LM Studio&lt;/p&gt; &lt;p&gt;A simple script to import models from HF Cache to LM Studio without using additional space 😎 just using symbolic links! We don't need 4TB local disk anymore!&lt;/p&gt; &lt;p&gt;Here link to the repo: &lt;a href="https://github.com/ivanfioravanti/lmstudio_hf"&gt;https://github.com/ivanfioravanti/lmstudio_hf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T22:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1it36b0</id>
    <title>Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;</title>
    <updated>2025-02-19T11:18:31+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt; &lt;img alt="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" src="https://preview.redd.it/d3bl014yx2ke1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddda4efa5606005673aaca2845b18430aa309c24" title="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d3bl014yx2ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T11:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1itrbny</id>
    <title>Small Models Struggle to Learn from Strong Reasoners</title>
    <updated>2025-02-20T05:41:14+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.12143"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itrbny/small_models_struggle_to_learn_from_strong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itrbny/small_models_struggle_to_learn_from_strong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1itr47x</id>
    <title>JoyCaption multimodal captioning model: GGUFs available; now working with KoboldCpp and Llama.cpp</title>
    <updated>2025-02-20T05:28:36+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;JoyCaption is an image captioning Visual Language Model (VLM) being built from the ground up as a free, open, and uncensored model for the community to use in training Diffusion models.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/fancyfeast/llama-joycaption-alpha-two-hf-llava"&gt;Link to project HF page.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fpgaminer/joycaption"&gt;Like to project Github page.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Jobaar/Llama-JoyCaption-Alpha-Two-GGUF"&gt;GGUF weights with image projector for Llama.cpp and KoboldCpp.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am not associated with the JoyCaption project or team.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr47x/joycaption_multimodal_captioning_model_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr47x/joycaption_multimodal_captioning_model_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itr47x/joycaption_multimodal_captioning_model_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T05:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1itqq7g</id>
    <title>Qwen2.5-VL Technical Report</title>
    <updated>2025-02-20T05:05:02+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.13923"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itqq7g/qwen25vl_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itqq7g/qwen25vl_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T05:05:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1itslms</id>
    <title>Magma: A Foundation Model for Multimodal AI Agents</title>
    <updated>2025-02-20T07:03:11+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://microsoft.github.io/Magma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itslms/magma_a_foundation_model_for_multimodal_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itslms/magma_a_foundation_model_for_multimodal_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteeqv</id>
    <title>New Wayfarer Large Model: a brutally challenging roleplay model trained to let you fail and die, now with better data and a larger base.</title>
    <updated>2025-02-19T19:36:59+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of AI models that coddle you with sunshine and rainbows? We heard you loud and clear. Last month, we shared Wayfarer (based on Nemo 12b), an open-source model that embraced death, danger, and gritty storytelling. The response was overwhelming—so we doubled down with Wayfarer Large.&lt;/p&gt; &lt;p&gt;Forged from Llama 3.3 70b Instruct, this model didn’t get the memo about being “nice.” We trained it to weave stories with teeth—danger, heartbreak, and the occasional untimely demise. While other AIs play it safe, Wayfarer Large thrives on risk, ruin, and epic stakes. We tested it on AI Dungeon a few weeks back, and players immediately became obsessed.&lt;/p&gt; &lt;p&gt;We’ve decided to open-source this model as well so anyone can experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-Large-70B-Llama-3.3"&gt;https://huggingface.co/LatitudeGames/Wayfarer-Large-70B-Llama-3.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or if you want to try this model without running it yourself, you can do so at &lt;a href="https://aidungeon.com"&gt;&lt;strong&gt;https://aidungeon.com&lt;/strong&gt;&lt;/a&gt; (Wayfarer Large requires a subscription while Wayfarer Small is free).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu01d8</id>
    <title>What’s recent open source LLMs have the largest context windows?</title>
    <updated>2025-02-20T14:42:43+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open WebUI 0.5.15 just added a new RAG feature called “Full Context Mode for Local Document Search (RAG). It says it “injects entire document content into context, improving accuracy for models with large context windows -ideal for deep context understanding”. Obviously I want to try this out and use a model with a larger context window. My limitations are 48 GB VRAM and 64 GB system memory. What are my best options given these limitations. I’m seeing most models are limited to 128k. What can I run beyond 128k at Q4 and still have enough VRAM for large context without absolutely killing my tokens per second? I just need like 2-3 t/s. I’m pretty patient. P.S. I know this question has been asked before, however, most of the results were from like 8 months ago. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu01d8/whats_recent_open_source_llms_have_the_largest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T14:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1itsnun</id>
    <title>Explanation &amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference</title>
    <updated>2025-02-20T07:07:21+00:00</updated>
    <author>
      <name>/u/YiPherng</name>
      <uri>https://old.reddit.com/user/YiPherng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"&gt; &lt;img alt="Explanation &amp;amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference" src="https://external-preview.redd.it/cW8EKiWvzQbht7_TCSgxsKZtbhoVL_boLT3a2KLDi7c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0997935da6291fe427ee6657d27d2cb0957b424" title="Explanation &amp;amp; Results of NSA - DeepSeek Introduces Ultra-Fast Long-Context Model Training and Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YiPherng"&gt; /u/YiPherng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://shockbs.pro/blog/deepseek-introduces-nsa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itsnun/explanation_results_of_nsa_deepseek_introduces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1itytpy</id>
    <title>Reasoning model based on Qwen2.5-Max will soon be released</title>
    <updated>2025-02-20T13:46:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess new &amp;amp; larger QwQ models are also coming soon?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;On February 20th, during Alibaba's earnings call, Alibaba Group CEO Wu Yongming stated that looking ahead, Alibaba will continue to focus on three main business types: domestic and international e-commerce, AI + cloud computing technology, and internet platform products. Over the next three years, Alibaba will increase investment in three areas around the strategic core of AI: AI infrastructure, basic model platforms and AI native applications, and the AI transformation of existing businesses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;At the same time, Wu Yongming revealed that Alibaba will also release a deep reasoning model based on Qwen2.5-Max in the near future.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteaew</id>
    <title>Google releases PaliGemma 2 mix - a VLM for many tasks</title>
    <updated>2025-02-19T19:32:14+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Gemma tech lead over here :)&lt;/p&gt; &lt;p&gt;Today, we released a new model, PaliGemma 2 mix! It's the same architecture as PaliGemma 2, but these are some checkpoints that work well for a bunch of tasks without having to fine-tune it.&lt;/p&gt; &lt;p&gt;Some links first&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official Google blog &lt;a href="https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688"&gt;https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The Hugging Face blog &lt;a href="https://huggingface.co/blog/paligemma2mix"&gt;https://huggingface.co/blog/paligemma2mix&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open models in &lt;a href="https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4"&gt;https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Free demo to try out &lt;a href="https://huggingface.co/spaces/google/paligemma2-10b-mix"&gt;https://huggingface.co/spaces/google/paligemma2-10b-mix&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So what can this model do?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image captioning (both short and long captions)&lt;/li&gt; &lt;li&gt;OCR&lt;/li&gt; &lt;li&gt;Question answering&lt;/li&gt; &lt;li&gt;Object detection&lt;/li&gt; &lt;li&gt;Image segmentation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So you can use the model for localization, image understanding, document understanding, and more! And as always, if you want even better results for your task, you can pick the base models and fine-tune them. The goal of this release was to showcase what can be done with PG2, which is a very good model for fine-tuning.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1itc3h7</id>
    <title>Training LLM on 1000s of GPUs made simple</title>
    <updated>2025-02-19T18:06:09+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt; &lt;img alt="Training LLM on 1000s of GPUs made simple" src="https://preview.redd.it/2wk7ntxpy4ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04be9bf215a025ca522b0d41193331c0824a527c" title="Training LLM on 1000s of GPUs made simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2wk7ntxpy4ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1itxm6l</id>
    <title>Linux Lazy Unmap Flush "LUF" Reducing TLB Shootdowns By 97%, Faster AI LLM Performance</title>
    <updated>2025-02-20T12:43:29+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"&gt; &lt;img alt="Linux Lazy Unmap Flush &amp;quot;LUF&amp;quot; Reducing TLB Shootdowns By 97%, Faster AI LLM Performance" src="https://external-preview.redd.it/XFz-Ged5fhiZf00xJLPX8r3w0bcDxstbdfJqpWstxas.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6aae0dfa26ec9efd8319878b27776e8805f5eab" title="Linux Lazy Unmap Flush &amp;quot;LUF&amp;quot; Reducing TLB Shootdowns By 97%, Faster AI LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Linux-Lazy-Unmap-Flush"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itxm6l/linux_lazy_unmap_flush_luf_reducing_tlb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T12:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1itvu5m</id>
    <title>R1 is insanely good, but falls short of o1 in generalization</title>
    <updated>2025-02-20T10:53:08+00:00</updated>
    <author>
      <name>/u/EmptyTuple</name>
      <uri>https://old.reddit.com/user/EmptyTuple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"&gt; &lt;img alt="R1 is insanely good, but falls short of o1 in generalization" src="https://b.thumbs.redditmedia.com/6dj1VvGs8U5eMA2APXR7Pu0Q2eB3-XWpiEXhDcu6Msk.jpg" title="R1 is insanely good, but falls short of o1 in generalization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmptyTuple"&gt; /u/EmptyTuple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1itvu5m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itvu5m/r1_is_insanely_good_but_falls_short_of_o1_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T10:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1itt2np</id>
    <title>The AI CUDA Engineer</title>
    <updated>2025-02-20T07:35:22+00:00</updated>
    <author>
      <name>/u/NunyaBuzor</name>
      <uri>https://old.reddit.com/user/NunyaBuzor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"&gt; &lt;img alt="The AI CUDA Engineer" src="https://external-preview.redd.it/aGM3dDZnNDR6OGtlMSXELHvaEW2TWH4Y-jofml_DDu990hjShx1lPnkzI0Tg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=802747a6a727b96cb8564ca2740aef0075558eee" title="The AI CUDA Engineer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunyaBuzor"&gt; /u/NunyaBuzor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u8ipxi34z8ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itt2np/the_ai_cuda_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T07:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1itr9th</id>
    <title>New AI Model | Ozone AI</title>
    <updated>2025-02-20T05:37:54+00:00</updated>
    <author>
      <name>/u/Perfect-Bowl-1601</name>
      <uri>https://old.reddit.com/user/Perfect-Bowl-1601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;We're excited to announce the release of our latest model: **Reverb-7b!** The Ozone AI team has been hard at work, and we believe this model represents a significant step forward in 7B performance. This model was trained on over 200 million tokens of distilled data from Claude 3.5 Sonnet and GPT-4o. This model is a fine-tune of Qwen 2.5 7b.&lt;/p&gt; &lt;p&gt;Based on our benchmarks, Reverb-7b is showing impressive results, particularly on MMLU Pro. We're seeing performance that appears to surpass other 7B models on the Open LLM Leaderboard, specifically with the challenging MMLU Pro dataset (see: &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"&gt;https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Our MMLU Pro results:&lt;/p&gt; &lt;p&gt;Biology: 0.6904 Business: 0.3143 Chemistry: 0.2314 Computer Science: 0.4000 Economics: 0.5758 Engineering: 0.3148 Health: 0.5183 History: 0.4934 Law: 0.3315 Math: 0.2983 Other: 0.4372 Philosophy: 0.4409 Physics: 0.2910 Psychology: 0.5990&lt;/p&gt; &lt;p&gt;Average Accuracy (across all MMLU Pro subjects): 0.4006&lt;/p&gt; &lt;p&gt;(More benchmarks are coming soon!)&lt;/p&gt; &lt;p&gt;Model Card &amp;amp; Download: &lt;a href="https://huggingface.co/ozone-ai/Reverb-7b"&gt;https://huggingface.co/ozone-ai/Reverb-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is only our third model release, and we're committed to pushing the boundaries of open-source LLMs. We have a 14B and 2B models currently in the works, so stay tuned for those releases in the coming days!&lt;/p&gt; &lt;p&gt;We're eager to hear your feedback! Download Reverb, give it a try, and let us know what you think.&lt;/p&gt; &lt;p&gt;Thanks for your support and we're excited to see what you do with Reverb-7b!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Bowl-1601"&gt; /u/Perfect-Bowl-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itr9th/new_ai_model_ozone_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T05:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ityftd</id>
    <title>Samsung is working on its own on-device LLM.</title>
    <updated>2025-02-20T13:26:39+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt; &lt;img alt="Samsung is working on its own on-device LLM." src="https://preview.redd.it/cgbfpkphpake1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bd286726a3a9cd81d352de72126809656fd7e96" title="Samsung is working on its own on-device LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgbfpkphpake1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1itv9ia</id>
    <title>Agent using Canva. Things are getting wild now...</title>
    <updated>2025-02-20T10:13:03+00:00</updated>
    <author>
      <name>/u/ljhskyso</name>
      <uri>https://old.reddit.com/user/ljhskyso</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt; &lt;img alt="Agent using Canva. Things are getting wild now..." src="https://external-preview.redd.it/NTlhcjg4czRyOWtlMY88yKM0XPFK9vDNwHuU8bb82IoeEzVPUXcqILOpddQA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d10dc91d97d69b43afa2b66abd3152b04b563263" title="Agent using Canva. Things are getting wild now..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljhskyso"&gt; /u/ljhskyso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hjbttwq4r9ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T10:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1itq30t</id>
    <title>Qwen/Qwen2.5-VL-3B/7B/72B-Instruct are out!!</title>
    <updated>2025-02-20T04:28:52+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The key enhancements of Qwen2.5-VL are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Visual Understanding: Improved ability to recognize and analyze objects, text, charts, and layouts within images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Agentic Capabilities: Acts as a visual agent capable of reasoning and dynamically interacting with tools (e.g., using a computer or phone).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Long Video Comprehension: Can understand videos longer than 1 hour and pinpoint relevant segments for event detection.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Visual Localization: Accurately identifies and localizes objects in images with bounding boxes or points, providing stable JSON outputs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured Output Generation: Can generate structured outputs for complex data like invoices, forms, and tables, useful in domains like finance and commerce.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T04:28:52+00:00</published>
  </entry>
</feed>
