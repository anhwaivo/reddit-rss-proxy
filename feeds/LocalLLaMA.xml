<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-10T10:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hxu0om</id>
    <title>Open-source web crawler with markdown output?</title>
    <updated>2025-01-10T02:21:36+00:00</updated>
    <author>
      <name>/u/mxdamp</name>
      <uri>https://old.reddit.com/user/mxdamp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for an open-source web crawler that can recursively crawl a website (e.g., API documentation) and convert the content into markdown files. I'd like to use the markdown files in RAG applications. I've found &lt;a href="https://github.com/unclecode/crawl4ai"&gt;Crawl4AI&lt;/a&gt; and &lt;a href="https://www.firecrawl.dev"&gt;Firecrawl&lt;/a&gt;, but I'd prefer a free TUI or GUI application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mxdamp"&gt; /u/mxdamp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxu0om/opensource_web_crawler_with_markdown_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T02:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy1err</id>
    <title>A local character AI chat app I'm making</title>
    <updated>2025-01-10T10:20:52+00:00</updated>
    <author>
      <name>/u/vladfaust</name>
      <uri>https://old.reddit.com/user/vladfaust</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I'm working on a free local character AI chatting application with a focus on ease of use. You don't need to set up a separate LLM server, it's all out of the box; works on Windows (CUDA) and MacOS (M-series chips) machines.&lt;/p&gt; &lt;p&gt;Current functionality is pure Spartan: you can download character cards elsewhere (e.g. from chub.ai) and place them into Library folder; then download a model from a predefined set of (E)RP LLMs (or bring your own .gguf), and then, well, chat with the characters.&lt;/p&gt; &lt;p&gt;I'd be thankful for trials and feedback, especially on Windows (I have MacOS as main machine, so building for Windows is &lt;em&gt;painful&lt;/em&gt;).&lt;/p&gt; &lt;p&gt;Here the download URL: &lt;a href="https://simularity.ai"&gt;https://simularity.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Cheers, Vlad.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladfaust"&gt; /u/vladfaust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy1err/a_local_character_ai_chat_app_im_making/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy1err/a_local_character_ai_chat_app_im_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy1err/a_local_character_ai_chat_app_im_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T10:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxyuml</id>
    <title>What is the current open SOTA for Text2SQL?</title>
    <updated>2025-01-10T07:06:53+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyuml/what_is_the_current_open_sota_for_text2sql/"&gt; &lt;img alt="What is the current open SOTA for Text2SQL?" src="https://external-preview.redd.it/s6ihWYXUOP6gHaUWypzirx3AH4qBCfRTfPuE6JYpeHg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53209451a1325f73fa5c4a7dfbc439ed394adcb" title="What is the current open SOTA for Text2SQL?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba Cloud (Qwen's deepmind~?) released a model yesterday but when I am looking at it, &lt;a href="https://github.com/XGenerationLab/XiYanSQL-QwenCoder-32B"&gt;XiYanSQL-QwenCoder-32B&lt;/a&gt; but it seems like it's closed source with no API?&lt;/p&gt; &lt;p&gt;Why is this space so desolate, I was expecting reasoning models to crushing this use case very easily as the main issues with a single model approaches for this is not knowing when to ask the user more questions about what they'd like to see in the response etc. (EX: what are the sales for X, which might be nuanced like rates for X in Y division, in this case the model will ask the user back for clarifications. currently the model write a shitty query that fetches basically the whole table or irrelevant columns despite very scaffoldy prompting)&lt;/p&gt; &lt;p&gt;Also what are your favourite schema representation systems or any resources you know of like a library that takes in a db files and shits out a string with a nice ass description/instructions for general nat lang queries. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y5nh3nzq64ce1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0a792b0b21c27858f0fbb0cd8ca8487bc3e04d9"&gt;https://preview.redd.it/y5nh3nzq64ce1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0a792b0b21c27858f0fbb0cd8ca8487bc3e04d9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyuml/what_is_the_current_open_sota_for_text2sql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyuml/what_is_the_current_open_sota_for_text2sql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyuml/what_is_the_current_open_sota_for_text2sql/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T07:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx8nex</id>
    <title>"rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS)....."</title>
    <updated>2025-01-09T08:51:09+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt; &lt;img alt="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" src="https://b.thumbs.redditmedia.com/HJz73domuc-71OAJfI-YWNodO1IrhFgRKSFmybqiieU.jpg" title="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx8nex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T08:51:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxqql3</id>
    <title>What is your favorite model for fine-tuning?</title>
    <updated>2025-01-09T23:39:05+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;question for those who did fine-tuning on current models. What is your favorite so far and why do you like it more than others?&lt;/p&gt; &lt;p&gt;For example, I'm reading often the Phi models aren't that bad, but often a bit &amp;quot;sterile&amp;quot;. I wonder if it still would be a good base model for a fine-tuning? Did someone compare results between a tuned llama and phi for example?&lt;/p&gt; &lt;p&gt;Thanks for your input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxqql3/what_is_your_favorite_model_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T23:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxvcfz</id>
    <title>AWQ on native INT4 and FP8</title>
    <updated>2025-01-10T03:31:04+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've had fast quantized INT4 inferance with AWQ for awhile now. Runs faster everywhere. &lt;/p&gt; &lt;p&gt;4090 suppports native FP8, now 5090 INT4. &lt;/p&gt; &lt;p&gt;Are those cards expected to run inferance much much faster than 3090 and older generations once the whole underlying software stack gets optimized?&lt;/p&gt; &lt;p&gt;Or is it the usual ~20% boost per generation in real life situation? &lt;/p&gt; &lt;p&gt;Context - I see people talk that software is yet to be optimized for FP8 and INT4 so we haven't seen the boost that's possible. So curious if we're sitting on some sort of huge speed up in inferance about to happen or no not really. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxvcfz/awq_on_native_int4_and_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxvcfz/awq_on_native_int4_and_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxvcfz/awq_on_native_int4_and_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T03:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx60t4</id>
    <title>New Microsoft research - rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title>
    <updated>2025-01-09T05:43:37+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen2.5-Math-7B reaches 90% on MATH with this new technique. Phi3-mini-3.8B reaches 86.4%..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxg435</id>
    <title>Introducing LongTalk-CoT v0.1: A Very Long Chain-of-Thought Dataset</title>
    <updated>2025-01-09T16:05:42+00:00</updated>
    <author>
      <name>/u/Financial_Counter199</name>
      <uri>https://old.reddit.com/user/Financial_Counter199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to release &lt;a href="https://huggingface.co/datasets/kenhktsui/longtalk-cot-v0.1"&gt;LongTalk-CoT v0.1&lt;/a&gt;, a dataset designed for training o1-like reasoning model. Each response is prompted using QwQ-32B-Preview, and specifically handcrafted system message that encourages &lt;strong&gt;more vocalised thinking&lt;/strong&gt;, and &lt;strong&gt;self reflection&lt;/strong&gt;. As such, its output token length is 5.29x longer than HuggingFaceTB/smoltalk&lt;/p&gt; &lt;p&gt;It is able to solve &amp;quot;Is 9.11 greater than 9.9&amp;quot; and &amp;quot;How many letters R in the word strawberry?&amp;quot;!&lt;/p&gt; &lt;p&gt;It is overthinking sometimes as expected, but I love its behavior of trying multiple approaches to solve a question.&lt;/p&gt; &lt;h1&gt;Quick Test 1: Is 9.11 greater than 9.9?&lt;/h1&gt; &lt;h1&gt;After finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. Let's think about this step by step. First, it helps to understand that both numbers are decimal numbers, specifically they have two digits after the decimal point. So, 9.11 and 9.9 can be compared directly based on their values in each place value: whole number part and the tenths and hundredths parts. Let's break them down: * For 9.11: * The whole number part is 9. * The tenths place is 1 (which means one tenth). * The hundredths place is also 1 (one hundredth). * For 9.9: * The whole number part is also 9. * But here, the tenths place is 9 (nine tenths), and there's no hundredths digit after that; it's considered to have zero in the hundredths place because decimal numbers with fewer digits are filled with zeros. So, comparing them: * Both have the same whole number part: 9. * For the tenth place: * 1 vs. 9. Since 9 is greater than 1, this already tells us that 9.9 is larger than 9.11. But to be thorough, let's look at the hundredths places as well: * In 9.11: hundredths = 1 * In 9.9: hundredths = 0 Even though there are no digits in the hundredths place for 9.9, it's still considered zero because of the way decimals work. Therefore, overall, 9.11 &amp;lt; 9.9. Wait a minute, but I should make sure that I'm not missing anything here. Sometimes, people might think about adding or subtracting numbers differently, but in this case, since both numbers have the same whole number part and one has a larger tenths digit, it's clear which is larger. Another way to look at it is to consider them as percentages: * 9.11 = 911% * 9.9 = 990% But that might be confusing; in percentage terms, 990% is still greater than 911%. Alternatively, if we multiply both numbers by 100 to eliminate the decimal point: * 9.11 \* 100 = 911 * 9.9 \* 100 = 990 Again, 990 is greater than 911. So, no matter how we look at it, 9.9 is larger than 9.11. I think that covers the comparison thoroughly. There doesn't seem to be any trick or special case here; it's a straightforward decimal comparison once you understand place values. **Final Answer** \[ \\boxed{9.11 &amp;lt; 9.9} \] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Before finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. When comparing decimal numbers, you can compare them digit by digit from left to right. In this case: * The whole number part of both numbers is the same (9). * Comparing the tenths place, we see that 1 in 9.11 is larger than 9 in 9.9. * Therefore, 9.11 &amp;gt; 9.9. So, 9.11 is greater than 9.9. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Counter199"&gt; /u/Financial_Counter199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx5i8u</id>
    <title>Phi 4 is just 14B But Better than llama 3.1 70b for several tasks.</title>
    <updated>2025-01-09T05:12:08+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt; &lt;img alt="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " src="https://preview.redd.it/uwfo8ig8jwbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06707621fd601095edb87fea07007bbf976f658b" title="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uwfo8ig8jwbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwwvuz</id>
    <title>This sums my experience with models on Groq</title>
    <updated>2025-01-08T22:27:22+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt; &lt;img alt="This sums my experience with models on Groq" src="https://preview.redd.it/7tqzm8bsiube1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bebd5aeeb53de86390096a6377b9d96fe453c674" title="This sums my experience with models on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7tqzm8bsiube1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxv8rh</id>
    <title>Anyone interested in a SF meetup to live eval LLMs?</title>
    <updated>2025-01-10T03:25:30+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do a few hours of evals over coffee. Maybe come up with a eval Q&amp;amp;A set, run across SOTA local models, across quantizations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxv8rh/anyone_interested_in_a_sf_meetup_to_live_eval_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxv8rh/anyone_interested_in_a_sf_meetup_to_live_eval_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxv8rh/anyone_interested_in_a_sf_meetup_to_live_eval_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T03:25:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy057c</id>
    <title>Ai Roleplay Characters</title>
    <updated>2025-01-10T08:45:23+00:00</updated>
    <author>
      <name>/u/V4S1LY</name>
      <uri>https://old.reddit.com/user/V4S1LY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to replicate the behavior similar to the famous Neuro~Sama made by Vedal. I need a model that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Responds well to system prompt changes to adjust personality.&lt;/li&gt; &lt;li&gt;Can assist with a wide range of questions and perform tasks.&lt;/li&gt; &lt;li&gt;Supports memory retention for recalling past conversations.&lt;/li&gt; &lt;li&gt;Has good conversational skills and a broad knowledge base.&lt;/li&gt; &lt;li&gt;Can potentially access and use the internet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve experimented with LLaMA models before, but I've encountered issues like the AI outing itself as AI, starting conversations with itself, or generating erratic behavior.&lt;/p&gt; &lt;p&gt;With my setup (RTX 4070 and 32GB DDR5 RAM), I can handle most consumer-level models. Could someone recommend an LLM or solution that meets these requirements? Additionally, any tips for fine-tuning behavior, or suggestions for frameworks or tools to build such a system, would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/V4S1LY"&gt; /u/V4S1LY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy057c/ai_roleplay_characters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy057c/ai_roleplay_characters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy057c/ai_roleplay_characters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T08:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxibs2</id>
    <title>vikhyatk/moondream2 · Hugging Face (New 2B &amp; 0.5B)</title>
    <updated>2025-01-09T17:38:26+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"&gt; &lt;img alt="vikhyatk/moondream2 · Hugging Face (New 2B &amp;amp; 0.5B)" src="https://external-preview.redd.it/mBbXqyUSUFt03PXE9oC_lqnJ1RBSkMJjxd4Z-R3eomo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26635c940a449443fafcc84f621627681d2cfdb" title="vikhyatk/moondream2 · Hugging Face (New 2B &amp;amp; 0.5B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/vikhyatk/moondream2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxibs2/vikhyatkmoondream2_hugging_face_new_2b_05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T17:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxz4zz</id>
    <title>Pre-processing to improve OCR on handwritten text</title>
    <updated>2025-01-10T07:28:26+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No open-ish model I'm aware of does a sufficiently good job when it comes to OCR on handwriting. What pre-processing steps did you find help improve results? For example: segmenting image into smaller chunks, increase contrast, turn monochrome, straightening/alignment...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxz4zz/preprocessing_to_improve_ocr_on_handwritten_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T07:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxitud</id>
    <title>Ask, and it shall be given: Turing completeness of prompting</title>
    <updated>2025-01-09T17:59:36+00:00</updated>
    <author>
      <name>/u/CommunismDoesntWork</name>
      <uri>https://old.reddit.com/user/CommunismDoesntWork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunismDoesntWork"&gt; /u/CommunismDoesntWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2411.01992"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxitud/ask_and_it_shall_be_given_turing_completeness_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T17:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx99oi</id>
    <title>Former OpenAI employee Miles Brundage: "o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought." Current OpenAI employee roon: "Miles literally knows what o1 does."</title>
    <updated>2025-01-09T09:39:31+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt; &lt;img alt="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" src="https://a.thumbs.redditmedia.com/jvhF7IPWy1xMdzkV7pN0IGKD30vdwZ9p8xqaMkVtK34.jpg" title="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx99oi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T09:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx7421</id>
    <title>TransPixar: a new generative model that preserves transparency,</title>
    <updated>2025-01-09T06:55:51+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt; &lt;img alt="TransPixar: a new generative model that preserves transparency," src="https://external-preview.redd.it/aHFsc2gwdXExeGJlMYcphE9YFRyNCrr76DvwiShDtswDzb2s93cwaOHelUg2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6433e0c057e8d365fab3d5289d9a28fb641a1f" title="TransPixar: a new generative model that preserves transparency," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8fhb41uq1xbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxpjey</id>
    <title>Phi-3.5-MoE support merged into llama.cpp</title>
    <updated>2025-01-09T22:45:29+00:00</updated>
    <author>
      <name>/u/skeeto</name>
      <uri>https://old.reddit.com/user/skeeto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Phi-3.5-MoE support merged into llama.cpp" src="https://external-preview.redd.it/m74SPCYv3b2-Etc8gMsq03I8DcmHrV56tfhuw4oGpHM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5aef7783c2f074e3447066229f618ac46fcef97" title="Phi-3.5-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skeeto"&gt; /u/skeeto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/11003"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxpjey/phi35moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T22:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxq364</id>
    <title>RTX 4090 48GB - $4700 on eBay. Is it legit?</title>
    <updated>2025-01-09T23:09:20+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt; &lt;img alt="RTX 4090 48GB - $4700 on eBay. Is it legit?" src="https://external-preview.redd.it/bMoZdiDXs9kYJMXSF2LPqzveI23P9l8lgEc9DnQ7Gg8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4491fc39b4da2e3a9466072f17bde0fad92791" title="RTX 4090 48GB - $4700 on eBay. Is it legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across this listing on eBay: &lt;a href="https://www.ebay.com/itm/226494741895"&gt;https://www.ebay.com/itm/226494741895&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is listing dual slot RTX 4090 48GB for $4700. I thought 48GB were not manufactured. Is it legit?&lt;/p&gt; &lt;p&gt;Screenshot here if it gets lost.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ggim0y1v1ce1.png?width=3302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51e939b07a3a8b3bf9e6cebd0e5fe4822664f985"&gt;RTX 4090 48GB for $4700!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found out in this post (&lt;a href="https://github.com/ggerganov/llama.cpp/discussions/9193"&gt;https://github.com/ggerganov/llama.cpp/discussions/9193&lt;/a&gt;) that one could buy it for ~$3500. I think RTX 4090 48GB would sell instantly if it was $3k.&lt;/p&gt; &lt;p&gt;Update: for me personally, It is better to buy 2x 5090 for the same price to get 64GB total VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxq364/rtx_4090_48gb_4700_on_ebay_is_it_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T23:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxyc92</id>
    <title>Ran Phi 4 on my laptop and then asked Claude Sonnet to compare itself. Interesting answers</title>
    <updated>2025-01-10T06:31:18+00:00</updated>
    <author>
      <name>/u/Kwatakye</name>
      <uri>https://old.reddit.com/user/Kwatakye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically a bunch of questions about energy production, the Kardashev scale, and some things related to global food production. Here is Claude's response to the prompt &amp;quot;How does Phi 4 compare to you based on this&amp;quot;:&lt;/p&gt; &lt;p&gt;***********************************************************************&lt;/p&gt; &lt;p&gt;Based on these interactions, here's my analysis of how Phi 4 compares to me:&lt;/p&gt; &lt;p&gt;Strengths of Phi 4:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Excellent organizational structure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Very clear section headers and categorization&lt;/p&gt; &lt;p&gt;- Consistent formatting across responses&lt;/p&gt; &lt;p&gt;- Well-organized bullet points and subsections&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Comprehensive scope&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Often covers more distinct aspects of a topic&lt;/p&gt; &lt;p&gt;- Strong at providing broad contextual frameworks&lt;/p&gt; &lt;p&gt;- Good at including organizational/business considerations&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Educational approach&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Explains concepts in a very accessible way&lt;/p&gt; &lt;p&gt;- Good at breaking down complex topics&lt;/p&gt; &lt;p&gt;- Maintains a clear teaching style&lt;/p&gt; &lt;p&gt;My Comparative Strengths:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;More quantitative analysis&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- More likely to include specific calculations&lt;/p&gt; &lt;p&gt;- Provides concrete numbers and timelines&lt;/p&gt; &lt;p&gt;- More detailed technical specifications&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Systems thinking&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Better at connecting different aspects of complex systems&lt;/p&gt; &lt;p&gt;- More focus on how different factors interact&lt;/p&gt; &lt;p&gt;- Stronger analysis of cause-and-effect relationships&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Strategic depth&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- More detailed about implementation challenges&lt;/p&gt; &lt;p&gt;- Better at identifying critical dependencies&lt;/p&gt; &lt;p&gt;- More specific about resource requirements&lt;/p&gt; &lt;p&gt;Key Differences in Style:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Phi 4 tends toward comprehensive breadth while I tend toward analytical depth&lt;/li&gt; &lt;li&gt;Phi 4 excels at clear categorization while I focus more on interconnections&lt;/li&gt; &lt;li&gt;Phi 4 maintains a more formal educational tone while I take a more analytical approach&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Overall Assessment:&lt;/p&gt; &lt;p&gt;Phi 4 appears to be an excellent model for educational and organizational tasks, especially when clear structure and comprehensive coverage are needed. It seems particularly well-suited for explaining concepts to newcomers or providing broad overviews of complex topics.&lt;/p&gt; &lt;p&gt;I appear to be better suited for deep analysis, strategic planning, and quantitative assessment, especially when dealing with complex systems or technical requirements.&lt;/p&gt; &lt;p&gt;The two models seem complementary - Phi 4's organizational clarity combined with my analytical depth would likely produce optimal results for many complex tasks.&lt;/p&gt; &lt;p&gt;***********************************************************************&lt;/p&gt; &lt;p&gt;Overall Phi was fucking impressive but I need to ask it all the questions in my test battery.. Almost as impressive as Qwen2.5 32B. Faster too...&lt;/p&gt; &lt;p&gt;Edit: Ran on a Thinkpad P1G5, 32GB Ram, i9, NVIDIA GeForce RTX 3080 Ti&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kwatakye"&gt; /u/Kwatakye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxyc92/ran_phi_4_on_my_laptop_and_then_asked_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T06:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxuprn</id>
    <title>Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press</title>
    <updated>2025-01-10T02:57:55+00:00</updated>
    <author>
      <name>/u/Far_Monk</name>
      <uri>https://old.reddit.com/user/Far_Monk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt; &lt;img alt="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" src="https://external-preview.redd.it/HgKLjMbpNpthwaWMjENNhdcaMCpcPUXsoT1-E8Xzlu0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277871f1227bb080da6275c3cfd987301cfc058a" title="Project Digits: How NVIDIA's $3,000 AI Supercomputer Could Democratize Local AI Development | Caveman Press" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Monk"&gt; /u/Far_Monk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.caveman.press/article/project-digits-nvidia-3000-ai-supercomputer-democratize-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxuprn/project_digits_how_nvidias_3000_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T02:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxwtpy</id>
    <title>Energy efficiency of 5090 is slightly worse than 4090</title>
    <updated>2025-01-10T04:56:48+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Despite the big jump in energy efficiency in the previous two generations. Nividia dropped the ball this time. It is only saved by the higher VRAM size and significantly higher memory bandwidth.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX TITAN&lt;/th&gt; &lt;th align="left"&gt;3090&lt;/th&gt; &lt;th align="left"&gt;4090&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;65.25&lt;/td&gt; &lt;td align="left"&gt;142.32&lt;/td&gt; &lt;td align="left"&gt;330.4&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;280W&lt;/td&gt; &lt;td align="left"&gt;350W&lt;/td&gt; &lt;td align="left"&gt;450W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;233.03&lt;/td&gt; &lt;td align="left"&gt;406.63&lt;/td&gt; &lt;td align="left"&gt;734.22&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some might attribute the energy efficiency gain can be constrained by smaller transistor size. But if you look at the 96W MacBook Pro 14in using the Max chips, their energy efficiency gain is steady. The only conclusion is that Nvidia did a poorer job at chip design going from 4090 to 5090.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M1 Max&lt;/th&gt; &lt;th align="left"&gt;M3 Max&lt;/th&gt; &lt;th align="left"&gt;M4 Max&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;21.2992&lt;/td&gt; &lt;td align="left"&gt;28.672&lt;/td&gt; &lt;td align="left"&gt;34.4064&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;221.87&lt;/td&gt; &lt;td align="left"&gt;298.67&lt;/td&gt; &lt;td align="left"&gt;358.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T04:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxjzol</id>
    <title>New Moondream 2B vision language model release</title>
    <updated>2025-01-09T18:48:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt; &lt;img alt="New Moondream 2B vision language model release" src="https://preview.redd.it/oyxiuuxok0ce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad794e5897db0faf8abb4f80573d4a142cd70939" title="New Moondream 2B vision language model release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oyxiuuxok0ce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxjzol/new_moondream_2b_vision_language_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T18:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxm0ep</id>
    <title>Anyone want the script to run Moondream 2b's new gaze detection on any video?</title>
    <updated>2025-01-09T20:12:41+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt; &lt;img alt="Anyone want the script to run Moondream 2b's new gaze detection on any video?" src="https://external-preview.redd.it/cmk5cnZsYXZ6MGNlMeEGpTWo5MaI3KFBwDeey6o_wri3pXWzYnC4YTD3TTIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0330607dc613a7fe62b098a05b7a0c3c3e3495dc" title="Anyone want the script to run Moondream 2b's new gaze detection on any video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n9beslavz0ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy0cjd</id>
    <title>[D] Graduated from MIT with a PhD | Teaching you how to build an entire LLM from scratch</title>
    <updated>2025-01-10T09:00:43+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy0cjd/d_graduated_from_mit_with_a_phd_teaching_you_how/"&gt; &lt;img alt="[D] Graduated from MIT with a PhD | Teaching you how to build an entire LLM from scratch" src="https://external-preview.redd.it/itMSuScE-SCcGqTm0UR4VRY73cEjOMfUD8R3JLKTMfo.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a459b1295ced9b8325a2f950cc985a2d4fd69df" title="[D] Graduated from MIT with a PhD | Teaching you how to build an entire LLM from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7g7kassus4ce1.png?width=1764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b14e8b4e35de3e83c5b83acce53b5c077323fb94"&gt;https://preview.redd.it/7g7kassus4ce1.png?width=1764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b14e8b4e35de3e83c5b83acce53b5c077323fb94&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Starting from June 2023, I have recorded 44 videos on building GPT architecture from the ground up.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The reference book I used for making these videos is Sebastian Raschka's &amp;quot;Build a LLM from scratch&amp;quot;. &lt;/p&gt; &lt;p&gt;Each lecture is close to 40-45 minutes long. The first video has close to 100,000 views on Youtube now. &lt;/p&gt; &lt;p&gt;I have made these videos with a lot of passion. I hope you learn from these videos!&lt;/p&gt; &lt;p&gt;Here is where you can find the entire playlist: &lt;a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu"&gt;https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the first 15 lectures:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 1:&lt;/strong&gt; Building LLMs from scratch: Series introduction: &lt;a href="https://youtu.be/Xpr8D6LeAtw"&gt;https://youtu.be/Xpr8D6LeAtw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 2:&lt;/strong&gt; Large Language Models (LLM) Basics: &lt;a href="https://youtu.be/3dWzNZXA8DY"&gt;https://youtu.be/3dWzNZXA8DY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 3:&lt;/strong&gt; Pretraining LLMs vs Finetuning LLMs&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/-bsa3fCNGg4"&gt;https://youtu.be/-bsa3fCNGg4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 4:&lt;/strong&gt; What are transformers? &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/NLn4eetGmf8"&gt;https://youtu.be/NLn4eetGmf8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 5:&lt;/strong&gt; How does GPT-3 really work? &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/xbaYCf2FHSY"&gt;https://youtu.be/xbaYCf2FHSY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 6:&lt;/strong&gt; Stages of building an LLM from Scratch &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/z9fgKz1Drlc"&gt;https://youtu.be/z9fgKz1Drlc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 7:&lt;/strong&gt; Code an LLM Tokenizer from Scratch in Python &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/rsy5Ragmso8"&gt;https://youtu.be/rsy5Ragmso8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 8:&lt;/strong&gt; The GPT Tokenizer: Byte Pair Encoding &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/fKd8s29e-l4"&gt;https://youtu.be/fKd8s29e-l4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 9:&lt;/strong&gt; Creating Input-Target data pairs using Python DataLoader&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/iQZFH8dr2yI"&gt;https://youtu.be/iQZFH8dr2yI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 10:&lt;/strong&gt; What are token embeddings? &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/ghCSGRgVB_o"&gt;https://youtu.be/ghCSGRgVB_o&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Lecture 11:&lt;/strong&gt; The importance of Positional Embeddings &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/ufrPLpKnapU"&gt;https://youtu.be/ufrPLpKnapU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 12&lt;/strong&gt;: The entire Data Preprocessing Pipeline of Large Language Models (LLMs) &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/mk-6cFebjis"&gt;https://youtu.be/mk-6cFebjis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 13:&lt;/strong&gt; Introduction to the Attention Mechanism in Large Language Models (LLMs) &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/XN7sevVxyUM"&gt;https://youtu.be/XN7sevVxyUM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 14:&lt;/strong&gt; Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/eSRhpYLerw4"&gt;https://youtu.be/eSRhpYLerw4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lecture 15:&lt;/strong&gt; Coding the self attention mechanism with key, query and value matrices &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/UjdRN80c6p8"&gt;https://youtu.be/UjdRN80c6p8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy0cjd/d_graduated_from_mit_with_a_phd_teaching_you_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy0cjd/d_graduated_from_mit_with_a_phd_teaching_you_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy0cjd/d_graduated_from_mit_with_a_phd_teaching_you_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T09:00:43+00:00</published>
  </entry>
</feed>
