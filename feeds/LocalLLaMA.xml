<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-22T20:05:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iv0z5w</id>
    <title>What would you do with 96GB of VRAM (quad 3090 setup)</title>
    <updated>2025-02-21T20:35:58+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for inspiration. Mostly curious about ways to get an LLM to learn a code base and become a coding mate I can discuss stuff with about the code base (coding style, bug hunting, new features, refactoring)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T20:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivq39n</id>
    <title>What is the *smallest* model with scores similar to gemini flash 1.5?</title>
    <updated>2025-02-22T18:56:00+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder, since new models are out every month, if there is a *small* model ( &amp;lt;=14B ) with scores comparable at least with the now retired gemini flash 1.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq39n/what_is_the_smallest_model_with_scores_similar_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq39n/what_is_the_smallest_model_with_scores_similar_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq39n/what_is_the_smallest_model_with_scores_similar_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivr2l1</id>
    <title>llm-commit: Auto-Generate Git Commit Messages with LLMs!</title>
    <updated>2025-02-22T19:37:39+00:00</updated>
    <author>
      <name>/u/Argetos96</name>
      <uri>https://old.reddit.com/user/Argetos96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;llm-commit: Auto-Generate Git Commit Messages with LLMs!&lt;/h1&gt; &lt;p&gt;Are you bored of writing commit messages? Here’s the solution: I built &lt;code&gt;llm-commit&lt;/code&gt;, a small plugin for Simon Willison’s &lt;code&gt;llm&lt;/code&gt; utility that uses staged Git changes to generate commit messages with a language model. It’s simple and saves time.&lt;/p&gt; &lt;p&gt;How it works:&lt;br /&gt; - Stage your changes: &lt;code&gt;git add .&lt;/code&gt;&lt;br /&gt; - Run: &lt;code&gt;llm commit&lt;/code&gt;&lt;br /&gt; And boom—a commit message based on your diff! Want to tweak it? Try:&lt;br /&gt; - &lt;code&gt;llm commit --model gpt-4 --max-tokens 150 --temperature 0.8 --yes&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/gntousakis/llm-commit"&gt;https://github.com/gntousakis/llm-commit&lt;/a&gt;. It’s a lightweight tool I hacked together to make Git life easier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Argetos96"&gt; /u/Argetos96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivr2l1/llmcommit_autogenerate_git_commit_messages_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivr2l1/llmcommit_autogenerate_git_commit_messages_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivr2l1/llmcommit_autogenerate_git_commit_messages_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T19:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivl80h</id>
    <title>Is this a good spec for local LLM?</title>
    <updated>2025-02-22T15:30:16+00:00</updated>
    <author>
      <name>/u/Efficient_Try8674</name>
      <uri>https://old.reddit.com/user/Efficient_Try8674</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://pcpartpicker.com/list/D294Zc"&gt;https://pcpartpicker.com/list/D294Zc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Try8674"&gt; /u/Efficient_Try8674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivl80h/is_this_a_good_spec_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivl80h/is_this_a_good_spec_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivl80h/is_this_a_good_spec_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T15:30:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iur927</id>
    <title>I tested Grok 3 against Deepseek r1 on my personal benchmark. Here's what I found out</title>
    <updated>2025-02-21T13:46:16+00:00</updated>
    <author>
      <name>/u/goddamnit_1</name>
      <uri>https://old.reddit.com/user/goddamnit_1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, the Grok 3 is here. And as a Whale user, I wanted to know if it's as big a deal as they are making out to be.&lt;/p&gt; &lt;p&gt;Though I know it's unfair for Deepseek r1 to compare with Grok 3 which was trained on 100k h100 behemoth cluster.&lt;/p&gt; &lt;p&gt;But I was curious about how much better Grok 3 is compared to Deepseek r1. So, I tested them on my personal set of questions on reasoning, mathematics, coding, and writing.&lt;/p&gt; &lt;p&gt;Here are my observations.&lt;/p&gt; &lt;h1&gt;Reasoning and Mathematics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 and Deepseek r1 are practically neck-and-neck in these categories.&lt;/li&gt; &lt;li&gt;Both models handle complex reasoning problems and mathematics with ease. Choosing one over the other here doesn't seem to make much of a difference.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 leads in this category. Its code quality, accuracy, and overall answers are simply better than Deepseek r1's.&lt;/li&gt; &lt;li&gt;Deepseek r1 isn't bad, but it doesn't come close to Grok 3. If coding is your primary use case, Grok 3 is the clear winner.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Writing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both models are equally better for creative writing, but I personally prefer Grok 3’s responses.&lt;/li&gt; &lt;li&gt;For my use case, which involves technical stuff, I liked the Grok 3 better. Deepseek has its own uniqueness; I can't get enough of its autistic nature.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Who Should Use Which Model?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 is the better option if you're focused on coding.&lt;/li&gt; &lt;li&gt;For reasoning and math, you can't go wrong with either model. They're equally capable.&lt;/li&gt; &lt;li&gt;If technical writing is your priority, Grok 3 seems slightly better than Deepseek r1 for my personal use cases, for schizo talks, no one can beat Deepseek r1.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a detailed analysis,&lt;a href="https://composio.dev/blog/grok-3-vs-deepseek-r1/"&gt; Grok 3 vs Deepseek r1&lt;/a&gt;, for a more detailed breakdown, including specific examples and test cases.&lt;/p&gt; &lt;p&gt;What are your experiences with the new Grok 3? Did you find the model useful for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goddamnit_1"&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T13:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivku0v</id>
    <title>Book Editing Advice</title>
    <updated>2025-02-22T15:12:15+00:00</updated>
    <author>
      <name>/u/LostHisDog</name>
      <uri>https://old.reddit.com/user/LostHisDog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there. I try to keep up with all this LLM stuff but it moves real fast. My main use case for running something locally is just to work through editing some books I've written in the past. I'd like to find a model that can keep a large context locally but I don't think that really exists right now for mortal hardware (3090 and 64gb RAM) but thought I would ask. It's still mostly a trade off with dumb model leaving room for larger context or smart model with smaller context as I understand it? My question really would be if right now there's anything that can take in 50,000 - 100,000 words or so and be able to talk about them somewhat intelligently on my hardware?&lt;/p&gt; &lt;p&gt;And assuming the answer is probably still no and likely will be for a while, does anyone know of API's or paid options that can work in that context range? I know google has notebook llm and it seems promising but I'm not really sure about there data scraping policies and would prefer to not have my stuff consumed by the matrix before I'm even done with it.&lt;/p&gt; &lt;p&gt;Appreciate any feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostHisDog"&gt; /u/LostHisDog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivku0v/book_editing_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivku0v/book_editing_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivku0v/book_editing_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T15:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivnm3m</id>
    <title>3D printing</title>
    <updated>2025-02-22T17:12:26+00:00</updated>
    <author>
      <name>/u/rorowhat</name>
      <uri>https://old.reddit.com/user/rorowhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wondering if ther is a LLM/difussion type of model where you dictate what you're looking to make, give dimensions etc and have a model give you a model you could print ? Or if there are any other tools that could assist on creating designs. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rorowhat"&gt; /u/rorowhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivnm3m/3d_printing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivnm3m/3d_printing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivnm3m/3d_printing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T17:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpmmj</id>
    <title>Mac 48GB M4 Pro 20 GPU sweet spot for 24-32B LLMs</title>
    <updated>2025-02-22T18:36:10+00:00</updated>
    <author>
      <name>/u/noless15k</name>
      <uri>https://old.reddit.com/user/noless15k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a quick follow-up to my past detailed posts about the performance of the M4 Pro, this time with long-ish (for local) context windows and newer models&lt;/p&gt; &lt;p&gt;General experience below is in LM Studio. These are rough estimates based on memory as I don't have my computer with me at the moment but I have been used these two models a lot recently.&lt;/p&gt; &lt;p&gt;32B Qwen2.5 DeepSeek R1 Distill with 32k context:&lt;/p&gt; &lt;p&gt;~ 8 minutes to get to first token &lt;/p&gt; &lt;p&gt;~ 3 tokens per second Q6_K_L GGUF&lt;/p&gt; &lt;p&gt;~ 5 tokens per second Q4 MLX&lt;/p&gt; &lt;p&gt;~ 40 GB of RAM&lt;/p&gt; &lt;p&gt;24B Mistral Small 3 with 32k context:&lt;/p&gt; &lt;p&gt;~ 6 minutes to get to first token&lt;/p&gt; &lt;p&gt;~ 5 tokens per second Q6_K_L GGUF&lt;/p&gt; &lt;p&gt;~ 28 GB of RAM&lt;/p&gt; &lt;p&gt;Side Question: LM Studio 0.3.10 supports Speculative Decoding, but I haven't found a helper model that is compatible with either of these. Does anyone know of one?&lt;/p&gt; &lt;p&gt;At the time I bought the Mac Mini for $2099 out the door ($100 off and B&amp;amp;H paid the tax as I opened a credit card with them) I felt some regret for not getting the 64GB model (which was not in stock). However more RAM for the M4 PRO wouldn't provide much utility beyond having more room for other apps. Larger context windows would be even slower and that's really all the extra ram would be good for, or perhaps a larger model, and that's the same problem. &lt;/p&gt; &lt;p&gt;I also could only find at the time the 48GB model paired with the 20GPU version of the M4 Pro. Turns out this gives a speed boost of 15% during token generation and 20% during prompt processing. So in terms of Mac's exorbitant pricing practice, I think 48GB RAM with the 20 core GPU is a better value than the 64GB / 16-core GPU at the same price point. Wanted to share in case this helps anyone choose.&lt;/p&gt; &lt;p&gt;I originality bought the 24GB / 16-core GPU model on sale for $1289 (tax included). The price was more reasonable, but it wasn't practical to use for anything larger than 7 or 14B parameters once context length increased past 8k.&lt;/p&gt; &lt;p&gt;I don't think the 36GB / 32-core M4 MAX is a better value (though when the Mac Studios come out that might change) given it costs $1k more being only available right now as a laptop and won't fit the 32B model at 32k context. But for Mistral 24B it might get to first token in under 5 minutes and likely get 7-8 tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noless15k"&gt; /u/noless15k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpmmj/mac_48gb_m4_pro_20_gpu_sweet_spot_for_2432b_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpmmj/mac_48gb_m4_pro_20_gpu_sweet_spot_for_2432b_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpmmj/mac_48gb_m4_pro_20_gpu_sweet_spot_for_2432b_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivo0gv</id>
    <title>DarkRapids, Local GPU rig build with style (water cooling)</title>
    <updated>2025-02-22T17:28:50+00:00</updated>
    <author>
      <name>/u/berni8k</name>
      <uri>https://old.reddit.com/user/berni8k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt; &lt;img alt="DarkRapids, Local GPU rig build with style (water cooling)" src="https://b.thumbs.redditmedia.com/k7wIbfmhIPkjHny_WwxeIVUQKpigJzDbZhExxg-v5To.jpg" title="DarkRapids, Local GPU rig build with style (water cooling)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It started off as a PC build but then progressed into a local GPU rig.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z2qwggz1qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0c9836a1cbacbc4a2d3ee492ca1cdebe88ff89e"&gt;https://preview.redd.it/9z2qwggz1qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0c9836a1cbacbc4a2d3ee492ca1cdebe88ff89e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Its main feature are its quad RTX 3090 that were collected overtime off the used market in various states of disrepair. All of them upgraded to watercoolng with AlphaCool waterblocks (that went for cheap because they are clearing stock). Most of these GPUs are also the higher power 420W TDP third party variants making this a lot of heat to deal with.&lt;/p&gt; &lt;p&gt;The 3 radiators packed into the case are all sitting on exhaust vents to keep the interior cool. This is important because the water temperature is ran very high at ~55°C making for rather hot exhaust air. This is the only way this amount of radiators can deal with dissipating the ~1700W of heat and still have fans run at reasonable speeds so that it doesn't sound like a server taking off.&lt;/p&gt; &lt;p&gt;CPU Cooling is done using a separate AIO on the front intake. This is because the TIM under the heat spreader of AMD Threadripper CPUs is still not very good, so even such a low core count chip can't deal with high ambient temperature, so the water that is cooling the GPUs is too hot. For this reason the CPU radiator is taking in fresh cold air and doing it at a rate that even the radiator exhaust is barely warm. This air can then still cool the GPU radiators.&lt;/p&gt; &lt;p&gt;For running LLM inference many of you on this reddit will know that you do not hit very high power usage per card. So running LLMs this rig can stay pretty quiet, even long prompt processing wont bother it since there is a lot of thermal mass to heat up. However other things like StableDifusion or training will make it pull some serious power and require the fans to ramp up a fair bit.&lt;/p&gt; &lt;p&gt;Total weight of this computer is 32kg ~70lb so i ended up adding a pair of handles to the top in order to make moving it a bit easier.&lt;/p&gt; &lt;p&gt;As for performance. Well it performs like a 4x RTX 3090 rig, plenty of LLM benchmarks for that on this corner of reddit.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; - AMD Threadripper Pro 3945WX 12 core&lt;br /&gt; - ASRock WRX80 Creator R2.0 board&lt;br /&gt; - 256GB of DDR4 3200 (8 sticks)&lt;br /&gt; - 4x GPUs: RTX 3090 (all on PCIe 16x 4.0)&lt;br /&gt; - 2x PSUs: 1500W Silverstone SST-ST1500 + 1000W Corsair HX1000&lt;br /&gt; - Thermaltake Core X71 case&lt;br /&gt; - AIO CPU cooler Enermax Liqtech II 240mm&lt;br /&gt; - GPU cooling custom loop with 360mm + 360mm + 240mm radiators&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mhtue5q02qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7661e688704dce04b4d31e659ddc3dd357bce917"&gt;https://preview.redd.it/mhtue5q02qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7661e688704dce04b4d31e659ddc3dd357bce917&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dp0p0qh12qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e66aa50b217c088d9926f95659d5e78d54a4fbbf"&gt;https://preview.redd.it/dp0p0qh12qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e66aa50b217c088d9926f95659d5e78d54a4fbbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qsls22u14qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dacfcd4fcfe078a29ffbc93fb81b251da7fc474"&gt;https://preview.redd.it/qsls22u14qke1.jpg?width=3468&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dacfcd4fcfe078a29ffbc93fb81b251da7fc474&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/berni8k"&gt; /u/berni8k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivo0gv/darkrapids_local_gpu_rig_build_with_style_water/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T17:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1iui6nk</id>
    <title>Starting next week, DeepSeek will open-source 5 repos</title>
    <updated>2025-02-21T04:13:54+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt; &lt;img alt="Starting next week, DeepSeek will open-source 5 repos" src="https://preview.redd.it/syeh0rmm3fke1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3667b8c2ba5c4d6506f21080ba3334e6724119" title="Starting next week, DeepSeek will open-source 5 repos" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syeh0rmm3fke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivdkoe</id>
    <title>What are the best uncensored/unfiltered small models(up to 22B) for philosophical conversation/brainstorming?</title>
    <updated>2025-02-22T07:42:12+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The models I tried act unnecessarily like morality police which kills the purpose of philosophical debates. what models would you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T07:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivhrbn</id>
    <title>Are you using local LLMs to power Cline, RooCode, Cursor etc.? What is your experience?</title>
    <updated>2025-02-22T12:34:15+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anybody using opensource models locally (like Phi-4 14B, Mistrall Small 24B, Qwen 14B, Deepseek Coder V2 Lite 16B) with local agentic code assistants like cline, cursor, RooCode, Pythagora and others? What are you using and what is your experience with them?&lt;/p&gt; &lt;p&gt;Last time I have checked was with Deepseek Coder 7B about a year ago and it wasn't usable, wondering how far it got and if people are using it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivhrbn/are_you_using_local_llms_to_power_cline_roocode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivhrbn/are_you_using_local_llms_to_power_cline_roocode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivhrbn/are_you_using_local_llms_to_power_cline_roocode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T12:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iva6j2</id>
    <title>There's also the new ROG Flow Z13 (2025) with 128GB LPDDR5X on board for $2,799</title>
    <updated>2025-02-22T04:08:43+00:00</updated>
    <author>
      <name>/u/ultrapcb</name>
      <uri>https://old.reddit.com/user/ultrapcb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The mem bus is still at 256bit and a M4 Pro or whatever is faster but 128gb vram at this price doesn't sound too bad or not?&lt;/p&gt; &lt;h1&gt;edit: to be clear, this is unified memory!&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ultrapcb"&gt; /u/ultrapcb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T04:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv45vg</id>
    <title>AMD Strix Halo 128GB performance on deepseek r1 70B Q8</title>
    <updated>2025-02-21T22:51:13+00:00</updated>
    <author>
      <name>/u/hardware_bro</name>
      <uri>https://old.reddit.com/user/hardware_bro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw a review on douying for Chinese mini PC AXB35-2 prototype with AI MAX+ pro 395 and 128GB memory. Running deepseek r1 Q8 on LM studio 0.3.9 with 2k context on windows, no flash attention, the reviewer said it is about 3token/sec.&lt;/p&gt; &lt;p&gt;source: douying id 141zhf666, posted on Feb 13.&lt;/p&gt; &lt;p&gt;For comparison: I have macbook pro m4 MAX 40core GPU 128GB, running LM studio 0.3.10, running deepseek r1 70B distilled Q8 with 2k context, no flash attention or k, v cache. 5.46tok/sec&lt;/p&gt; &lt;p&gt;Update test the mac using MLX instead of GGUF format:&lt;/p&gt; &lt;p&gt;Using MLX Deepseek R1 distill Llama-70B 8bit.&lt;/p&gt; &lt;p&gt;2k context, output 1140tokens at 6.29 tok/sec.&lt;/p&gt; &lt;p&gt;8k context, output 1365 tokens at 5.59 tok/sec&lt;/p&gt; &lt;p&gt;13k max context, output 1437 tokens at 6.31 tok/sec, 1.1% context full&lt;/p&gt; &lt;p&gt;13k max context, output 1437 tokens at 6.36 tok/sec, 1.4% context full&lt;/p&gt; &lt;p&gt;13k max context, output 3422 tokens at 5.86 tok/sec, 3.7% context full&lt;/p&gt; &lt;p&gt;13k max context, output 1624 tokens at 5.62 tok/sec, 4.6% context full&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hardware_bro"&gt; /u/hardware_bro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T22:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivj7xz</id>
    <title>How do you use multimodal models?</title>
    <updated>2025-02-22T13:54:04+00:00</updated>
    <author>
      <name>/u/TheGlobinKing</name>
      <uri>https://old.reddit.com/user/TheGlobinKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noob here... I often use text-generation-webui for running quantized (gguf) LLMs on my laptop, but I have no idea how to use visual language models (e.g. &lt;a href="https://huggingface.co/jiviai/Jivi-RadX-v1"&gt;https://huggingface.co/jiviai/Jivi-RadX-v1&lt;/a&gt;) or the new Ovis2. I was wondering if there is a similar tool to easily work with those models (loading pictures and so on) or do I need to learn python?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGlobinKing"&gt; /u/TheGlobinKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivj7xz/how_do_you_use_multimodal_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivj7xz/how_do_you_use_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivj7xz/how_do_you_use_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T13:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv45hh</id>
    <title>You can now do function calling with DeepSeek R1</title>
    <updated>2025-02-21T22:50:42+00:00</updated>
    <author>
      <name>/u/ido-pluto</name>
      <uri>https://old.reddit.com/user/ido-pluto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"&gt; &lt;img alt="You can now do function calling with DeepSeek R1" src="https://external-preview.redd.it/3lluEZUQskcTLR8tskEWHfqfXh4UA47uxDjeSpFlBzc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=344959dbe67223a6bb5d5f24659e31e96a57a19c" title="You can now do function calling with DeepSeek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ido-pluto"&gt; /u/ido-pluto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://node-llama-cpp.withcat.ai/blog/v3.6-deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T22:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivn6pj</id>
    <title>Google AI Studio Free - What's the daily limits?</title>
    <updated>2025-02-22T16:55:01+00:00</updated>
    <author>
      <name>/u/Sostrene_Blue</name>
      <uri>https://old.reddit.com/user/Sostrene_Blue</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm new to Google AI Studio Free and I'm really impressed so far. &lt;/p&gt; &lt;p&gt;It seems like I can use it daily without any restrictions. Is that actually true? Are there any limitations I should be aware of? &lt;/p&gt; &lt;p&gt;Also, does the &amp;quot;grounding with Google Search&amp;quot; feature also have unlimited usage? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sostrene_Blue"&gt; /u/Sostrene_Blue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivn6pj/google_ai_studio_free_whats_the_daily_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T16:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv72vu</id>
    <title>Are there any LLMs with less than 1m parameters?</title>
    <updated>2025-02-22T01:24:28+00:00</updated>
    <author>
      <name>/u/UselessSoftware</name>
      <uri>https://old.reddit.com/user/UselessSoftware</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that's a weird request and the model would be useless, but I'm doing a proof-of-concept port of llama2.c to DOS and I want a model that can fit inside 640 KB of RAM.&lt;/p&gt; &lt;p&gt;Anything like a 256K or 128K model?&lt;/p&gt; &lt;p&gt;I want to get LLM inferencing working on the original PC. 😆&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UselessSoftware"&gt; /u/UselessSoftware &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T01:24:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv6zou</id>
    <title>Ovis2 34B ~ 1B - Multi-modal LLMs from Alibaba International Digital Commerce Group</title>
    <updated>2025-02-22T01:19:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on qwen2.5 series, they covered all sizes from 1B to 32B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/AIDC-AI/ovis2-67ab36c7e497429034874464"&gt;https://huggingface.co/collections/AIDC-AI/ovis2-67ab36c7e497429034874464&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We are pleased to announce the release of Ovis2, our latest advancement in multi-modal large language models (MLLMs). Ovis2 inherits the innovative architectural design of the Ovis series, aimed at structurally aligning visual and textual embeddings. As the successor to Ovis1.6, Ovis2 incorporates significant improvements in both dataset curation and training methodologies.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T01:19:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivq82p</id>
    <title>PocketPal Update: Roleplay &amp; AI Assistant Management Made Easy</title>
    <updated>2025-02-22T19:01:25+00:00</updated>
    <author>
      <name>/u/Ill-Still-6859</name>
      <uri>https://old.reddit.com/user/Ill-Still-6859</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt; &lt;img alt="PocketPal Update: Roleplay &amp;amp; AI Assistant Management Made Easy" src="https://external-preview.redd.it/xFz-PydBf6J5PbD1cEzU0c0JBwrOIBAU7nRQ1kNYrMk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbe4bf095fcf49915b200d8be106d02079e7e96a" title="PocketPal Update: Roleplay &amp;amp; AI Assistant Management Made Easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;just released &lt;strong&gt;Pals&lt;/strong&gt; in &lt;a href="https://github.com/a-ghorbani/pocketpal-ai"&gt;PocketPal&lt;/a&gt; &lt;strong&gt;(v1.8.3+)&lt;/strong&gt;, so wanted to share with you folks.&lt;/p&gt; &lt;p&gt;Pals is basically a feature to make it easier to manage AI assistants and roleplay system prompts/setups. If you often tweak system prompts for LLMs on device, this should save you some time I guess.&lt;/p&gt; &lt;h1&gt;What Pals Lets You Do&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Define Assistant Types:&lt;/strong&gt; simple system prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Roleplay Mode:&lt;/strong&gt; quickly create structured roleplay scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Choose Default Models:&lt;/strong&gt; assign a specific model for each Pal, so as soon as you select to use it it loads the model too.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quick Activation:&lt;/strong&gt; select or switch Pals directly from the chat input.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Prompt Generation:&lt;/strong&gt; llms help also generating system prompts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you need help writing system prompts, you can use an LLM to generate them inside PocketPal. I've found Llama 3.2 3B works well for this.&lt;/p&gt; &lt;p&gt;For more details and usage instructions check out this: &lt;a href="https://github.com/a-ghorbani/pocketpal-ai/discussions/221"&gt;https://github.com/a-ghorbani/pocketpal-ai/discussions/221&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;As always would love to hear what you think.&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Update &lt;strong&gt;PocketPal&lt;/strong&gt; to the latest version. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: atm through TestFlight: &lt;a href="https://testflight.apple.com/join/B3KE74MS"&gt;https://testflight.apple.com/join/B3KE74MS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://play.google.com/store/apps/details?id=com.pocketpalai"&gt;https://play.google.com/store/apps/details?id=com.pocketpalai&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Go to the &lt;strong&gt;Pals&lt;/strong&gt; screen and set up an assistant or roleplay character.&lt;/li&gt; &lt;li&gt;Assign a model, system prompt, or generate one using an LLM, etc&lt;/li&gt; &lt;li&gt;Use your Pal from the chat whenever you need it.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ivq82p/video/m2zgersxmqke1/player"&gt;https://reddit.com/link/1ivq82p/video/m2zgersxmqke1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Still-6859"&gt; /u/Ill-Still-6859 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivq82p/pocketpal_update_roleplay_ai_assistant_management/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T19:01:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivfddq</id>
    <title>Is it worth spending so much time and money on small LLMs?</title>
    <updated>2025-02-22T09:52:13+00:00</updated>
    <author>
      <name>/u/ML-Future</name>
      <uri>https://old.reddit.com/user/ML-Future</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"&gt; &lt;img alt="Is it worth spending so much time and money on small LLMs?" src="https://preview.redd.it/n9rafd86xnke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046f7f56c2b33ae73d2249b62632828de46d3b5" title="Is it worth spending so much time and money on small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ML-Future"&gt; /u/ML-Future &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9rafd86xnke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfddq/is_it_worth_spending_so_much_time_and_money_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T09:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivgqhe</id>
    <title>What are your use cases for small (1-3-8B) models?</title>
    <updated>2025-02-22T11:29:03+00:00</updated>
    <author>
      <name>/u/silveroff</name>
      <uri>https://old.reddit.com/user/silveroff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m curious what you guys doing with tiny 1-3B or little bigger like 8-9B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silveroff"&gt; /u/silveroff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivgqhe/what_are_your_use_cases_for_small_138b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T11:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivkpdo</id>
    <title>Wayfarer Large is (surprisingly) great + Example Chats</title>
    <updated>2025-02-22T15:06:10+00:00</updated>
    <author>
      <name>/u/Sunija_Dev</name>
      <uri>https://old.reddit.com/user/Sunija_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; &lt;a href="https://pastebin.com/BEXz8399"&gt;Example Chat 1&lt;/a&gt; &lt;a href="https://pastebin.com/nH5iYzFe"&gt;/ 2&lt;/a&gt; It works with normal RP (= not text adventure). And it's great.&lt;/p&gt; &lt;p&gt;Maybe you had the same situation as me, seeing the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;announcement of Wayfarer Large 70b&lt;/a&gt;...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a textadventure model&lt;/li&gt; &lt;li&gt;that is brutal and will kill you&lt;/li&gt; &lt;li&gt;and is a Llama3.3 finetune&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...and thinking: &lt;strong&gt;Wow, that's like a who's-who of things that I'm not interested in.&lt;/strong&gt; I don't use a textadventure style, I usually don't want to die in my RP, and Llama3 is so sloppy/repetitiony that even finetunes usually don't get rid of it. So, it was rather desperation when I downloaded Wayfarer Large, threw it in my normal setup aaaand... well, you read the title. Let's talk details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Works with &amp;quot;normal&amp;quot; RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite it being a textadventure model, you can just use it like any other model without adapting your setup. My example character has an adventurey setting, but the models also works with slice-of-life cards. Or whatever you're into.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Shortform RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wayfarer is one of the few models that writes short posts (&lt;a href="https://pastebin.com/BEXz8399"&gt;see example&lt;/a&gt;). If you like that is definitely subjective. But there are some advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No space for slop/repeptiton (and even if, you'd notice it quickly)&lt;/li&gt; &lt;li&gt;Usable even with 1.5 tok/s&lt;/li&gt; &lt;li&gt;You get to interact more without waiting for generation/reading&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Simply good RP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Often finetunes just focus on &amp;quot;less slop&amp;quot;, but I think there are more things that make RP good (&lt;a href="https://docs.google.com/document/d/1OwBgvdQkz57g6uNzeBGk3ozru8jwga3NdGahKOfuhc8/edit?usp=sharing"&gt;you can read more on my RP ramblings here&lt;/a&gt;). And despite the posts being short, Wayfarer fits everything necessary in them.&lt;/p&gt; &lt;p&gt;It moves the plot forward and is fairly intelligent. The dialog feels natural, sometimes cracking jokes and being witty. And it references the context (surroundings and stuff) properly, which is a bit of a pet-peeve for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Not crazy evil&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They advertised it as a maniac, but it's... fine. I bet you can prompt it to be a crazy murder-hobo, but it never randomly tried to kill me. It just doesn't have a strong positivity bias and you can have fun arguments with it. Which, I guess (?) is what people rather want, than a murder-hobo. I'd say it has great &amp;quot;emotional range&amp;quot; - it can be angry at you, but it doesn't have to.&lt;/p&gt; &lt;p&gt;It is not as crazy as DeepSeek-R1 that suddenly throws mass murder in your highschool drama. If R1 is &lt;em&gt;Game of Thrones&lt;/em&gt;, Wayfarer is &lt;em&gt;Lord of the Rings&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Keep in mind: I didn't adapt my prompts at all to fit Wayfarer. You can find my system prompt and char card at the end of the example chat. So, with better prompting, you can definitely get more out of the model.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rarely gets stuck in situations where it doesn't progress the story.&lt;/li&gt; &lt;li&gt;Very rarely switches to &amp;quot;You&amp;quot; style.&lt;/li&gt; &lt;li&gt;Shortform isn't everbodies favorite. But you might be able to change that via prompts?&lt;/li&gt; &lt;li&gt;Doesn't like to write character's thoughts.&lt;/li&gt; &lt;li&gt;Doesn't super strictly follow character cards. Maybe an issue with my prompt.&lt;/li&gt; &lt;li&gt;Doesn't not describes surroundings as much as I'd like.&lt;/li&gt; &lt;li&gt;Still some positivity bias in normal prompting...?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How can I run it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I run this quant (&lt;a href="https://huggingface.co/VoidStare/Wayfarer-Large-70B-Llama-3.3-EXL2-4.65bpw-h6"&gt;VoidStare_Wayfarer-Large-70B-Llama-3.3-EXL2-4.65bpw-h6&lt;/a&gt;) on 2x3090 (48GB vram). With a 3090+3060 (=36GB vram) you can run a 3bpw quant. Since it's posts are short, running it partially on CPU could be fine too.&lt;/p&gt; &lt;p&gt;Also, if you want to support the creators, you can run it with an &lt;a href="https://aidungeon.com/"&gt;aidungeon subscription&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So, is it a perfect model? No, obviously not.&lt;/p&gt; &lt;p&gt;But to me, it's the most interesting since Mistral-123b large finetunes. And, besides using it as-is, I bet merging it or finetuning on top could be very interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sunija_Dev"&gt; /u/Sunija_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivkpdo/wayfarer_large_is_surprisingly_great_example_chats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T15:06:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpa6r</id>
    <title>Abusing WebUI Artifacts (Again)</title>
    <updated>2025-02-22T18:21:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt; &lt;img alt="Abusing WebUI Artifacts (Again)" src="https://external-preview.redd.it/cDhodWx2cjZncWtlMQu5ROv8uFTKESGnRAtwEPoYjV9P5uC6sL5S6dTjkckO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96024f10c2840d152729af0e38edf6258ccf9cd5" title="Abusing WebUI Artifacts (Again)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eav6zr6gqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivfwta</id>
    <title>Finally stable</title>
    <updated>2025-02-22T10:30:39+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt; &lt;img alt="Finally stable" src="https://preview.redd.it/67mcka284oke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d58089abaf3cb6614bb83158b3a7bf5ad67876d7" title="Finally stable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Lazarus – Dual RTX 3090 Build&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;GPUs: 2x RTX 3090 @ 70% TDP&lt;/p&gt; &lt;p&gt;CPU: Ryzen 9 9950X&lt;/p&gt; &lt;p&gt;RAM: 64GB DDR5 @ 5600MHz&lt;/p&gt; &lt;p&gt;Total Power Draw (100% Load): ~700watts&lt;/p&gt; &lt;p&gt;GPU temps are stable at 60-70c at max load.&lt;/p&gt; &lt;p&gt;These RTX 3090s were bought used with water damage, and I’ve spent the last month troubleshooting and working on stability. After extensive cleaning, diagnostics, and BIOS troubleshooting, today I finally managed to fit a full 70B model entirely in GPU memory.&lt;/p&gt; &lt;p&gt;Since both GPUs are running at 70% TDP, I’ve temporarily allowed one PCIe power cable to feed two PCIe inputs, though it's still not optimal for long-term stability.&lt;/p&gt; &lt;p&gt;Currently monitoring temps and perfmance—so far, so good!&lt;/p&gt; &lt;p&gt;Let me know if you have any questions or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67mcka284oke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T10:30:39+00:00</published>
  </entry>
</feed>
