<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-21T00:52:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mvpdl4</id>
    <title>Anyone tried running llama cpp with Vulkan on Android?</title>
    <updated>2025-08-20T19:53:17+00:00</updated>
    <author>
      <name>/u/Icy_Advance_2514</name>
      <uri>https://old.reddit.com/user/Icy_Advance_2514</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im trying to run llama cpp on pixel phones and i wonder if anyone had success before? There is an issue on qualcomm gpu s with vulcan, but anyone tried with Mali?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Advance_2514"&gt; /u/Icy_Advance_2514 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvpdl4/anyone_tried_running_llama_cpp_with_vulkan_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvpdl4/anyone_tried_running_llama_cpp_with_vulkan_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvpdl4/anyone_tried_running_llama_cpp_with_vulkan_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvtgkj</id>
    <title>Lightweight browser tool to run local models (Gemma, Llama, Zephyr, Phi, Qwen, Mistral) with private document Q&amp;A - no installation required</title>
    <updated>2025-08-20T22:28:03+00:00</updated>
    <author>
      <name>/u/gpt872323</name>
      <uri>https://old.reddit.com/user/gpt872323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone getting started with local setups and prioritizing privacy, &lt;a href="https://lite.askcyph.ai"&gt;https://lite.askcyph.ai&lt;/a&gt; offers a lightweight, browser-based way to work with local models such as Gemma, Llama, Zephyr, Phi, and Mistral, plus simple document Q&amp;amp;A, all running client-side.&lt;/p&gt; &lt;p&gt;There are tons of sites that offer free hosted models, but privacy has been an issue. This makes it easier without installation required for someone without much tech background.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt872323"&gt; /u/gpt872323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvtgkj/lightweight_browser_tool_to_run_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvtgkj/lightweight_browser_tool_to_run_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvtgkj/lightweight_browser_tool_to_run_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T22:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6wwe</id>
    <title>nvidia/parakeet-tdt-0.6b-v3 (now multilingual)</title>
    <updated>2025-08-20T06:14:15+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt; &lt;img alt="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" src="https://external-preview.redd.it/12PzLvQjZXrvyzotsfsH7vxtU3vJRsRc5ZD3WiNviO0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a6ac17d880f0cd5d00920d58cc8f2aa4530205" title="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;parakeet-tdt-0.6b-v3 is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription. It extends the parakeet-tdt-0.6b-v2 model by expanding language support from English to 25 European languages. The model automatically detects the language of the audio and transcribes it without requiring additional prompting. It is part of a series of models that leverage the Granary [1, 2] multilingual corpus as their primary training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mviuzq</id>
    <title>Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)</title>
    <updated>2025-08-20T16:00:19+00:00</updated>
    <author>
      <name>/u/aquarat</name>
      <uri>https://old.reddit.com/user/aquarat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt; &lt;img alt="Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)" src="https://external-preview.redd.it/MaUSRoNLAJ1LyKjA9wGag9Te4pvp93JxB__k_YVVUBI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a08347bd804d947ea5c5b82647272167dc6fca" title="Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd really like to get something decent running locally, like one of the Deepseek models. I figure this will need 600 GBs of VRAM to run comfortably with one of the Unsloth models. Buying this amount of VRAM via Nvidia GPUs isn't workable for me, but the AMD Strix Halo 395+ machines should make this possible, eventually 😅.&lt;/p&gt; &lt;p&gt;An option is the Framework Desktop, but about 2 months back HP ran a very short-lived special on their HP Z2 Mini G1a Strix machines... so I bought two. I found the ROCM system/libs to be pretty unusable but things are improving.&lt;/p&gt; &lt;p&gt;One machine runs Ubuntu (kernel 6.14.0-27) and another runs Fedora Rawhide (FR is really nice...) (kernel 6.17.0-0.rc0.250808g37816488247d.14.fc43.x86_64)&lt;/p&gt; &lt;p&gt;I recently found this repository via a post on Framework's forum: &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes"&gt;https://github.com/kyuz0/amd-strix-halo-toolboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified the Dockerfile slightly to also build llama.cpp's RPC server and then tried out the result. To my surprise it worked with GPT OSS 120b (Unsloth Q4_K_XL variant).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/92p4bsxv17kf1.png?width=1710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed952d17fbb988151abf8a2523ed8cf84759f15"&gt;https://preview.redd.it/92p4bsxv17kf1.png?width=1710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed952d17fbb988151abf8a2523ed8cf84759f15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some caveats: one machine is running Ubuntu, another is running Fedora Rawhide with a bleeding edge kernel. They're connected via 2.5 gigabit ethernet. &lt;code&gt;nvtop&lt;/code&gt; doesn't work very well on the Ubuntu machine, I assume because the kernel is old.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 5294.35 ms / 2491 tokens ( 2.13 ms per token, 470.50 tokens per second) eval time = 3586.47 ms / 109 tokens ( 32.90 ms per token, 30.39 tokens per second) total time = 8880.82 ms / 2600 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and a follow-up prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 5082.28 ms / 2313 tokens ( 2.20 ms per token, 455.11 tokens per second) eval time = 152837.44 ms / 4085 tokens ( 37.41 ms per token, 26.73 tokens per second) total time = 157919.71 ms / 6398 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm using a modified version of the &lt;code&gt;rocm-7rc-rocwmma&lt;/code&gt; docker image:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;RUN cmake -S . -B build \ -DGGML_HIP=ON \ -DAMDGPU_TARGETS=gfx1151 \ -DCMAKE_BUILD_TYPE=Release \ -DLLAMA_HIP_UMA=ON \ -DGGML_HIP_ROCWMMA_FATTN=ON \ -DGGML_RPC=ON \ &amp;amp;&amp;amp; cmake --build build --config Release -- -j$(nproc) \ &amp;amp;&amp;amp; cmake --install build --config Release RUN find build -type f -name 'libggm*.so' -exec cp -v {} /opt/rocm-7.0/lib/ \; RUN cp ./build/bin/rpc-server /usr/local/bin/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and I run it like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Build the image docker build -t &amp;lt;your account on docker hub&amp;gt;/llama-rocm-7rc-rocwmma:2508201616 -f Dockerfile.rocm-7rc-rocwmma --push . // secondary node / start first docker run --rm -ti --device /dev/dri --device /dev/kfd --group-add video --group-add render -p 50052:50052 --name llama --security-opt seccomp=unconfined docker.io/aquarat/llama-rocm-7rc-rocwmma:2508201616 rpc-server -H 0.0.0.0 // primary node docker run --rm -ti -v /home/user/models-llama:/models --name llama --device /dev/dri --device /dev/kfd --group-add video --group-add render -p 11434:11434 --security-opt seccomp=unconfined docker.io/aquarat/llama-rocm-7rc-rocwmma:2508201616 llama-server --no-mmap -ngl 999 -fa -m /models/gpt-oss/gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf --port 11434 --host 0.0.0.0 -c 100000 --jinja --reasoning-format auto --rpc 192.168.0.39:50052 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I thought this might be usable to someone 🤷‍♂️&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l6z8a6ak67kf1.png?width=631&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f13cb6fb46388ad264eb42481ad735465aeb751"&gt;https://preview.redd.it/l6z8a6ak67kf1.png?width=631&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f13cb6fb46388ad264eb42481ad735465aeb751&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aquarat"&gt; /u/aquarat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjjxe</id>
    <title>guide : running gpt-oss with llama.cpp -ggerganov</title>
    <updated>2025-08-20T16:24:30+00:00</updated>
    <author>
      <name>/u/onwardforward</name>
      <uri>https://old.reddit.com/user/onwardforward</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"&gt; &lt;img alt="guide : running gpt-oss with llama.cpp -ggerganov" src="https://external-preview.redd.it/0MtZInNIGRZV6H6dIhZ8EGkGtej94yvJcqiqENXoH1U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e1421e8cea5aa0f8f2819555a163e1481ff84d0" title="guide : running gpt-oss with llama.cpp -ggerganov" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onwardforward"&gt; /u/onwardforward &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15396"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvp0kn</id>
    <title>cursor will increase in price , The good thing is that we have local models</title>
    <updated>2025-08-20T19:40:16+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt; &lt;img alt="cursor will increase in price , The good thing is that we have local models" src="https://external-preview.redd.it/wrLxmwkxE0sboHYe9DL7M2A9cBYiHIRiSvubxC7TZHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0319732839ae255898da934baa62523680f329b8" title="cursor will increase in price , The good thing is that we have local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the cursor will increase in price. Right now, you have an elastic price, but after September 15, you will be charged more.&lt;/p&gt; &lt;p&gt;blog : &lt;a href="https://cursor.com/blog/aug-2025-pricing"&gt;https://cursor.com/blog/aug-2025-pricing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;price : &lt;a href="https://docs.cursor.com/en/account/pricing#auto"&gt;https://docs.cursor.com/en/account/pricing#auto&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5jfqnrb98kf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f23a973f7d39ab4ff7f46e79d3bcc014dacdb61"&gt;https://preview.redd.it/i5jfqnrb98kf1.png?width=689&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f23a973f7d39ab4ff7f46e79d3bcc014dacdb61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ytoam8gk88kf1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5db3b26f6f79cda3f7cc6717f5956e371c2587e3"&gt;https://preview.redd.it/ytoam8gk88kf1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5db3b26f6f79cda3f7cc6717f5956e371c2587e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvp0kn/cursor_will_increase_in_price_the_good_thing_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvbzvh</id>
    <title>Qwen 30B Instruct vs GPT-OSS 20B for real life coding</title>
    <updated>2025-08-20T11:20:38+00:00</updated>
    <author>
      <name>/u/Mobile_Ice1759</name>
      <uri>https://old.reddit.com/user/Mobile_Ice1759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Would like some opinions besides benchmarks for those 2 models (or maybe additional one) from people who use it for production applications. Web (PHP/JS), iOS (Swift). As Im GPU poor and have 1x3090 these are the best local options for me now.&lt;/p&gt; &lt;p&gt;Both models sucks with the whole codebases (qwen cli, aider), so I'm making some summaries which then I give to it along with some context.&lt;/p&gt; &lt;p&gt;Naturally GPT works a bit faster, but I encounter a problem where I have to switch models for different problems, like UI or back-end, even though they are not consistently better versus each other. I'm looking for anyone who can get me along the way with models parameters, workflow, etc with going on this setup.&lt;/p&gt; &lt;p&gt;Mostly all my problems are solved via paid services, but there are 2 projects now, where I can't/won't share data and trying to think of solution without spending half a budget on making a lab or purchasing cloud gpu.&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mobile_Ice1759"&gt; /u/Mobile_Ice1759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T11:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvv6bv</id>
    <title>Useful Recipes IK-Llama</title>
    <updated>2025-08-20T23:39:56+00:00</updated>
    <author>
      <name>/u/Infamous_Jaguar_2151</name>
      <uri>https://old.reddit.com/user/Infamous_Jaguar_2151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted invite everyone interested to share recipes and tokens/sec results that have worked for you in ik-llama.&lt;/p&gt; &lt;p&gt;Below are mine so far — mostly for GLM models on multi-GPU + CPU setups. If you spot any optimizations, I’d love to hear them. I’m running ubergarm quants. I’m new to this so if it looks off feel free to let me know.&lt;/p&gt; &lt;p&gt;My machine: EPYC 9225, 768GB DDR5-6000 RAM, dual RTX 4090 (soon to be swapped for 6000 Pro Max-Q).&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;GLM 4.5 Air — 19 tokens/sec&lt;/p&gt; &lt;p&gt;~/ik_llama.cpp/build/bin/llama-server \ --model &amp;quot;/path/to/models/GLM 4.5 Air/GLM-4.5-Air-IQ5_K-00001-of-00002.gguf&amp;quot; \ --alias &amp;quot;ubergarm/GLM-4.5-Air-IQ5_K&amp;quot; \ --chat-template chatglm4 \ --ctx-size 32768 \ -fa -fmoe \ -ctk q8_0 -ctv q8_0 \ -ub 4096 -b 4096 \ -ngl 99 \ --split-mode layer \ -ot exps=CPU \ --parallel 1 \ --threads 20 \ --host 127.0.0.1 \ --port 8080 \ --no-mmap&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;GLM 4.5 — 12 tokens/sec&lt;/p&gt; &lt;p&gt;~/ik&lt;em&gt;llama.cpp/build/bin/llama-server \ --model &amp;quot;/path/to/models/GLM-4.5/GLM-4.5-IQ4_K-00001-of-00005.gguf&amp;quot; \ --alias &amp;quot;ubergarm/GLM-4.5-IQ4_K&amp;quot; \ --ctx-size 32768 \ -fa -fmoe \ -ctk q8_0 -ctv q8_0 \ -ub 4096 -b 4096 \ -ngl 99 \ --split-mode layer \ -ot exps=CPU \ -ot 'blk.(3|4).ffn&lt;/em&gt;.&lt;em&gt;=CUDA0' \ -ot 'blk.(5|6).ffn_.&lt;/em&gt;=CUDA1' \ --parallel 1 \ --threads 24 \ --host 127.0.0.1 \ --port 8080 \ --no-mmap&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infamous_Jaguar_2151"&gt; /u/Infamous_Jaguar_2151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvv6bv/useful_recipes_ikllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvv6bv/useful_recipes_ikllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvv6bv/useful_recipes_ikllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T23:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mve5hp</id>
    <title>Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis</title>
    <updated>2025-08-20T13:01:02+00:00</updated>
    <author>
      <name>/u/Educational_Cry_7951</name>
      <uri>https://old.reddit.com/user/Educational_Cry_7951</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt; &lt;img alt="Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis" src="https://a.thumbs.redditmedia.com/bAUgORU1f1Mo2dKJy7wG0byeBvquE5LZqpWQak3Vgr0.jpg" title="Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you’ve used modern reasoning-focused LLMs, you’ve probably seen it happen: the model starts solving your problem, then analyzes its own reasoning, then re-analyzes that, spiraling into thousands of tokens of circular “thinking.” It’s expensive, slow, and sometimes worse than a non reasoning model.&lt;/p&gt; &lt;p&gt;Today, we’re excited to share &lt;strong&gt;Datarus-R1-14B-Preview&lt;/strong&gt;, a new &lt;strong&gt;open-weight reasoning model&lt;/strong&gt; designed to avoid this overthinking trap while hitting state-of-the-art results on coding and reasoning benchmarks.&lt;/p&gt; &lt;h1&gt;Key points:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;14B parameters — but outperforms much larger models.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;18–49% fewer tokens&lt;/strong&gt; than competitors for the same reasoning tasks.&lt;/li&gt; &lt;li&gt;New training method focused on &lt;strong&gt;adaptive&lt;/strong&gt; multi-step reasoning.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it out &amp;amp; resources:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chat and test the model: &lt;a href="https://chat.datarus.ai"&gt;chat.datarus.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Website: &lt;a href="https://datarus.ai"&gt;datarus.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jupyter Agent for interactive workflows: &lt;a href="https://github.com/DatarusAI/Datarus-JupyterAgent"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model weights (open): &lt;a href="https://huggingface.co/DatarusAI/Datarus-R1-14B-preview"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Preprint: &lt;a href="https://arxiv.org/pdf/2508.13382"&gt;ArXiv 2508.13382&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear what you all think, especially if you give the &lt;strong&gt;Preview&lt;/strong&gt; a spin or integrate the Jupyter agent into your workflows!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z0nb5gw4a6kf1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b5295d0021f25a5efefb6c6ea98ef8c5a53e367"&gt;https://preview.redd.it/z0nb5gw4a6kf1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b5295d0021f25a5efefb6c6ea98ef8c5a53e367&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Cry_7951"&gt; /u/Educational_Cry_7951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvuzzl</id>
    <title>Offline AI models for background noise removal and voice isolation</title>
    <updated>2025-08-20T23:32:14+00:00</updated>
    <author>
      <name>/u/healthiswealth0</name>
      <uri>https://old.reddit.com/user/healthiswealth0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Izotope 11 doesn't give results comparable to Adobe Podcast, but AP can only process max 4h/recording and it's online only.&lt;/p&gt; &lt;p&gt;Is there any offline AI model I can use which outputs similar quality as AP? I have RTX4090 so GPU is not an issue. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/healthiswealth0"&gt; /u/healthiswealth0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvuzzl/offline_ai_models_for_background_noise_removal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T23:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3hcr</id>
    <title>GPT 4.5 vs DeepSeek V3.1</title>
    <updated>2025-08-20T03:06:43+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt; &lt;img alt="GPT 4.5 vs DeepSeek V3.1" src="https://preview.redd.it/5c3gbyx3c3kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcee32647c7f5d8e6d02cf6e6eb7d06bb63cacab" title="GPT 4.5 vs DeepSeek V3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5c3gbyx3c3kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnetu</id>
    <title>New Trainable Sparsity Method I've been working on!</title>
    <updated>2025-08-20T18:41:56+00:00</updated>
    <author>
      <name>/u/nano-tech-warrior</name>
      <uri>https://old.reddit.com/user/nano-tech-warrior</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt; &lt;img alt="New Trainable Sparsity Method I've been working on!" src="https://preview.redd.it/mpxhgfb1y7kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1be78ec5f54af38b4819955caf62751a49149e6" title="New Trainable Sparsity Method I've been working on!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing CWIC a trainable sparsity paradigm that beats SOTA methods, enabling 80% sparsity and 4x+ speedups on CPU.&lt;/p&gt; &lt;p&gt;Something I've been working on with friends at &lt;a href="http://crystalai.org"&gt;crystalai.org&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;It works on models as small as 1b, outperforming TEAL R-sparse and friends.&lt;br /&gt; We are releasing code at &lt;a href="https://github.com/crystal-ai-org/cwic"&gt;https://github.com/crystal-ai-org/cwic&lt;/a&gt;&lt;br /&gt; read more at the blog &lt;a href="https://crystalai.org/blog/2025-08-18-compute-where-it-counts"&gt;https://crystalai.org/blog/2025-08-18-compute-where-it-counts&lt;/a&gt;&lt;br /&gt; if your interested in our our work feel free to reach out at &lt;a href="https://x.com/crystalAIorg"&gt;https://x.com/crystalAIorg&lt;/a&gt;, we love collaboration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nano-tech-warrior"&gt; /u/nano-tech-warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mpxhgfb1y7kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7kk2</id>
    <title>Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528</title>
    <updated>2025-08-20T06:54:36+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt; &lt;img alt="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" src="https://b.thumbs.redditmedia.com/H3XSTIYwTWYE6zf6TJkmBkaHUW2BDiBZVxdTdMDbz8s.jpg" title="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://github.com/cpldcpu/LRMTokenEconomy"&gt;here &lt;/a&gt;for more background information on the evaluation.&lt;/p&gt; &lt;p&gt;It appears they significantly reduced overthinking for prompts that can can be answered from model knowledge and math problems. There are still some cases where it creates very long CoT though for logic puzzles.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv7kk2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvgw9k</id>
    <title>DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC</title>
    <updated>2025-08-20T14:48:46+00:00</updated>
    <author>
      <name>/u/alexmrv</name>
      <uri>https://old.reddit.com/user/alexmrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt; &lt;img alt="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" src="https://external-preview.redd.it/FW1JlH9sSss0Pq8rzoWxjFpJJZK922NxO1y6uOe6VUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d145871e4a70112dabd181298b3fe1e73ada2a35" title="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been experimenting with memory systems for AI agents, and I wanted to share a prototype I've built: DiffMem. It's a lightweight, Git-based memory backend that stores &amp;quot;current state&amp;quot; knowledge in Markdown files while using Git's commit history for tracking evolution. The goal is efficient, scalable memory for long-horizon agents. Think personal assistants that accumulate knowledge over years without bloating queries.&lt;/p&gt; &lt;h1&gt;Why Git for AI Memory?&lt;/h1&gt; &lt;p&gt;Traditional approaches (databases, vector stores) work, but they can get messy with evolving personal data. DiffMem flips this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Current-State Focus&lt;/strong&gt;: Only the &amp;quot;now&amp;quot; view is in active files (e.g., current relationships or facts). This keeps search/indexing lean. BM25 queries hit a compact surface, reducing token overhead in LLM contexts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;History in the Background&lt;/strong&gt;: Changes live in Git diffs/logs. Agents query the present by default but can dive into &amp;quot;how did this evolve?&amp;quot; via targeted diffs (e.g., &lt;code&gt;git diff HEAD~1 file.md&lt;/code&gt;), without loading full histories.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benefits for Engineers&lt;/strong&gt;: No schemas/migrations. Just edit Markdown. Git handles versioning, branching (e.g., monthly timelines), and audits for free. It's durable (plaintext, distributed) and hackable.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt;: Analyzes transcripts, creates/updates entities, stages in Git's working tree (commit explicit for atomicity).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Manager&lt;/strong&gt;: Assembles depth-based context (basic: core blocks; deep: full files; temporal: with Git history).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Searcher&lt;/strong&gt;: LLM-orchestrated BM25 for semantic-aware retrieval.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a PoC. Functional but rough (e.g., manual index rebuilds, basic error handling). Full code, examples, and repo guide on GitHub.&lt;/p&gt; &lt;h1&gt;Why Share This?&lt;/h1&gt; &lt;p&gt;This is R&amp;amp;D exploring how version control can power evolvable agents. We're not claiming it's revolutionary, but it solves real pain points like memory sprawl and temporal reasoning. Future ideas: agent-driven pruning (archive low-strength memories to branches), collaborative repos for multi-agent systems, or hybrid with embeddings.&lt;/p&gt; &lt;p&gt;I'd love honest feedback: Does this resonate? What breaks? Ideas for improvements/collaborations? PRs welcome.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;https://github.com/Growth-Kinetics/DiffMem&lt;/a&gt;&lt;br /&gt; License: MIT&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexmrv"&gt; /u/alexmrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T14:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvgg6u</id>
    <title>Doing continued pre-training with Unsloth?</title>
    <updated>2025-08-20T14:32:02+00:00</updated>
    <author>
      <name>/u/Thisisdog92</name>
      <uri>https://old.reddit.com/user/Thisisdog92</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to experiment with continued pre-training to teach a model domain specific facts (law) in a non-english language, but the barrier to entry seems a bit daunting. My dataset is in the range of ~2B tokens.&lt;/p&gt; &lt;p&gt;Unsloth has a &lt;a href="https://docs.unsloth.ai/basics/continued-pretraining"&gt;guide&lt;/a&gt;, and also posted &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1d86k5y/continued_pretraining_2x_faster_notebook_to/"&gt;here&lt;/a&gt; on reddit about using QLoRA for continued pre-training, which seems both easier and less resource intensive than doing full continued pre-training. At the same time information is a bit sparse.&lt;/p&gt; &lt;p&gt;Has anyone here done continued pre-training using unsloth? How are the results compared to proper pre-training? &lt;/p&gt; &lt;p&gt;Is it possible to do it on an instruction tuned model or is it important to use a base model? Not having to do instruction tuning would make things a lot easier.&lt;/p&gt; &lt;p&gt;Are there any recommendations around specific models (eg qwen 3), where some respond better to training than others? Is it best to stay away from MoE models?&lt;/p&gt; &lt;p&gt;If using Unsloth (and 4bit QLoRA), should you train on a Q4 model, or should the model itself be a higher quant?&lt;/p&gt; &lt;p&gt;Any recommendations around training hyperparameters doing cpt with Unsloth? Ie learning rate, LoRA rank and alpha, context length, batch size etc.&lt;/p&gt; &lt;p&gt;Is there anything regarding language one should be mindful of? My language (swedish) is usually moderately to well supported, so tokenizations should not be an issue.&lt;/p&gt; &lt;p&gt;How should I format my dataset? Most of my documents are quite large, to the tune of 10k-20k tokens. Do I just train on these larger documents or should I split them? Any suggestions on tools for preparing that?&lt;/p&gt; &lt;p&gt;I know these are a lot of questions, but I haven't been able to find solid answers to them and I'm hoping the answers to this post could help more people in the future who want to experiment with continued pre-training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thisisdog92"&gt; /u/Thisisdog92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgg6u/doing_continued_pretraining_with_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgg6u/doing_continued_pretraining_with_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgg6u/doing_continued_pretraining_with_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T14:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvol0o</id>
    <title>Running Qwen3-Coder-30B-A3 Q4_LM in Cursor with Agent Mode unlocked</title>
    <updated>2025-08-20T19:24:29+00:00</updated>
    <author>
      <name>/u/ConfidentDinner6648</name>
      <uri>https://old.reddit.com/user/ConfidentDinner6648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing ways to make Cursor usable without relying only on their default “auto” model (which honestly feels pretty bad). While experimenting, I noticed something interesting:&lt;/p&gt; &lt;p&gt;If you run a model locally and just register it under the name &lt;code&gt;gpt-4o&lt;/code&gt;, Cursor unlocks &lt;strong&gt;Agent Mode&lt;/strong&gt; (function calling, todo list, etc.) and everything works as if it were an official endpoint.&lt;/p&gt; &lt;p&gt;I tried this with &lt;strong&gt;Qwen3-Coder-30B-A3 Q4_LM&lt;/strong&gt; (through LM Studio + ngrok) and here’s what I got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms Gemini Flash and Gemini Pro on many coding tasks&lt;/li&gt; &lt;li&gt;In some cases, feels close to Sonnet 4 (which is wild for a quantized 30B)&lt;/li&gt; &lt;li&gt;Function calling works smoothly, no errors so far&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This obviously isn’t official support, but it shows that Cursor &lt;em&gt;could&lt;/em&gt; support local/self-hosted models natively without much issue.&lt;/p&gt; &lt;p&gt;Anyone else tried running Qwen3 (or others) inside Cursor like this? Curious to hear results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentDinner6648"&gt; /u/ConfidentDinner6648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvts3i</id>
    <title>2x RTX 5060ti 16GB - inference benchmarks in Ollama</title>
    <updated>2025-08-20T22:41:21+00:00</updated>
    <author>
      <name>/u/avedave</name>
      <uri>https://old.reddit.com/user/avedave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"&gt; &lt;img alt="2x RTX 5060ti 16GB - inference benchmarks in Ollama" src="https://b.thumbs.redditmedia.com/k4_tJqT09ffBK0-BkHQm_oeGLgR4XZG78TAvB_JX2Fk.jpg" title="2x RTX 5060ti 16GB - inference benchmarks in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Despite the recommendations of most Redditors, I chose not to fish a used 3090 out of a dumpster for $1,000. Instead, I bought two brand-new NVIDIA RTX 5060 Ti 16GB cards for a total of $800.&lt;/p&gt; &lt;p&gt;I am pretty happy with the inference results in Ollama!&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quantization: Q4_K_M (all models)&lt;/li&gt; &lt;li&gt;Prompt: &amp;quot;Write a 500-word essay containing recommendations for travel arrangements from Warsaw to New York, assuming it’s the year 1900.&amp;quot;&lt;/li&gt; &lt;li&gt;NVIDIA drivers: 575.64.03&lt;/li&gt; &lt;li&gt;CUDA version: 12.9&lt;/li&gt; &lt;li&gt;Ollama version: 0.11.4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Duration&lt;/th&gt; &lt;th align="left"&gt;Prompt Processing&lt;/th&gt; &lt;th align="left"&gt;Response Processing&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 1B&lt;/td&gt; &lt;td align="left"&gt;0m:4s&lt;/td&gt; &lt;td align="left"&gt;249 tokens/s&lt;/td&gt; &lt;td align="left"&gt;212 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 4B&lt;/td&gt; &lt;td align="left"&gt;0m:8s&lt;/td&gt; &lt;td align="left"&gt;364 tokens/s&lt;/td&gt; &lt;td align="left"&gt;108 token/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 12B&lt;/td&gt; &lt;td align="left"&gt;0m:18s&lt;/td&gt; &lt;td align="left"&gt;305 tokens/s&lt;/td&gt; &lt;td align="left"&gt;44 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 27B&lt;/td&gt; &lt;td align="left"&gt;0m:42s&lt;/td&gt; &lt;td align="left"&gt;217 tokens/s&lt;/td&gt; &lt;td align="left"&gt;22 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 70B&lt;/td&gt; &lt;td align="left"&gt;7m:31s&lt;/td&gt; &lt;td align="left"&gt;22 tokens/s&lt;/td&gt; &lt;td align="left"&gt;3.04 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Conclusions / Observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'd be happy to see a direct comparison, but I believe that for inference, 2x5060ti 16GB is a much better option than 1x3090 24GB&lt;/li&gt; &lt;li&gt;Load times for all models were between 1 and 10 seconds, so if you are worried about 5060ti being just PCIe 5 x8 - I don't think that an issue at all&lt;/li&gt; &lt;li&gt;Even during the lengthy inference of DeepSeek R1 70B each GPU was consuming around just 40W (while the card is rated at max 180W)&lt;/li&gt; &lt;li&gt;The temperature of GPUs was around 60C&lt;/li&gt; &lt;li&gt;The last two observations probably mean there's some room for improvement - I'd be happy to see any suggestions!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avedave"&gt; /u/avedave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvts3i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvts3i/2x_rtx_5060ti_16gb_inference_benchmarks_in_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T22:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvo9ko</id>
    <title>Using large-scale search to discover fast GPU kernels</title>
    <updated>2025-08-20T19:12:53+00:00</updated>
    <author>
      <name>/u/jafioti</name>
      <uri>https://old.reddit.com/user/jafioti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt; &lt;img alt="Using large-scale search to discover fast GPU kernels" src="https://external-preview.redd.it/b-ktLeXWioxuV4hQoMUJJnc-Er8yy-L0PCsHQ7N2a0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd17cc6390a58faaef58b46b4cf1bfc8a445b756" title="Using large-scale search to discover fast GPU kernels" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance. &lt;a href="https://github.com/luminal-ai/luminal"&gt;https://github.com/luminal-ai/luminal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It takes high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.&lt;/p&gt; &lt;p&gt;You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: &lt;a href="https://youtu.be/P2oNR8zxSAA"&gt;https://youtu.be/P2oNR8zxSAA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.&lt;/p&gt; &lt;p&gt;We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.&lt;/p&gt; &lt;p&gt;The aim is to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo above and I’d love to hear your thoughts!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mvo9ko/video/dshypdss48kf1/player"&gt;https://reddit.com/link/1mvo9ko/video/dshypdss48kf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jafioti"&gt; /u/jafioti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9ko/using_largescale_search_to_discover_fast_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvw3hz</id>
    <title>NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model</title>
    <updated>2025-08-21T00:20:54+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt; &lt;img alt="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" src="https://b.thumbs.redditmedia.com/e1iw4XluNhKx2_uQFrUBfjQ-KXChA-T80tHk2Ay_3VI.jpg" title="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvw3hz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T00:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvl0zk</id>
    <title>Qwen-Image-Edit #6 overall on LMArena, best open model image editor</title>
    <updated>2025-08-20T17:17:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt; &lt;img alt="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" src="https://preview.redd.it/90yj5wnyj7kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c77694f269b30f417ff8568342a83f0ba81a1ec2" title="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Surprised they didn't vote this one higher, I felt like the edits I saw Qwen make online were pretty good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90yj5wnyj7kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T17:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvn50l</id>
    <title>Guys it's official, the nano banana model on lm arena is Google's</title>
    <updated>2025-08-20T18:32:13+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OfficialLoganK/status/1957908528925909391"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvfdja</id>
    <title>IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit</title>
    <updated>2025-08-20T13:51:04+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt; &lt;img alt="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" src="https://preview.redd.it/moddapg5j6kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6316f1a44b3898869b91f28f4d1774a35db2491" title="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Solar storms don’t just make pretty auroras—they can scramble GPS, disrupt flights, degrade satellite comms, and stress power grids. To get ahead of that, IBM and NASA have open‑sourced Surya on Hugging Face: a foundation model trained on years of Solar Dynamics Observatory (SDO) data to make space‑weather forecasting more accurate and accessible.&lt;/p&gt; &lt;p&gt;What Surya is&lt;/p&gt; &lt;p&gt;A mid‑size foundation model for heliophysics that learns general “features of the Sun” from large SDO image archives.&lt;/p&gt; &lt;p&gt;Built to support zero/few‑shot tasks like flare probability, CME risk, and geomagnetic indices (e.g., Kp/Dst) with fine‑tuning.&lt;/p&gt; &lt;p&gt;Released with open weights and recipes so labs, universities, and startups can adapt it without massive compute.&lt;/p&gt; &lt;p&gt;Why this matters&lt;/p&gt; &lt;p&gt;Early, reliable alerts help airlines reroute, satellite operators safe‑mode hardware, and grid operators harden the network before a hit.&lt;/p&gt; &lt;p&gt;Open sourcing lowers the barrier for regional forecasters and fosters reproducible science (shared baselines, comparable benchmarks).&lt;/p&gt; &lt;p&gt;We’re in an active solar cycle—better lead times now can prevent expensive outages and service disruptions.&lt;/p&gt; &lt;p&gt;How to try it (technical)&lt;/p&gt; &lt;p&gt;Pull the model from Hugging Face and fine‑tune on your target label: flare class prediction, Kp nowcasting, or satellite anomaly detection.&lt;/p&gt; &lt;p&gt;Start with SDO preprocessing pipelines; add lightweight adapters/LoRA for event‑specific fine‑tuning to keep compute modest.&lt;/p&gt; &lt;p&gt;Evaluate on public benchmarks (Kp/Dst) and report lead time vs. skill scores; stress test on extreme events.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moddapg5j6kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjj8q</id>
    <title>Seed-OSS-36B-Instruct</title>
    <updated>2025-08-20T16:23:50+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Introduction:&lt;/p&gt; &lt;p&gt;Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.&lt;/p&gt; &lt;p&gt;We release this series of models to the open-source community under the Apache-2.0 license.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Flexible Control of Thinking Budget&lt;/strong&gt;: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Reasoning Capability&lt;/strong&gt;: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Intelligence&lt;/strong&gt;: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Friendly&lt;/strong&gt;: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: Trained with up-to-512K long context natively.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve posted on here a couple times sharing my project. I'm training LLM’s from scratch on 1800’s London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I’ve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here’s the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up “Lord Palmerston” and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I’m not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I’m just doing this for fun so if anyone would like to collaborate let me know, I’m open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
