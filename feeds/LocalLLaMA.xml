<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-08T16:07:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kh3g7f</id>
    <title>Did anyone try out Mistral Medium 3?</title>
    <updated>2025-05-07T17:38:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt; &lt;img alt="Did anyone try out Mistral Medium 3?" src="https://external-preview.redd.it/Z3k2eW51bjJiZXplMcUDN_ixsC3ErmhxAmaSJ8XxFt_ddYdhD_A2seyNDJhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=380661f2e13d59675a91c29257e8bd38b702503f" title="Did anyone try out Mistral Medium 3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I briefly tried Mistral Medium 3 on OpenRouter, and I feel its performance might not be as good as Mistral's blog claims. (The video shows the best result out of the 5 shots I ran. ) &lt;/p&gt; &lt;p&gt;Additionally, I tested having it recognize and convert the benchmark image from the blog into JSON. However, it felt like it was just randomly converting things, and not a single field matched up. Could it be that its input resolution is very low, causing compression and therefore making it unable to recognize the text in the image? &lt;/p&gt; &lt;p&gt;Also, I don't quite understand why it uses 5-shot in the GPTQ diamond and MMLU Pro benchmarks. Is that the default number of shots for these tests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6w9w0rl2beze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T17:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh6kh3</id>
    <title>Qwen3 MMLU-Pro Computer Science LLM Benchmark Results</title>
    <updated>2025-05-07T19:43:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt; &lt;img alt="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" src="https://preview.redd.it/3yuv5m5qxeze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ed0dfa07bd3b2e9b138176b2104e26c7a51e6e4" title="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive &lt;strong&gt;Qwen 3 evaluations&lt;/strong&gt; across a range of formats and quantisations, focusing on &lt;strong&gt;MMLU-Pro&lt;/strong&gt; (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-235B-A22B&lt;/strong&gt; (via Fireworks API) tops the table at &lt;strong&gt;83.66%&lt;/strong&gt; with ~55 tok/s.&lt;/li&gt; &lt;li&gt;But the &lt;strong&gt;30B-A3B Unsloth&lt;/strong&gt; quant delivered &lt;strong&gt;82.20%&lt;/strong&gt; while running locally at ~45 tok/s and with zero API spend.&lt;/li&gt; &lt;li&gt;The same Unsloth build is ~5x faster than Qwen's &lt;strong&gt;Qwen3-32B&lt;/strong&gt;, which scores &lt;strong&gt;82.20%&lt;/strong&gt; as well yet crawls at &amp;lt;10 tok/s.&lt;/li&gt; &lt;li&gt;On Apple silicon, the &lt;strong&gt;30B MLX&lt;/strong&gt; port hits &lt;strong&gt;79.51%&lt;/strong&gt; while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;0.6B&lt;/strong&gt; micro-model races above 180 tok/s but tops out at &lt;strong&gt;37.56%&lt;/strong&gt; - that's why it's not even on the graph (50 % performance cut-off).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, Alibaba/Qwen - you really whipped the llama's ass! And to OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. &lt;em&gt;This&lt;/em&gt; is the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yuv5m5qxeze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:43:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1khs34q</id>
    <title>Best Open source Speech to text+ diarization models</title>
    <updated>2025-05-08T14:53:31+00:00</updated>
    <author>
      <name>/u/Hungry-Ad-1177</name>
      <uri>https://old.reddit.com/user/Hungry-Ad-1177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, hope you’re doing well. I’m currently working on a project where I need to convert audio conversations between a customer and agents into text.&lt;/p&gt; &lt;p&gt;Since most recordings involve up to three speakers, could you please suggest some top open-source models suited for this task, particularly those that support speaker diarization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry-Ad-1177"&gt; /u/Hungry-Ad-1177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khs34q/best_open_source_speech_to_text_diarization_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khs34q/best_open_source_speech_to_text_diarization_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khs34q/best_open_source_speech_to_text_diarization_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T14:53:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1khb7rs</id>
    <title>The new MLX DWQ quant is underrated, it feels like 8bit in a 4bit quant.</title>
    <updated>2025-05-07T22:59:41+00:00</updated>
    <author>
      <name>/u/mzbacd</name>
      <uri>https://old.reddit.com/user/mzbacd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed it was added to MLX a few days ago and started using it since then. It's very impressive, like running an 8bit model in a 4bit quantization size without much performance loss, and I suspect it might even finally make the 3bit quantization usable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;edit:&lt;br /&gt; just made a DWQ quant one from unquantized version:&lt;br /&gt; &lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mzbacd"&gt; /u/mzbacd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T22:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbxg4</id>
    <title>QwQ Appreciation Thread</title>
    <updated>2025-05-07T23:33:37+00:00</updated>
    <author>
      <name>/u/OmarBessa</name>
      <uri>https://old.reddit.com/user/OmarBessa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt; &lt;img alt="QwQ Appreciation Thread" src="https://external-preview.redd.it/iUbtHN7RzxrcJ1LnOytJyYZIsd6RNnT0J4eou-hgYFg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8377a20d6ad8107b227ddbef333fbae642705" title="QwQ Appreciation Thread" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561"&gt;https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Taken from: &lt;a href="https://fiction.live/stories/Fiction-liveBench-May-06-2025/oQdzQvKHw8JyXbN87"&gt;Regarding-the-Table-Design - Fiction-liveBench-May-06-2025 - Fiction.live&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mean guys, don't get me wrong. The new Qwen3 models are great, but QwQ still holds quite decently. If it weren't for its overly verbose thinking...yet look at this. &lt;strong&gt;It is still basically sota in long context comprehension among open-source models.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OmarBessa"&gt; /u/OmarBessa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1khrcle</id>
    <title>Llama nemotron model</title>
    <updated>2025-05-08T14:22:22+00:00</updated>
    <author>
      <name>/u/Basic-Pay-9535</name>
      <uri>https://old.reddit.com/user/Basic-Pay-9535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thoughts on the new llama nemotron reasoning model by nvidia ? how would you compare it to other open source and closed reasoning models. And what are your top reasoning models ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Basic-Pay-9535"&gt; /u/Basic-Pay-9535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khrcle/llama_nemotron_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khrcle/llama_nemotron_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khrcle/llama_nemotron_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T14:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0hcd</id>
    <title>Cracking 40% on SWE-bench verified with open source models &amp; agents &amp; open-source synth data</title>
    <updated>2025-05-07T15:39:46+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt; &lt;img alt="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" src="https://preview.redd.it/4lwtc2sgpdze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f581dfebc0968cbf87949bad4b08918a6afa989" title="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know that finetuning &amp;amp; RL work great for getting great LMs for agents -- the problem is where to get the training data!&lt;/p&gt; &lt;p&gt;We've generated 50k+ task instances for 128 popular GitHub repositories, then trained our own LM for SWE-agent. The result? We achieve 40% pass@1 on SWE-bench Verified -- a new SoTA among open source models.&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;em&gt;everything&lt;/em&gt;, and we're excited to see what you build with it! This includes the agent (SWE-agent), the framework used to generate synthetic task instances (SWE-smith), and our fine-tuned LM (SWE-agent-LM-32B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4lwtc2sgpdze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9018</id>
    <title>OpenCodeReasoning - new Nemotrons by NVIDIA</title>
    <updated>2025-05-07T21:22:11+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1khlxzj</id>
    <title>If you could make a MoE with as many active and total parameters as you wanted. What would it be?</title>
    <updated>2025-05-08T09:35:09+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1khgir9</id>
    <title>Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?</title>
    <updated>2025-05-08T03:28:06+00:00</updated>
    <author>
      <name>/u/GrungeWerX</name>
      <uri>https://old.reddit.com/user/GrungeWerX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt; &lt;img alt="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" src="https://external-preview.redd.it/eNoMp-cBhW5B3lKr_XZAgD1Qku1SepqMDIM_aLwv22o.png?width=140&amp;amp;height=82&amp;amp;crop=140:82,smart&amp;amp;auto=webp&amp;amp;s=b758fb52ae557d8179140e36108651b61ff7d08a" title="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only person that's noticed that GLM-4's outputs are eerily similar to Gemini Pro 2.5 in formatting? I copy/pasted a prompt in several different SOTA LLMs - GPT-4, DeepSeek, Gemini 2.5 Pro, Claude 2.7, and Grok. Then I tried it in GLM-4, and was like, wait a minute, where have I seen this formatting before? Then I checked - it was in &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;. Now, I'm not saying that GLM-4 is Gemini 2.5 Pro, of course not, but could it be a hacked earlier version? Or perhaps (far more likely) they used it as a template for how GLM does its outputs? Because Gemini is the &lt;strong&gt;only&lt;/strong&gt; LLM that does it this way where it gives you three Options w/parentheticals describing tone, and then finalizes it by saying &amp;quot;Choose the option that best fits your tone&amp;quot;. Like, &lt;strong&gt;almost exactly the same.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just tested it out on Gemini 2.0 and Gemini Flash. Neither of these versions do this. This is only done by Gemini 2.5 Pro and GLM-4. None of the other Closed-source LLMs do this either, like chat-gpt, grok, deepseek, or claude.&lt;/p&gt; &lt;p&gt;I'm not complaining. And if the Chinese were to somehow hack their LLM and released a quantized open source version to the world - despite how unlikely this is - I wouldn't protest...much. &amp;gt;.&amp;gt;&lt;/p&gt; &lt;p&gt;But jokes aside, anyone else notice this?&lt;/p&gt; &lt;p&gt;Some samples:&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d"&gt;https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e"&gt;https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05"&gt;https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f"&gt;https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrungeWerX"&gt; /u/GrungeWerX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T03:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzwe9</id>
    <title>New mistral model benchmarks</title>
    <updated>2025-05-07T15:16:25+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt; &lt;img alt="New mistral model benchmarks" src="https://preview.redd.it/hrtrvrvnmdze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a47a4a215c33b3670819e5b09e20d25a73074d7" title="New mistral model benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hrtrvrvnmdze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh579e</id>
    <title>Qwen 3 evaluations</title>
    <updated>2025-05-07T18:48:14+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt; &lt;img alt="Qwen 3 evaluations" src="https://preview.redd.it/8f8g366goeze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd68bf0ab81adb00446d201fbee1d90070c68389" title="Qwen 3 evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive Qwen 3 evaluations across a range of formats and quantisations, focusing on MMLU-Pro (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;p&gt;1️⃣ Qwen3-235B-A22B (via Fireworks API) tops the table at 83.66% with ~55 tok/s.&lt;/p&gt; &lt;p&gt;2️⃣ But the 30B-A3B Unsloth quant delivered 82.20% while running locally at ~45 tok/s and with zero API spend.&lt;/p&gt; &lt;p&gt;3️⃣ The same Unsloth build is ~5x faster than Qwen's Qwen3-32B, which scores 82.20% as well yet crawls at &amp;lt;10 tok/s.&lt;/p&gt; &lt;p&gt;4️⃣ On Apple silicon, the 30B MLX port hits 79.51% while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/p&gt; &lt;p&gt;5️⃣ The 0.6B micro-model races above 180 tok/s but tops out at 37.56% - that's why it's not even on the graph (50 % performance cut-off).&lt;/p&gt; &lt;p&gt;All local runs were done with @lmstudio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;Conclusion: Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, @Alibaba_Qwen - you really whipped the llama's ass! And to @OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. This is the future!&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/wolframrvnwlf/status/1920186645384478955?s=46"&gt;https://x.com/wolframrvnwlf/status/1920186645384478955?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8f8g366goeze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1khmh5m</id>
    <title>Anyone get speculative decoding to work for Qwen 3 on LM Studio?</title>
    <updated>2025-05-08T10:11:47+00:00</updated>
    <author>
      <name>/u/jaxchang</name>
      <uri>https://old.reddit.com/user/jaxchang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got it working in llama.cpp, but it's being slower than running Qwen 3 32b by itself in LM Studio. Anyone tried this out yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaxchang"&gt; /u/jaxchang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T10:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1khmaah</id>
    <title>5 commands to run Qwen3-235B-A22B Q3 inference on 4x3090 + 32-core TR + 192GB DDR4 RAM</title>
    <updated>2025-05-08T09:59:25+00:00</updated>
    <author>
      <name>/u/EmilPi</name>
      <uri>https://old.reddit.com/user/EmilPi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, thanks Qwen team for the generosity, and Unsloth team for quants.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;: optimized for my build, your options may vary (e.g. I have slow RAM, which does not work above 2666MHz, and only 3 channels of RAM available). This set of commands downloads GGUFs into llama.cpp's folder build/bin folder. If unsure, use full paths. I don't know why, but llama-server may not work if working directory is different.&lt;/p&gt; &lt;p&gt;End result: 125-180 tokens per second read speed (prompt processing), 12-15 tokens per second write speed (generation) - depends on prompt/response/context length. I use 8k context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;0. You need CUDA installed (so, I kinda lied) and available in your PATH:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/"&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Download &amp;amp; Compile llama.cpp:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp ; cd llama.cpp cmake -B build -DBUILD_SHARED_LIBS=ON -DLLAMA_CURL=OFF -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_USE_GRAPHS=ON ; cmake --build build --config Release --parallel 32 cd build/bin &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. Download quantized model (that almost fits into 96GB VRAM) files:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for i in {1..3} ; do curl -L --remote-name &amp;quot;https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-0000${i}-of-00003.gguf?download=true&amp;quot; ; done &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;3. Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --port 1234 \ --model ./Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf \ --alias Qwen3-235B-A22B-Thinking \ --temp 0.6 --top-k 20 --min-p 0.0 --top-p 0.95 \ -ngl 95 --split-mode layer -ts 22,23,24,26 \ -c 8192 -ctk q8_0 -ctv q8_0 -fa \ --main-gpu 3 \ --no-mmap \ -ot 'blk\.[2-3]1\.ffn.*=CPU' \ -ot 'blk\.[5-8]1\.ffn.*=CPU' \ -ot 'blk\.9[0-1]\.ffn.*=CPU' \ --threads 32 --numa distribute &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmilPi"&gt; /u/EmilPi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:59:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbz70</id>
    <title>Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)</title>
    <updated>2025-05-07T23:35:56+00:00</updated>
    <author>
      <name>/u/eding42</name>
      <uri>https://old.reddit.com/user/eding42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt; &lt;img alt="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" src="https://external-preview.redd.it/qU1K5b6t8nYIjVW3n7kpmgcB3rS2YYANfyJcs8RztyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4675a9fdccab6a8f9da5be381fa2c1bd1fe534bf" title="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe the 24 GB Arc B580 model that got leaked will be announced? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eding42"&gt; /u/eding42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/intel/status/1920241029804064796"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1khl779</id>
    <title>Auto Thinking Mode Switch for Qwen3 / Open Webui Function</title>
    <updated>2025-05-08T08:40:24+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt; &lt;img alt="Auto Thinking Mode Switch for Qwen3 / Open Webui Function" src="https://external-preview.redd.it/Aave_1tyS2RPiuZJIgxgvsnWGpciBGyuudbUEh0fSQA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d772752d670d6ca5d635248fb38c7dc6830a14b" title="Auto Thinking Mode Switch for Qwen3 / Open Webui Function" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/AaronFeng753/Better-Qwen3"&gt;&lt;strong&gt;https://github.com/AaronFeng753/Better-Qwen3&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is an open webui function for Qwen3 models, it can automatically turn on/off the thinking process by using the LLM itself to evaluate the difficulty of your request.&lt;/p&gt; &lt;p&gt;You will need to edit the code to config the OpenAI compatible API URL and the Model name.&lt;/p&gt; &lt;p&gt;(And yes, it works with local LLM, I'm using one right now, ollama and lm studio both has OpenAI compatible API)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cntv1xrcsize1.png?width=846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef5af10d3badbdeaa6d9558fdce7895de4f5c3"&gt;https://preview.redd.it/cntv1xrcsize1.png?width=846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef5af10d3badbdeaa6d9558fdce7895de4f5c3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T08:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1khqz92</id>
    <title>Intel Promises More Arc GPU Action at Computex - Battlemage Goes Pro With AI-Ready Memory Capacities</title>
    <updated>2025-05-08T14:06:43+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khqz92/intel_promises_more_arc_gpu_action_at_computex/"&gt; &lt;img alt="Intel Promises More Arc GPU Action at Computex - Battlemage Goes Pro With AI-Ready Memory Capacities" src="https://external-preview.redd.it/Fn-jI2IQ5AcXbD0Bt1vpe7afAh8S1-d1oMm_VEMcTUQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d173e4817ba5709a8969ba44d6ed200538e00e2" title="Intel Promises More Arc GPU Action at Computex - Battlemage Goes Pro With AI-Ready Memory Capacities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-promises-arc-gpu-action-at-computex-battlemage-pro-ai-ready-memory-capacities/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khqz92/intel_promises_more_arc_gpu_action_at_computex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khqz92/intel_promises_more_arc_gpu_action_at_computex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T14:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1khqyds</id>
    <title>GMK EVO-X2 AI Max+ 395 Mini-PC review!</title>
    <updated>2025-05-08T14:05:38+00:00</updated>
    <author>
      <name>/u/Corylus-Core</name>
      <uri>https://old.reddit.com/user/Corylus-Core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/embed/UXjg6Iew9lg?si=ki1HzFMO6nuy4oaX"&gt;GMK EVO-X2 AI Max+ 395 Mini-PC review!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corylus-Core"&gt; /u/Corylus-Core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khqyds/gmk_evox2_ai_max_395_minipc_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khqyds/gmk_evox2_ai_max_395_minipc_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khqyds/gmk_evox2_ai_max_395_minipc_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T14:05:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1khs277</id>
    <title>Aider benchmarks for Qwen3-235B-A22B that were posted here were apparently faked</title>
    <updated>2025-05-08T14:52:27+00:00</updated>
    <author>
      <name>/u/tjuene</name>
      <uri>https://old.reddit.com/user/tjuene</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khs277/aider_benchmarks_for_qwen3235ba22b_that_were/"&gt; &lt;img alt="Aider benchmarks for Qwen3-235B-A22B that were posted here were apparently faked" src="https://external-preview.redd.it/yXqXdu-V1zQwQHC-lYAJfW59I54R-k04O4eLGM6BROQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=909d548ecf3ca2af543b4af0384879360a7594a8" title="Aider benchmarks for Qwen3-235B-A22B that were posted here were apparently faked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tjuene"&gt; /u/tjuene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Aider-AI/aider/pull/3908#issuecomment-2863328652"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khs277/aider_benchmarks_for_qwen3235ba22b_that_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khs277/aider_benchmarks_for_qwen3235ba22b_that_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T14:52:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9qlx</id>
    <title>No local, no care.</title>
    <updated>2025-05-07T21:53:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt; &lt;img alt="No local, no care." src="https://preview.redd.it/f0l4hjmklfze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ceb732f3829a0007aaaa683f507cd9116cadc51" title="No local, no care." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0l4hjmklfze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1khql0u</id>
    <title>Intel to launch Arc Pro B60 graphics card with 24GB memory at Computex - VideoCardz.com</title>
    <updated>2025-05-08T13:49:27+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No word on pricing yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-to-launch-arc-pro-b60-graphics-card-with-24gb-memory-at-computex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khql0u/intel_to_launch_arc_pro_b60_graphics_card_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khql0u/intel_to_launch_arc_pro_b60_graphics_card_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T13:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1khlw98</id>
    <title>ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation</title>
    <updated>2025-05-08T09:31:33+00:00</updated>
    <author>
      <name>/u/searcher1k</name>
      <uri>https://old.reddit.com/user/searcher1k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"&gt; &lt;img alt="ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation" src="https://external-preview.redd.it/CFr9_cLjVeZwkAhQkscmJVhOJEQi2DmpjFaEwPPrd7A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=837ef81c969d8d59aca68353cf9f08b45dab3dd4" title="ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.17671"&gt;https://arxiv.org/abs/2503.17671&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/searcher1k"&gt; /u/searcher1k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1khlw98"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1khq3ul</id>
    <title>Introducing the Intelligent Document Processing (IDP) Leaderboard – A Unified Benchmark for OCR, KIE, VQA, Table Extraction, and More</title>
    <updated>2025-05-08T13:27:29+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The most comprehensive benchmark to date for evaluating document understanding capabilities of Vision-Language Models (VLMs).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt;&lt;br /&gt; A unified evaluation suite covering 6 core IDP tasks across 16 datasets and 9,229 documents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key Information Extraction (KIE)&lt;/li&gt; &lt;li&gt;Visual Question Answering (VQA)&lt;/li&gt; &lt;li&gt;Optical Character Recognition (OCR)&lt;/li&gt; &lt;li&gt;Document Classification&lt;/li&gt; &lt;li&gt;Table Extraction&lt;/li&gt; &lt;li&gt;Long Document Processing (LongDocBench)&lt;/li&gt; &lt;li&gt;(Coming soon: Confidence Score Calibration)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each task uses multiple datasets, including real-world, synthetic, and newly annotated ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights from the Benchmark&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Flash leads overall&lt;/strong&gt;, but surprisingly underperforms its predecessor on OCR and classification.&lt;/li&gt; &lt;li&gt;All models struggled with long document understanding – top score was just 69.08%.&lt;/li&gt; &lt;li&gt;Table extraction remains a bottleneck — especially for long, sparse, or unstructured tables.&lt;/li&gt; &lt;li&gt;Surprisingly, GPT-4o's performance &lt;em&gt;decreased&lt;/em&gt; in the latest version (&lt;em&gt;gpt-4o-2024-11-20&lt;/em&gt;) compared to its earlier release (&lt;em&gt;gpt-4o-2024-08-06&lt;/em&gt;).&lt;/li&gt; &lt;li&gt;Token usage (and thus cost) varies dramatically across models — GPT-4o-mini was the most expensive per request due to high token usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why does this matter?&lt;/strong&gt;&lt;br /&gt; There’s currently no unified benchmark that evaluates all IDP tasks together — most leaderboards (e.g., OpenVLM, Chatbot Arena) don’t deeply assess document understanding.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Document Variety&lt;/strong&gt;&lt;br /&gt; We evaluated models on a wide range of documents: Invoices, forms, receipts, charts, tables (structured + unstructured), handwritten docs, and even diacritics texts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get Involved&lt;/strong&gt;&lt;br /&gt; We’re actively updating the benchmark with new models and datasets.&lt;/p&gt; &lt;p&gt;This is developed with collaboration from IIT Indore and Nanonets.&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://idp-leaderboard.org/"&gt;https://idp-leaderboard.org/&lt;/a&gt;&lt;br /&gt; Release blog: &lt;a href="https://idp-leaderboard.org/details/"&gt;https://idp-leaderboard.org/details/&lt;/a&gt;&lt;br /&gt; GithHub: &lt;a href="https://github.com/NanoNets/docext/tree/main/docext/benchmark"&gt;https://github.com/NanoNets/docext/tree/main/docext/benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to share your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khq3ul/introducing_the_intelligent_document_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khq3ul/introducing_the_intelligent_document_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khq3ul/introducing_the_intelligent_document_processing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T13:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1khpf0m</id>
    <title>Smoothie Qwen: A lightweight adjustment tool for smoothing token probabilities in the Qwen models to encourage balanced multilingual generation.</title>
    <updated>2025-05-08T12:55:52+00:00</updated>
    <author>
      <name>/u/likejazz</name>
      <uri>https://old.reddit.com/user/likejazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khpf0m/smoothie_qwen_a_lightweight_adjustment_tool_for/"&gt; &lt;img alt="Smoothie Qwen: A lightweight adjustment tool for smoothing token probabilities in the Qwen models to encourage balanced multilingual generation." src="https://preview.redd.it/ctoabtdg2kze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a07c38c69823d8ec768a987766643b8ad2f4845" title="Smoothie Qwen: A lightweight adjustment tool for smoothing token probabilities in the Qwen models to encourage balanced multilingual generation." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Smoothie Qwen&lt;/strong&gt; is a lightweight adjustment tool that smooths token probabilities in Qwen models, enhancing balanced multilingual generation capabilities. We've uploaded pre-adjusted models to our &lt;a href="https://huggingface.co/collections/dnotitia/smoothie-qwen3-6811896ebb3a255de7b5b437"&gt;Smoothie Qwen Collection on 🤗 Hugging Face&lt;/a&gt; for your convenience:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smoothie-Qwen3 Collection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-0.6B"&gt;dnotitia/Smoothie-Qwen3-0.6B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-1.7B"&gt;dnotitia/Smoothie-Qwen3-1.7B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-4B"&gt;dnotitia/Smoothie-Qwen3-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-8B"&gt;dnotitia/Smoothie-Qwen3-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-14B"&gt;dnotitia/Smoothie-Qwen3-14B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-32B"&gt;dnotitia/Smoothie-Qwen3-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-30B-A3B"&gt;dnotitia/Smoothie-Qwen3-30B-A3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen3-235B-A22B"&gt;dnotitia/Smoothie-Qwen3-235B-A22B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Smoothie-Qwen2.5 Collection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-0.5B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-0.5B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-1.5B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-3B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-7B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-7B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-14B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-14B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-32B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-32B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/dnotitia/Smoothie-Qwen2.5-72B-Instruct"&gt;dnotitia/Smoothie-Qwen2.5-72B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/dnotitia/smoothie-qwen"&gt;https://github.com/dnotitia/smoothie-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/likejazz"&gt; /u/likejazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ctoabtdg2kze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khpf0m/smoothie_qwen_a_lightweight_adjustment_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khpf0m/smoothie_qwen_a_lightweight_adjustment_tool_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T12:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1khjrtj</id>
    <title>Building LLM Workflows - - some observations</title>
    <updated>2025-05-08T06:54:56+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on some relatively complex LLM workflows for the past year (not continuously, on and off). Here are some conclusions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Decomposing each task to the smallest steps and prompt chaining works far better than just using a single prompt with CoT. turning each step of the CoT into its own prompt and checking/sanitizing outputs reduces errors.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using XML tags to structure the system prompt, prompt etc works best (IMO better than JSON structure but YMMV)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You have to remind the LLM that its only job is to work as a semantic parser of sorts, to merely understand and transform the input data and NOT introduce data from its own &amp;quot;knowledge&amp;quot; into the output.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NLTK, SpaCY, FlairNLP are often good ways to independently verify the output of an LLM (eg: check if the LLM's output has a sequence of POS tags you want etc). The great thing about these libraries is they're fast and reliable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ModernBERT classifiers are often just as good at LLMs if the task is small enough. Fine-tuned BERT-style classifiers are usually better than LLM for focused, narrow tasks.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM-as-judge and LLM confidence scoring is extremely unreliable, especially if there's no &amp;quot;grounding&amp;quot; for how the score is to be arrived at. Scoring on vague parameters like &amp;quot;helpfulness&amp;quot; is useless - -eg: LLMs often conflate helpfulness with professional tone and length of response. Scoring has to either be grounded in multiple examples (which has its own problems - - LLMs may make the wrong inferences from example patterns), or a fine-tuned model is needed. If you're going to fine-tune for confidence scoring, might as well use a BERT model or something similar.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;In Agentic loops, the hardest part is setting up the conditions where the LLM exits the loop - - using the LLM to decide whether or not to exit is extremely unreliable (same reason as LLM-as-judge issues).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Performance usually degrades past 4k tokens (input context window) ... this is often only seen once you've run thousands of iterations. If you have a low error threshold, even a 5% failure rate in the pipeline is unacceptable, keeping all prompts below 4k tokens helps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32B models are good enough and reliable enough for most tasks, if the task is structured properly.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured CoT (with headings and bullet points) is often better than unstructured &lt;code&gt;&amp;lt;thinking&amp;gt;Okay, so I must...etc&lt;/code&gt; tokens. Structured and concise CoT stays within the context window (in the prompt as well as examples), and doesn't waste output tokens.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-consistency helps, but that also means running each prompt multiple times - - forces you to use smaller models and smaller prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Writing your own CoT is better than relying on a reasoning model. Reasoning models are a good way to collect different CoT paths and ideas, and then synthesize your own.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The long-term plan is always to fine-tune everything. Start with a large API-based model and few-shot examples, and keep tweaking. Once the workflows are operational, consider creating fine-tuning datasets for some of the tasks so you can shift to a smaller local LLM or BERT. Making balanced datasets isn't easy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;when making a dataset for fine-tuning, make it balanced by setting up a categorization system/orthogonal taxonomy so you can get complete coverage of the task. Use MECE framework.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've probably missed many points, these were the first ones that came to mind.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T06:54:56+00:00</published>
  </entry>
</feed>
