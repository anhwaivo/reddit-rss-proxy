<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-16T04:24:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jcdhw6</id>
    <title>Help with local build - Dual 5090s worth it?</title>
    <updated>2025-03-16T04:19:40+00:00</updated>
    <author>
      <name>/u/Mutinix</name>
      <uri>https://old.reddit.com/user/Mutinix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thinking of buying the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 9 9950X&lt;/li&gt; &lt;li&gt;NVIDIA GeForce RTX 5090&lt;/li&gt; &lt;li&gt; ASUS ProArt X870E-Creator&lt;/li&gt; &lt;li&gt;64GB Kingston Fury Beast&lt;/li&gt; &lt;li&gt;1200W FSP Power Supply&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm an AI newbie. I've been running 12B - 14B models on my M3 Macbook. I'm hoping to use the new PC to run bigger models, plus throw in some Stable Diffusion in there. This is a big expense - So I'm wondering if dual 5090s are worth it? I tend to keep my PCs for a loooong time. I still have my 970 build and it's 9 years old. I'm thinking of turning that in to a server and running PiHole and some other stuff on it. Coming back to the point - I read a few posts saying that I should just go for a server rather than consumer hardware, so I'm a bit conflicted on that front. I think have a consumer PC will be beneficial for the future and I can use it for anything I want.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mutinix"&gt; /u/Mutinix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdhw6/help_with_local_build_dual_5090s_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdhw6/help_with_local_build_dual_5090s_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdhw6/help_with_local_build_dual_5090s_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T04:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jba8c1</id>
    <title>Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM</title>
    <updated>2025-03-14T18:05:43+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt; &lt;img alt="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" src="https://b.thumbs.redditmedia.com/HVn5TgokJMLlPcDoL4fVZ2_Vk4oxK6Eh7mcozu3sLRs.jpg" title="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Gemma 3 (12B) up to &lt;strong&gt;6x longer context lengths&lt;/strong&gt; with Unsloth than Hugging Face + FA2 on a 24GB GPU. 27B also fits in 24GB!&lt;/p&gt; &lt;p&gt;We also saw &lt;strong&gt;infinite exploding gradients&lt;/strong&gt; when using older GPUs (Tesla T4s, RTX 2080) with float16 for Gemma 3. Newer GPUs using float16 like A100s also have the same issue - I auto fix this in Unsloth!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are also double BOS tokens which ruin finetunes for Gemma 3 - Unsloth auto corrects for this as well!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth now supports&lt;/strong&gt; &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;everything&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; This includes &lt;strong&gt;full fine-tuning&lt;/strong&gt;, pretraining, and support for all models (like &lt;strong&gt;Mixtral&lt;/strong&gt;, MoEs, Cohere etc. models) and algorithms like DoRA&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3-4B-it&amp;quot;, load_in_4bit = True, load_in_8bit = False, # [NEW!] 8bit full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Gemma 3 (27B) fits in 22GB VRAM. You can read our in depth blog post about the new changes: &lt;a href="https://unsloth.ai/blog/gemma3"&gt;unsloth.ai/blog/gemma3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tune Gemma 3 (4B) for free using our&lt;/strong&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab notebook&lt;/strong&gt;&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;li&gt;We uploaded Dynamic 4-bit quants, and it's even more effective due to Gemma 3's multi modality. See all Gemma 3 Uploads including GGUF, 4-bit etc: &lt;a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7xnidddi3poe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75c2f0fad10c4e170d1455269118d0fff4c38baf"&gt;Gemma 3 27B quantization errors&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made a &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;Guide to run Gemma 3&lt;/a&gt; properly and fixed issues with GGUFs not working with vision - reminder the correct params according to the Gemma team are &lt;strong&gt;temperature = 1.0, top_p = 0.95, top_k = 64&lt;/strong&gt;. According to the Ollama team, you should use temp = 0.1 in Ollama for now due to some backend differences. Use temp = 1.0 in llama.cpp, Unsloth, and other backends!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gemma 3 Dynamic 4-bit instruct quants:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-unsloth-bnb-4bit"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-unsloth-bnb-4bit"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let me know if you have any questions and hope you all have a lovely Friday and weekend! :) Also to update Unsloth do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab Notebook&lt;/strong&gt;&lt;/a&gt;.ipynb) with free GPU to finetune, do inference, data prep on Gemma 3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T18:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbswbb</id>
    <title>Google Gemma 3 Function Calling Example</title>
    <updated>2025-03-15T11:17:23+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbswbb/google_gemma_3_function_calling_example/"&gt; &lt;img alt="Google Gemma 3 Function Calling Example" src="https://external-preview.redd.it/JJibL4hSkxnr4q6DhVJEM2Ko2IVarP3wTtQMaq5A-OY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6991b50067a6a482105c820e8ab33f6c65d7eb48" title="Google Gemma 3 Function Calling Example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.philschmid.de/gemma-function-calling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbswbb/google_gemma_3_function_calling_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbswbb/google_gemma_3_function_calling_example/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T11:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcb180</id>
    <title>Is there an ethical/copyright reason that OpenAI/Google etc. don’t release their older models?</title>
    <updated>2025-03-16T01:58:50+00:00</updated>
    <author>
      <name>/u/gonegirlinterrupted</name>
      <uri>https://old.reddit.com/user/gonegirlinterrupted</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just to clarify, I know we can access older versions through the API, when I mean release I mean specifically their first or second model version in some sort of open source capacity. Just wondering if there is a clear reason that I’m missing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gonegirlinterrupted"&gt; /u/gonegirlinterrupted &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcb180/is_there_an_ethicalcopyright_reason_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcb180/is_there_an_ethicalcopyright_reason_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcb180/is_there_an_ethicalcopyright_reason_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T01:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbwjav</id>
    <title>A quick blog on serving Multi-LoRA Adapters</title>
    <updated>2025-03-15T14:39:29+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwjav/a_quick_blog_on_serving_multilora_adapters/"&gt; &lt;img alt="A quick blog on serving Multi-LoRA Adapters" src="https://preview.redd.it/m1uvfboq7voe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2ce6665ee4d730ba8d9180ea6dcb5e1847b3a20" title="A quick blog on serving Multi-LoRA Adapters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1uvfboq7voe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwjav/a_quick_blog_on_serving_multilora_adapters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwjav/a_quick_blog_on_serving_multilora_adapters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T14:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbrf3z</id>
    <title>This M2 Ultra v2 M3 Ultra benchmark by Matt Tech Talks is just wrong!</title>
    <updated>2025-03-15T09:30:50+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrf3z/this_m2_ultra_v2_m3_ultra_benchmark_by_matt_tech/"&gt; &lt;img alt="This M2 Ultra v2 M3 Ultra benchmark by Matt Tech Talks is just wrong!" src="https://external-preview.redd.it/_KRDmpyK0VZKT3ecW_LXf4RyMu89oGSa7dfrQMy211g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=203660775a9e061a35755c30e8f0cb5c0192515d" title="This M2 Ultra v2 M3 Ultra benchmark by Matt Tech Talks is just wrong!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the outburst, but I can't see M2 Ultra numbers so low in benchmarks any more.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvkobl58mtoe1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34930c4a39ca9669762e6331c64b63ac23fd451e"&gt;https://preview.redd.it/tvkobl58mtoe1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34930c4a39ca9669762e6331c64b63ac23fd451e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have used M2 Ultra 192GB 76 GPU cores and M3 Ultra 512GB 80 GPU cores.&lt;/p&gt; &lt;p&gt;I repeated same test, 3 times per machine and these were mine results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GGUF M2 Ultra 82.75 tok/sec (much higher than 58!)&lt;/li&gt; &lt;li&gt;GGUF M3 Ultra 88.08 tok/sec&lt;br /&gt;&lt;/li&gt; &lt;li&gt;MLX M2 Ultra 119.32 tok/sec&lt;br /&gt;&lt;/li&gt; &lt;li&gt;MLX M3 Ultra 118.74 tok/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here the YouTube video: &lt;a href="https://www.youtube.com/watch?v=2Hgk46_E_1c"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote a thread on X on this &lt;a href="https://x.com/ivanfioravanti/status/1900841506912493847"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrf3z/this_m2_ultra_v2_m3_ultra_benchmark_by_matt_tech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrf3z/this_m2_ultra_v2_m3_ultra_benchmark_by_matt_tech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrf3z/this_m2_ultra_v2_m3_ultra_benchmark_by_matt_tech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T09:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc3jow</id>
    <title>Openweb UI, LM Studio or which interface is your favorite .... and why? (Apple users)</title>
    <updated>2025-03-15T19:52:18+00:00</updated>
    <author>
      <name>/u/EmergencyLetter135</name>
      <uri>https://old.reddit.com/user/EmergencyLetter135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Ollama with Openweb UI on a Mac Studio M1 Ultra with 128 GB RAM for half a year and am basically happy with it. I use different LLM models of Huggingface mostly in the range of 24B to 32B parameters in the Q8 versions for text work. I have also set up RAGs. Now I'm going to install LM Studio on our new Mac Mini for smaller tasks and I'm curious whether the interface will inspire me even more. What experiences have you had with the different systems? What are your recommendations for Apple users?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmergencyLetter135"&gt; /u/EmergencyLetter135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3jow/openweb_ui_lm_studio_or_which_interface_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3jow/openweb_ui_lm_studio_or_which_interface_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3jow/openweb_ui_lm_studio_or_which_interface_is_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T19:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbpnht</id>
    <title>I've made a forked Sesame-CSM repo containing some QoL improvements to Sesame.</title>
    <updated>2025-03-15T07:16:04+00:00</updated>
    <author>
      <name>/u/zenforic</name>
      <uri>https://old.reddit.com/user/zenforic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This repo, called &lt;a href="https://github.com/zenforic/csm-multi"&gt;csm-multi&lt;/a&gt;, allows for generating audio multiple times without having to reload the models every time (since a fair few implementations require re-running the scripts). I did make a fair bit of edits to two different scripts to accomplish this, so big thanks to the original authors and those original sources are linked within the repo's readme. It also allows for optional definable multi-speaker generations that combine into a single audio file (with split versions being saved separately as well). Lastly, reference audio can be added (with captioning, i.e. with whisper) to lock in a speaker consistently.&lt;/p&gt; &lt;p&gt;This should work relatively easily on linux. but Sesame is a fair bit more difficult for windows. The gist is, use triton-windows 3.1 instead of 3.2 (this also means MSVC and cuda toolkit are required), python 3.10, get bitsandbytes cuda installed, optionally upgrade torch to 2.6.0 (AFTER installing requirements, as silentcipher will try to install 2.4, the 2.4 requirements aren't breaking if changed) and if using the default hugging face downloads, ensure you have repo access to both sesame's csm1b and meta's meta-llama-3.2 and login with `huggingface-cli login` and use an access token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zenforic"&gt; /u/zenforic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpnht/ive_made_a_forked_sesamecsm_repo_containing_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T07:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcd51c</id>
    <title>Split screen LLM Chat / Web App Prototyping and LLM Powered Dataset Creation</title>
    <updated>2025-03-16T03:58:33+00:00</updated>
    <author>
      <name>/u/AdElectronic8073</name>
      <uri>https://old.reddit.com/user/AdElectronic8073</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of tools I built for myself that might appeal to the Local Llama crowd.&lt;/p&gt; &lt;p&gt;Split screen LLM Chat / Web App Prototyping with the CodeMirror editor - lets you chat with the Open AI API compatible model of your choice and code at the same time. Stitches the code windows together for preview and single HTML file download of the output. &lt;a href="https://github.com/dmeldrum6/LLMPrototyping"&gt;https://github.com/dmeldrum6/LLMPrototyping&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM-Powered Dataset Creation Tool - lets you use an Open AI API compatible LLM to generate DataSets for training. &lt;a href="https://github.com/dmeldrum6/LLMDatasetBuilder"&gt;https://github.com/dmeldrum6/LLMDatasetBuilder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdElectronic8073"&gt; /u/AdElectronic8073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcd51c/split_screen_llm_chat_web_app_prototyping_and_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcd51c/split_screen_llm_chat_web_app_prototyping_and_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcd51c/split_screen_llm_chat_web_app_prototyping_and_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T03:58:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbjhu1</id>
    <title>Llama 3.3 keeping you all safe from sun theft. Thank the Lord.</title>
    <updated>2025-03-15T01:03:44+00:00</updated>
    <author>
      <name>/u/Ok-Application-2261</name>
      <uri>https://old.reddit.com/user/Ok-Application-2261</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"&gt; &lt;img alt="Llama 3.3 keeping you all safe from sun theft. Thank the Lord." src="https://preview.redd.it/j9xfffiw5roe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da69f10083402ccec3b64a47beaa338bc3599611" title="Llama 3.3 keeping you all safe from sun theft. Thank the Lord." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Application-2261"&gt; /u/Ok-Application-2261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j9xfffiw5roe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbjhu1/llama_33_keeping_you_all_safe_from_sun_theft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T01:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbywp5</id>
    <title>DeepSeek R1 Distill Qwen 7B Q4 large context (up to 128K) tests</title>
    <updated>2025-03-15T16:27:20+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbywp5/deepseek_r1_distill_qwen_7b_q4_large_context_up/"&gt; &lt;img alt="DeepSeek R1 Distill Qwen 7B Q4 large context (up to 128K) tests" src="https://b.thumbs.redditmedia.com/pdxqKuXMc_yWV28JSRz8JG-pNWkHOHkLaC_eNKmySkM.jpg" title="DeepSeek R1 Distill Qwen 7B Q4 large context (up to 128K) tests" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WE need more large context tests on local models so here is my first attempt.&lt;/p&gt; &lt;p&gt;I used M3 Ultra 512 GB + LM Studio with:&lt;br /&gt; - GGUF Flash Attention on, 128K context&lt;br /&gt; - MLX, 128K context&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0soy1rnqvoe1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6552e6b08b0fb1d89418a337b312e105708bdf1e"&gt;MLX vs llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MLX super fast in q4! &lt;/p&gt; &lt;p&gt;Detailed data here.&lt;/p&gt; &lt;p&gt;Size,tok/sec,secs to first token&lt;br /&gt; GGUF&lt;br /&gt; - 2K,83.7,1.8&lt;br /&gt; - 16K,59.6,13.8&lt;br /&gt; - 32K,44.0,35.1&lt;br /&gt; - 64K,29.4,98.9&lt;br /&gt; - 128K,17.7,310.85&lt;br /&gt; MLX&lt;br /&gt; - 2K,116.4,1.6&lt;br /&gt; - 16K,90.6,13.0&lt;br /&gt; - 32K,68.75,35.3&lt;br /&gt; - 64K,44.5,107.5&lt;br /&gt; - 128K,26.7,364.1&lt;/p&gt; &lt;p&gt;I used first 55 chapters of Pride and Prejudice from Jane Austen for this test. Up to 32K context the quality of output is good, after that becomes worst and worst.&lt;/p&gt; &lt;p&gt;Which model should I try now? A reasoning one was not the best choice honestly, but I had it locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbywp5/deepseek_r1_distill_qwen_7b_q4_large_context_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbywp5/deepseek_r1_distill_qwen_7b_q4_large_context_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbywp5/deepseek_r1_distill_qwen_7b_q4_large_context_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T16:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jca6xz</id>
    <title>How can I save my progress between different server instances?</title>
    <updated>2025-03-16T01:14:07+00:00</updated>
    <author>
      <name>/u/MassivePalpitation29</name>
      <uri>https://old.reddit.com/user/MassivePalpitation29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with LLaMa 3.3 70b by renting vast.ai servers. Because I'm on a budget, I destroy the instance after using it so that I don't have pay for storage there. Is there a way I can save what I do each session and some how upload it to new instances? Or is there a better way for me to go about using heavy duty LLMs on a budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MassivePalpitation29"&gt; /u/MassivePalpitation29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jca6xz/how_can_i_save_my_progress_between_different/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jca6xz/how_can_i_save_my_progress_between_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jca6xz/how_can_i_save_my_progress_between_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T01:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbyolh</id>
    <title>Diffusion Language Models in 2 minutes</title>
    <updated>2025-03-15T16:17:22+00:00</updated>
    <author>
      <name>/u/CasulaScience</name>
      <uri>https://old.reddit.com/user/CasulaScience</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"&gt; &lt;img alt="Diffusion Language Models in 2 minutes" src="https://external-preview.redd.it/l0zCB3Hsl6Zyg_xqk4dL1W5McbBOHzb917AfOF8NKWc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8423b4d3358e31cc2c420bbdb7ab3ad555fb7a3" title="Diffusion Language Models in 2 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CasulaScience"&gt; /u/CasulaScience &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/_6jekTwBxow?si=yvEamKu9ommm8v7T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T16:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbrwqf</id>
    <title>Deep Research Tools: Am I the only one feeling...underwhelmed? (OpenAI, Google, Open Source)</title>
    <updated>2025-03-15T10:07:54+00:00</updated>
    <author>
      <name>/u/mimirium_</name>
      <uri>https://old.reddit.com/user/mimirium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been diving headfirst into these &amp;quot;Deep Research&amp;quot; AI tools lately - OpenAI's thing, Google's Gemini version, Perplexity, even some of the open-source ones on GitHub. You know, the ones that promise to do all the heavy lifting of in-depth research for you. I was so hyped!&lt;/p&gt; &lt;p&gt;I mean, the idea is amazing, right? Finally having an AI assistant that can handle literature reviews, synthesize data, and write full reports? Sign me up! But after using them for a while, I keep feeling like something's missing.&lt;/p&gt; &lt;p&gt;Like, the biggest issue for me is accuracy. I’ve had to fact-check so many things, and way too often it's just plain wrong. Or even worse, it makes up sources that don't exist! It's also pretty surface-level. It can pull information, sure, but it often misses the whole context. It's rare I find truly new insights from it. Also, it just grabs stuff from the web without checking if a source is a blog or a peer reviewed journal. And once it starts down a wrong path, its so hard to correct the tool.&lt;/p&gt; &lt;p&gt;And don’t even get me started on the limitations with data access - I get it, it's early days. But being able to pull private information would be so useful!&lt;/p&gt; &lt;p&gt;I can see the potential here, I really do. Uploading files, asking tough questions, getting a structured report… It’s a big step, but I was kinda hoping for a breakthrough in saving time. I am just left slightly unsatisfied and wishing for something a little bit better.&lt;/p&gt; &lt;p&gt;So, am I alone here? What have your experiences been like? Has anyone actually found one of these tools that nails it, or are we all just beta-testing expensive (and sometimes inaccurate) search engines?&lt;/p&gt; &lt;p&gt;TL;DR: These &amp;quot;Deep Research&amp;quot; AI tools are cool, but they still have accuracy issues, lack context, and need more data access. Feeling a bit underwhelmed tbh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mimirium_"&gt; /u/mimirium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T10:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbufek</id>
    <title>Local LLM on cheap machine, a one page summary</title>
    <updated>2025-03-15T12:51:41+00:00</updated>
    <author>
      <name>/u/gitcommitshow</name>
      <uri>https://old.reddit.com/user/gitcommitshow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"&gt; &lt;img alt="Local LLM on cheap machine, a one page summary" src="https://preview.redd.it/ft673vxiouoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc8946332f8bbd79475fa1a4c79d32c90025ac4f" title="Local LLM on cheap machine, a one page summary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gitcommitshow"&gt; /u/gitcommitshow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ft673vxiouoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T12:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc5iqi</id>
    <title>AI Scientists By Sakana AI passed ICLR review bar!!!</title>
    <updated>2025-03-15T21:21:33+00:00</updated>
    <author>
      <name>/u/mansurul11</name>
      <uri>https://old.reddit.com/user/mansurul11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An amazing experiment was conducted by Sakana.ai. They collaborated with ICLR workshop organizers to submit three original research papers, all originated and written entirely by this AI scientist. The review process was double-blind, but reviewers were informed that three out of the 43 submitted papers were original research from an AI scientist. 🤯&lt;/p&gt; &lt;p&gt;TLDR from the blog post: The AI Scientist-v2, after being given a broad topic to conduct research on, generated a paper titled “Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization”. This paper reported a negative result that The AI Scientist encountered while trying to innovate on novel regularization methods for training neural networks that can improve their compositional generalization. This manuscript received an average reviewer score of 6.33 at the ICLR workshop, placing it above the average acceptance threshold.&lt;/p&gt; &lt;p&gt;&lt;a href="https://sakana.ai/ai-scientist-first-publication/"&gt;https://sakana.ai/ai-scientist-first-publication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mansurul11"&gt; /u/mansurul11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T21:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbn4a4</id>
    <title>DeepSeek's owner asked R&amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?</title>
    <updated>2025-03-15T04:24:47+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt; &lt;img alt="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" src="https://external-preview.redd.it/4hsJXx_AcJxGPq4lJQrABcvHj2M_s3N2kv5MhyZHOZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=337e519de49d47a13546c9c6c2c323282203aee5" title="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/amir/status/1900583042659541477"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T04:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcchtq</id>
    <title>Baidu releases X1, a (closed?) model that matches R1 and ERNIE 4.5, that matches GPT 4.5</title>
    <updated>2025-03-16T03:20:25+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Baidu_Inc/status/1901094083508220035"&gt;https://x.com/Baidu_Inc/status/1901094083508220035&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T03:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc1nw1</id>
    <title>Actual Electricity Consumption and Cost to Run Local LLMs. From Gemma3 to QwQ.</title>
    <updated>2025-03-15T18:28:09+00:00</updated>
    <author>
      <name>/u/QuantuisBenignus</name>
      <uri>https://old.reddit.com/user/QuantuisBenignus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tokens/WattHour and Tokens/US cent calculated for 17 local LLMs, including the new Gemma3 models. Wall plug power measured for each run under similar conditions and prompt.&lt;/p&gt; &lt;p&gt;Table, graph and formulas for estimate here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QuantiusBenignus/Zshelf/discussions/2"&gt;https://github.com/QuantiusBenignus/Zshelf/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Average, consumer-grade hardware and local LLMs quantized to Q5 on average.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuantuisBenignus"&gt; /u/QuantuisBenignus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T18:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbpesk</id>
    <title>Block Diffusion</title>
    <updated>2025-03-15T06:58:36+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt; &lt;img alt="Block Diffusion" src="https://external-preview.redd.it/ajRoczdkcGd4c29lMVSExINF4ZLZPdurGgKB_nt4a17rDQ79hHF6tCkUtZhB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0a9c1a1e32d732770b3bfd59c242574fc96a2e3" title="Block Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ze05rmrgxsoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T06:58:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc3fkd</id>
    <title>I hope uncensored gemma3b come soon enough... the model is unbearable boring as it is know.</title>
    <updated>2025-03-15T19:47:15+00:00</updated>
    <author>
      <name>/u/pumukidelfuturo</name>
      <uri>https://old.reddit.com/user/pumukidelfuturo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly had more fun with Darkest Muse or even the Gemma2 9b simpo version (which is my fav model).&lt;/p&gt; &lt;p&gt;I'm not even talking about NSFW stuff, i'm just chatting with it and its visions about everything are just lame, safe, boring and such... the lack of personality it just bores me too much. It's lame vanilla corpo mumbo jumbo style all over the place. If i wanted that i'd use Llama 3 instead.&lt;/p&gt; &lt;p&gt;I hope trainers can fix this and make this fun somewhat. It's gonna be a hard job. I'm just experiencing brainrot of how dull it is. It's dumb as a rock.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pumukidelfuturo"&gt; /u/pumukidelfuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T19:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbyg29</id>
    <title>GPT-Sovits V3 TTS (407M) Release - 0-Shot Voice Cloning , Multi Language</title>
    <updated>2025-03-15T16:06:43+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/releases/tag/20250228v3"&gt;https://github.com/RVC-Boss/GPT-SoVITS/releases/tag/20250228v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Version 3 of GPT Sovits released two weeks ago and I havent really seen any discussion about it outside of China. &lt;/p&gt; &lt;p&gt;The new version increased the parameter count from 167m to 407m, also the voice cloning capability has improved a lot over the previous versions. Both 0 shot (uses a single audio sample shorter then 10 seconds) and trained voices are now a lot closer to the original and it is capable of staying in the emotion of the sample more consistently. &lt;/p&gt; &lt;p&gt;GPT Sovits supports English, Chinese, Japanese, Korean and Cantonese. From my personal testing it currently is the best option for 0 shot voice cloning in Japanese.&lt;/p&gt; &lt;p&gt;Here is a link to the machine translated changelog: &lt;a href="https://github-com.translate.goog/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7"&gt;https://github-com.translate.goog/RVC-Boss/GPT-SoVITS/wiki/GPT‐SoVITS‐v3‐features-(新特性)?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=ja&amp;amp;_x_tr_pto=wapp&lt;/a&gt;?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=ja&amp;amp;_x_tr_pto=wapp)&lt;/p&gt; &lt;p&gt;Note: the audio examples on their Github page are still from V2 not V3. Also once you start the Gradio interface you need to select v3 from the dropdown menu as it defaults to v2 still.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T16:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc9meu</id>
    <title>Who's still running ancient models?</title>
    <updated>2025-03-16T00:44:12+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to take a pause from my experiments today, gemma3, mistralsmall, phi4, qwq, qwen, etc and marvel at how good they are for their size. A year ago most of us thought that we needed 70B to kick ass. 14-32B is punching super hard. I'm deleting my Q2/Q3 llama405B, and deepseek dyanmic quants.&lt;/p&gt; &lt;p&gt;I'm going to re-download guanaco, dolphin-llama2, vicuna, wizardLM, nous-hermes-llama2, etc&lt;br /&gt; For old times sake. It's amazing how far we have come and how fast. Some of these are not even 2 years old! Just a year plus! I'm going to keep some ancient model and run them so I can remember and don't forget and to also have more appreciation for what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T00:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbwk65</id>
    <title>Made a ManusAI alternative that run locally</title>
    <updated>2025-03-15T14:40:35+00:00</updated>
    <author>
      <name>/u/fawendeshuo</name>
      <uri>https://old.reddit.com/user/fawendeshuo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I have been working with a friend on a fully local Manus that can run on your computer, it started as a fun side project but it's slowly turning into something useful.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/Fosowl/agenticSeek"&gt;https://github.com/Fosowl/agenticSeek&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We already have a lot of features :: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Web agent:&lt;/strong&gt; Autonomous web search and web browsing with selenium&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code agent:&lt;/strong&gt; Semi-autonomous coding ability, automatic trial and retry&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File agent:&lt;/strong&gt; Bash execution and file system interaction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Routing system:&lt;/strong&gt; The best agent is selected given the user prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt; : save and load previous conversation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API tool:&lt;/strong&gt; We will integrate many API tool, for now we only have webi and flight search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory system&lt;/strong&gt; : Individual agent memory and compression. Quite experimental but we use a summarization model to compress the memory over time. it is disabled by default for now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text to speech &amp;amp; Speech to text&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Coming features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tasks planning&lt;/strong&gt; (development started) : Breaks down tasks and spins up the right agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User Preferences Memory&lt;/strong&gt; (in development)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR System&lt;/strong&gt; – Enables the agent to see what you are seing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Agent&lt;/strong&gt; – Chat with personal documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How does it differ from openManus ?&lt;/p&gt; &lt;p&gt;We want to run everything locally and avoid the use of fancy frameworks, build as much from scratch as possible.&lt;/p&gt; &lt;p&gt;We still have a long way to go and probably will never match openManus in term of capabilities but it is more accessible, it show how easy it is to created a hyped product like ManusAI.&lt;/p&gt; &lt;p&gt;We are a very small team of 2 from France and Taiwan. We are seeking feedback, love and and contributors!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fawendeshuo"&gt; /u/fawendeshuo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T14:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcbt5l</id>
    <title>These guys never rest!</title>
    <updated>2025-03-16T02:41:35+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt; &lt;img alt="These guys never rest!" src="https://preview.redd.it/4hmgoyhlsyoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455f74ad35ad822af5cb2fe29f909a6835248ce7" title="These guys never rest!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hmgoyhlsyoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T02:41:35+00:00</published>
  </entry>
</feed>
