<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-07T08:25:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lto2in</id>
    <title>best llm for 32 gb ram and 8gb vram</title>
    <updated>2025-07-07T07:38:23+00:00</updated>
    <author>
      <name>/u/diddy_stroker</name>
      <uri>https://old.reddit.com/user/diddy_stroker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;as the title suggests i would like to run the best llm possible for my system and i am really new to llms so i really have no idea where to start help is really appreciated . ( my system has 32gb ddr5 6000mhz cl 36 ram and a amd rx 7600 with 8gb vram )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diddy_stroker"&gt; /u/diddy_stroker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T07:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltkdjz</id>
    <title>LogiQ CLI Beta | Full LMStudio Support.</title>
    <updated>2025-07-07T03:52:39+00:00</updated>
    <author>
      <name>/u/x8ko_dev</name>
      <uri>https://old.reddit.com/user/x8ko_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we're working on a full CLI that currently beats a fair amount of other projects in benchmarks when running SOTA models, We're looking to get more testers running local models to see how it performs versus other projects that have local support.&lt;/p&gt; &lt;p&gt;We'd love for you to join us and help out in real world testing, we're willing to make almost any changes and are constantly working to improve, Our goal is to be the best and we will get to that point.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/fA4upHvMsK"&gt;https://discord.gg/fA4upHvMsK&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/xyOz-dev/LogiQCLI"&gt;&lt;strong&gt;https://github.com/xyOz-dev/LogiQCLI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x8ko_dev"&gt; /u/x8ko_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltkdjz/logiq_cli_beta_full_lmstudio_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltkdjz/logiq_cli_beta_full_lmstudio_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltkdjz/logiq_cli_beta_full_lmstudio_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T03:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lto3t9</id>
    <title>PGDS approach to full model inference on consumer grade GPUs</title>
    <updated>2025-07-07T07:40:48+00:00</updated>
    <author>
      <name>/u/finnabrahamson</name>
      <uri>https://old.reddit.com/user/finnabrahamson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;p&gt;We propose a novel inference-time optimization method for resource-constrained deployment of large language models (LLMs), enabling high-quality output from models too large to fit into a single consumer-grade GPU. This technique—Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR)—leverages coarse inference from a quantized or low-parameter proxy model to define a hyperspherical subregion of the solution space. This subregion is then mapped onto the full-precision model, from which only the minimal required model weights are dynamically loaded and computed.&lt;/p&gt; &lt;p&gt;To reduce redundant early-layer computation and maximize efficiency, PG-DSIR determines a static insertion point within the full model’s architecture where the low-precision representation aligns most closely with a hidden state of the larger model. This alignment is determined through cosine similarity across the hidden states of the full model, enabling direct embedding injection midstream. A lightweight corrective module (e.g., a LoRA or learned delta predictor) bridges the precision gap between the coarse embedding and the ground truth embedding, improving alignment and preserving output quality.&lt;/p&gt; &lt;p&gt;Our technique draws conceptual inspiration from hybrid latent-space workflows in image generation (e.g., LCM + SD schedulers) and departs from traditional quantization, distillation, or Mixture-of-Experts (MoE) routing by enabling continuous, per-inference specialization of the model footprint. The resulting system provides a principled and geometry-driven pathway to real-time inference using ultra-large models on modest hardware, dramatically reducing both VRAM usage and computational overhead.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Introduction: Toward Focused Large-Model Inference via Geometric Slicing and Targeted Insertion&lt;/p&gt; &lt;p&gt;Large Language Models have achieved unprecedented capability at the cost of significant resource demands. Models such as Meta’s LLaMA 3 70B, for example, require over 140 GB of VRAM to run in full precision—placing them well beyond reach for consumer or even prosumer-grade hardware. Traditional strategies to reduce inference cost include quantization, distillation, parameter pruning, and expert routing. These techniques, while effective, trade off flexibility, output fidelity, or require extensive fine-tuning.&lt;/p&gt; &lt;p&gt;We introduce Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR) as an alternative strategy, rooted in a geometric understanding of latent representations. Our method operates under the key insight that inference can be reconceived not as a global pass through the entire parameter space of a model, but as a locally focused traversal through a high-dimensional latent graph—constrained by an informed estimate of the solution space.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Conceptual Overview&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The method begins with a prompt passed to a smaller, quantized proxy model (e.g., 7B), which produces a low-precision embedding in a shared latent space. Although this representation lacks the full nuance of a high-precision model, it defines a directional “search vector” within the solution space. By interpreting this vector as a hypersphere in the full model’s higher-precision latent space, we delimit the relevant solution subregion for the current prompt.&lt;/p&gt; &lt;p&gt;Rather than processing the entire 70B model, we instead extract and load only the parameter subset required to refine that coarse embedding within its solution subregion. This step parallels the logic of MoE routing but bypasses the need for discrete experts or static routing logic, instead constructing a dynamically sliced micro-expert composed only of what the full model knows about the specific problem space defined by the prompt.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Insertion Point Resolution&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To avoid recomputation of early transformer blocks—often the most computationally expensive—we perform embedding handoff into the full model at an internal layer corresponding to the hidden state most similar to the coarse embedding. This “insertion point” is found by analyzing cosine similarity between the proxy embedding and the hidden states of the full model when run on the same prompt. Importantly, this mapping is prompt-agnostic and only needs to be computed once per proxy/full model pair.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Precision Bridging and Correction&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even with accurate slicing and entry point resolution, discrepancies will remain between the proxy embedding and the full model’s expected hidden state. We address this through a corrective module—either a learned LoRA, linear mapping, or shallow neural delta predictor—trained on embedding pairs generated via dual model evaluation on a large prompt corpus. This allows us to cleanly bridge the two latent spaces with negligible overhead.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Implications&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The proposed PG-DSIR pipeline enables inference from large-scale models like LLaMA 3 70B or Mixtral on consumer GPUs (e.g., RTX 3060–4090), significantly reducing required VRAM and compute without necessitating global model transformation. Moreover, this method maintains the full model’s capabilities and expressiveness, differing from quantization approaches that often suffer from irrecoverable degradation.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;By treating inference as a navigational process through high-dimensional geometry, PG-DSIR transforms the challenge of large model execution into a targeted optimization problem. We believe this technique can unlock a new era of ultra-large model accessibility, enabling research, development, and deployment of frontier models on commodity hardware—without compromising capability.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finnabrahamson"&gt; /u/finnabrahamson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T07:40:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt18hg</id>
    <title>Are Qwen3 Embedding GGUF faulty?</title>
    <updated>2025-07-06T13:27:20+00:00</updated>
    <author>
      <name>/u/espadrine</name>
      <uri>https://old.reddit.com/user/espadrine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Embedding has great retrieval results on &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, I tried it in &lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF"&gt;llama.cpp&lt;/a&gt;. The results were much worse than competitors. I have an FAQ benchmark that looks a bit like this:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3 8B&lt;/td&gt; &lt;td&gt;18.70%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral&lt;/td&gt; &lt;td&gt;53.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI (text-embedding-3-large)&lt;/td&gt; &lt;td&gt;55.87%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google (text-embedding-004)&lt;/td&gt; &lt;td&gt;57.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cohere (embed-v4.0)&lt;/td&gt; &lt;td&gt;58.50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Voyage AI&lt;/td&gt; &lt;td&gt;60.54%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Qwen3 is the only one that I am not using an API for, but I would assume that the F16 GGUF shouldn't have that big of an impact on performance compared to the raw model, say using TEI or vLLM.&lt;/p&gt; &lt;p&gt;Does anybody have a similar experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/espadrine"&gt; /u/espadrine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T13:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltcwbx</id>
    <title>Local LLM for business</title>
    <updated>2025-07-06T21:41:22+00:00</updated>
    <author>
      <name>/u/Acceptable_Factor817</name>
      <uri>https://old.reddit.com/user/Acceptable_Factor817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I own a mid size electrical contracting bussiness, about 35 employees. I'm thinking of implementing a local ai server maybe mixtral 8x7B to increase the efficiency of the business. My main reason is for book keeping/receipt processing, finance etc as of now but I'm hoping to train on other areas. any other ideas on how this could help my business. Is it worth implementing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Factor817"&gt; /u/Acceptable_Factor817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltcwbx/local_llm_for_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltcwbx/local_llm_for_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltcwbx/local_llm_for_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T21:41:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltonwy</id>
    <title>Should I remove these?</title>
    <updated>2025-07-07T08:18:23+00:00</updated>
    <author>
      <name>/u/Opening_Cash_4532</name>
      <uri>https://old.reddit.com/user/Opening_Cash_4532</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I am QLORA finetuning a Llama instruct model but when I am creating the dataset via its chat template applied, it prints &amp;quot;Cutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024&amp;quot; into my data at everyline in the json file. Should I be removing/cleaning them? Do they harm the attention mechanism by making the model to focus on these or do they create noise? &lt;/p&gt; &lt;p&gt;Also when I look at the chat template via this code,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&amp;quot;meta-llama/Llama-3.1-8B-Instruct&amp;quot;) print(tokenizer.chat_template) &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;It prints out these:&lt;br /&gt; {{- bos_token }}&lt;/p&gt; &lt;p&gt;{%- if custom_tools is defined %}&lt;/p&gt; &lt;p&gt;{%- set tools = custom_tools %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- if not tools_in_user_message is defined %}&lt;/p&gt; &lt;p&gt;{%- set tools_in_user_message = true %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- if not date_string is defined %}&lt;/p&gt; &lt;p&gt;{%- set date_string = &amp;quot;26 Jul 2024&amp;quot; %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- if not tools is defined %}&lt;/p&gt; &lt;p&gt;{%- set tools = none %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{#- This block extracts the system message, so we can slot it into the right place. #}&lt;/p&gt; &lt;p&gt;{%- if messages[0]['role'] == 'system' %}&lt;/p&gt; &lt;p&gt;{%- set system_message = messages[0]['content']|trim %}&lt;/p&gt; &lt;p&gt;{%- set messages = messages[1:] %}&lt;/p&gt; &lt;p&gt;{%- else %}&lt;/p&gt; &lt;p&gt;{%- set system_message = &amp;quot;&amp;quot; %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{#- System message + builtin tools #}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- if builtin_tools is defined or tools is not none %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Environment: ipython\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- if builtin_tools is defined %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Tools: &amp;quot; + builtin_tools | reject('equalto', 'code_interpreter') | join(&amp;quot;, &amp;quot;) + &amp;quot;\n\n&amp;quot;}}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Cutting Knowledge Date: December 2023\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Today Date: &amp;quot; + date_string + &amp;quot;\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- if tools is not none and not tools_in_user_message %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;You have access to the following functions. To call a function, please respond with JSON for a function call.&amp;quot; }}&lt;/p&gt; &lt;p&gt;{{- 'Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.' }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Do not use variables.\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- for t in tools %}&lt;/p&gt; &lt;p&gt;{{- t | tojson(indent=4) }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endfor %}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{{- system_message }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt; &lt;p&gt;{#- Custom tools are passed in a user message with some extra guidance #}&lt;/p&gt; &lt;p&gt;{%- if tools_in_user_message and not tools is none %}&lt;/p&gt; &lt;p&gt;{#- Extract the first user message so we can plug it in here #}&lt;/p&gt; &lt;p&gt;{%- if messages | length != 0 %}&lt;/p&gt; &lt;p&gt;{%- set first_user_message = messages[0]['content']|trim %}&lt;/p&gt; &lt;p&gt;{%- set messages = messages[1:] %}&lt;/p&gt; &lt;p&gt;{%- else %}&lt;/p&gt; &lt;p&gt;{{- raise_exception(&amp;quot;Cannot put tools in the first user message when there's no first user message!&amp;quot;) }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{{- '&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;\n\n' -}}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Given the following functions, please respond with a JSON for a function call &amp;quot; }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;with its proper arguments that best answers the given prompt.\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{{- 'Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.' }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;Do not use variables.\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- for t in tools %}&lt;/p&gt; &lt;p&gt;{{- t | tojson(indent=4) }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endfor %}&lt;/p&gt; &lt;p&gt;{{- first_user_message + &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;}}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- for message in messages %}&lt;/p&gt; &lt;p&gt;{%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}&lt;/p&gt; &lt;p&gt;{{- '&amp;lt;|start_header_id|&amp;gt;' + message['role'] + '&amp;lt;|end_header_id|&amp;gt;\n\n'+ message['content'] | trim + '&amp;lt;|eot_id|&amp;gt;' }}&lt;/p&gt; &lt;p&gt;{%- elif 'tool_calls' in message %}&lt;/p&gt; &lt;p&gt;{%- if not message.tool_calls|length == 1 %}&lt;/p&gt; &lt;p&gt;{{- raise_exception(&amp;quot;This model only supports single tool-calls at once!&amp;quot;) }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- set tool_call = message.tool_calls[0].function %}&lt;/p&gt; &lt;p&gt;{%- if builtin_tools is defined and tool_call.name in builtin_tools %}&lt;/p&gt; &lt;p&gt;{{- '&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\n\n' -}}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|python_tag|&amp;gt;&amp;quot; + tool_call.name + &amp;quot;.call(&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- for arg_name, arg_val in tool_call.arguments | items %}&lt;/p&gt; &lt;p&gt;{{- arg_name + '=&amp;quot;' + arg_val + '&amp;quot;' }}&lt;/p&gt; &lt;p&gt;{%- if not loop.last %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;, &amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- endfor %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;)&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- else %}&lt;/p&gt; &lt;p&gt;{{- '&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\n\n' -}}&lt;/p&gt; &lt;p&gt;{{- '{&amp;quot;name&amp;quot;: &amp;quot;' + tool_call.name + '&amp;quot;, ' }}&lt;/p&gt; &lt;p&gt;{{- '&amp;quot;parameters&amp;quot;: ' }}&lt;/p&gt; &lt;p&gt;{{- tool_call.arguments | tojson }}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;}&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- if builtin_tools is defined %}&lt;/p&gt; &lt;p&gt;{#- This means we're in ipython mode #}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|eom_id|&amp;gt;&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- else %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- elif message.role == &amp;quot;tool&amp;quot; or message.role == &amp;quot;ipython&amp;quot; %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;ipython&amp;lt;|end_header_id|&amp;gt;\n\n&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- if message.content is mapping or message.content is iterable %}&lt;/p&gt; &lt;p&gt;{{- message.content | tojson }}&lt;/p&gt; &lt;p&gt;{%- else %}&lt;/p&gt; &lt;p&gt;{{- message.content }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;p&gt;{%- endfor %}&lt;/p&gt; &lt;p&gt;{%- if add_generation_prompt %}&lt;/p&gt; &lt;p&gt;{{- '&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\n\n' }}&lt;/p&gt; &lt;p&gt;{%- endif %}&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Opening_Cash_4532"&gt; /u/Opening_Cash_4532 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltonwy/should_i_remove_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltonwy/should_i_remove_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltonwy/should_i_remove_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T08:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsxxo2</id>
    <title>Python Implementation of Google's MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</title>
    <updated>2025-07-06T10:20:49+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sigridjineth/muvera-py"&gt;https://github.com/sigridjineth/muvera-py&lt;/a&gt;&lt;br /&gt; I have created the Python implementation to make the FDE algorithm more accessible while maintaining complete fidelity to the original C++ implementation. Every function and parameter has been carefully mapped to ensure identical behavior.&lt;/p&gt; &lt;h1&gt;What is FDE (Read below)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fixed-Dimensional Encoding (FDE) solves a fundamental problem in modern search systems: how to efficiently search through billions of documents when each document is represented by hundreds of vectors (as in ColBERT-style models).&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Traditional search&lt;/strong&gt;: Document = 1 vector → Fast but inaccurate&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modern multi-vector search&lt;/strong&gt;: Document = 100s of vectors → Accurate but extremely slow&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The FDE Solution&lt;/h1&gt; &lt;p&gt;FDE transforms multiple vectors into a single fixed-size vector while preserving the similarity relationships. The magic is that the dot product between two FDE vectors approximates the original Chamfer similarity between the multi-vector sets.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsxxo2/python_implementation_of_googles_muvera/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T10:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lte7m8</id>
    <title>use Blender MCP with a ready made asset pack</title>
    <updated>2025-07-06T22:39:33+00:00</updated>
    <author>
      <name>/u/fiddler64</name>
      <uri>https://old.reddit.com/user/fiddler64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried out the Blender MCP Tutorial &lt;a href="https://www.youtube.com/watch?v=lCyQ717DuzQ"&gt;https://www.youtube.com/watch?v=lCyQ717DuzQ&lt;/a&gt; and it was really underwhelming, all the objects and materials are as basic as it gets. I guess that's the limit of using python to create mesh within blender. &lt;/p&gt; &lt;p&gt;So my question is - is there some sort of mcp server to an asset pack (on fab.com, blender market, or local) that I can use to tell llm to get stuff from to put into blender rather than creating its own mesh. On that note, can an mcp server have pics instead of text as description for the functions for the llm to invoke?&lt;/p&gt; &lt;p&gt;Sorry if this is the wrong place to ask, and my english as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fiddler64"&gt; /u/fiddler64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lte7m8/use_blender_mcp_with_a_ready_made_asset_pack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lte7m8/use_blender_mcp_with_a_ready_made_asset_pack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lte7m8/use_blender_mcp_with_a_ready_made_asset_pack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T22:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lspzn3</id>
    <title>128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.</title>
    <updated>2025-07-06T01:59:10+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt; &lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt; &lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt; &lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt; &lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt; &lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;3014.18 ± 1.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 0.6B Q8_0&lt;/td&gt; &lt;td align="left"&gt;604.15 MiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;191.63 ± 0.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1289.11 ± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;3.56 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;91.46 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;357.71 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.09 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;249.45 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.24 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;300.02 ± 0.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 32B Q4_0&lt;/td&gt; &lt;td align="left"&gt;17.42 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.39 ± 0.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;48.92 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;50.70 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.05 ± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;56.33 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt; &lt;td align="left"&gt;42.55 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;16.00 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;1023.81 ± 3.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;63.87 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;238.17 ± 0.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.21 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;25.17 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;202.50 ± 0.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt; &lt;td align="left"&gt;137.11 GiB&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.17 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt; &lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt; &lt;p&gt;Now, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output token throughput (tok/s) (256)&lt;/th&gt; &lt;th align="left"&gt;Prompt processing t/s (4096)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.68&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt; &lt;td align="left"&gt;19.76&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;25.96&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;27.26&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;32.3&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;38.55&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt; &lt;td align="left"&gt;36.96&lt;/td&gt; &lt;td align="left"&gt;350&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt; &lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T01:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt79jg</id>
    <title>Nvidia RTX 5060 Ti 16GB for local LLM inference with Olllama + Open WebUI</title>
    <updated>2025-07-06T17:46:10+00:00</updated>
    <author>
      <name>/u/Philhippos</name>
      <uri>https://old.reddit.com/user/Philhippos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! Like many here, I am super excited to locally run open source LLMs using Open WebUI, LMStudio etc., and figured that a RTX 5060 Ti would be a good budget starting point. So I got it with a cheap gaming PC a few days ago. Its whole purpose for me at the moment is to learn how to configure everything (using Ollama, pipelines, Google Search integration, integrating vector databases, LightRAG, LangGraph etc.), and later I think I could set up some knowledge bases to support me at some repetitive tasks.&lt;/p&gt; &lt;p&gt;Below you can find some performance metrics of the models I ran so far.&lt;/p&gt; &lt;p&gt;At work I plan to set up a similar configuration but as a server with an RTX 6000 Pro with 96 GB VRAM, so several users can use 32B Models in parallel.&lt;/p&gt; &lt;p&gt;For my private starter setup, I tried to stay below 1000€, so I got the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Graphics card: VGP NVIDIA RTX 5060 Ti 16GB Inno3D Twin X2&lt;/li&gt; &lt;li&gt;CPU: Ryzen 7 5700X / 8 x 3.40 GHz / Turbo 4.60 - AM4 Socket Vermeer &lt;/li&gt; &lt;li&gt;Motherboard: SoAM4 Gigabyte B550M DS3H AC Wifi mATX (PCI Express 4.0 x16)&lt;/li&gt; &lt;li&gt;Memory: 16GB G.Skill Aegis DDR4 RAM at 3200 MHz&lt;/li&gt; &lt;li&gt;SSD: 1TB M.2 SSD PCI-E NVMe NV3 Bulk (Read 6000 MBs, Write 4000 MBs)&lt;/li&gt; &lt;li&gt;Power supply: SQ-WHITE 700 Watt super silent power supply – 80+&lt;/li&gt; &lt;li&gt;Win 11 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As LLM engine, I use Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference Speeds tested with Open WebUI:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gemma3:12b: 37.1 token/s&lt;/li&gt; &lt;li&gt;deepseek-r1:14b: 36 token/s&lt;/li&gt; &lt;li&gt;qwen3:14b: 39.3 token/s&lt;/li&gt; &lt;li&gt;mistral-small3.2:24b: 11.6 token/s --&amp;gt; but here partial CPU offloading seems to take place&lt;/li&gt; &lt;li&gt;gemma3n:e4b: 29.11 token/s&lt;/li&gt; &lt;li&gt;qwen3:4b: 104.6 token/s&lt;/li&gt; &lt;li&gt;gemma3:4b: 96.1 token/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of the models were in Q4_K_M and. gguf format. The prompt I used to test was &amp;quot;Hello&amp;quot;. If I should try some more models, just let me know.&lt;/p&gt; &lt;p&gt;I think what's especially interesting is that mistral-small3.2:24b automatically gets partially offloaded to the CPU, but the speed remains okay-ish. Calling &amp;quot;ollama ps&amp;quot; tells me that the size would be 26 GB, with 45%/55% CPU/GPU offloading. I am a bit confused, since on the &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt; model page for &lt;a href="https://ollama.com/library/mistral-small3.2"&gt;mistral-small3.2&lt;/a&gt; a size of only 15GB is stated.&lt;/p&gt; &lt;p&gt;I also tried a 3bit quantized version of Qwen3:32B, but its output was very bad.&lt;/p&gt; &lt;p&gt;Next year I am thinking about getting a used RTX 3090 with 24 GB of VRAM or a 5090 with 32 GB of VRAM (I hope the 700W powersupply would support that), in case I figure that larger models offer a significant improvement in quality. I also realized that the case I got is too small for many versions of these cards, so an upgrade might become a bit tricky. Unfortunately I cannot run popular models like Gemma 3 27B or Qwen 3 32B at the moment on the RTX 5060 Ti with 16GB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My conclusion on the RTX 5060 Ti 16GB for running LLMs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So for the price I paid I am happy with the setup. I like especially that the power consumption in idle for the whole system is only around 65 Watts, and under load stays below 270 Watts. I use Ngrok to make my Open WebUI interface available to me wherever I am, so as the PC is always running at home, I really appreciate the low idle power consumption. However, for anyone already having a capable PC at home, I think getting a used RTX 3090 with 24 GB VRAM and more CUDA cores would be a better investment than the RTX 5060 Ti - as long as the RTX 3090 fits into the case.&lt;/p&gt; &lt;p&gt;I also already plan some upgrades, like increasing to 32GB (or 64 GB) of RAM. I recognized that several times I tried to load Mistral-Small3.2, Open WebUI threw an error. I assume that was because due to other system processes my PC ran out of RAM when trying to load.&lt;/p&gt; &lt;p&gt;At the moment, I also struggle a bit with effectively setting the context sizes for the LLMs, both in Open WebUI and directly with the &amp;quot;model create&amp;quot; and &amp;quot;PARAMETER num_ctx&amp;quot; in Ollama. A saw plenty other people struggling with that on reddit etc, and indeed the behavior there seems pretty strange to me: even if I try to set huge context sizes, after calling the model, &amp;quot;ollama ps&amp;quot; only shows that the size of the loaded model barely (if at all) increased. When using the models with the apparently increased context sizes, it neither feels like anything changed. So if anyone has a solution that really adjusts the context size for the models to use in Open WebUI, I would be happy to read it.&lt;/p&gt; &lt;p&gt;I hope this helps some people out there, and let me know if you have some suggestions for some further performance improvements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Philhippos"&gt; /u/Philhippos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt79jg/nvidia_rtx_5060_ti_16gb_for_local_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt79jg/nvidia_rtx_5060_ti_16gb_for_local_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt79jg/nvidia_rtx_5060_ti_16gb_for_local_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T17:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt8j4u</id>
    <title>Llamacpp | Samsung s24+ | Snapdragon 8 Gen 3 + Adreno 750 | Real world testing with Qwen3-4B</title>
    <updated>2025-07-06T18:37:59+00:00</updated>
    <author>
      <name>/u/73tada</name>
      <uri>https://old.reddit.com/user/73tada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Performance Summary based on real-world testing:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q4_0 Model:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU-only: 8.30 tokens/second (recommended)&lt;/li&gt; &lt;li&gt;GPU (25 layers): 8.81 tokens/second (competitive)&lt;/li&gt; &lt;li&gt;GPU excels at prompt processing (57.86 vs 41.60 tok/s)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q5_K_M Model:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU-only: 7.15 tokens/second (much better)&lt;/li&gt; &lt;li&gt;GPU (25 layers): 2.67 tokens/second (avoid GPU for this format)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Test prompt was:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How can I draw a simple 360x240 box in html using the canvas&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;llamacpp&lt;/em&gt; was built on device with Termux, on a phone released in Jan 2024. Not going to win any awards for speed, however it's certainly usable!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/73tada"&gt; /u/73tada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt8j4u/llamacpp_samsung_s24_snapdragon_8_gen_3_adreno/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt8j4u/llamacpp_samsung_s24_snapdragon_8_gen_3_adreno/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt8j4u/llamacpp_samsung_s24_snapdragon_8_gen_3_adreno/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T18:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltnpsl</id>
    <title>How good is Qwen3-14B for local use? Any benchmarks vs other models?</title>
    <updated>2025-07-07T07:14:32+00:00</updated>
    <author>
      <name>/u/abubakkar_s</name>
      <uri>https://old.reddit.com/user/abubakkar_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'm looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\_14B depending on naming). I know it's been getting some hype lately, but I wanted to hear from people who’ve actually used it.&lt;/p&gt; &lt;p&gt;* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?&lt;/p&gt; &lt;p&gt;* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?&lt;/p&gt; &lt;p&gt;* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?&lt;/p&gt; &lt;p&gt;* How does it do with coding, long-context tasks, or general instruction following?&lt;/p&gt; &lt;p&gt;Would like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abubakkar_s"&gt; /u/abubakkar_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T07:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltg9ji</id>
    <title>M4 Max VS M3 Ultra Qwen3 mlx inference</title>
    <updated>2025-07-07T00:16:33+00:00</updated>
    <author>
      <name>/u/SuperPumpkin314</name>
      <uri>https://old.reddit.com/user/SuperPumpkin314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/"&gt; &lt;img alt="M4 Max VS M3 Ultra Qwen3 mlx inference" src="https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdf1d0b9ee516b55b56aba895742768fd4fba77f" title="M4 Max VS M3 Ultra Qwen3 mlx inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems compared with llama.cpp, mlx has greatly improved LLM inference with Apple Silicone. &lt;/p&gt; &lt;p&gt;I was looking at the Qwen3 inference benchmarks &lt;a href="https://x.com/awnihannun/status/1917050679467835880?s=61"&gt;https://x.com/awnihannun/status/1917050679467835880?s=61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165"&gt;https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I believe it was done on unbinned M4 max, and I get the corresponding numbers with my M3 ultra (binned version, 28c CPU, 60c GPU).&lt;/p&gt; &lt;p&gt;- 0.6B: 394 t/s&lt;/p&gt; &lt;p&gt;- 1.7B: 294 t/s&lt;/p&gt; &lt;p&gt;- 4B: 173 t/s&lt;/p&gt; &lt;p&gt;- 8B: 116 t/s&lt;/p&gt; &lt;p&gt;- 14B: 71 t/s&lt;/p&gt; &lt;p&gt;- 30B /A3B: 101 t/s&lt;/p&gt; &lt;p&gt;- 32B: 33 t/s&lt;/p&gt; &lt;p&gt;From this comparison, it seems&lt;/p&gt; &lt;p&gt;- M3U binned only get faster when activated parameters exceed 4B, and the advanges are actually not that big.&lt;/p&gt; &lt;p&gt;- For small LLMs with &amp;lt;=3B activated parameters, including 30B/A3B moe, M4 max is significantly faster.&lt;/p&gt; &lt;p&gt;There are many previous discussions on choosing between two models, and I was also so hesitant when I made the choice and I ended up with M3U binned.&lt;/p&gt; &lt;p&gt;But from this results, it seems from a local LLM inference perspective, maxed M4 max should be the to-go choice? My rationals are&lt;/p&gt; &lt;p&gt;- M4 max has much better single core cpu/gpu performance, which is more helpful for most daily tasks and programming tasks.&lt;/p&gt; &lt;p&gt;- max M4 max has 128gb memory, which allows you try a even bigger model, e.g., Qwen3 235B A22B&lt;/p&gt; &lt;p&gt;- For local LLM inference, small LLMs are more usable, it's barely feasible to use &amp;gt;32B models in daily tasks. And with this assumption, M4 max seems to win in most cases?&lt;/p&gt; &lt;p&gt;What should be the correct take-aways from this comparison?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperPumpkin314"&gt; /u/SuperPumpkin314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T00:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltgh9h</id>
    <title>GitHub - tallesborges/agentic-system-prompts: A collection of system prompts and tool definitions from production AI coding agents</title>
    <updated>2025-07-07T00:27:07+00:00</updated>
    <author>
      <name>/u/Thin_Commission_8109</name>
      <uri>https://old.reddit.com/user/Thin_Commission_8109</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgh9h/github_tallesborgesagenticsystemprompts_a/"&gt; &lt;img alt="GitHub - tallesborges/agentic-system-prompts: A collection of system prompts and tool definitions from production AI coding agents" src="https://external-preview.redd.it/0Wivc8zh2a4qNYeBjwLvtx-oCar3y8G36lxh7M_r1ig.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a527ff104e74d194936a2aa782fa015d799afea3" title="GitHub - tallesborges/agentic-system-prompts: A collection of system prompts and tool definitions from production AI coding agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Commission_8109"&gt; /u/Thin_Commission_8109 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tallesborges/agentic-system-prompts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgh9h/github_tallesborgesagenticsystemprompts_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgh9h/github_tallesborgesagenticsystemprompts_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T00:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbr1t</id>
    <title>Best reasoning model for Apple silicon with 128GB</title>
    <updated>2025-07-06T20:52:23+00:00</updated>
    <author>
      <name>/u/FuguSandwich</name>
      <uri>https://old.reddit.com/user/FuguSandwich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an MacBook M4 Max with 128 GB and LM Studio. Playing around with Gemma 3 models and Llama 4 Scout. What is the best reasoning model that will fit into my RAM? &lt;/p&gt; &lt;p&gt;Also, running HF Diffusers app. Running SD3 Medium for txt2img, anything else I should be looking at?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FuguSandwich"&gt; /u/FuguSandwich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltm49x</id>
    <title>Training 8 repos of UI code</title>
    <updated>2025-07-07T05:32:50+00:00</updated>
    <author>
      <name>/u/pomatotappu</name>
      <uri>https://old.reddit.com/user/pomatotappu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, in my company, we have 8 repos of ui code. We mainly use React and our own internally developed component library which is a seperate repo. Now, the problem statement is to develop a chat app similar to open ai that can generate code using our components library or code that follows our rules/style of code. The local model needs to have the context of our entire 8 repos. How do i go about achieving this? What are the different approaches to it? Which local llms are currently great for such coding tasks? &lt;/p&gt; &lt;p&gt;FYI, we should be able to train the model on our company's macbook air m1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pomatotappu"&gt; /u/pomatotappu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T05:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt254p</id>
    <title>Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds</title>
    <updated>2025-07-06T14:09:23+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"&gt; &lt;img alt="Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds" src="https://external-preview.redd.it/TUcdBWYS71oGoBa_L8NsLNVv7XxT4A7gDIlhLZYekVM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8dda3fcfc7ea8d542e1d5d4a0ee80c722521587c" title="Zhipu (company behind GLM) secures $1.4 billion strategic investment from Shanghai state funds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/07/04/zhipu-secures-1-4-billion-strategic-investment-from-shanghai-state-funds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt254p/zhipu_company_behind_glm_secures_14_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T14:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbg2s</id>
    <title>Narrative Beam Search workflow in Open WebUI</title>
    <updated>2025-07-06T20:39:40+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"&gt; &lt;img alt="Narrative Beam Search workflow in Open WebUI" src="https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fed0ead8ff149a552dc79fcd54b637dc557727" title="Narrative Beam Search workflow in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A variant of beam search which runs from the point of view of different system prompts. The workflow runs in an optimising LLM proxy that sends an artifact back to Open WebUI that listens to the data from the pending completion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/nbs.py"&gt;Code&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/067r3vt8ebbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsz4hk</id>
    <title>Huawei's Pangu AI Rocked by Unverified Claims of Fraud from Alleged Team Member</title>
    <updated>2025-07-06T11:36:43+00:00</updated>
    <author>
      <name>/u/Rich-Mushroom-8360</name>
      <uri>https://old.reddit.com/user/Rich-Mushroom-8360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;br /&gt; after reading the traslation of this article, I found there're many details, is it possible true or just a fake story?&lt;/p&gt; &lt;p&gt;gemini's traslation:&lt;/p&gt; &lt;p&gt;This is a full translation of the provided text. The original is a deeply emotional and accusatory letter from a self-proclaimed Huawei employee. The translation aims to preserve the tone, technical details, and cultural nuances of the original piece.&lt;/p&gt; &lt;p&gt;The Fall of Pangu: The Heartbreak and Darkness of the Huawei Noah's Ark Pangu LLM Development Journey&lt;/p&gt; &lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am an employee of the Pangu LLM team at Huawei's Noah's Ark Lab.&lt;/p&gt; &lt;p&gt;First, to verify my identity, I will list some details:&lt;/p&gt; &lt;p&gt;The current director of Noah's Ark Lab is Wang Yunhe, who was formerly the head of the Algorithm Application Department, later renamed the Small Model Lab. The former director of Noah's Ark was Yao Jun (whom everyone called Teacher Yao). Several lab directors include: Tang Ruiming (Ming-ge, Team Ming, has since left), Shang Lifeng, Zhang Wei (Wei-ge), Hao Jianye (Teacher Hao), and Liu Wulong (referred to as Director Wulong). Many other key members and experts have also left one after another.&lt;/p&gt; &lt;p&gt;We belong to an organization called the &amp;quot;Fourth Field Army&amp;quot; (四野). Under the Fourth Field Army, there are many &amp;quot;columns&amp;quot; (纵队); the foundational language model team is the Fourth Column. Wang Yunhe's small model team is the Sixteenth Column. We participated in gatherings in Suzhou, with various monthly deadlines. During the &amp;quot;problem-tackling sessions&amp;quot; in Suzhou, &amp;quot;mission orders&amp;quot; were issued, requiring us to meet targets before set deadlines. The Suzhou gatherings brought people from all over to the Suzhou Research Institute. We usually stayed in hotels, such as one in Lu Zhi (甪直), separated from our families and children.&lt;/p&gt; &lt;p&gt;During the Suzhou gatherings, Saturday was a default workday. It was exhausting, but there was afternoon tea on Saturdays, and one time we even had crayfish. Our workstations at the Suzhou Research Institute were moved once, from one building to another. The buildings at the Suzhou Institute have European-style architecture, with a large slope at the entrance, and the scenery inside is beautiful. Trips to the Suzhou gatherings would last at least a week, sometimes longer. Many people couldn't go home for one or even two months.&lt;/p&gt; &lt;p&gt;Noah's Ark was once rumored to be research-oriented, but after I joined, because we were working on the large model project under the Fourth Field Army, the project members completely turned into a delivery-focused team, swamped with routine meetings, reviews, and reports. We often had to apply just to run experiments. The team needed to interface with numerous business lines like Terminal's Celia (小艺), Huawei Cloud, and ICT, and the delivery pressure was immense.&lt;/p&gt; &lt;p&gt;The Pangu model developed by Noah's Ark was initially codenamed &amp;quot;Pangu Zhizi&amp;quot; (盘古智子). At first, it was only available as an internal webpage that required an application for trial use. Later, due to pressure, it was integrated into Welink and opened for public beta.&lt;/p&gt; &lt;p&gt;The recent controversy surrounding the accusations that the Pangu LLM plagiarized Qwen has been all over the news. As a member of the Pangu team, I've been tossing and turning every night, unable to sleep. Pangu's brand has been so severely damaged. On one hand, I selfishly worry about my own career development and feel that my past hard work was for nothing. On the other hand, I feel a sense of vindication now that someone has started exposing these things. For countless days and nights, we gritted our teeth in anger, powerless, as certain individuals internally reaped endless benefits through repeated fraud. This suppression and humiliation have gradually eroded my affection for Huawei, leaving me dazed and confused, lost and aimless, often questioning my life and self-worth.&lt;/p&gt; &lt;p&gt;I admit that I am a coward. As a humble worker, I dare not oppose people like Wang Yunhe with their powerful connections, let alone a behemoth like Huawei. I am terrified of losing my job, as I have a family and children to support. That's why I deeply admire the whistleblower from the bottom of my heart. However, when I see the internal attempts to whitewash and cover up the facts to deceive the public, I can no longer tolerate it. I want to be brave for once and follow my conscience. Even if I harm myself by 800, I hope to damage the enemy by 1,000. I have decided to publicize what I have seen and heard here (some of which is from colleagues) about the &amp;quot;legendary story&amp;quot; of the Pangu LLM.&lt;/p&gt; &lt;p&gt;Huawei has indeed primarily trained its large models on Ascend cards (the Small Model Lab has quite a few Nvidia cards, which they used for training before transitioning to Ascend). I was once captivated by Huawei's determination to &amp;quot;build the world's second choice,&amp;quot; and I used to have deep feelings for the company. We went through trials and tribulations with Ascend, from being full of bugs to now being able to train models, and we invested immense effort and sacrifice.&lt;/p&gt; &lt;p&gt;Initially, our computing power was very limited, and we trained models on the 910A. At that time, it only supported fp16, and the training stability was far worse than bf16. Pangu started working on MoE (Mixture of Experts) very early. In 2023, the main focus was on training a 38B MoE model and a subsequent 71B dense model. The 71B dense model was expanded to become the first-generation 135B dense model, and later, the main models were gradually trained on the 910B.&lt;/p&gt; &lt;p&gt;Both the 71B and 135B models had a huge, fundamental flaw: the tokenizer. The tokenizer used back then had extremely low encoding efficiency. Every single symbol, number, space, and even Chinese character took up one token. As you can imagine, this wasted a tremendous amount of computing power and resulted in poor model performance. At that time, the Small Model Lab happened to have a vocabulary they had trained themselves. Teacher Yao suspected that the model's tokenizer was the problem (and in hindsight, his suspicion was undoubtedly correct). So, he decided to have the 71B and 135B models switch tokenizers, as the Small Model Lab had experimented with this before. The team stitched together two tokenizers and began the replacement process. The replacement for the 71B model failed. The 135B model, using a more refined embedding initialization strategy, finally succeeded in changing its vocabulary after being continually trained on at least 1T of data. But as you can imagine, the performance did not improve.&lt;/p&gt; &lt;p&gt;Meanwhile, other domestic companies like Alibaba and Zhipu AI were training on GPUs and had already figured out the right methods. The gap between Pangu and its competitors grew wider and wider. An internal 230B dense model, trained from scratch, failed for various reasons, pushing the project to the brink of collapse. Facing pressure from several deadlines and strong internal skepticism about Pangu, the team's morale hit rock bottom. With extremely limited computing power, the team struggled and tried many things. For example, they accidentally discovered that the 38B MoE model at the time did not have the expected MoE effect. So they removed the MoE parameters, reverting it to a 13B dense model. Since the 38B MoE originated from a very early Pangu Alpha 13B with a relatively outdated architecture, the team made a series of changes, such as switching from absolute position encoding to RoPE, removing bias, and switching to RMSNorm. Given the failures with the tokenizer and the experience of changing vocabularies, this model's vocabulary was also replaced with the one used by Wang Yunhe's Small Model Lab's 7B model. This 13B model was later expanded and continually trained, becoming the second-generation 38B dense model (which was the main mid-range Pangu model for several months) and was once quite competitive. However, because the larger 135B model had an outdated architecture and was severely damaged by the vocabulary change (later analysis revealed that the stitched-together vocabulary had even more serious bugs), its performance after continued training still lagged far behind leading domestic models like Qwen. The internal criticism and pressure from leadership grew even stronger. The team was practically in a desperate situation.&lt;/p&gt; &lt;p&gt;Under these circumstances, Wang Yunhe and his Small Model Lab stepped in. They claimed to have inherited and modified the parameters from the old 135B model, and by training on just a few hundred billion tokens, they improved various metrics by an average of about ten points. In reality, this was their first masterpiece of &amp;quot;shell-wrapping&amp;quot; (套壳, i.e., putting a new shell on another company's model) applied to a large model. At Huawei, laymen lead experts, so the leadership had no concept of how absurd this was; they just thought there must be some algorithmic innovation. After internal analysis, it was discovered that they had actually continued training on Qwen 1.5 110B, adding layers, expanding the FFN dimensions, and incorporating some mechanisms from the Pangu-Pi paper to reach about 135B parameters. In fact, the old 135B had 107 layers, while this new model only had 82, and various other configurations were different. After training, the distribution of many parameters in the new, mysterious 135B model was almost identical to Qwen 110B. Even the class name in the model's code was &amp;quot;Qwen&amp;quot; at the time; they were too lazy to even change it. This model later became the so-called 135B V2. And this model was provided to many downstream teams, including external customers.&lt;/p&gt; &lt;p&gt;This incident was a huge blow to those of us colleagues who were doing our work seriously and honestly. Many people internally, including those in the Terminal and Huawei Cloud divisions, knew about this. We all joked that we should stop calling it the Pangu model and call it the &amp;quot;Qiangu&amp;quot; model instead (a pun combining Qwen and Pangu). At the time, team members wanted to report this to the BCG (Business Conduct Guidelines) office, as it was major business fraud. But later, it was said that a leader stopped them, because higher-level leaders (like Teacher Yao, and possibly Director Xiong and Elder Zha) also found out later but did nothing about it. Getting good results through shell-wrapping was also beneficial to them. This event caused several of the team's strongest members to become disheartened, and talk of resignation became commonplace.&lt;/p&gt; &lt;p&gt;At this point, Pangu seemed to find a turning point. Since the Pangu models mentioned earlier were mostly based on continued training and modification, Noah's Ark at that time had no grasp of training technology from scratch, let alone on Ascend's NPUs. Thanks to the strenuous efforts of the team's core members, Pangu began training its third-generation models. After immense effort, the data architecture and training algorithms gradually caught up with the industry. The people from the Small Model Lab had nothing to do with this hardship.&lt;/p&gt; &lt;p&gt;Initially, the team members had no confidence and started with just a 13B model. But later, they found the results were quite good. So this model was later expanded again, becoming the third-generation 38B, codenamed 38B V3. I'm sure many brothers in the product lines are familiar with this model. At that time, this model's tokenizer was an extension of Llama's vocabulary (a common practice in the industry). Meanwhile, Wang Yunhe's lab created another vocabulary (which later became the vocabulary for the Pangu series). The two vocabularies were forced into a &amp;quot;horse race&amp;quot; (a competitive trial), which ended with no clear winner. So, the leadership immediately decided that the vocabularies should be unified, and Wang Yunhe's should be used. Consequently, the 135B V3 (known externally as Pangu Ultra), which was trained from scratch, adopted this tokenizer. This also explains the confusion many brothers who used our models had: why two models of the same V3 generation, but different sizes, used different tokenizers.&lt;/p&gt; &lt;p&gt;From the bottom of our hearts, we feel that the 135B V3 was the pride of our Fourth Column team at the time. It was the first truly full-stack, self-developed, properly from-scratch-trained, hundred-billion-parameter-level model from Huawei, and its performance was comparable to competitors in early 2024. Writing this, I am already in tears. It was so incredibly difficult. To ensure stable training, the team conducted a large number of comparative experiments and performed timely rollbacks and restarts whenever the model's gradients showed anomalies. This model truly achieved what was later stated in the technical report: not a single loss spike throughout the entire training process. We overcame countless difficulties. We did it. We are willing to guarantee the authenticity of this model's training with our lives and honor. How many sleepless nights did we spend for its training? How wronged and aggrieved did we feel when we were being worthless in internal forums? We persevered.&lt;/p&gt; &lt;p&gt;We are the ones who were truly burning our youth to build up China's domestic computing foundation... Away from home, we gave up our families, our holidays, our health, and our entertainment. We risked everything. The hardships and difficulties involved cannot be fully described in a few words. At various mobilization meetings, when we shouted slogans like &amp;quot;Pangu will prevail, Huawei will prevail,&amp;quot; we were genuinely and deeply moved.&lt;/p&gt; &lt;p&gt;However, all the fruits of our hard work were often casually taken by the Small Model Lab. Data? They just demanded it. Code? They just took it and even required us to help adapt it so it could be run with a single click. We used to joke that the Small Model Lab was the &amp;quot;mouse-clicking lab.&amp;quot; We did the hard work; they reaped the glory. It really is true what they say: &amp;quot;You are carrying a heavy burden so that someone else can live a peaceful life.&amp;quot; Under these circumstances, more and more of our comrades could no longer hold on and chose to leave. Seeing those brilliant colleagues leave one by one, I felt both regret and sadness. In this battle-like environment, we were more like comrades-in-arms than colleagues. They were also great teachers from whom I could learn countless technical things. Seeing them go to outstanding teams like ByteDance's Seed, Deepseek, Moonshot AI, Tencent, and Kuaishou, I am genuinely happy for them and wish them the best for escaping this exhausting and dirty place. I still vividly remember what a colleague who left said: &amp;quot;Coming here was a disgrace to my technical career. Every day I stay here is a waste of life.&amp;quot; The words were harsh, but they left me speechless. I worried about my own lack of technical expertise and my inability to adapt to the high-turnover environment of internet companies, which kept me from taking the step to resign despite thinking about it many times.&lt;/p&gt; &lt;p&gt;Besides dense models, Pangu later began exploring MoE models. Initially, a 224B MoE model was trained. In parallel, the Small Model Lab launched its second major shell-wrapping operation (minor incidents may have included other models, like a math model), which is the now infamous Pangu-Pro MoE 72B. This model was internally claimed to have been expanded from the Small Model Lab's 7B model (even if true, this contradicts the technical report, let alone the fact that it was continued training on a shell of Qwen 2.5's 14B). I remember that just a few days after they started training, their internal evaluation scores immediately caught up with our 38B V3 at the time. Many brothers in the AI System Lab knew about their shell-wrapping operation because they needed to adapt the model, but for various reasons, they couldn't bring justice to light. In fact, for this model that was trained for a very long time afterward, I am surprised that HonestAGI was able to detect this level of similarity. The computing power spent on &amp;quot;washing&amp;quot; the parameters to continue training would have been more than enough to train a model of the same size from scratch. I heard from colleagues that they used many methods to wash away Qwen's watermark, even intentionally training it on dirty data. This provides an unprecedented case study for the academic community researching model &amp;quot;lineage.&amp;quot; New lineage detection methods in the future can be tested on this.&lt;/p&gt; &lt;p&gt;In late 2024 and early 2025, after the release of Deepseek v3 and r1, our team was hit hard by their stunning technical level and faced even greater skepticism. To keep up with the trend, Pangu imitated Deepseek's model size and began training a 718B MoE model. At this time, the Small Model Lab struck again. They chose to shell-wrap and continue training on Deepseek-v3. They trained the model by freezing the parameters loaded from Deepseek. Even the directory for loading the checkpoint was named deepseekv3—they didn't even bother to change it. How arrogant is that? In contrast, some colleagues with true technical integrity were training another 718B MoE from scratch, but they encountered all sorts of problems. But obviously, how could this model ever be better than a direct shell-wrap? If it weren't for the team leader's insistence, it would have been shut down long ago.&lt;/p&gt; &lt;p&gt;Huawei's cumbersome process management severely slows down the R&amp;amp;D pace of large models, with things like version control, model lineage, various procedures, and traceability requirements. Ironically, the Small Model Lab's models never seem to be bound by these processes. They can shell-wrap whenever they want, continue training whenever they want, and endlessly demand computing resources. This stark, almost surreal contrast illustrates the current state of process management: &amp;quot;The magistrates are allowed to set fires, but the common people are not even allowed to light lamps.&amp;quot; How ridiculous? How tragic? How hateful? How shameful!&lt;/p&gt; &lt;p&gt;After the HonestAGI incident, we were forced into endless internal discussions and analyses on how to handle public relations and &amp;quot;respond.&amp;quot; Admittedly, the original analysis might not have been strong enough, giving Wang Yunhe and the Small Model Lab an opportunity to argue and twist the truth. For this, I have felt sick to my stomach these past two days, constantly questioning the meaning of my life and whether there is any justice in the world. I'm not playing along anymore. I'm going to resign. I am also applying to have my name removed from the author list of some of the Pangu technical reports. Having my name on those reports is a stain on my life that I can never erase. At the time, I never thought they would be brazen enough to open-source it. I never thought they would dare to fool the world like this and promote it so heavily. At that time, perhaps I was holding onto a sliver of wishful thinking and didn't refuse to be listed as an author. I believe many of my dedicated comrades were also forced onto this pirate ship or were unaware of the situation. But this can't be undone. I hope to spend the rest of my life doing solid, meaningful work to atone for my weakness and indecisiveness back then.&lt;/p&gt; &lt;p&gt;Writing this late at night, I am already in tears, sobbing uncontrollably. I remember when some outstanding colleagues were leaving, I asked them with a wry smile if they were going to post a long, customary farewell message on the internal forum to expose the situation. They replied, &amp;quot;No, it's a waste of time, and I'm afraid it would make things even worse for you all.&amp;quot; At that moment, I felt a deep sense of sorrow, because my comrades, with whom I had once fought for a common ideal, had completely lost faith in Huawei. We used to joke that we were using the Communist Party's &amp;quot;millet plus rifles&amp;quot; (meager resources) while the organization had the style of the Kuomintang (corrupt and bureaucratic).&lt;/p&gt; &lt;p&gt;There was a time when I was proud that we were using &amp;quot;millet plus rifles&amp;quot; to defeat foreign guns and cannons.&lt;/p&gt; &lt;p&gt;Now, I am tired. I want to surrender.&lt;/p&gt; &lt;p&gt;To this day, I still sincerely hope that Huawei can learn its lesson, do Pangu right, make Pangu world-class, and bring Ascend to the level of Nvidia. The internal phenomenon of &amp;quot;bad money driving out good&amp;quot; has caused Noah's Ark, and even Huawei, to rapidly lose a large number of outstanding large model talents. I believe they are now shining in various teams like Deepseek, realizing their ambitions and talents, and contributing to the fierce AI competition between China and the US. I often lament that Huawei doesn't lack talent; it simply doesn't know how to retain it. If these people were given the right environment, the right resources, fewer shackles, and less political infighting, what would stop Pangu from succeeding?&lt;/p&gt; &lt;p&gt;Finally: I swear on my life, character, and honor that everything I have written above is true (at least within my limited knowledge). I do not have the high level of technical skill or the opportunity to conduct a thorough and solid analysis, nor do I dare to use internal records as direct evidence for fear of being caught through information security. But I believe many of my former comrades will vouch for me. To my brothers still inside Huawei, including those in the product lines we served, I believe the countless details in this article will resonate with your own impressions and corroborate my claims. You too may have been deceived, but these cruel truths will not remain buried. The traces of our struggle should not be distorted and buried either.&lt;/p&gt; &lt;p&gt;Having written so much, certain people will surely want to find me and silence me. The company might even try to shut me up or hold me accountable. If that happens, my personal safety, and even that of my family, could be threatened. For my own protection, I will report that I am safe to everyone daily in the near future.&lt;/p&gt; &lt;p&gt;If I disappear, just consider it my sacrifice for truth and ideals, for the better development of computing power and AI in Huawei and even in China. I am willing to be buried in that place where I once fought.&lt;/p&gt; &lt;p&gt;Goodbye, Noah's Ark.&lt;/p&gt; &lt;p&gt;Written in the early morning of July 6, 2024, in Shenzhen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich-Mushroom-8360"&gt; /u/Rich-Mushroom-8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsz4hk/huaweis_pangu_ai_rocked_by_unverified_claims_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T11:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbrlf</id>
    <title>🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)</title>
    <updated>2025-07-06T20:53:01+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt; &lt;img alt="🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" src="https://preview.redd.it/bwd1gqkrfbbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57eba9e11159f3d51759d5ca917254faf9332203" title="🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We have been exploring various open-source Text-to-Speech (TTS) models, and decided to create a Hugging Face demo space that makes it easy to compare their quality side-by-side.&lt;/p&gt; &lt;p&gt;The demo features &lt;strong&gt;12 popular TTS models&lt;/strong&gt;, all tested using a consistent prompt, so you can quickly hear and compare their synthesized speech and choose the best one for your audio projects.&lt;/p&gt; &lt;p&gt;Would love to get feedback or suggestions!&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary"&gt;Check out the demo space and detailed comparison here!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-part-2"&gt;Check out the blog: Choosing the Right Text-to-Speech Model: Part 2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Share your use-case and we will update this space as required! &lt;/p&gt; &lt;p&gt;Which TTS model sounds most natural to you?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwd1gqkrfbbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltgayn</id>
    <title>Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA</title>
    <updated>2025-07-07T00:18:28+00:00</updated>
    <author>
      <name>/u/woct0rdho</name>
      <uri>https://old.reddit.com/user/woct0rdho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt; &lt;img alt="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" src="https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094a72eeef2838a876e8835f533b8146c2ead2a9" title="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &amp;lt; 20% GPU usage. It's been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search 'qwen3 30b a3b lora' on HuggingFace, that's... interesting)&lt;/p&gt; &lt;p&gt;This should be made easier. I've made a fused version of Qwen3 MoE Layer that's much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.&lt;/p&gt; &lt;p&gt;There is still room for further optimization, but you can try it now and train your own LoRA.&lt;/p&gt; &lt;p&gt;Also, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woct0rdho"&gt; /u/woct0rdho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/woct0rdho/transformers-qwen3-moe-fused"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T00:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltamap</id>
    <title>Cheapest way to stack VRAM in 2025?</title>
    <updated>2025-07-06T20:04:49+00:00</updated>
    <author>
      <name>/u/gnad</name>
      <uri>https://old.reddit.com/user/gnad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;4x RTX 3060 (48 GB)&lt;/li&gt; &lt;li&gt;4x P100 (64 GB)&lt;/li&gt; &lt;li&gt;3x P40 (72 GB)&lt;/li&gt; &lt;li&gt;3x RX 9060 (48 GB)&lt;/li&gt; &lt;li&gt;4x MI50 32GB (128GB)&lt;/li&gt; &lt;li&gt;3x RTX 4060 ti/5060 ti (48 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Edit: add more suggestion from comments. &lt;/p&gt; &lt;p&gt;Which GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnad"&gt; /u/gnad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltfgoy</id>
    <title>I drew a silly comic about Llama model</title>
    <updated>2025-07-06T23:37:41+00:00</updated>
    <author>
      <name>/u/Organic-Mechanic-435</name>
      <uri>https://old.reddit.com/user/Organic-Mechanic-435</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt; &lt;img alt="I drew a silly comic about Llama model" src="https://a.thumbs.redditmedia.com/ntblmHJuZXo-K_j-B7phe4Ko7b3I1mCMnzblLD25_K8.jpg" title="I drew a silly comic about Llama model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a roleplayer using SillyTavern. Llama models are often used as 'base' for fine tunes in Huggingface. Seeing what people can do with local models also fascinate me. &lt;sup&gt;^&lt;/sup&gt; Hello!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Organic-Mechanic-435"&gt; /u/Organic-Mechanic-435 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ltfgoy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T23:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt4y1z</id>
    <title>Self-hosted AI coding that just works</title>
    <updated>2025-07-06T16:09:28+00:00</updated>
    <author>
      <name>/u/send_me_a_ticket</name>
      <uri>https://old.reddit.com/user/send_me_a_ticket</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;: VSCode + RooCode + LM Studio + Devstral + Ollama + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long Post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Responds reasonably well on a variety of hardware.&lt;/li&gt; &lt;li&gt;Doesn’t hallucinate outdated syntax.&lt;/li&gt; &lt;li&gt;Costs $0 (except electricity).&lt;/li&gt; &lt;li&gt;Understands less common languages, e.g., KQL, Flutter, etc.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After experimenting with several setups, here’s the combo I found that actually works.&lt;br /&gt; Please forgive any mistakes and feel free to let me know of any improvements you are aware of.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br /&gt; Tested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.&lt;br /&gt; Should work on both low, and high-end setups, your mileage may vary.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;VSCode +(with) RooCode +(connected to) LM Studio +(running) Devstral +(and) Ollama +(running) snowflake-arctic-embed2 +(supported by) docs-mcp-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Why both LM Studio &amp;amp; Ollama? I am using LM Studio for LLM inference (great UI, OpenAI-compatible API), but doesn't support running embeddings in parallel. Ollama handles embeddings nicely but changing model parameters is painful. Hence, they complement each other.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Setup Process for users saying this is too complicated &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install &lt;code&gt;VSCode&lt;/code&gt; then get &lt;code&gt;RooCode&lt;/code&gt; Extension&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;LMStudio&lt;/code&gt; and pull &lt;code&gt;Devstral&lt;/code&gt; large language model that fits your computer&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;Ollama&lt;/code&gt; and pull &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; embedding model &lt;em&gt;(or any other embedding model you like)&lt;/em&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;NodeJS&lt;/code&gt;, depending on which config you prefer &lt;em&gt;(recommend Docker)&lt;/em&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Include &lt;code&gt;docs-mcp-server&lt;/code&gt; in your RooCode MCP configuration &lt;em&gt;(see json below)&lt;/em&gt;&lt;br /&gt; ---&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;VSCode + RooCode&lt;/strong&gt;&lt;br /&gt; RooCode is a VS Code extension that enables agentic coding and has MCP support.&lt;/p&gt; &lt;p&gt;VS Code: &lt;a href="https://code.visualstudio.com/download"&gt;https://code.visualstudio.com/download&lt;/a&gt;&lt;br /&gt; Alternative - VSCodium: &lt;a href="https://github.com/VSCodium/vscodium/releases"&gt;https://github.com/VSCodium/vscodium/releases&lt;/a&gt; - No telemetry&lt;/p&gt; &lt;p&gt;RooCode: &lt;a href="https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline"&gt;https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative to this setup is Zed Editor: &lt;a href="https://zed.dev/download"&gt;https://zed.dev/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: &lt;a href="https://github.com/send-me-a-ticket/zedforwindows"&gt;github.com/send-me-a-ticket/zedforwindows&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://lmstudio.ai/download"&gt;https://lmstudio.ai/download&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nice UI with real-time logs&lt;/li&gt; &lt;li&gt;GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num_gpu and num_ctx parameters&lt;/li&gt; &lt;li&gt;Good (better?) OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://ollama.com/download"&gt;https://ollama.com/download&lt;/a&gt;&lt;br /&gt; Used only for running &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Devstral (Unsloth finetune)&lt;/strong&gt;&lt;br /&gt; Solid coding model with good tool usage.&lt;/p&gt; &lt;p&gt;I use &lt;code&gt;devstral-small-2505@iq2_m&lt;/code&gt;, which fully fits within 10GB VRAM. token context 32768.&lt;br /&gt; Other variants &amp;amp; parameters may work depending on your hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;snowflake-arctic-embed2&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://ollama.com/library/snowflake-arctic-embed2"&gt;https://ollama.com/library/snowflake-arctic-embed2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recommend Docker use instead of NPX, for security and ease of use.&lt;/p&gt; &lt;p&gt;Portainer is my recommended extension for ease of use - &lt;a href="https://hub.docker.com/extensions/portainer/portainer-docker-extension"&gt;https://hub.docker.com/extensions/portainer/portainer-docker-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/arabold/docs-mcp-server"&gt;https://github.com/arabold/docs-mcp-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the &lt;em&gt;correct&lt;/em&gt; syntax for &lt;em&gt;your&lt;/em&gt; version of language implementation, and avoid hallucinations.&lt;/p&gt; &lt;p&gt;You &lt;em&gt;should&lt;/em&gt; also be able to run &lt;code&gt;localhost:6281&lt;/code&gt; to open web UI for the &lt;code&gt;docs-mcp-server&lt;/code&gt;, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.&lt;/p&gt; &lt;p&gt;You can implement this MCP server as following -&lt;/p&gt; &lt;p&gt;&lt;em&gt;Docker version (needs Docker Installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;docker&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;run&amp;quot;, &amp;quot;-i&amp;quot;, &amp;quot;--rm&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6280:6280&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6281:6281&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_KEY&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;, &amp;quot;-v&amp;quot;, &amp;quot;docs-mcp-data:/data&amp;quot;, &amp;quot;ghcr.io/arabold/docs-mcp-server:latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:11434/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;snowflake-arctic-embed2&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NPX version (needs NodeJS installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:11434/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;snowflake-arctic-embed2&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Adding documentation for your language&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ask AI to use the &lt;code&gt;scrape_docs&lt;/code&gt; tool with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;url&lt;/strong&gt; (link to the documentation),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;library&lt;/strong&gt; (name of the documentation/programming language),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;version&lt;/strong&gt; (version of the documentation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you can also provide (optional):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;maxPages&lt;/strong&gt; (maximum number of pages to scrape, default is 1000).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;maxDepth&lt;/strong&gt; (maximum navigation depth, default is 3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt; (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').&lt;/li&gt; &lt;li&gt;&lt;strong&gt;followRedirects&lt;/strong&gt; (whether to follow HTTP 3xx redirects, default is true).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can ask AI to use &lt;strong&gt;search_docs&lt;/strong&gt; tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.&lt;/p&gt; &lt;p&gt;This stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.&lt;br /&gt; The MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/send_me_a_ticket"&gt; /u/send_me_a_ticket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T16:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lthtbn</id>
    <title>8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top.</title>
    <updated>2025-07-07T01:35:48+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt; &lt;img alt="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." src="https://b.thumbs.redditmedia.com/YH48KR3uSeLFmipFEDX0ai8FZ4TwQmmccEKSaaUx3Fk.jpg" title="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a &lt;a href="https://www.designarena.ai/"&gt;research&lt;/a&gt; project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when &lt;a href="https://www.designarena.ai/vote"&gt;voting&lt;/a&gt;, model names are hidden, so people don't immediately know which models generated what. &lt;/p&gt; &lt;p&gt;From the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. &lt;/p&gt; &lt;p&gt;Anything surprise you here? What models have you noticed been the best for UI/UX and frontend development? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lthtbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T01:35:48+00:00</published>
  </entry>
</feed>
