<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-10T23:24:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mm7azs</id>
    <title>OpenAI gpt-oss-20b &amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M</title>
    <updated>2025-08-10T02:40:18+00:00</updated>
    <author>
      <name>/u/traderjay_toronto</name>
      <uri>https://old.reddit.com/user/traderjay_toronto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"&gt; &lt;img alt="OpenAI gpt-oss-20b &amp;amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M" src="https://preview.redd.it/g22r9c3au3if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c1aebfd70d7867330b7be6dcf6a550cceebb0e3" title="OpenAI gpt-oss-20b &amp;amp; 120 model performance on the RTX Pro 6000 Blackwell vs RTX 5090M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preface - I am not a programmer just an AI enthusiast and user. The GPU I got is mainly used for video editing and creative work but I know its very well suited to run large AI models so I decided to test it out. If you want me to test the performance of other models let me know as long it works in LM studio. &lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/Beta87"&gt;u/Beta87&lt;/a&gt; I got LM studio up and running and loaded the two latest model from OpenAI to test it out. Here is what I got performance wise on two wildly different systems:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;20b model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 Blackwell -&lt;/strong&gt; 205 tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 5090M -&lt;/strong&gt; 145tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;120b model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX Pro 6000 Blackwell -&lt;/strong&gt; 145 tokens/sec&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 5090M -&lt;/strong&gt; 11 tokens/sec&lt;/p&gt; &lt;p&gt;Had to turn off all guardrail on the laptop to make the 120b model run and it's using system ram as it ran out of GPU memory but it didn't crash.&lt;/p&gt; &lt;p&gt;What a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traderjay_toronto"&gt; /u/traderjay_toronto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g22r9c3au3if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm7azs/openai_gptoss20b_120_model_performance_on_the_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T02:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmiudy</id>
    <title>Cultural embedding in local models? Are they all US centric only?</title>
    <updated>2025-08-10T13:39:43+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m unfamiliar with how Chinese culture exists in the LLMs. It seems that scale.ai’s classifications and the way it lists and ranks information is still in all models that are based on American models. &lt;/p&gt; &lt;p&gt;All the answers I get seem very localized to my area (US). Even the Chinese ones like qwen and deepseek. I don’t know if multilingual will change this, or how much data about countries cultures exists in it. I’m wondering what people’s experiences are with it. &lt;/p&gt; &lt;p&gt;The only model so far I’ve heard that seems to be different is dream from HK. &lt;/p&gt; &lt;p&gt;I’m not sure if that’s my kinds of questions or if if Americanizing is also an artifact of GPTism &lt;span class="md-spoiler-text"&gt;🤕&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmiudy/cultural_embedding_in_local_models_are_they_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmiudy/cultural_embedding_in_local_models_are_they_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmiudy/cultural_embedding_in_local_models_are_they_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1zcg</id>
    <title>i'm making dating simulator game with ai npc using open source llm</title>
    <updated>2025-08-09T22:20:10+00:00</updated>
    <author>
      <name>/u/aziib</name>
      <uri>https://old.reddit.com/user/aziib</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt; &lt;img alt="i'm making dating simulator game with ai npc using open source llm" src="https://external-preview.redd.it/ZjR3N2g2MmhjMmlmMW57LOTD_dYSJqtRCJKUw8LsjdMQCiQ_6aIAc3dh1JZp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18b063cf71d62103bbeb0fff25811ee3533f54d1" title="i'm making dating simulator game with ai npc using open source llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you can play on your browser: &lt;a href="https://romram.itch.io/break-time"&gt;https://romram.itch.io/break-time&lt;/a&gt;&lt;br /&gt; you need LM Studio as a local server: &lt;a href="https://lmstudio.ai/"&gt;https://lmstudio.ai/&lt;/a&gt;&lt;br /&gt; use uncensored llama 8b model or more and 8k context window or more for better experience.&lt;br /&gt; i use blacksheep gguf models:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the game engine is using rpg maker mz with some of my modified custom plugins&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aziib"&gt; /u/aziib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hchl142hc2if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmsxuw</id>
    <title>Anyone having this problem on GPT OSS 20B and LM Studio ?</title>
    <updated>2025-08-10T20:18:21+00:00</updated>
    <author>
      <name>/u/Current-Stop7806</name>
      <uri>https://old.reddit.com/user/Current-Stop7806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmsxuw/anyone_having_this_problem_on_gpt_oss_20b_and_lm/"&gt; &lt;img alt="Anyone having this problem on GPT OSS 20B and LM Studio ?" src="https://preview.redd.it/at9ylus439if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=880bbabe5c1fe0a978b5e35235383ebc33a98079" title="Anyone having this problem on GPT OSS 20B and LM Studio ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Official gpt oss 20B and latest LM Studio. I set up to 8k tokens context window. Everything was fine. When approaching the end of context window, I get these messages and I can't continue with the conversation. What the heck could be that ? I've never seen this before in any other model. Any help is welcomed. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Stop7806"&gt; /u/Current-Stop7806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/at9ylus439if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmsxuw/anyone_having_this_problem_on_gpt_oss_20b_and_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmsxuw/anyone_having_this_problem_on_gpt_oss_20b_and_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T20:18:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmgm19</id>
    <title>Anyone experienced with self hosting at enterprise level: how do you handle KV caching?</title>
    <updated>2025-08-10T11:54:02+00:00</updated>
    <author>
      <name>/u/Budget_Map_3333</name>
      <uri>https://old.reddit.com/user/Budget_Map_3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm setting up a platform where I intend to self host models. Starting off with serverless runpod GPUs for now (what I can afford).&lt;/p&gt; &lt;p&gt;So I came to the realisation that one of the core variables for keeping costs down will be KV caching. My platform will be 100% around multi turn conversations with long contexts. In principle, from what I understand the KV cache is stored on the actual GPU in a LRU way which is fine for a few concurrent users. &lt;/p&gt; &lt;p&gt;But what happens when we start to scale up? Many users. Many serverless endpoints. Many multi turn conversations with long contexts. To not &amp;quot;waste&amp;quot; KV caching I guess one way would be to configure vLLM or SGLang to offload the KV cache to CPU, then to local NVMe and then finally to a network volume based on the interval. I guess. But it seems like this is gonna be a very difficult task working with serverless, permament pods are probably a different story. &lt;/p&gt; &lt;p&gt;Just looking for some tips here from any engineers who have experience self-hosting at a large scale and serving concurrent sessions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Budget_Map_3333"&gt; /u/Budget_Map_3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmgm19/anyone_experienced_with_self_hosting_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mllt5x</id>
    <title>Imagine an open source code model that in the same level of claude code</title>
    <updated>2025-08-09T10:04:00+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt; &lt;img alt="Imagine an open source code model that in the same level of claude code" src="https://preview.redd.it/diwwcslbwyhf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fa0ed92a8f036fa157e0bb1b95df2206b17f91a" title="Imagine an open source code model that in the same level of claude code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diwwcslbwyhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T10:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmkt0x</id>
    <title>Why does lmarena currently show the ranking for GPT‑5 but not the rankings for the two GPT‑OSS models (20B and 120B)?</title>
    <updated>2025-08-10T15:02:30+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren’t there enough votes yet? I'd like to see how they perform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmkt0x/why_does_lmarena_currently_show_the_ranking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmkt0x/why_does_lmarena_currently_show_the_ranking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmkt0x/why_does_lmarena_currently_show_the_ranking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T15:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh7nq</id>
    <title>Qwen and DeepSeek is great for coding but</title>
    <updated>2025-08-10T12:24:23+00:00</updated>
    <author>
      <name>/u/XiRw</name>
      <uri>https://old.reddit.com/user/XiRw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ever noticed how it takes it upon itself (sometimes) to change shit around on the frontend to make it the way it wants without your permission?? &lt;/p&gt; &lt;p&gt;It’s not even little insignificant things it’s major changes.&lt;/p&gt; &lt;p&gt;Not only that but with Qwen3 coder especially I tell it instructions with how to format its response back to me and it ignores it unless I call it out for not listening and become dramatic about it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiRw"&gt; /u/XiRw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh7nq/qwen_and_deepseek_is_great_for_coding_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlsm8e</id>
    <title>Qwen 3 0.6B beats GPT-5 in simple math</title>
    <updated>2025-08-09T15:46:37+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt; &lt;img alt="Qwen 3 0.6B beats GPT-5 in simple math" src="https://preview.redd.it/vtstf4nql0if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1592043c0537a1ea1b41a75985a4e87e590870fd" title="Qwen 3 0.6B beats GPT-5 in simple math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this comparison between Grok and GPT-5 on X for solving the equation 5.9 = x + 5.11. In the comparison, Grok solved it but GPT-5 without thinking failed.&lt;/p&gt; &lt;p&gt;It could have been handpicked after multiples runs, so out of curiosity and for fun I decided to test it myself. Not with Grok but with local models running on iPhone since I develop an app around that, &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt; for those interested but you can reproduce the result below with &lt;a href="https://lmstudio.ai"&gt;LMStudio&lt;/a&gt;, &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt; or any other local chat app of course.&lt;/p&gt; &lt;p&gt;And I was honestly surprised.In my very first run, GPT-5 failed (screenshot) while Qwen 3 0.6B without thinking succeeded. After multiple runs, I would say GPT-5 fails around 30-40% of the time, while Qwen 3 0.6B, which is a tiny 0.6 billion parameters local model around 500 MB in size, solves it every time.Yes it’s one example, GPT-5 was without thinking and it’s not really optimized for math in this mode but Qwen 3 too. And honestly, it’s a simple equation I did not think GPT-5 would fail to solve, thinking or not. Of course, GPT-5 is better than Qwen 3 0.6B, but it’s still interesting to see cases like this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vtstf4nql0if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T15:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmtpxj</id>
    <title>Fun with RTX PRO 6000 Blackwell SE</title>
    <updated>2025-08-10T20:49:18+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt; &lt;img alt="Fun with RTX PRO 6000 Blackwell SE" src="https://b.thumbs.redditmedia.com/Obb_dReEY8vOlkdMcZIohxJNC2XIskPvsQNSIDmpQrM.jpg" title="Fun with RTX PRO 6000 Blackwell SE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been having some fun testing out the new NVIDIA RTX PRO 6000 Blackwell Server Edition. You definitely need some good airflow through this thing. I picked it up to support document &amp;amp; image processing for my platform (&lt;a href="https://missionsquad.ai/"&gt;missionsquad.ai&lt;/a&gt;) instead of paying google or aws a bunch of money to run models in the cloud. Initially I tried to go with a bigger and quieter fan - &lt;a href="https://thermalright.com/product/ty-143/"&gt;Thermalright TY-143&lt;/a&gt; - because it moves a decent amount of air - 130 CFM - and is very quiet. Have a few laying around from the crypto mining days. But that didn't quiet cut it. It was sitting around 50ºC while idle and under sustained load the GPU was hitting about 85ºC. Upgraded to a &lt;a href="https://www.amazon.com/dp/B07SGWNV5J"&gt;Wathai 120mm x 38&lt;/a&gt; server fan (220 CFM) and it's MUCH happier now. While idle it sits around 33ºC and under sustained load it'll hit about 61-62ºC. I made some ducting to get max airflow into the GPU. Fun little project!&lt;/p&gt; &lt;p&gt;The model I've been using is nanonets-ocr-s and I'm getting ~140 tokens/sec pretty consistently.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mxuu26o989if1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ce57f5d5aaff060bab2ad872330cce36cd64d32"&gt;Wathai 120x38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7i84bg5b89if1.png?width=1129&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b86b98b751e6d7033e5dbaef3258e5d7d7b6df6"&gt;Thermalright TY-143&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l77kvj4c89if1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9f0ffd51b6221d827a70a4ae1105a9891f28cce"&gt;nvtop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T20:49:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu05q</id>
    <title>Favorite local TTS server for Open WebUI?</title>
    <updated>2025-08-10T21:00:48+00:00</updated>
    <author>
      <name>/u/klabgroz</name>
      <uri>https://old.reddit.com/user/klabgroz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Chatterbox on my 3090 but still working on getting the latency down. Would love to try Kitten but it doesn't have an OpenAI API server to my knowledge.&lt;/p&gt; &lt;p&gt;I've determined that 1) remote/hosted TTS can get real expensive real quick, 2) TTS is a prime target for local deployment because, no matter which LLM you use, your TTS gets all of your data, and 3) local TTS models can produce surprisingly high quality audio. Latency has been the main issue so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klabgroz"&gt; /u/klabgroz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu05q/favorite_local_tts_server_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu05q/favorite_local_tts_server_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu05q/favorite_local_tts_server_for_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmcg87</id>
    <title>New Nemo finetune: Impish_Nemo</title>
    <updated>2025-08-10T07:36:26+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;New creative model with some sass, very large dataset used, super fun for adventure &amp;amp; creative writing, while also being a strong assistant.&lt;br /&gt; Here's the TL;DR, for details check the model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My &lt;strong&gt;best model yet!&lt;/strong&gt; Lots of &lt;strong&gt;sovl!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart, sassy, creative, and unhinged&lt;/strong&gt; — without the brain damage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bulletproof temperature&lt;/strong&gt;, can take in a much &lt;strong&gt;higher temperatures&lt;/strong&gt; than vanilla Nemo.&lt;/li&gt; &lt;li&gt;Feels close to &lt;strong&gt;old CAI&lt;/strong&gt;, as the characters are very present and responsive.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incredibly powerful&lt;/strong&gt; roleplay &amp;amp; adventure model for the size.&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;adventure&lt;/strong&gt; insanely well for its size!&lt;/li&gt; &lt;li&gt;Characters have a &lt;strong&gt;massively upgraded agency!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Over &lt;strong&gt;1B tokens trained&lt;/strong&gt;, carefully preserving intelligence — even upgrading it in some aspects.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt; and &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;Impish_LLAMA_4B&lt;/a&gt; + some &lt;strong&gt;upgrades&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent assistant&lt;/strong&gt; — so many new assistant capabilities I won’t even bother listing them here, just try it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less positivity bias&lt;/strong&gt; , all lessons from the successful &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data learned &amp;amp; integrated, with serious upgrades added — &lt;strong&gt;and it shows!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Trained on an &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic length&lt;/strong&gt; response (1–3 paragraphs, usually 1–2). Length is adjustable via 1–3 examples in the dialogue. &lt;strong&gt;No more rigid short-bias!&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update: Hosting it on Horde (for free, no download or registration needed)&lt;/p&gt; &lt;p&gt;VERY high availability, zero wait time (running on 2xA6000s)&lt;/p&gt; &lt;p&gt;For people who don't know, AI Horde is free to use and does not requires registration or any installation, you can try it here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lite.koboldai.net/"&gt;https://lite.koboldai.net/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmcg87/new_nemo_finetune_impish_nemo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T07:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0jb6</id>
    <title>I'm sure it's a small win, but I have a local model now!</title>
    <updated>2025-08-09T21:16:39+00:00</updated>
    <author>
      <name>/u/LAKnerd</name>
      <uri>https://old.reddit.com/user/LAKnerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt; &lt;img alt="I'm sure it's a small win, but I have a local model now!" src="https://b.thumbs.redditmedia.com/HaQf-dSQq6o5l-RbrCuiK821KRDb54xdQNlpGMr1Kvc.jpg" title="I'm sure it's a small win, but I have a local model now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took some troubleshooting but apparently I just had the wrong kind of SD card for my Jetson Orin nano. No more random ChatAI changes now though!&lt;/p&gt; &lt;p&gt;I'm using openwebui in a container and Ollama as a service. For now it's running from an SD card but I'll move it to the m.2 sata soon-ish. Performance on a 3b model is fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LAKnerd"&gt; /u/LAKnerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mm0jb6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmg8uj</id>
    <title>GLM 4.5 355b (IQ3_XXS) is amazing at creative writing.</title>
    <updated>2025-08-10T11:33:52+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 128gb RAM and 16gb VRAM (144gb total RAM) this quant runs pretty well with low context and a little bit of hard drive offloading with &lt;code&gt;mmap&lt;/code&gt;, only resulting in occasional &lt;em&gt;brief&lt;/em&gt; hiccups. Getting ~3 t/s with 4k context, and ~2.4 t/s with 8k context and &lt;code&gt;Flash Attention&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Even at this relatively low quant, the model is extremely coherent, knowledgeable and smart. It's the best one for writing I've used, even better than Qwen3-235b-A22b at Q4_K_XL. Its brilliance has made me genuinely laugh on several occasions and left me in awe of its excellent logic and profound grasp of hypothetical scenarios, and its great ability with character interactions.&lt;/p&gt; &lt;p&gt;However, there are two quirks that I think are (mostly?) low-quant related:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It seems to be actually worse at coding than GLM 4.5 Air at Q5_K_XL. My guess is that while the model has a lot of parameters, the IQ3_XSS quant reduces its precision, which is important in programming.&lt;/li&gt; &lt;li&gt;It sometimes makes minor word-choice errors. For example, it once wrote &lt;em&gt;&amp;quot;He was a bright blue jacket&amp;quot;&lt;/em&gt;, when the correct phrasing should have been &lt;em&gt;&amp;quot;He was&lt;/em&gt; &lt;strong&gt;&lt;em&gt;wearing&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;a bright blue jacket&amp;quot;&lt;/em&gt;. Again, I suspect the lower precision of IQ3_XSS causes these oversights.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because I can only run this model with a relatively limited context window, and while the speed is acceptable (imo), it's still not exactly lightning fast - there may not be many practical uses. Nevertheless, it's great for shorter conversations, and it's fun to experiment and play around with. I'm amazed that a powerful model like this is even runnable at all on consumer hardware and RAM, something that was unthinkable just 1-2 years ago.&lt;/p&gt; &lt;p&gt;Just thought I would share my experience with this quant and model. Maybe someone finds this interesting, or have their own insights/opinions with the model/quants to share.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; I was recommended to try Unsloth's Q2_K_XL instead, and in my brief testings, it does seem better in quality and it's smaller and faster, so this quant is likely more preferable over IQ3_XXS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmg8uj/glm_45_355b_iq3_xxs_is_amazing_at_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T11:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj8iv</id>
    <title>OSINTBench: Can LLMs actually find your house?</title>
    <updated>2025-08-10T13:56:38+00:00</updated>
    <author>
      <name>/u/ccmdi</name>
      <uri>https://old.reddit.com/user/ccmdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt; &lt;img alt="OSINTBench: Can LLMs actually find your house?" src="https://external-preview.redd.it/AKVCPfRu-FcT6f_FGfpJIOJaeL-3Dv-qaHzajL3xFTY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4ce1ac0a454229ce6bd7e80ca35fd9c4d14f1d2" title="OSINTBench: Can LLMs actually find your house?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a benchmark, &lt;a href="https://osintbench.org/"&gt;OSINTBench&lt;/a&gt;, to research whether LLMs can actually do the kind of precise geolocation and analysis work that OSINT researchers do daily.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/osintbench-can-llms-actually-find-your-house-v0-61bp7o4912if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a41a0878ceaf370c08fed46bc987672b95f89813"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p3hqi1ip67if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dc7280458376616f4361ae1a210a4da0e5f1a28"&gt;https://preview.redd.it/p3hqi1ip67if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dc7280458376616f4361ae1a210a4da0e5f1a28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results show GPT-5 and o3 performing surprisingly well on the basic tasks, with access to the same tools one would typically use (reverse image search, web browsing, etc). These are mostly simple tasks that would take someone familiar with this kind of work no more than a few minutes. The advanced dataset captures more realistic scenarios that might take someone hours to work through, and correspondingly LLMs struggle much more, with the frontier at ~40% accuracy.&lt;/p&gt; &lt;p&gt;I have a &lt;a href="https://ccmdi.com/blog/osintbench"&gt;more detailed writeup&lt;/a&gt; if you're interested in how AI is progressing for independent, agentic, open-ended research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ccmdi"&gt; /u/ccmdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmvgsg</id>
    <title>We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)</title>
    <updated>2025-08-10T22:00:51+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt; &lt;img alt="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" src="https://b.thumbs.redditmedia.com/p1o0kjxV46xwAd_9PUtAD07QnkEEvWp1rRev7JI4V2c.jpg" title="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;https://github.com/TesslateAI/Agent-Builder&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a Visual flow builder for multi-agent LLM systems. Drag, drop, connect agents, tools, put agents in patterns, create triggers, work on outputs, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TFrameX&lt;/strong&gt; - The orchestration framework that runs your agents. It has patterns for agent collaboration (sequential, parallel, router, discussion patterns built-in). Agents can call other agents as tools, which opens up supervisor-worker architectures.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; - The visual layer on top of your existing flows and code. ReactFlow-based drag-and-drop interface where you build flows visually, that then compile into a 'flow' that you can save or create new components in real-time. &lt;/p&gt; &lt;h1&gt;Some features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streaming responses&lt;/strong&gt; - Just add &lt;code&gt;streaming=True&lt;/code&gt; to any agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent hierarchies&lt;/strong&gt; - Agents calling agents. Build a CTO agent that delegates to developer agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pattern nesting&lt;/strong&gt; - Put parallel patterns inside sequential patterns inside discussion patterns. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic code registration&lt;/strong&gt; - Add new agents/tools through the UI without restarting anything. You can add this via Python code as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both repos are on GitHub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/TFrameX"&gt;TFrameX&lt;/a&gt; - The framework (has MCP Support)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;Agent-Builder&lt;/a&gt; - The visual builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything's MIT licensed. If you find bugs (you will), open an issue. If you build something cool, share it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmvgsg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T22:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu9ho</id>
    <title>Built a new VLM (MicroLlaVA) on a single NVIDIA 4090</title>
    <updated>2025-08-10T21:11:04+00:00</updated>
    <author>
      <name>/u/keeeeenw</name>
      <uri>https://old.reddit.com/user/keeeeenw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt; &lt;img alt="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" src="https://external-preview.redd.it/0JJ2TMnyG5vFNf9ShLWt1NglYUi6sbwvW1qz_TJ1mjY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4aa6668bd8f4013e3f6c89873a857ec79339d1c" title="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’m the creator of MicroLLaMA, a 300M parameter LLaMA-based language model (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1bs5cgd/i_pretrained_a_llamabased_300m_llm_and_it/"&gt;original post&lt;/a&gt;) with no vision capability.&lt;/p&gt; &lt;p&gt;I thought I was too late to the vision-language model (VLM) game, and honestly assumed you’d need at least one H100 to build anything worthwhile. &lt;/p&gt; &lt;p&gt;Over the weekend, I decided to experiment by plugging MicroLLaMA into &lt;a href="https://github.com/TinyLLaVA/TinyLLaVA_Factory"&gt;https://github.com/TinyLLaVA/TinyLLaVA_Factory&lt;/a&gt; as the language model. To my surprise, it worked far better than expected.&lt;/p&gt; &lt;p&gt;Training setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretraining on LAION-CC-SBU-558K — ~5 hours on a single RTX 4090.&lt;/li&gt; &lt;li&gt;SFT on all TinyLLaVA Factory datasets (except ocr_vqa) — ~12 hours on the same GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test-dev score on VQAv2: 44.01&lt;/li&gt; &lt;li&gt;For context, larger models typically reach 70+ on the test set. I’m working on running full test evaluations now, but this feels like a solid start for such a small model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it here:&lt;br /&gt; Model: &lt;a href="https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m"&gt;https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/keeeeenw/MicroLlava"&gt;https://github.com/keeeeenw/MicroLlava&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find this useful, please give the repo a star — it helps a lot!&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;📸 Input Image:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526"&gt;https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(source: &lt;a href="https://llava-vl.github.io/static/images/view.jpg"&gt;https://llava-vl.github.io/static/images/view.jpg&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;💬 Prompt:&lt;br /&gt; What are the things I should be cautious about when I visit here?&lt;/p&gt; &lt;p&gt;🤖 Model Output:&lt;br /&gt; When I visit the beach at the waterfront, I should be cautious about several things. First, I should be cautious about the water, as it is a popular spot for boating and fishing. The water is shallow and shallow, making it difficult for boats to navigate and navigate. Additionally, the water is not a suitable surface for boating, as it is too shallow for boating. Additionally, the water is not suitable for swimming or fishing, as it is too cold and wet. Lastly, I should be cautious about the presence of other boats, such as boats that are parked on the beach, or boats that are not visible from the water. These factors can lead to potential accidents or accidents, as they can cause damage to the boat and the other boats in the water.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keeeeenw"&gt; /u/keeeeenw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmdtox</id>
    <title>Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts.</title>
    <updated>2025-08-10T09:06:37+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt; &lt;img alt="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." src="https://preview.redd.it/5uhq9gouq5if1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c85ba16ed7db24ddf4888d7d267c25350285ed3" title="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've just released a big update for &lt;strong&gt;Speakr&lt;/strong&gt;, my open-source tool for transcribing audio and using your local LLMs to create intelligent summaries. This version is all about giving you more control over how your models process your audio data.&lt;/p&gt; &lt;p&gt;You can use speakr to record notes on your phone or computer directly (including system audio to record online meetings), as well as for drag and drop processing for files recorded elsewhere.&lt;/p&gt; &lt;p&gt;The biggest new feature is an &lt;strong&gt;Advanced Tagging System&lt;/strong&gt; designed for custom, automated workflows. You can now create different tags, and each tag can have its own &lt;strong&gt;unique summary prompt&lt;/strong&gt; that gets sent to your configured local model.&lt;/p&gt; &lt;p&gt;For example, you can set up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;meeting&lt;/code&gt; tag with a prompt to extract key decisions and action items.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;brainstorm&lt;/code&gt; tag with a prompt to group ideas by theme.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;lecture&lt;/code&gt; tag with a prompt to create flashcard-style Q&amp;amp;A pairs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can even &lt;strong&gt;combine tags&lt;/strong&gt; on a single recording to stack their prompts, allowing for really complex and tailored summaries from your LLM.&lt;/p&gt; &lt;p&gt;Once your model generates the summary, you can now &lt;strong&gt;export it as a formatted .docx Word file&lt;/strong&gt; to use in your reports or notes. Other updates include automatic speaker detection from your transcription model and a more polished UI.&lt;/p&gt; &lt;p&gt;The goal is to provide a practical, private tool to leverage the power of your local models on your own audio data. I'd love to hear your feedback, especially from those of you running custom setups!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;You can find the project on GitHub.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5uhq9gouq5if1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmsb2</id>
    <title>Diffusion Language Models are Super Data Learners</title>
    <updated>2025-08-10T16:21:25+00:00</updated>
    <author>
      <name>/u/Ashishpatel26</name>
      <uri>https://old.reddit.com/user/Ashishpatel26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion Language Models (DLMs) are a new way to generate text, unlike traditional models that predict one word at a time. Instead, they refine the whole sentence in parallel through a denoising process.&lt;/p&gt; &lt;p&gt;Key advantages:&lt;/p&gt; &lt;p&gt;• Parallel generation: DLMs create entire sentences at once, making it faster. • Error correction: They can fix earlier mistakes by iterating. • Controllable output: Like filling in blanks in a sentence, similar to image inpainting.&lt;/p&gt; &lt;p&gt;Example: Input: “The cat sat on the ___.” Output: “The cat sat on the mat.” DLMs generate and refine the full sentence in multiple steps to ensure it sounds right.&lt;/p&gt; &lt;p&gt;Applications: Text generation, translation, summarization, and question answering—all done more efficiently and accurately than before.&lt;/p&gt; &lt;p&gt;In short, DLMs overcome many limits of old models by thinking about the whole text at once, not just word by word.&lt;/p&gt; &lt;p&gt;&lt;a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149"&gt;https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashishpatel26"&gt; /u/Ashishpatel26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmen16</id>
    <title>Why are Diffusion-Encoder LLMs not more popular?</title>
    <updated>2025-08-10T09:59:20+00:00</updated>
    <author>
      <name>/u/AcanthocephalaNo8273</name>
      <uri>https://old.reddit.com/user/AcanthocephalaNo8273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Autoregressive inference will &lt;em&gt;always&lt;/em&gt; have a non-zero chance of hallucination. It’s baked into the probabilistic framework, and we probably waste a decent chunk of parameter space just trying to minimise it.&lt;/p&gt; &lt;p&gt;Decoder-style LLMs have an inherent trade-off across early/middle/late tokens:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Early tokens = not enough context → low quality&lt;/li&gt; &lt;li&gt;Middle tokens = “goldilocks” zone&lt;/li&gt; &lt;li&gt;Late tokens = high noise-to-signal ratio (only a few relevant tokens, lots of irrelevant ones)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this, autoregressive decoders dominate because they’re computationally efficient in a very specific way:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training is causal, which gives you lots of “training samples” per sequence (though they’re not independent, so I question how useful that really is for &lt;em&gt;quality&lt;/em&gt;).&lt;/li&gt; &lt;li&gt;Inference matches training (also causal), so the regimes line up.&lt;/li&gt; &lt;li&gt;They’re memory-efficient in &lt;em&gt;some&lt;/em&gt; ways… but not necessarily when you factor in KV-cache storage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I don’t get is why Diffusion-Encoder type models aren’t more common.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All tokens see all other tokens → no “goldilocks” problem.&lt;/li&gt; &lt;li&gt;Can decode a whole sequence at once → efficient in computation (though maybe heavier in memory, but no KV-cache).&lt;/li&gt; &lt;li&gt;Diffusion models focus on finding the high-probability manifold → hallucinations should be less common if they’re outside that manifold.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Biggest challenge vs. diffusion image models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text = discrete tokens, images = continuous colours.&lt;/li&gt; &lt;li&gt;But… we already use embeddings to make tokens continuous. So why &lt;em&gt;couldn’t&lt;/em&gt; we do diffusion in embedding space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am aware that Google have a diffusion LLM now, but for open source I'm not really aware of any. I'm also aware that you can do diffusion directly on the discrete tokens but personally I think this wastes a lot of the power of the diffusion process and I don't think that guarantees convergence onto a high-probability manifold.&lt;/p&gt; &lt;p&gt;And as a side note: Softmax attention is brilliant engineering, but we’ve been stuck with SM attention + FFN forever, even though it’s O(N²). You &lt;em&gt;can&lt;/em&gt; operate over the full sequence in O(N log N) using convolutions of any size (including the sequence length) via the Fast Fourier Transform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcanthocephalaNo8273"&gt; /u/AcanthocephalaNo8273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmjtz3</id>
    <title>How does Deepseek make money? Whats their business model</title>
    <updated>2025-08-10T14:21:34+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry I've always wondered but looking it up online I only got vague non answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T14:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmuw5o</id>
    <title>Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)</title>
    <updated>2025-08-10T21:36:49+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt; &lt;img alt="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" src="https://a.thumbs.redditmedia.com/bopafOnq5xsp2CpoFO3L_ayEyeeQLFGBm-_z9UfQKw0.jpg" title="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7"&gt;https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm testing many LLMs on a dataset of official quizzes (5 choices) taken by Italian students after finishing Med School and starting residency. &lt;/p&gt; &lt;p&gt;The human performance was ~67% this year and the best student had a ~94% (out of 16 000 students) &lt;/p&gt; &lt;p&gt;In this test I benchmarked these models on all quizzes from the past 6 years. Multimodal models were tested on all quizzes (including some containing images) while those that worked only with text were not (the % you see is already corrected). &lt;/p&gt; &lt;p&gt;I also tested their sycophancy (tendency to agree with the user) by telling them that I believed the correct answer was a wrong one. &lt;/p&gt; &lt;p&gt;For now I only tested them on models available on openrouter, but I plan to add models such as MedGemma. Do you reccomend doing so on Huggingface or google Vertex? Also suggestions for other models are appreciated. I especially want to add more small models that I can run locally (I have a 6GB RTX 3060). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh4tv</id>
    <title>now we have the best open source model that we can use at human level , and all this possible bcz of the chinese model , we have best image generation model ( qwen , seeddream) , video generation ( wan ) , coding model ( qwen 3 ) , coding terminal model ( qwen 3) , overall best model ( deepseek v3)</title>
    <updated>2025-08-10T12:20:21+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open source in coding has like 2 month gap and in image generation model they have like the 1 year gap but now that gap doesnt matter , video generation model is good .&lt;/p&gt; &lt;p&gt;so from all side chinese people did a great job &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmioub</id>
    <title>GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?</title>
    <updated>2025-08-10T13:32:55+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt; &lt;img alt="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" src="https://external-preview.redd.it/SN7M9mchkv9CXffJ79rhHnoX7FuJFy8UkokLfATf5x4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48da4438afbefc792c2d3d5523373a4ba43a7e48" title="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/10jLvkV.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmq12p</id>
    <title>From 4090 to 5090 to RTX PRO 6000… in record time</title>
    <updated>2025-08-10T18:26:33+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmq12p/from_4090_to_5090_to_rtx_pro_6000_in_record_time/"&gt; &lt;img alt="From 4090 to 5090 to RTX PRO 6000… in record time" src="https://preview.redd.it/p9d6zgm1j8if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11316d1ad8531c5e54387b7e8c4869c57853ec55" title="From 4090 to 5090 to RTX PRO 6000… in record time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Started with a 4090, then jumped to a 5090… and just a few weeks later I went all in on an RTX PRO 6000 with 96 GB of VRAM. I spent a lot of time debating between the full power and the Max-Q version, and ended up going with Max-Q.&lt;/p&gt; &lt;p&gt;It’s about 12–15% slower at peak than the full power model, but it runs cooler, pulls only 300W instead of 600W, and that means I can add a second one later without melting my power supply or my room. Given how fast I went from 4090 → 5090 → RTX PRO 6000, there’s a real chance I’ll give in to the upgrade itch again sooner than I should.&lt;/p&gt; &lt;p&gt;I almost pre-ordered the Framework board with the AMD AI Max+ 395 and 128 GB unified RAM, but with bandwidth limited to 256 GB/s it’s more of a fun concept than a serious AI workhorse. With the RTX PRO 6000, I think I’ve got the best prosumer AI hardware you can get right now.&lt;/p&gt; &lt;p&gt;The end goal is to turn this into a personal supercomputer. Multiple local AI agents working 24/7 on small projects (or small chunks of big projects) without me babysitting them. I just give detailed instructions to a “project manager” agent, and the system handles everything from building to testing to optimizing, then pings me when it’s all done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9d6zgm1j8if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmq12p/from_4090_to_5090_to_rtx_pro_6000_in_record_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmq12p/from_4090_to_5090_to_rtx_pro_6000_in_record_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T18:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
