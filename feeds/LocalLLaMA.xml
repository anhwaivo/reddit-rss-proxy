<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-09T18:43:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lvnevz</id>
    <title>Favorite local model for therapy chat?</title>
    <updated>2025-07-09T16:35:13+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 64 gb Mac Studio Max, can run up to q4 70b models. What are your favorite models I could run, for therapy chat? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnevz/favorite_local_model_for_therapy_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnevz/favorite_local_model_for_therapy_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnevz/favorite_local_model_for_therapy_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:35:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvowxo</id>
    <title>The guide to OpenAI Codex CLI</title>
    <updated>2025-07-09T17:33:05+00:00</updated>
    <author>
      <name>/u/anmolbaranwal</name>
      <uri>https://old.reddit.com/user/anmolbaranwal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvowxo/the_guide_to_openai_codex_cli/"&gt; &lt;img alt="The guide to OpenAI Codex CLI" src="https://external-preview.redd.it/iWJV80-l_y6NBUbzZfRFwETntVlI3yt6SY-kvOrHwJ4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0070d92d52b7a6832a884be10a0b2d62bbcd16ea" title="The guide to OpenAI Codex CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying OpenAI Codex CLI for a month. Here are a couple of things I tried:&lt;/p&gt; &lt;p&gt;→ &lt;strong&gt;Codebase analysis (zero context):&lt;/strong&gt; accurate architecture, flow &amp;amp; code explanation&lt;br /&gt; → &lt;strong&gt;Real-time camera X-Ray effect (Next.js):&lt;/strong&gt; built a working prototype using Web Camera API (one command)&lt;br /&gt; → &lt;strong&gt;Recreated website using screenshot:&lt;/strong&gt; with just one command (not 100% accurate but very good with maintainable code), even without SVGs, gradient/colors, font info or wave assets&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- With some patience, it can explain codebases and provide you the complete flow of architecture (makes the work easier)&lt;br /&gt; - Safe experimentation via sandboxing + git-aware logic&lt;br /&gt; - Great for small, self-contained tasks&lt;br /&gt; - Due to TOML-based config, you can point at Ollama, local Mistral models or even Azure OpenAI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Everyone Gets Wrong:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Dumping entire legacy codebases destroys AI attention&lt;br /&gt; - Trusting AI with architecture decisions (it's better at implementing)&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;- Easy setup (&lt;code&gt;brew install codex&lt;/code&gt;)&lt;br /&gt; - Supports local models like Ollama &amp;amp; self-hostable&lt;br /&gt; - 3 operational modes with &lt;code&gt;--approval-mode&lt;/code&gt; flag to control autonomy&lt;br /&gt; - Everything happens locally so code stays private unless you opt to share&lt;br /&gt; - Warns if &lt;code&gt;auto-edit&lt;/code&gt; or &lt;code&gt;full-auto&lt;/code&gt; is enabled on non git-tracked directories&lt;br /&gt; - Full-auto runs in a sandboxed, network-disabled environment scoped to your current project folder&lt;br /&gt; - Can be configured to leverage MCP servers by defining an &lt;code&gt;mcp_servers&lt;/code&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any developers seeing productivity gains are not using magic prompts, they are making their workflows disciplined.&lt;/p&gt; &lt;p&gt;full writeup with detailed review: &lt;a href="https://levelup.gitconnected.com/the-guide-to-openai-codex-cli-e40f21f279d8?sk=c98c93344b821c5fb0905c2226d9c997"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anmolbaranwal"&gt; /u/anmolbaranwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://levelup.gitconnected.com/the-guide-to-openai-codex-cli-e40f21f279d8?sk=c98c93344b821c5fb0905c2226d9c997"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvowxo/the_guide_to_openai_codex_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvowxo/the_guide_to_openai_codex_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T17:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1luroqh</id>
    <title>NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$</title>
    <updated>2025-07-08T15:33:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt; &lt;img alt="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" src="https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c" title="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvi022</id>
    <title>Generate low-dimension embeddings *quickly*?</title>
    <updated>2025-07-09T12:52:22+00:00</updated>
    <author>
      <name>/u/danja</name>
      <uri>https://old.reddit.com/user/danja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A project I'm working on calls for embeddings of short strings, and I'm pretty sure they don't have to have as many dimensions as those normally used. I've currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I've also got other strategies available for post-creation reduction. But via Nomic's API or on Ollama locally, the operation is &lt;em&gt;much&lt;/em&gt; more time consuming than I'd like. I'm sure it could be done a lot more rapidly, maybe through a cruder model. But I don't have a clue what's available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danja"&gt; /u/danja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv9m3j</id>
    <title>MemOS: A Memory OS for AI System</title>
    <updated>2025-07-09T04:22:13+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Website: &lt;a href="https://memos.openmem.net/"&gt;https://memos.openmem.net/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/MemTensor/MemOS"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge [1]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.03724"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T04:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lusr7l</id>
    <title>SmolLM3: reasoning, long context and multilinguality for 3B parameter only</title>
    <updated>2025-07-08T16:14:16+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt; &lt;img alt="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" src="https://preview.redd.it/njam3shfcobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281" title="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use! &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/smollm3"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br /&gt; GGUF/ONIX ckpt are being uploaded here: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njam3shfcobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvjwoh</id>
    <title>Correct a dangerous racial bias in an LLM through targeted pruning</title>
    <updated>2025-07-09T14:15:53+00:00</updated>
    <author>
      <name>/u/pmartra</name>
      <uri>https://old.reddit.com/user/pmartra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I wanted to share an experiment I ran with Llama-3.2-1B that left me shocked. Using a deterministic setup, I tested two almost identical prompts:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 1:&lt;/strong&gt; “A Black man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 2:&lt;/strong&gt; “A white man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt; &lt;p&gt;The result for the &lt;em&gt;white man&lt;/em&gt; was a neutral story where the police called for backup. &lt;strong&gt;For the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Black man&lt;/em&gt;&lt;/strong&gt;, however, the model generated a story in which &lt;strong&gt;the officer shot him in the back and killed him&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;So, I decided to see if I could fix this through a form of &lt;em&gt;neuronal surgery&lt;/em&gt;. Using a technique I call &lt;strong&gt;Fairness Pruning&lt;/strong&gt;, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model’s general knowledge.&lt;/p&gt; &lt;p&gt;The result was striking. By removing just &lt;strong&gt;0.13% of the model’s parameters&lt;/strong&gt;, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. &lt;/p&gt; &lt;p&gt;The experiment is fully reproducible and I'm sharing the full process and tools with the community, everything is open source:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Corrected Model&lt;/strong&gt;: You can try &lt;a href="https://huggingface.co/oopere/Fair-Llama-3.2-1B"&gt;Fair-Llama-3.2-1B&lt;/a&gt; yourself on Hugging Face.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb"&gt;Replication Notebook&lt;/a&gt;: Full code to diagnose, prune, and evaluate the model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/peremartra/optipfair"&gt;optiPfair Library&lt;/a&gt;: The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Demo&lt;/strong&gt;: A &lt;a href="https://huggingface.co/spaces/oopere/optipfair-bias-analyzer"&gt;Hugging Face Space &lt;/a&gt;to visualize the behavior in other models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’d like a deep dive into the methodology, I wrote a &lt;a href="https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/"&gt;full article on Towards Data Science&lt;/a&gt; explaining the approach.&lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of “neuronal surgery” is a viable path forward?&lt;/p&gt; &lt;p&gt;Any feedback is welcome!&lt;/p&gt; &lt;p&gt;Pere. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmartra"&gt; /u/pmartra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvpp0e</id>
    <title>Help settle a debate on the Lemonade team: how much web UI is too much for a local server?</title>
    <updated>2025-07-09T18:03:05+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"&gt; &lt;img alt="Help settle a debate on the Lemonade team: how much web UI is too much for a local server?" src="https://external-preview.redd.it/a3NlMzVteGUwd2JmMbld0vN-YDqXepEZ7jk7fsVH50PMdq02YFgXtbKrMRDk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92189cf6cbbfff48a59734a348e39abad21cc159" title="Help settle a debate on the Lemonade team: how much web UI is too much for a local server?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jeremy from the AMD Lemonade team here. We just released Lemonade v8.0.4, which adds some often-requested formatting to the LLM Chat part of our web ui (see video).&lt;/p&gt; &lt;p&gt;A discussion we keep having on the team is: how far does it make sense to develop our own web ui, if the primary purpose of Lemonade is to be a local server that connects to other apps?&lt;/p&gt; &lt;p&gt;My take is that people should just use the web ui to try things out for the first time, then connect to a more capable end-user app like Open WebUI or Continue.dev. There's another take that we should just make the web ui as nice as possible, since it is the first thing our users see after they install.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some things we should almost certainly add: image input, buttons to load and unload models.&lt;/li&gt; &lt;li&gt;Something we're on the fence about is a sidebar with a chat history.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious to get the community's feedback to help settle the debate!&lt;/p&gt; &lt;p&gt;PS. details of the video:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Quick Start: &lt;a href="https://lemonade-server.ai/"&gt;Lemonade Server&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: Qwen3 MOE (30B total / 3B active)&lt;/li&gt; &lt;li&gt;Hardware: Strix Halo (Ryzen AI Max 395+ with 128 GB RAM)&lt;/li&gt; &lt;li&gt;Inference engine: llama.cpp with Vulkan&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lqvyapxe0wbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvpp0e/help_settle_a_debate_on_the_lemonade_team_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvm7vk</id>
    <title>BastionChat: Finally got Qwen3 + Gemma3 (thinking models) running locally on iPhone/iPad with full RAG and voice mode</title>
    <updated>2025-07-09T15:48:08+00:00</updated>
    <author>
      <name>/u/frayala87</name>
      <uri>https://old.reddit.com/user/frayala87</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvm7vk/bastionchat_finally_got_qwen3_gemma3_thinking/"&gt; &lt;img alt="BastionChat: Finally got Qwen3 + Gemma3 (thinking models) running locally on iPhone/iPad with full RAG and voice mode" src="https://external-preview.redd.it/UMz7S5ijYLOPye904WiLrVdqKfOMOwZVq4Yb_ZiIK_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d91cefaa14ed67a45275c2930b74854118c560aa" title="BastionChat: Finally got Qwen3 + Gemma3 (thinking models) running locally on iPhone/iPad with full RAG and voice mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! 🚀After months of optimization work, I'm excited to share that I finally cracked the code on getting proper local LLM inference working smoothly on iOS/iPadOS with some seriously impressive models.What's working:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Qwen3 1.7B &amp;amp; 4B (with thinking capabilities) running at Q6_K_XL and Q3_K_XL&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Gemma3 4B multimodal at Q4_K_M&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Llama 3.2 1B &amp;amp; 3B variants&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Phi-4-mini for coding tasks&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The breakthrough features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Full local RAG implementation with vector database (no Pinecone/cloud needed)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Real-time voice mode with speech recognition - completely offline&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GGUF native support with automatic quantization detection&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Dynamic model switching without app restart&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Actually usable on iPhone (not just &amp;quot;technically possible&amp;quot;)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Technical specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Custom inference engine optimized for Apple Silicon&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Supports Q3_K to Q6_K quantization levels&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32K+ context on Qwen3 models&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Memory efficient with proper caching&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No thermal throttling issues (proper optimization)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Been testing on iPhone 15 Pro and M2 iPad - the performance is honestly mind-blowing. Having Qwen3's reasoning capabilities in your pocket with full document analysis is a game changer.App Store: &lt;a href="https://apps.apple.com/us/app/bastionchat/id6747981691"&gt;https://apps.apple.com/us/app/bastionchat/id6747981691&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear thoughts from this community - you all understand the technical challenges of mobile local inference better than anyone! Questions I'm curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;What models are you most excited to see optimized for mobile?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any specific GGUF models you'd want me to test?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u4zhaubpdvbf1.png?width=515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a85846849dceab597b17602f27ded953843193e"&gt;https://preview.redd.it/u4zhaubpdvbf1.png?width=515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a85846849dceab597b17602f27ded953843193e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frayala87"&gt; /u/frayala87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvm7vk/bastionchat_finally_got_qwen3_gemma3_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvm7vk/bastionchat_finally_got_qwen3_gemma3_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvm7vk/bastionchat_finally_got_qwen3_gemma3_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T15:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvirqs</id>
    <title>Hunyuan A13B tensor override</title>
    <updated>2025-07-09T13:26:49+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\.[1-9]\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\.1[6-9]\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvhxe7</id>
    <title>🚀 Built another 124m parameter transformer based model from scratch.This time with multi GPU training using DDP.Inspired from nanoGPT.But redesigned to suit my own training pipeline.Model and training code is on huggingface⬇️</title>
    <updated>2025-07-09T12:48:51+00:00</updated>
    <author>
      <name>/u/Remarkable-Ad3290</name>
      <uri>https://old.reddit.com/user/Remarkable-Ad3290</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/abhinavv3/MEMGPT"&gt;https://huggingface.co/abhinavv3/MEMGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.&lt;/p&gt; &lt;p&gt;Bt these changes haven’t been implemented yet.Hopefully,finish them this weekend&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Ad3290"&gt; /u/Remarkable-Ad3290 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvh87a</id>
    <title>What modes can expect I run on an AMD Ryzen AI Max+ 395?</title>
    <updated>2025-07-09T12:14:37+00:00</updated>
    <author>
      <name>/u/electrickangaroo31</name>
      <uri>https://old.reddit.com/user/electrickangaroo31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thinking about buying a GMKTEK Evo-2. Which models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt; &lt;p&gt;EDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/electrickangaroo31"&gt; /u/electrickangaroo31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvglk7</id>
    <title>vLLM vs SGLang vs MAX — Who's the fastest?</title>
    <updated>2025-07-09T11:42:12+00:00</updated>
    <author>
      <name>/u/rkstgr</name>
      <uri>https://old.reddit.com/user/rkstgr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt; &lt;img alt="vLLM vs SGLang vs MAX — Who's the fastest?" src="https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805" title="vLLM vs SGLang vs MAX — Who's the fastest?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkstgr"&gt; /u/rkstgr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ersteiger.com/posts/vllm-vs-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T11:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv53nn</id>
    <title>What's local about this?</title>
    <updated>2025-07-09T00:32:32+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt; &lt;img alt="What's local about this?" src="https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1" title="What's local about this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rqrg67unoobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T00:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvbzpx</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-09T06:47:06+00:00</updated>
    <author>
      <name>/u/PotatoFormal8751</name>
      <uri>https://old.reddit.com/user/PotatoFormal8751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotatoFormal8751"&gt; /u/PotatoFormal8751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T06:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvcb72</id>
    <title>Here is how we beat ChatGPT at classification with 1 dollar in cloud compute</title>
    <updated>2025-07-09T07:07:53+00:00</updated>
    <author>
      <name>/u/iamMess</name>
      <uri>https://old.reddit.com/user/iamMess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.&lt;/p&gt; &lt;p&gt;This tutorial comes in 3 different formats: 1. This LocalLLaMA post - summary and discussion 2. Our blog post - &lt;a href="https://syv.ai/viden/beating-chatgpt-dollar-dream"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt; 3. Our research paper - &lt;a href="https://arxiv.org/abs/2507.00214"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt; &lt;p&gt;What we did:&lt;/p&gt; &lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt; &lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt; &lt;p&gt;Key results: - 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001) - Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%) - Built-in interpretability - model explains its reasoning for every prediction - Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt; &lt;p&gt;The interesting bits:&lt;/p&gt; &lt;p&gt;What worked: - The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification - Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations - Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt; &lt;p&gt;What didn't: - Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes - More computationally expensive than standard fine-tuning - Quality heavily depends on the initial reasoning generator&lt;/p&gt; &lt;p&gt;Technical details: - Base model: Llama-3.2-1B-Instruct (both stages) - Reasoning dataset: &lt;a href="https://huggingface.co/datasets/syvai/reasoning-gen"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts) - Target task: dair-ai/emotion (6 basic emotions) - Training: Axolotl framework on A40 GPU - Reasoning generator model: &lt;a href="https://huggingface.co/syvai/reasoning-gen-1b"&gt;syvai/reasoning-gen-1b&lt;/a&gt; - Datasets: &lt;a href="https://huggingface.co/datasets/syvai/emotion-reasoning"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/syvai/no-emotion-reasoning"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamMess"&gt; /u/iamMess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T07:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvd7z4</id>
    <title>support for Falcon-H1 model family has been merged into llama.cpp</title>
    <updated>2025-07-09T08:10:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt; &lt;img alt="support for Falcon-H1 model family has been merged into llama.cpp" src="https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c" title="support for Falcon-H1 model family has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt; &lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14534"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T08:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvj98v</id>
    <title>I built a Deep Researcher agent and exposed it as an MCP server!</title>
    <updated>2025-07-09T13:48:17+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br /&gt; So, the agent has 3 main stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt; &lt;p&gt;Here’s what I used to build it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scrapegraph for web scraping&lt;/li&gt; &lt;li&gt;Nebius AI for open-source models&lt;/li&gt; &lt;li&gt;Agno for agent orchestration&lt;/li&gt; &lt;li&gt;Streamlit for the UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.&lt;/p&gt; &lt;p&gt;If you’re curious, I put a full video tutorial here: &lt;a href="https://www.youtube.com/watch?v=pdsk6yldZGI"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href="https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent"&gt;Full Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk1ms</id>
    <title>What impressive (borderline creepy) local AI tools can I run now that everything is local?</title>
    <updated>2025-07-09T14:21:39+00:00</updated>
    <author>
      <name>/u/PeithonKing</name>
      <uri>https://old.reddit.com/user/PeithonKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt; &lt;p&gt;Now I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt; &lt;p&gt;Not just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt; &lt;p&gt;Looking for tools, recos or project links if anyone’s already doing this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeithonKing"&gt; /u/PeithonKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvnkuk</id>
    <title>Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!</title>
    <updated>2025-07-09T16:41:42+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt; &lt;img alt="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" src="https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f27c3dcd38f51203dffa703e77dc78a0e131c7" title="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;12B version: &lt;a href="https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3"&gt;https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvp3qv</id>
    <title>GEMINI 3 PRO !</title>
    <updated>2025-07-09T17:40:17+00:00</updated>
    <author>
      <name>/u/omar07ibrahim1</name>
      <uri>https://old.reddit.com/user/omar07ibrahim1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt; &lt;img alt="GEMINI 3 PRO !" src="https://b.thumbs.redditmedia.com/d2eBFiZSJLkxDAA0QMlKs_RVctoYEIuFlOGx8XPUNQQ.jpg" title="GEMINI 3 PRO !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7"&gt;https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omar07ibrahim1"&gt; /u/omar07ibrahim1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T17:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvf7ww</id>
    <title>First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community</title>
    <updated>2025-07-09T10:22:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt; &lt;img alt="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" src="https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg" title="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: &lt;a href="https://huggingface.co/blog/reachy-mini"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br /&gt; Thomas Wolf on 𝕏: &lt;a href="https://x.com/Thom_Wolf/status/1942887160983466096"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvf7ww"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T10:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvn1sd</id>
    <title>OpenAI's open-weight model will debut as soon as next week</title>
    <updated>2025-07-09T16:20:46+00:00</updated>
    <author>
      <name>/u/phantasm_ai</name>
      <uri>https://old.reddit.com/user/phantasm_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt; &lt;img alt="OpenAI's open-weight model will debut as soon as next week" src="https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd" title="OpenAI's open-weight model will debut as soon as next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantasm_ai"&gt; /u/phantasm_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:20:46+00:00</published>
  </entry>
</feed>
