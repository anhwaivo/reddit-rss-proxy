<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-12T07:34:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hzh274</id>
    <title>Dell Computer Recommendations</title>
    <updated>2025-01-12T06:55:46+00:00</updated>
    <author>
      <name>/u/vincewit</name>
      <uri>https://old.reddit.com/user/vincewit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you a budget of 2,000 to 3,000 US$ to purchase a system, BUT YOU MUST purchase from Dell (in the near future) what system would you recommend as a Win 11 Workstation hosting and experimenting with ai models locally? Having trouble mapping out dell's pc lineup. &lt;/p&gt; &lt;p&gt;Initially the AI would be used to help process, search, summarize, cross-reference and analyze hundreds of documents/archives using some sort of to-be-determined RAG system. We would then move forward using the system to help transcribe and index audio interviews, better process and index documents we scan as well as photos of objects.&lt;/p&gt; &lt;p&gt;It would also be used for general/short and long form generative AI, if possible using the library outlined above.&lt;/p&gt; &lt;p&gt;I realize it is probably far better sourcing and building your own system, but we are locked into using dell for a number of reasons. &lt;/p&gt; &lt;p&gt;I have been looking at dell’s AI ready OptiPlex’s and Precision line workstations. I am not well versed with their line-up but it seems possible to get a PC with 14&lt;sup&gt;th&lt;/sup&gt; gen I7, 32 gigs of ram and a video card with 8 or 12 gigs of vram. For&lt;/p&gt; &lt;p&gt;It have been looking for a precision with the an thunderbolt or occulink so we might be able to later add a egpu without messing with dell’s warranty, but I have not hat much luck. I realize that would introduce a bottleneck. It is also hard to determine what systems, have additional slots to add a second GPU. I understand how that works in theory, but I do not know if I would trust myself to do it. In most cases I think I would then need to upgrade the power supply.&lt;/p&gt; &lt;p&gt;Any recommendations would be welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vincewit"&gt; /u/vincewit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzh274/dell_computer_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzh274/dell_computer_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzh274/dell_computer_recommendations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T06:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz1a01</id>
    <title>48gb vs 96gb VRAM for fine-tuning</title>
    <updated>2025-01-11T17:43:37+00:00</updated>
    <author>
      <name>/u/salec65</name>
      <uri>https://old.reddit.com/user/salec65</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A popular configuration for local hosting is 2x 24gb GPUs (3090's for example). This would let me access LLama3.3 Q4 which I find myself using often. In order to go beyond this, one either has to add more consumer GPUs, which gets tricky in standard desktops or rackmount cases, or switch to workstation/server GPUs where they can be more efficiently packed in.&lt;/p&gt; &lt;p&gt;For someone that is about to start really getting into fine-tuning models but hasn't quite understood when to use QLoRA/LoRA/FFT as well as starting to use larger prompts, I am curious whether it'd be worth it (or necessary) to go the extra mile and get myself setup for 96gb or more.&lt;/p&gt; &lt;p&gt;Some of my goals include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Code/Data generation - Generating documents w/ a specific syntax (xml-ish) based on prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Domain specific Q&amp;amp;A&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creative personas and characters for unique dialog&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While I plan to spin up instances of 2x 3090 and 2x A6000 to see for myself, I am very interested to hear from others with experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salec65"&gt; /u/salec65 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz1a01/48gb_vs_96gb_vram_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzc2rp</id>
    <title>What is the best model for writing academic papers?</title>
    <updated>2025-01-12T01:59:58+00:00</updated>
    <author>
      <name>/u/PuzzleheadedPitch316</name>
      <uri>https://old.reddit.com/user/PuzzleheadedPitch316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing an academic paper on Economics and I would like help from AI when writing by giving instructions and being able to adapt APA&lt;/p&gt; &lt;p&gt;I am looking for a model that writes in academic language, like for a thesis. I have a 4060 ti 16gb vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuzzleheadedPitch316"&gt; /u/PuzzleheadedPitch316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzc2rp/what_is_the_best_model_for_writing_academic_papers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzc2rp/what_is_the_best_model_for_writing_academic_papers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzc2rp/what_is_the_best_model_for_writing_academic_papers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T01:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyukc2</id>
    <title>GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;D</title>
    <updated>2025-01-11T12:09:03+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt; &lt;img alt="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" src="https://external-preview.redd.it/2MaaUSNtf5DLbq6ZpF876OWYQdcOtASsj6e_pAKWpKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec156925c4109c28028bb52b1517ca8eb977cd5a" title="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tegridydev/dnd-llm-game?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzcu0a</id>
    <title>ML Fundamentals / LLMs Certificate</title>
    <updated>2025-01-12T02:40:54+00:00</updated>
    <author>
      <name>/u/ke7cfn</name>
      <uri>https://old.reddit.com/user/ke7cfn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm wondering is there any modern certificate program on ML Fundamentals which includes in depth detail into LLMs that seems as a good foundation for a software / data / infrastructure engineer to find related work ??&lt;/p&gt; &lt;p&gt;I know there were historically many ML related coursework from the past 15-20 years or so but that older courses may not include a more specific focus on LLMs. I'm interested in a Certificate that is modern and hopefully in the affordable domain..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ke7cfn"&gt; /u/ke7cfn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcu0a/ml_fundamentals_llms_certificate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcu0a/ml_fundamentals_llms_certificate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcu0a/ml_fundamentals_llms_certificate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz50c0</id>
    <title>Denser Reward for RLHF PPO Training</title>
    <updated>2025-01-11T20:27:42+00:00</updated>
    <author>
      <name>/u/Leading-Contract7979</name>
      <uri>https://old.reddit.com/user/Leading-Contract7979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to share our recent work &amp;quot;Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model&amp;quot;! &lt;/p&gt; &lt;p&gt;In this paper, &lt;strong&gt;we study the granularity of action space in RLHF PPO training&lt;/strong&gt;, assuming only binary preference labels. Our proposal is to &lt;strong&gt;assign reward to each semantically complete text segment&lt;/strong&gt;, rather than per-token (maybe over-granular) or bandit reward (sparse). We further &lt;strong&gt;design techniques to ensure the effectiveness and stability of RLHF PPO training under the denser {segment, token}-level rewards&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our Segment-level RLHF PPO and its Token-level PPO variant outperform bandit PPO&lt;/strong&gt; across AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks under various backbone LLMs (Llama Series, Phi Series).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2501.02790"&gt;https://arxiv.org/pdf/2501.02790&lt;/a&gt; &lt;ol&gt; &lt;li&gt;Benckmark results are available at: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO?tab=readme-ov-file#benckmark-results--released-models"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO?tab=readme-ov-file#benckmark-results--released-models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Method illustration at: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO/blob/main/method.png"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO/blob/main/method.png&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/yinyueqin/DenseRewardRLHF-PPO"&gt;https://github.com/yinyueqin/DenseRewardRLHF-PPO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Prior work on token-level reward model for RLHF: &lt;a href="https://arxiv.org/abs/2306.00398"&gt;https://arxiv.org/abs/2306.00398&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading-Contract7979"&gt; /u/Leading-Contract7979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz50c0/denser_reward_for_rlhf_ppo_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvfjq</id>
    <title>What do you think of AI employees?</title>
    <updated>2025-01-11T13:03:35+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seeing a surge in start-ups and large enterprises building AI employees.&lt;/p&gt; &lt;p&gt;A good number of well-funded start-ups are building AI SDRs, SWEs, marketing agents, and Customer success agents. Even Salesforce is working on AgentForce to create no-code salesforce automation agents.&lt;/p&gt; &lt;p&gt;This trend is growing faster than I thought; dozens of start-ups are probably in YC this year.&lt;/p&gt; &lt;p&gt;I’m not sure if any of them are in production doing the jobs in the real world, and also, these agents may require a dozen integrations to be anywhere close to being functional.&lt;/p&gt; &lt;p&gt;As much as I like LLMs, they still don’t seem capable of handling edge cases in real-world jobs. They may be suitable for building automated pipelines for tightly scoped tasks, but replacing humans seems far-fetched.&lt;/p&gt; &lt;p&gt;Salesforce Chairman Mark Benioff even commented on not hiring human employees anymore; though it could be their sneaky marketing, it shows their intent.&lt;/p&gt; &lt;p&gt;What do you think of this AI employee in general the present and future? I would love to hear your thoughts if you’re building something simillar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyrml</id>
    <title>[Mini Rant] Are LLMs trapped in English and the assistant paradigms?</title>
    <updated>2025-01-11T15:52:33+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;br /&gt; It feels like we’re trapped in two mainstream paradigms, and it’s starting to get on my nerves. Let me explain:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLMs (too) focused on English&lt;/strong&gt;&lt;br /&gt; We’re seeing more and more models—Qwen, Mistral, Llama 3.x, etc.—that claim “multilingual” abilities. And if you look closely, everyone approaches the problem differently. However, my empirical scenarios often fail to deliver a good experience with those LLMs, even at a 70B scale.&lt;br /&gt; Yes, I understand English reaches the largest audience, but by focusing everything on English, we’re limiting the nuanced cultural and stylistic richness of other languages (French, Spanish, Italian, etc.).&lt;br /&gt; As a result, we rarely see new “styles” or modes of reasoning outside of English.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The “assistant” obsession&lt;/strong&gt;&lt;br /&gt; Everyone wants to build a conversation assistant. Sure, it’s a popular use case,&lt;br /&gt; but it kind of locks us into a single format: a Q&amp;amp;A flow with a polite, self-censored style.&lt;br /&gt; We forget these are token generators that could be tweaked for creative text manipulation or other forms of generation.&lt;br /&gt; I really wish we’d explore more diverse use cases: scenario generation, data-to-text, or other conversation protocols that aren’t so uniform.&lt;/p&gt; &lt;p&gt;I understand that model publishers invest significant resources into performing benchmarks and enhancing multilingual capabilities. For instance, Aya Expanse by Cohere For AI represents a notable advancement in this area. Despite these efforts, in real-world scenarios, I’ve never been able to achieve the same level of performance in French as in English with open-source models. Conversely, closed-source models maintain a more consistent performance across languages, which is frustrating because I’d prefer using open-source models.&lt;/p&gt; &lt;p&gt;Am I the only one who feels we’re stuck between “big English-only LLMs” and “conversation assistant” paradigms? I think there’s so much potential out there for better multilingual support and more interesting use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzg0hd</id>
    <title>How does Llama-fication or Mistral-fication of open source models work?</title>
    <updated>2025-01-12T05:44:52+00:00</updated>
    <author>
      <name>/u/InevitablePhysics151</name>
      <uri>https://old.reddit.com/user/InevitablePhysics151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this post by Daniel Han where he explains Llama-fication of microsoft's phi-4 model.&lt;/p&gt; &lt;p&gt;Does anyone understand the implementation details on these steps?&lt;/p&gt; &lt;p&gt;Model repositories mostly contain config JSON files. What are the functions of these files? How can changes made to them be iteratively tested on low resource systems (NVIDIA GeForce GTX 1650) for small language models?&lt;/p&gt; &lt;p&gt;Can someone share any videos, blogs or articles where these changes are implemented?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;Link to post by Daniel Han: &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7283548654622126080/"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7283548654622126080/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitablePhysics151"&gt; /u/InevitablePhysics151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzg0hd/how_does_llamafication_or_mistralfication_of_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzg0hd/how_does_llamafication_or_mistralfication_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzg0hd/how_does_llamafication_or_mistralfication_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T05:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzew4v</id>
    <title>Speculative decoding isn't coming to ollama anytime soon, any alternatives?</title>
    <updated>2025-01-12T04:36:53+00:00</updated>
    <author>
      <name>/u/ServeAlone7622</name>
      <uri>https://old.reddit.com/user/ServeAlone7622</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to this recently &lt;a href="https://github.com/ollama/ollama/pull/8134"&gt;rejected PR&lt;/a&gt; ollama isn't going to bring draft models and speculative decoding in any time soon. I'd very much like to have this feature. I tried it out on mlx and it seems to be more than a token speed up. It seems to take the &amp;quot;voice&amp;quot; of the draft model and integrate it into the larger model. I guess this is a type of steering?&lt;/p&gt; &lt;p&gt;Imagine giving something like small stories to a 128k context model!&lt;/p&gt; &lt;p&gt;In any event, I'd use mlx but my use case isn't purely apple. &lt;/p&gt; &lt;p&gt;Does anyone have suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ServeAlone7622"&gt; /u/ServeAlone7622 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T04:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu2dh</id>
    <title>LocalGLaDOS - running on a real LLM-rig</title>
    <updated>2025-01-11T11:34:21+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt; &lt;img alt="LocalGLaDOS - running on a real LLM-rig" src="https://external-preview.redd.it/EfE2n_bbhcmfaS9RbA5FtQq7jGIahU2UIGm8g-a1Uag.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ce4ca891cbd89dfa15f29ba5ffa968064f42e85" title="LocalGLaDOS - running on a real LLM-rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-GHKTocDF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyils</id>
    <title>Nvidia 50x0 cards are not better than their 40x0 equivalents</title>
    <updated>2025-01-11T15:40:50+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking closely at the specs, I found 40x0 equivalents for the new 50x0 cards except for 5090. Interestingly, all 50x0 cards are not as energy efficient as the 40x0 cards. Obviously, GDDR7 is the big reason for the significant boost in memory bandwidth for 50x0.&lt;/p&gt; &lt;p&gt;Unless you really need FP4 and DLSS4, there are not that strong a reason to buy the new cards. For the 4070Super/5070 pair, the former can be 15% faster in prompt processing and the latter is 33% faster in inference. If you value prompt processing, it might even make sense to buy the 4070S.&lt;/p&gt; &lt;p&gt;As I mentioned in another thread, this gen is more about memory upgrade than the actual GPU upgrade.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;4070 Super&lt;/th&gt; &lt;th align="left"&gt;5070&lt;/th&gt; &lt;th align="left"&gt;4070Ti Super&lt;/th&gt; &lt;th align="left"&gt;5070Ti&lt;/th&gt; &lt;th align="left"&gt;4080 Super&lt;/th&gt; &lt;th align="left"&gt;5080&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;141.93&lt;/td&gt; &lt;td align="left"&gt;123.37&lt;/td&gt; &lt;td align="left"&gt;176.39&lt;/td&gt; &lt;td align="left"&gt;175.62&lt;/td&gt; &lt;td align="left"&gt;208.9&lt;/td&gt; &lt;td align="left"&gt;225.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;220&lt;/td&gt; &lt;td align="left"&gt;250&lt;/td&gt; &lt;td align="left"&gt;285&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;320&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;656.12&lt;/td&gt; &lt;td align="left"&gt;493.49&lt;/td&gt; &lt;td align="left"&gt;618.93&lt;/td&gt; &lt;td align="left"&gt;585.39&lt;/td&gt; &lt;td align="left"&gt;652.8&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GB/s&lt;/td&gt; &lt;td align="left"&gt;504&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;896&lt;/td&gt; &lt;td align="left"&gt;736&lt;/td&gt; &lt;td align="left"&gt;960&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Price at Launch&lt;/td&gt; &lt;td align="left"&gt;$599&lt;/td&gt; &lt;td align="left"&gt;$549&lt;/td&gt; &lt;td align="left"&gt;$799&lt;/td&gt; &lt;td align="left"&gt;$749&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz254t</id>
    <title>New finetune Negative_LLAMA_70B</title>
    <updated>2025-01-11T18:20:50+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's January 2025, and still, there are very few models out there that have successfully tackled LLM's positivity bias. &lt;strong&gt;LLAMA 3.3&lt;/strong&gt; was received in the community with mixed feelings. It is an exceptional assistant, and superb at instruction following (&lt;strong&gt;highest IFEVAL&lt;/strong&gt; to date, and by a large margin too.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem-&lt;/strong&gt; it is very predictable, dry, and of course, plaugued with positivity bias like all other LLMs. &lt;strong&gt;Negative_LLAMA_70B&lt;/strong&gt; is &lt;strong&gt;not&lt;/strong&gt; an unalignment-focused model (even though it's pretty uncensored), but it is my attempt to address positivity bias while keeping the exceptional intelligence of the &lt;strong&gt;LLAMA 3.3 70B&lt;/strong&gt; base model. Is the base 3.3 smarter than my finetune? I'm pretty sure it is, however, Negative_LLAMA_70B is still pretty damn smart.&lt;/p&gt; &lt;p&gt;The model was &lt;strong&gt;NOT&lt;/strong&gt; overcooked with unalignment, so it won't straight up throw morbid or depressing stuff at you, but if you were to ask it to write a story, or engage in an RP, you would notice &lt;strong&gt;slightly&lt;/strong&gt; darker undertones. In a long trip, the character takes in a story- their legs will be hurt and would feel tired, in &lt;strong&gt;Roleplay&lt;/strong&gt; when you seriously piss off a character- it might hit you (without the need to explicitly prompt such behavior in the character card).&lt;/p&gt; &lt;p&gt;Also, &lt;strong&gt;toxic-dpo&lt;/strong&gt; and other morbid unalignment datasets were &lt;strong&gt;not&lt;/strong&gt; used. I did include a private dataset that should allow total freedom in both &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt;, and quite a lot of various assistant-oriented tasks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B?not-for-all-audiences=true#tldr"&gt;&lt;/a&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less positivity bias&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;exceptionally good&lt;/strong&gt; at following the character card.&lt;/li&gt; &lt;li&gt;Characters feel more &lt;strong&gt;'alive'&lt;/strong&gt;, and will occasionally &lt;strong&gt;initiate stuff on their own&lt;/strong&gt; (without being prompted to, but fitting to their character).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong ability&lt;/strong&gt; to comprehend and roleplay &lt;strong&gt;uncommon physical and mental characteristics&lt;/strong&gt;. TL;DR Strong Roleplay &amp;amp; Creative writing abilities. Less positivity bias. Very smart assistant with low refusals. exceptionally good at following the character card. Characters feel more 'alive', and will occasionally initiate stuff on their own (without being prompted to, but fitting to their character). Strong ability to comprehend and roleplay uncommon physical and mental characteristics.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X: &lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt;hf: &lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview&lt;/a&gt; blog: &lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;https://novasky-ai.github.io/posts/sky-t1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzcxby</id>
    <title>moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others.</title>
    <updated>2025-01-12T02:46:02+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt; &lt;img alt="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." src="https://preview.redd.it/c73tl9ko7hce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecac7f710d04af5ce2b21d2e0825db84a321f21" title="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c73tl9ko7hce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz0n8c</id>
    <title>GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</title>
    <updated>2025-01-11T17:15:58+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt; &lt;img alt="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" src="https://external-preview.redd.it/fWekNX9cjJo2NgR6zTyYnqvItoILS5GvTDAQC2foz30.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13f97a5793ce6881c43646a9bce53d9dbbf16b98" title="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzfjmp</id>
    <title>Parking Systems analysis and Report Generation with Computer vision and Ollama</title>
    <updated>2025-01-12T05:15:40+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt; &lt;img alt="Parking Systems analysis and Report Generation with Computer vision and Ollama " src="https://external-preview.redd.it/ZDUxcHMwOGt5aGNlMZNdfj6QUni_z9Bf_NJiTzUymfkgPwnfSrss06zjR7A1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f892ce79dd89dd0ae0171dc7ac8e70e942f8504" title="Parking Systems analysis and Report Generation with Computer vision and Ollama " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2tf8yz7kyhce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T05:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz2rar</id>
    <title>Why we don't know researchers behind DeepSeek?</title>
    <updated>2025-01-11T18:48:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero interviews, zero social activity. Zero group photos, none about us page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hze1xk</id>
    <title>6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!</title>
    <updated>2025-01-12T03:48:19+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt; &lt;img alt="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" src="https://external-preview.redd.it/eDd2Nndjb3ppaGNlMUVpVcA3yZ4wjFwvAE4TdXYq4bpkJwG-QulsV1F4T0eu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=475a3c9c78a9d27249a2c72bcf664f75b7f9639c" title="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r3w7zbozihce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T03:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzd3xs</id>
    <title>Qwen releases Qwen Chat (online)</title>
    <updated>2025-01-12T02:56:09+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chat.qwenlm.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz7l7d</id>
    <title>OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model</title>
    <updated>2025-01-11T22:23:27+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt; &lt;img alt="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" src="https://preview.redd.it/nhsep8z3xfce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b460c09bdabd5d02e8e8e46757749c4711b75f" title="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhsep8z3xfce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T22:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz5caf</id>
    <title>Tutorial: Run Moondream 2b's new gaze detection on any video</title>
    <updated>2025-01-11T20:42:31+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt; &lt;img alt="Tutorial: Run Moondream 2b's new gaze detection on any video" src="https://external-preview.redd.it/a2VmczhmdHllZmNlMTF40J1mEmizgXzWsZQRgxJwv14NVEzVGBQqF-uixs9J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6d357b58a592f06ed596b1615e185d70bfedfdf" title="Tutorial: Run Moondream 2b's new gaze detection on any video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9ofbftyefce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz97my</id>
    <title>they don’t know how good gaze detection is on moondream</title>
    <updated>2025-01-11T23:38:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt; &lt;img alt="they don’t know how good gaze detection is on moondream" src="https://external-preview.redd.it/anBia3RnaGhhZ2NlMSTi0DO1FtxEm4mYFQVOtZR8uuj4lv59wjB_E-Pc4Mjr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3206ad0d968f5e06525f4113c574566e35551fb1" title="they don’t know how good gaze detection is on moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xgysp5nhagce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T23:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzany5</id>
    <title>We are an AI company now!</title>
    <updated>2025-01-12T00:47:37+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt; &lt;img alt="We are an AI company now!" src="https://preview.redd.it/0yl0970umgce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53963d0db45722eea8467f27c91ca48e5a7cf6fc" title="We are an AI company now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0yl0970umgce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T00:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz28ld</id>
    <title>Bro whaaaat?</title>
    <updated>2025-01-11T18:24:57+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt; &lt;img alt="Bro whaaaat?" src="https://preview.redd.it/cwi5l2ziqece1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6895d12163dd294798940a5c5b6368da7f91b2f" title="Bro whaaaat?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwi5l2ziqece1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:24:57+00:00</published>
  </entry>
</feed>
