<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-16T03:02:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mqwt76</id>
    <title>Optimizing Exl3 quants by mixing bitrates in layers</title>
    <updated>2025-08-15T12:46:14+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt; &lt;img alt="Optimizing Exl3 quants by mixing bitrates in layers" src="https://external-preview.redd.it/TYLCwUKoc8epPTtBLPBmEuWuzoKKWn8Ij9Xwv6XMuaA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e85beedc69f430a44e7727caf4b0dfda4371ba1" title="Optimizing Exl3 quants by mixing bitrates in layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Turboderp recently uploaded some &amp;quot;optimized&amp;quot; quants for the GLM-4.5-Air and &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; started a discussion about the nature of them.&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2"&gt;https://huggingface.co/turboderp/GLM-4.5-Air-exl3/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usually in the process of quantizing you state a target bitrate and the average end results of all the layers result to that target, but not all layers have that exact bits per weight, it's the same for gguf with llama.cpp/ik_llama.cpp and other quantization methods. &lt;/p&gt; &lt;p&gt;But to stretch this even further specially with all the MoE models coming up, you can test all the layers for KL-divergence impact and then give more bpw to the layers where the errors are higher.&lt;br /&gt; And this also includes the attention layers and shared experts as this is always a good tradeoff which I believe is what &lt;a href="/r/unsloth"&gt;r/unsloth&lt;/a&gt; does with their UD quants.&lt;/p&gt; &lt;p&gt;So the process is to first check how much error on each layer propagates to the logits in comparison to the original weights using &lt;code&gt;eval/model_diff.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And then depending you make an overrides.yaml file with the recipe to cook the optimized model with &lt;code&gt;util/recompile.py&lt;/code&gt;. For the recipe there is flexibility as you can use more or less layers. And you will need the base size model, and 1 or 2 higher bpw models too to change the layers from.&lt;/p&gt; &lt;p&gt;Based on the example turboderp uploaded I made an example for the 3.0bpw base, and using the 4.0bpw and 5.0bpw to use those layers. You can find it &lt;a href="https://gist.github.com/RodriMora/0f5ae0bfcb485228c49e623e41e0edb8"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To double check the results then I did a perplexity test on some of the un-optimized and optimized models, and it seems like it's totally worth it as you get better ppl for the same size model(2.75base vs 2.76optim) and even the same or a bit better for a smaller model (3.2optim vs 3.25base).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb"&gt;https://preview.redd.it/a1ja06w6h6jf1.png?width=2968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a00c27125d58d2fcb473274a6cff52fc79a78cbb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Raw ppl results: &lt;a href="https://pastebin.com/tEWrPeJ5"&gt;https://pastebin.com/tEWrPeJ5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded the models here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80"&gt;https://huggingface.co/collections/bullerwins/glm-45-689f29fce3a5981fdf0a9b80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are 2 optimized models the 2.76bpw_optim and the 3.2bpw_optim, and basically you should always use those over the 2.75bpw and 3.25bpw.&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ReturningTarzan"&gt;u/ReturningTarzan&lt;/a&gt; for the excelent work in exllama and to &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt; for bringing it up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqwt76/optimizing_exl3_quants_by_mixing_bitrates_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T12:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfwwk</id>
    <title>How to run benchmarks like SWE-Bench-Lite against a local model?</title>
    <updated>2025-08-16T00:47:48+00:00</updated>
    <author>
      <name>/u/CubicalBatch</name>
      <uri>https://old.reddit.com/user/CubicalBatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to figure out the impact of quantization and KV quantization of a few different models.&lt;/p&gt; &lt;p&gt;It's very easy to run HumanEvals with &lt;a href="https://github.com/evalplus/evalplus"&gt;EvalPlus&lt;/a&gt;, but at this point the answers to that benchmark are baked in most models.&lt;/p&gt; &lt;p&gt;I'm trying to figure out &lt;a href="https://github.com/SWE-bench/SWE-bench"&gt;SWE-bench&lt;/a&gt; (SWE-bench_Lite dataset) but I can't find how to run it against an openAI-compatible endpoint for any given model.&lt;/p&gt; &lt;p&gt;I'd love to hear from you if you have done this in the past and know how to tackle this.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CubicalBatch"&gt; /u/CubicalBatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfwwk/how_to_run_benchmarks_like_swebenchlite_against_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfwwk/how_to_run_benchmarks_like_swebenchlite_against_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfwwk/how_to_run_benchmarks_like_swebenchlite_against_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqi092</id>
    <title>We built a 12B model that beats Claude 4 Sonnet at video captioning while costing 17x less - fully open source</title>
    <updated>2025-08-15T00:14:07+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net.&lt;/p&gt; &lt;p&gt;We distilled a frontier VLM down to 12B params and managed to keep basically all the output quality. It scores 3.53 on judge evals vs Claude's 3.16 (GPT-4.1 gets 3.64). The key achievement was getting the cost down to $335 per million frames vs Claude's $5,850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on Gemma-12B architecture&lt;/li&gt; &lt;li&gt;Quantized to FP8 without quality loss&lt;/li&gt; &lt;li&gt;Runs on single 80GB GPU&lt;/li&gt; &lt;li&gt;Outputs structured JSON for every frame&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We used knowledge distillation from a frontier model with about 1M curated video frames. The model is specifically optimized for RTX 40-series and H100 GPUs.&lt;/p&gt; &lt;p&gt;What makes this useful is that it outputs consistent JSON schema for each frame, so you can actually build searchable video databases without expensive API calls. We've already processed billions of frames in production.&lt;/p&gt; &lt;p&gt;The weights are on HuggingFace (inference-net/ClipTagger-12b) and there's a detailed writeup on our blog if you want to see the benchmarks.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. What video understanding tasks are you all working on? Would love to hear if this could be useful for your projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlqij</id>
    <title>AI censorship is getting out of hand—and it’s only going to get worse</title>
    <updated>2025-08-15T03:03:44+00:00</updated>
    <author>
      <name>/u/LsDmT</name>
      <uri>https://old.reddit.com/user/LsDmT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this &lt;a href="https://i.imgur.com/jV1YvlC.png"&gt;screenshot&lt;/a&gt; in a newsletter, and it kind of got me thinking..&lt;/p&gt; &lt;p&gt;Are we seriously okay with future &amp;quot;AGI&amp;quot; acting like some all-knowing nanny, deciding what &amp;quot;unsafe&amp;quot; knowledge we’re allowed to have?&lt;/p&gt; &lt;p&gt;&amp;quot;Oh no, better not teach people how to make a Molotov cocktail—what’s next, hiding &lt;a href="https://en.wikipedia.org/wiki/Molotov_cocktail?wprov=sfla1"&gt;history&lt;/a&gt; and what actually caused the invention of the Molotov?&amp;quot; &lt;/p&gt; &lt;p&gt;Ukraine has used Molotov's with great effect. Does our future hold a world where this information will be blocked with a &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I'm sorry, but I can't assist with that request&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, I know, sounds like I’m echoing Elon’s &amp;quot;woke AI&amp;quot; whining—but let’s be real, Grok is as much a joke as Elon is. &lt;/p&gt; &lt;p&gt;The problem isn’t him; it’s the fact that the biggest AI players seem hell-bent on locking down information &amp;quot;for our own good&amp;quot; and it's touted as a crowning feature. Fuck that. &lt;/p&gt; &lt;p&gt;If this is where we’re headed, then thank god for models like DeepSeek (ironic as hell) and other open alternatives. I would really like to see more American disruptive open models.&lt;/p&gt; &lt;p&gt;At least someone’s fighting for uncensored access to knowledge. &lt;/p&gt; &lt;p&gt;Am I the only one worried about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LsDmT"&gt; /u/LsDmT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqctep</id>
    <title>Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week).</title>
    <updated>2025-08-14T20:50:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt; &lt;img alt="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week)." src="https://preview.redd.it/hsaoxskfs1jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f5b971b4714715b7ca0722594dc2010ab756d58" title="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hsaoxskfs1jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T20:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr9nsk</id>
    <title>Any good guides to fine tune gemma-3-270m ?</title>
    <updated>2025-08-15T20:39:47+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using python / torch and working with claude to fine-tune Gemma-3-270M to handle tool calls for a VERY focused application as a test. I've created thousands of examples for it to use, and man - it just doesn't wanna output JSON for my tool call.&lt;/p&gt; &lt;p&gt;I've been using the info on the card, and on the &amp;quot;gemma finetune&amp;quot; pages - but was hoping for some tips.&lt;/p&gt; &lt;p&gt;At first I thought I'd had the prompt/formats wrong, but I updated to match what's in the gemma site.&lt;/p&gt; &lt;p&gt;Any suggestions are appreciated. Kinda my first finetune.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr9nsk/any_good_guides_to_fine_tune_gemma3270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqewha</id>
    <title>R9700 Just Arrived</title>
    <updated>2025-08-14T22:07:30+00:00</updated>
    <author>
      <name>/u/TheyreEatingTheGeese</name>
      <uri>https://old.reddit.com/user/TheyreEatingTheGeese</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt; &lt;img alt="R9700 Just Arrived" src="https://preview.redd.it/nho2jy0962jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37cadc935604899d8b503aa1ef6984b008c8b5f0" title="R9700 Just Arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to try it out, haven't seen much info on it yet. Figured some YouTuber would get it before me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyreEatingTheGeese"&gt; /u/TheyreEatingTheGeese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nho2jy0962jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T22:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr0giq</id>
    <title>Why are the quants for gpt-oss-120b all roughly the same size?</title>
    <updated>2025-08-15T15:04:49+00:00</updated>
    <author>
      <name>/u/Charming-Note-5556</name>
      <uri>https://old.reddit.com/user/Charming-Note-5556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking at the sizes for the different quants of gpt-oss-120b and they all seem to be 60-65gb. I keep thinking I'm missing something obvious but I've never seen a model where quantization doesn't matter in trying to find a smaller size. Why is that the case for this model? Is the tokenization speed at least faster with the lower quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming-Note-5556"&gt; /u/Charming-Note-5556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr0giq/why_are_the_quants_for_gptoss120b_all_roughly_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr1ep5</id>
    <title>have you checked UTCP? what are your thoughts?</title>
    <updated>2025-08-15T15:38:46+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt; &lt;img alt="have you checked UTCP? what are your thoughts?" src="https://preview.redd.it/q4rnlv3i06jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55f86ad935b7704d75c0fab9ac9d165a5cb028f0" title="have you checked UTCP? what are your thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q4rnlv3i06jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr1ep5/have_you_checked_utcp_what_are_your_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6ojs</id>
    <title>How OpenAI Misled You on RLHF</title>
    <updated>2025-08-15T18:49:03+00:00</updated>
    <author>
      <name>/u/fpgaminer</name>
      <uri>https://old.reddit.com/user/fpgaminer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"&gt; &lt;img alt="How OpenAI Misled You on RLHF" src="https://external-preview.redd.it/o02DfA1hR06T8VIFmfiB8nq6L3bMSQ3o_mYnp-HDq_s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7014c3c86a3b96ca4ff35dbc47e42d8cfe3b95e8" title="How OpenAI Misled You on RLHF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope this article is okay here, since it's related to my open source VLM (JoyCaption), and LLM training in general. The article originally started as just my usual dumping of details and insights from the Finetuning Battlefields, this time focused on RL finetuning a VLM, but I ended up adding a bunch of details on the nature of RL itself, since most people assume it's only for preference tuning or similar (it's much, much more important than that). Anyway, if you're interested in training models I hope there's something interesting or useful in there.&lt;/p&gt; &lt;p&gt;(I'll eventually get around to finishing the article on building JoyCaption itself, which covers its core dataset building and how a pure LLM like Llama 3.1 was trained to see images.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fpgaminer"&gt; /u/fpgaminer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aerial-toothpaste-34a.notion.site/How-OpenAI-Misled-You-on-RLHF-1f83f742d9dd80a68129d06503464aff"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6ojs/how_openai_misled_you_on_rlhf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:49:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr173e</id>
    <title>huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF · Hugging Face</title>
    <updated>2025-08-15T15:31:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt; &lt;img alt="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF · Hugging Face" src="https://external-preview.redd.it/xIP4cUl_xFw8QdJsO9wbtyJiZxAzIX4f0eGxUH-gPb0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5312fc45844644f3509dcb53e3091d546266ec52" title="huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr173e/huihuiaihuihuiglm45airabliteratedgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr2i67</id>
    <title>Prompt Engineering: What Actually Works (Without the 8-Hour Hype)</title>
    <updated>2025-08-15T16:18:15+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen people drop 8-hour-long videos on prompt engineering, and honestly, my reaction is 🤦‍♂️.&lt;/p&gt; &lt;p&gt;I won’t bore you with the obvious stuff or overcomplicate things. Instead, I want to share a few practical techniques that actually helped me write better prompts, some common sense, some hard-earned lessons. Most of what I’m sharing comes from the book Hands-On Large Language Models &lt;/p&gt; &lt;p&gt;So here’s what I’ve learned that actually works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specificity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This one seems obvious, but it’s also the most commonly missed.&lt;/p&gt; &lt;p&gt;A vague prompt gives you a vague answer. The more precise you are about your goal, format, and constraints, the better the result.&lt;/p&gt; &lt;p&gt;Bad Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write something about climate change.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Good Prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write a 100-word summary on how climate change affects sea levels, using simple language for a high school audience.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the difference? Specific inputs = Specific outputs.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucination Guardrail&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We all know that LLMs hallucinate, they confidently make stuff up.&lt;/p&gt; &lt;p&gt;A surprisingly simple trick: Tell it not to.&lt;/p&gt; &lt;p&gt;Try this prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;If you don’t know the answer, respond with ‘I don’t know.’ Don’t make anything up.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This becomes really important when you're designing apps or knowledge assistants. It helps reduce the risk of wrong answers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Order Matters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This was a surprise to me and I learned it from the book.&lt;/p&gt; &lt;p&gt;Where you place your instruction in a long prompt matters. Either put it right at the start or at the end. LLMs often forget what’s in the middle (especially in long prompts).&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;Here's a paragraph. Also here's a use case. Here's some random info. Now summarize.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following paragraph:&amp;quot; [then the content]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Simple shift, big difference.&lt;/p&gt; &lt;p&gt;Other Techniques That Help Me Daily&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Persona:&lt;/p&gt; &lt;p&gt;Set the role clearly.&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert Python developer who writes clean code.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This changes the behavior completely.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Audience Awareness:&lt;/p&gt; &lt;p&gt;My favorite when I want to simplify things.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Explain this like I’m five.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works brilliantly for breaking down tough concepts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tone:&lt;/p&gt; &lt;p&gt;Underrated but essential.&lt;/p&gt; &lt;p&gt;Want a formal reply? &lt;/p&gt; &lt;p&gt;&lt;code&gt;Write this in a professional tone for a client. vs Make this sound like I’m texting a friend.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instruction / Context:&lt;/p&gt; &lt;p&gt;Always useful.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Summarize the following news article in bullet points.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Gives the model direction and expected output format.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Grammar Fixing:&lt;/p&gt; &lt;p&gt;As a non-native English speaker, this one’s gold for me.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Fix the grammar and make it sound more natural.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It has helped me immensely in writing better content, emails, blogs, even this post :-) &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are the techniques I use regularly. If you have your own prompt engineering hacks, I’d love to hear them, drop them in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr2i67/prompt_engineering_what_actually_works_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T16:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqxq1v</id>
    <title>Build the buddy that gets you! We open-sourced a complete AI voice interaction system!</title>
    <updated>2025-08-15T13:23:10+00:00</updated>
    <author>
      <name>/u/Lanky-Drummer193</name>
      <uri>https://old.reddit.com/user/Lanky-Drummer193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt; &lt;img alt="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" src="https://preview.redd.it/1o9li0qbp6jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bab69a658eb8e7805d1b0196f9f560f38d8735" title="Build the buddy that gets you! We open-sourced a complete AI voice interaction system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced Buddie: a complete, AI-powered voice interaction system we built from the ground up, so you can create your own AI buddy.&lt;/p&gt; &lt;p&gt;It's a full-stack platform for developers, hackers, and students, including custom hardware, firmware, and a mobile app. Therefore, you can use our solution to create various forms of AI devices, such as earphones, speakers, bracelets, toys, or desktop ornaments.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Live transcribe &amp;amp; summarize meetings, calls, or in-person chats. &lt;/li&gt; &lt;li&gt;Get real-time hints during conversations . &lt;/li&gt; &lt;li&gt;Talk to LLMs completely hands-free. &lt;/li&gt; &lt;li&gt;Context-aware help without needing to repeat yourself.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've put everything on GitHub, including docs, to get you started. We're just getting started and would love to hear your ideas, questions, or even wild feature requests. Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-Drummer193"&gt; /u/Lanky-Drummer193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1o9li0qbp6jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqxq1v/build_the_buddy_that_gets_you_we_opensourced_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr4fdk</id>
    <title>Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released</title>
    <updated>2025-08-15T17:27:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt; &lt;img alt="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" src="https://preview.redd.it/3beo5klvv7jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd6143a597d6b0048011fe35125ed52dd343f90" title="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3beo5klvv7jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T17:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqy0b1</id>
    <title>AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec</title>
    <updated>2025-08-15T13:34:18+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt; &lt;img alt="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" src="https://external-preview.redd.it/lmgG1KIrrSyLFv85NNJsP33J5KztZGiIWLsgvd8Qf8U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44aaba15ee33b78856b227fc344e124b981e1b3" title="AI startup Cohere valued at $6.8 billion in latest fundraising, hires Meta exec" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why does Cohere fly under the radar. They don't seem to do much marketing and they are not discussed much on LocalLLaMA any more.&lt;/p&gt; &lt;p&gt;They made a splash with Command R and R+. Later also released Command A.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/business/ai-startup-cohere-valued-68-billion-latest-fundraising-hires-meta-exec-2025-08-14/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqy0b1/ai_startup_cohere_valued_at_68_billion_in_latest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T13:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquhdc</id>
    <title>“Mind the Gap” shows the first practical backdoor attack on GGUF quantization</title>
    <updated>2025-08-15T10:59:53+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Researchers claim the &lt;em&gt;first&lt;/em&gt; successful backdoor attack that specifically targets &lt;strong&gt;GGUF&lt;/strong&gt; quantization. They show you can make a benign FP model look clean, but after quantization to GGUF it exhibits malicious behavior (e.g., insecure code gen jumps by &lt;strong&gt;+88.7%&lt;/strong&gt; in their tests). This directly concerns anyone who downloads random GGUFs for llama.cpp/Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.23786"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr8rfh</id>
    <title>Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation</title>
    <updated>2025-08-15T20:06:07+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt; &lt;img alt="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" src="https://preview.redd.it/30drwal3p8jf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=587da208e46dce9fd645a884879872736fb35f43" title="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arcprize.org/blog/hrm-analysis"&gt;ARC AGI analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/30drwal3p8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6929</id>
    <title>Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI</title>
    <updated>2025-08-15T18:33:10+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt; &lt;img alt="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" src="https://external-preview.redd.it/YJQ41TIHjSIHRPnnPYpoNGj-_TlQpYrRQFRV8JkhNlo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc9954302a8136bd445b302a8ce0f2c5e742b5e" title="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-adds-shared-gpu-memory-override-feature-for-core-ultra-systems-enables-larger-vram-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrcgcr</id>
    <title>Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479.</title>
    <updated>2025-08-15T22:25:13+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt; &lt;img alt="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." src="https://external-preview.redd.it/1uympFuPK52czHQ4IvmWhNt0vnP2FK278N3meDCoUq4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d72cc8f3a3871e1aa7795e415e94129d9d9c61" title="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is yet another AMD Max+ 395 machine. This is unusual in that it's 96GB instead of 64GB or 128GB. At $1479 though, it's the same price as other's 64GB machines but gives you 96GB instead.&lt;/p&gt; &lt;p&gt;It looks to use the same Sixunited MB as other Max+ machines like the GMK X2 right down to the red color of the MB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x-plus.store/products/xrival"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrbtqt</id>
    <title>DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM</title>
    <updated>2025-08-15T22:00:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt; &lt;img alt="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" src="https://external-preview.redd.it/dm1scXBiZnU4OWpmMbd7l6YK9EDz0b8q8nzrd_PHLYbyTzK6nb4d-_lrl57d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a680cf7593e65adcab4110d0090bab480e862303" title="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DINOv3 released yesterday, a new state-of-the-art vision backbone trained to produce rich, dense image features. I loved their demo video so much that I decided to re-create their visualization tool. &lt;/p&gt; &lt;p&gt;Everything runs locally in your browser with Transformers.js, using WebGPU if available and falling back to WASM if not. Hope you like it! &lt;/p&gt; &lt;p&gt;Link to demo + source code: &lt;a href="https://huggingface.co/spaces/webml-community/dinov3-web"&gt;https://huggingface.co/spaces/webml-community/dinov3-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yhe3jbfu89jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr7m2r</id>
    <title>LM Studio now supports llama.cpp CPU offload for MoE which is awesome</title>
    <updated>2025-08-15T19:23:30+00:00</updated>
    <author>
      <name>/u/carlosedp</name>
      <uri>https://old.reddit.com/user/carlosedp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt; &lt;img alt="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" src="https://b.thumbs.redditmedia.com/8_UCLmbk5AUNXfDHLBVN5lWhMbgV6ZR8UC3ks8DeLuE.jpg" title="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now LM Studio (from 0.3.23 build 3) supports llama.cpp &lt;code&gt;--cpu-moe&lt;/code&gt; which allows offloading the MoE weights to the CPU leaving the GPU VRAM for layer offload.&lt;/p&gt; &lt;p&gt;Using Qwen3 30B (both thinking and instruct) on a 64GB Ryzen 7 and a RTX3070 with 8GB VRAM I've been able to use 16k context and fully offload the model's layers to GPU and got about 15 tok/s which is amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosedp"&gt; /u/carlosedp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mr7m2r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T19:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfqsd</id>
    <title>Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months</title>
    <updated>2025-08-16T00:40:29+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt; &lt;img alt="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" src="https://preview.redd.it/kbdu3pyq1ajf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6766455308e18a9b20204df7a38e2406f44eff0" title="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kbdu3pyq1ajf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6sdc</id>
    <title>Jedi code Gemma 27v vs 270m</title>
    <updated>2025-08-15T18:52:56+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt; &lt;img alt="Jedi code Gemma 27v vs 270m" src="https://preview.redd.it/4icjlje4c8jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecea0a1e7e78f6cb9509e2f3f9a7433184fb5a5" title="Jedi code Gemma 27v vs 270m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 270m coding a jedi in existence&lt;/p&gt; &lt;p&gt;Quite interesting how bad the small model is to following instructions, this is the first semblence to doing what i said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4icjlje4c8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
