<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-05T21:23:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j3gahy</id>
    <title>NVIDIA’s GeForce RTX 4090 With 96GB VRAM Reportedly Exists; The GPU May Enter Mass Production Soon, Targeting AI Workloads.</title>
    <updated>2025-03-04T17:31:10+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/"&gt;https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly highly interested. If this will be true.&lt;/p&gt; &lt;p&gt;Price around 6k. &lt;/p&gt; &lt;p&gt;Source; &amp;quot;The user did confirm that the one with a 96 GB VRAM won't guarantee stability and that its cost, due to a higher VRAM, will be twice the amount you would pay on the 48 GB edition. As per the user, this is one of the reasons why the factories are considering making only the 48 GB edition but may prepare the 96 GB in about 3-4 months.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j429et</id>
    <title>Why is Qwen 2.5 32b Coder the best local text analysis LLM?</title>
    <updated>2025-03-05T12:46:48+00:00</updated>
    <author>
      <name>/u/custodiam99</name>
      <uri>https://old.reddit.com/user/custodiam99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried a lot of models, even Llama 3.3 70b q_4 with 32k context, but Qwen 2.5 32b q_8 Coder is the best &lt;em&gt;text analysis model&lt;/em&gt; I can find. It really mirrors the source text and can explore very nuanced details. It makes very good word lists (obviously not concordance) from the input file. Can there be a connection between the &lt;strong&gt;coder function&lt;/strong&gt; and the &lt;em&gt;text analysis functionality&lt;/em&gt;? Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/custodiam99"&gt; /u/custodiam99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T12:46:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4dk36</id>
    <title>QWQ-32B Out now on Ollama!</title>
    <updated>2025-03-05T20:49:44+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt; &lt;img alt="QWQ-32B Out now on Ollama!" src="https://b.thumbs.redditmedia.com/O181SWrHVmIh9sGbRY12DzYdKUiOWkWGCwaGeUM_eJQ.jpg" title="QWQ-32B Out now on Ollama!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9immyermoxme1.png?width=818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a95127f64fb4e06bf7252b854cbc5cc6d712558"&gt;https://preview.redd.it/9immyermoxme1.png?width=818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a95127f64fb4e06bf7252b854cbc5cc6d712558&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5knz36hooxme1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=377c42ffbc3c7e8175ee5e2b107c770fc30469a6"&gt;https://preview.redd.it/5knz36hooxme1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=377c42ffbc3c7e8175ee5e2b107c770fc30469a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LINK: &lt;a href="https://ollama.com/library/qwq:32b-q8_0"&gt;https://ollama.com/library/qwq:32b-q8_0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j48403</id>
    <title>I built an In-Memory GGUF Merging tool</title>
    <updated>2025-03-05T17:12:12+00:00</updated>
    <author>
      <name>/u/Enough-Meringue4745</name>
      <uri>https://old.reddit.com/user/Enough-Meringue4745</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, in case anyone is interested I put up a Draft PR on llama.cpp for merging split GGUF's into one completely in memory- so I don't have to write 500gb+ of files just to merge split R1 files. I've only tested it on linux using libfuse3, but its been working.&lt;/p&gt; &lt;p&gt;PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12189"&gt;https://github.com/ggml-org/llama.cpp/pull/12189&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I got tired of having to deal with split GGUFs for vllm or ollama, so I whipped this one up. &lt;/p&gt; &lt;p&gt;When you compile llama.cpp you'll end up with a &lt;code&gt;llama-gguf-fuse&lt;/code&gt; binary. &lt;code&gt;fuse_mount&lt;/code&gt; in this case is a directory containing &lt;code&gt;merged.gguf&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) ➜ bin git:(master) ✗ ./llama-gguf-fuse /home/acidhax/.cache/huggingface/hub/models--unsloth--r1-1776-GGUF/snapshots/30aa0a022fc707acd380c555ef38b4de7891b3b2/Q4_K_M/r1-1776-Q4_K_M ~/fuse_mount Searching with glob pattern: /home/acidhax/.cache/huggingface/hub/models--unsloth--r1-1776-GGUF/snapshots/30aa0a022fc707acd380c555ef38b4de7891b3b2/Q4_K_M/r1-1776-Q4_K_M*-of-*.gguf Detected 9 splits Trying to open: /home/acidhax/.cache/huggingface/hub/models--unsloth--r1-1776-GGUF/snapshots/30aa0a022fc707acd380c555ef38b4de7891b3b2/Q4_K_M/r1-1776-Q4_K_M-00001-of-00009.gguf Merged file built: 404430187040 bytes, 1026 segments, 9 split files, 1025 tensors &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enough-Meringue4745"&gt; /u/Enough-Meringue4745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48403/i_built_an_inmemory_gguf_merging_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48403/i_built_an_inmemory_gguf_merging_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j48403/i_built_an_inmemory_gguf_merging_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4cc6y</id>
    <title>Try QwQ-32B on Hugging Face</title>
    <updated>2025-03-05T20:00:15+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4cc6y/try_qwq32b_on_hugging_face/"&gt; &lt;img alt="Try QwQ-32B on Hugging Face" src="https://external-preview.redd.it/lBZs0Q7c65_lYRXO4ivgUAuYiqvkD7hvWkRogLEWXvw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d8af51e17945c43660cee6e1f3aae2b98b5f55" title="Try QwQ-32B on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/Qwen/QwQ-32B-Demo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4cc6y/try_qwq32b_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4cc6y/try_qwq32b_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3vbfh</id>
    <title>Ollama v0.5.13 has been released</title>
    <updated>2025-03-05T04:52:19+00:00</updated>
    <author>
      <name>/u/Inevitable-Rub8969</name>
      <uri>https://old.reddit.com/user/Inevitable-Rub8969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt; &lt;img alt="Ollama v0.5.13 has been released" src="https://b.thumbs.redditmedia.com/ZjpiHa9AL7vA9lJSktHNr9Xm0TdCOx4eLZRQfB67ZNA.jpg" title="Ollama v0.5.13 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f"&gt;https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Rub8969"&gt; /u/Inevitable-Rub8969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T04:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j49sbd</id>
    <title>Is there a statistically significant difference in logical reasoning performance between DeepSeek R1 and Perplexity R1 1776?</title>
    <updated>2025-03-05T18:18:23+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: After running a McNemar’s statistical test on &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; benchmark results (lineage-128), there’s no statistically significant difference between DeepSeek R1 and Perplexity R1 1776 logical reasoning performance. They both perform similarly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You may have seen my recent posts containing benchmark results of DeepSeek R1 and Perplexity R1 1776 models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If not, a quick summary: I tested both models in my logical reasoning &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; benchmark. Initially R1 1776 performed much worse compared to the original DeepSeek R1. After Perplexity fixed the problem with the serving stack both models started performing equally well when tested via OpenRouter (R1 1776 appears to be slightly better, but the difference is very small).&lt;/p&gt; &lt;p&gt;It kept bugging me if there is really a meaningful difference between the two models, so I decided to put my remaining OpenRouter credits to some good use and cooked a statistical hypothesis test that would answer this question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Initial plan&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After a quick research I decided to use McNemar’s test to see if there is a statistically significant difference in the performance of both models. It's commonly used in machine learning to compare the performance of classifier models. My case is similar enough.&lt;/p&gt; &lt;p&gt;&lt;a href="https://machinelearningmastery.com/mcnemars-test-for-machine-learning/"&gt;https://machinelearningmastery.com/mcnemars-test-for-machine-learning/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since both models have almost perfect accuracy for smaller lineage-bench problem sizes, I decided to generate additional set of 400 lineage-128 quizzes and test both models on this new set. The logic behind this is that the increased difficulty will make the difference between the performance of both models (if there is any) more pronounced.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First a quick look at the lineage-128 results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Nr&lt;/th&gt; &lt;th align="left"&gt;model_name&lt;/th&gt; &lt;th align="left"&gt;lineage-128&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;deepseek/deepseek-r1&lt;/td&gt; &lt;td align="left"&gt;0.688&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;perplexity/r1-1776&lt;/td&gt; &lt;td align="left"&gt;0.685&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can observe the accuracy is almost equal in both models. Also with this problem size my benchmark is still far from being saturated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contingency table&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next step was to create a contingency table based on the answers to lineage-128 quizzes generated by both models.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;...&lt;/th&gt; &lt;th align="left"&gt;DeepSeek R1 correct&lt;/th&gt; &lt;th align="left"&gt;DeepSeek R1 incorrect&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;R1 1776 correct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;203&lt;/td&gt; &lt;td align="left"&gt;71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;R1 1776 incorrect&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;73&lt;/td&gt; &lt;td align="left"&gt;53&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;McNemar's test&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;McNemar’s test in our case checks whether one model is more likely than the other to be correct on items where the other is wrong.&lt;/p&gt; &lt;p&gt;The null hypothesis here is that there is no difference in the the proportion of questions on which Model A answers correctly while Model B answers incorrectly and the proportion of questions on which Model B answers correctly while Model A answers incorrectly.&lt;/p&gt; &lt;p&gt;We can already see that it's almost the same value, but let's calculate the test statistics anyway.&lt;/p&gt; &lt;p&gt;X&lt;sup&gt;2&lt;/sup&gt; = (71-73)&lt;sup&gt;2&lt;/sup&gt; / (71+73) = 0.027(7)&lt;/p&gt; &lt;p&gt;This test statistics value corresponds to pvalue of around 0.868. Since p &amp;gt; 0.05, we can't reject the null hypothesis. Therefore the difference in performance between both models is not statistically significant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There is no statistically significant difference in performance of DeepSeek R1 and Perplexity R1 1776 in lineage-128. But maybe for some reason there is a statistically significant difference only in lineage-64? I could generate more samples and... oh no, I'm almost out of OpenRouter credits.&lt;/p&gt; &lt;p&gt;PS. While searching for the DeepSeek R1 provider in OpenRouter I checked Nebius AI, Minimax and Parasail in 200 lineage-128 quizzes. Nebius scored 0.595, Minimax 0.575 and Parasail 0.680. I had no problems with Parasail - it's quite fast and cheaper than alternatives, definitely recommended.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j49sbd/is_there_a_statistically_significant_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j49sbd/is_there_a_statistically_significant_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j49sbd/is_there_a_statistically_significant_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T18:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j46odf</id>
    <title>AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)</title>
    <updated>2025-03-05T16:13:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46odf/ai_moderates_movies_so_editors_dont_have_to/"&gt; &lt;img alt="AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)" src="https://external-preview.redd.it/cWQweG1yeGRid21lMYil-mvvsBaemYDUlRSkvBniidK7oobmA4FpXb-7Z2sJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4525abd7367f7b0c28bd3fca735471fdb17fa34" title="AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h1mv5sxdbwme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46odf/ai_moderates_movies_so_editors_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j46odf/ai_moderates_movies_so_editors_dont_have_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T16:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j47yei</id>
    <title>FULL LEAKED v0 by Vercel System Prompts (100% Real)</title>
    <updated>2025-03-05T17:05:47+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 05/03/2025)&lt;/p&gt; &lt;p&gt;I managed to get the full system prompts from v0 by Vercel. OVER 1.4K LINES.&lt;/p&gt; &lt;p&gt;There is some interesting stuff you should go and check.&lt;/p&gt; &lt;p&gt;This is 100% real, got it by myself. I managed to extract the full prompts with all the tags included, like &amp;lt;thinking&amp;gt;.&lt;/p&gt; &lt;p&gt;And now the question is, what model is v0 using? soon. 👀&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/x1xhlol/v0-system-prompts"&gt;https://github.com/x1xhlol/v0-system-prompts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:05:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j48k5p</id>
    <title>From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!</title>
    <updated>2025-03-05T17:30:01+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"&gt; &lt;img alt="From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!" src="https://external-preview.redd.it/-QMLQ4lTDWvel2mDwd9Tw9u7Idn4ILKomy3yQX099fQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52a3abf47f6d7769d0954522ed244bd266923078" title="From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/theroyallab/YALS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4dk98</id>
    <title>Open-Source Multi-turn Slack Agent with LangGraph + Arcade</title>
    <updated>2025-03-05T20:49:55+00:00</updated>
    <author>
      <name>/u/MostlyGreat</name>
      <uri>https://old.reddit.com/user/MostlyGreat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing the &lt;a href="https://github.com/ArcadeAI/SlackAgent"&gt;source code&lt;/a&gt; for something we built that might save you a ton of headaches as you're trying to build your own agents - a fully functional Slack agent that can handle multi-turn, tool-calling with real auth flows without making you want to throw your laptop out the window. It supports Gmail, Calendar, GitHub, etc.&lt;/p&gt; &lt;p&gt;Here's also a &lt;a href="https://www.loom.com/share/806e82e8d8e5482090bff780d0168278"&gt;quick video demo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What makes this actually useful:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles complex auth flows - OAuth, 2FA, the works (not just toy examples with hardcoded API keys)&lt;/li&gt; &lt;li&gt;Uses end-user credentials - No sketchy bot tokens with permanent access or limited to one just one user&lt;/li&gt; &lt;li&gt;Multi-service support - Seamlessly jumps between GitHub, Google Calendar, etc. with proper token management&lt;/li&gt; &lt;li&gt;Multi-turn conversations - LangGraph orchestration that maintains context through authentication flows&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Real things it can do:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pull data from private GitHub repos (after proper auth)&lt;/li&gt; &lt;li&gt;Post comments as the actual user&lt;/li&gt; &lt;li&gt;Check and create calendar events&lt;/li&gt; &lt;li&gt;Read and manage Gmail&lt;/li&gt; &lt;li&gt;Web search and crawling via SERP and Firecrawl&lt;/li&gt; &lt;li&gt;Maintain conversation context through the entire flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just recorded a demo showing it handling a complete workflow: checking a private PR, commenting on it, checking my calendar, and scheduling a meeting with the PR authors - all with proper auth flows, not fake demos.&lt;/p&gt; &lt;h1&gt;Why we built this:&lt;/h1&gt; &lt;p&gt;We were tired of seeing agent demos where &amp;quot;tool-using&amp;quot; meant calling weather APIs or other toy examples. We wanted to show what's possible when you give agents proper enterprise-grade auth handling.&lt;/p&gt; &lt;p&gt;It's built to be deployed on Modal and only requires Python 3.10+, Poetry, OpenAI and Arcade API keys to get started. The setup process is straightforward and well-documented in the repo.&lt;/p&gt; &lt;h1&gt;All open source:&lt;/h1&gt; &lt;p&gt;Everything is up on GitHub so you can dive into the implementation details, especially how we used LangGraph for orchestration and &lt;a href="http://Arcade.dev"&gt;Arcade.dev&lt;/a&gt; for tool integration.&lt;/p&gt; &lt;p&gt;The repo explains how we solved the hard parts around:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Token management&lt;/li&gt; &lt;li&gt;LangGraph nodes for auth flow orchestration&lt;/li&gt; &lt;li&gt;Handling auth retries and failures&lt;/li&gt; &lt;li&gt;Proper scoping of permissions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the repo: &lt;a href="https://github.com/ArcadeAI/SlackAgent"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy building!&lt;/p&gt; &lt;p&gt;P.S. In testing, one dev gave it access to the &lt;a href="https://docs.arcade.dev/toolkits/entertainment/spotify"&gt;Spotify tools&lt;/a&gt;. Two days later they had a playlist called &amp;quot;Songs to Code Auth Flows To&amp;quot; with suspiciously specific lyrics. 🎵🔐&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MostlyGreat"&gt; /u/MostlyGreat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4d5fr</id>
    <title>Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎</title>
    <updated>2025-03-05T20:33:03+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt; &lt;img alt="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" src="https://preview.redd.it/ye3dq51qlxme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4849458d901ac4ba1ba6d60edc9a283317fab5bc" title="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thing is friggin sweet!! Can’t wait to fire it up and load up full DeepSeek 671b on this monster! It does look slightly different than the promotional photos I saw online which is a little concerning, but for $800 🤷‍♂️. They’ve got it mounted in some kind of acrylic case or something, it’s in there pretty good, can’t seem to remove it easily. As soon as I figure out how to plug it up to my monitor, I’ll give you guys a report. Seems to be missing DisplayPort and no HDMI either. Must be some new type of port that I might need an adapter for. That’s what I get for being on the bleeding edge I guess. 🤓&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ye3dq51qlxme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j46y3s</id>
    <title>The Mac Studio has been benchmarked with Llama 3.1 405B</title>
    <updated>2025-03-05T16:24:26+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"&gt; &lt;img alt="The Mac Studio has been benchmarked with Llama 3.1 405B" src="https://b.thumbs.redditmedia.com/KgXuoJ3u9SQcUqdCeM_SCtqNLq6m2wfX0B_6FzjMdlk.jpg" title="The Mac Studio has been benchmarked with Llama 3.1 405B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's my guess based on HF models size in GGUF format. MaziyarPanahi/Meta-Llama-3.1-405B-Instruct-GGUF Q3_K_S&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j46y3s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T16:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b8kc</id>
    <title>QwQ 32B-GGUF quants available!</title>
    <updated>2025-03-05T19:15:35+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"&gt; &lt;img alt="QwQ 32B-GGUF quants available!" src="https://external-preview.redd.it/RTxLEWJN9iyy-yoootrctgBomXSk2KFDde9mtFbC1SI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9a36c0f53de7d1bab8eff48da8fe7dc3f2a37c4" title="QwQ 32B-GGUF quants available!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j47frd</id>
    <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
    <updated>2025-03-05T16:44:39+00:00</updated>
    <author>
      <name>/u/OC2608</name>
      <uri>https://old.reddit.com/user/OC2608</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This TTS method was made using Qwen 2.5. I think it's similar to Llasa. Not sure if already posted.&lt;/p&gt; &lt;p&gt;Hugging Face Space: &lt;a href="https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS"&gt;https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2503.01710"&gt;https://arxiv.org/pdf/2503.01710&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub Repository: &lt;a href="https://github.com/SparkAudio/Spark-TTS"&gt;https://github.com/SparkAudio/Spark-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/SparkAudio/Spark-TTS-0.5B"&gt;https://huggingface.co/SparkAudio/Spark-TTS-0.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demos: &lt;a href="https://sparkaudio.github.io/spark-tts/"&gt;https://sparkaudio.github.io/spark-tts/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OC2608"&gt; /u/OC2608 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T16:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4914s</id>
    <title>QwQ 32b demo available</title>
    <updated>2025-03-05T17:48:47+00:00</updated>
    <author>
      <name>/u/ryseek</name>
      <uri>https://old.reddit.com/user/ryseek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen uploaded an app, which supposedly uses QwQ 32b &lt;a href="https://huggingface.co/spaces/Qwen/QwQ-32B-Demo"&gt;https://huggingface.co/spaces/Qwen/QwQ-32B-Demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;No weights released yet. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryseek"&gt; /u/ryseek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4bi0g</id>
    <title>brainless Ollama naming about to strike again</title>
    <updated>2025-03-05T19:26:23+00:00</updated>
    <author>
      <name>/u/gpupoor</name>
      <uri>https://old.reddit.com/user/gpupoor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt; &lt;img alt="brainless Ollama naming about to strike again" src="https://preview.redd.it/hnw7tvbo9xme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff02886ac4815c2bc162e157351ae6d100b824d0" title="brainless Ollama naming about to strike again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpupoor"&gt; /u/gpupoor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hnw7tvbo9xme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j42py0</id>
    <title>OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp; 20+ Rich Interactions</title>
    <updated>2025-03-05T13:11:17+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt; &lt;img alt="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" src="https://preview.redd.it/jw78717wevme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992ec58ae3641fac0387c04211452b938e040836" title="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw78717wevme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T13:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j44vep</id>
    <title>Mac Studio just got 512GB of memory!</title>
    <updated>2025-03-05T14:55:01+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For $10,499 (in US), you get 512GB of memory and 4TB storage @ 819 GB/s memory bandwidth. This could be enough to run Llama 3.1 405B @ 8 tps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j417qh</id>
    <title>llama.cpp is all you need</title>
    <updated>2025-03-05T11:45:16+00:00</updated>
    <author>
      <name>/u/s-i-e-v-e</name>
      <uri>https://old.reddit.com/user/s-i-e-v-e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only started paying somewhat serious attention to locally-hosted LLMs earlier this year.&lt;/p&gt; &lt;p&gt;Went with ollama first. Used it for a while. Found out by accident that it is using llama.cpp. Decided to make life difficult by trying to compile the llama.cpp ROCm backend from source on Linux for a somewhat unsupported AMD card. Did not work. Gave up and went back to ollama.&lt;/p&gt; &lt;p&gt;Built a simple story writing helper cli tool for myself based on file includes to simplify lore management. Added ollama API support to it.&lt;/p&gt; &lt;p&gt;ollama randomly started to use CPU for inference while &lt;code&gt;ollama ps&lt;/code&gt; claimed that the GPU was being used. Decided to look for alternatives.&lt;/p&gt; &lt;p&gt;Found koboldcpp. Tried the same ROCm compilation thing. Did not work. Decided to run the regular version. To my surprise, it worked. Found that it was using vulkan. Did this for a couple of weeks.&lt;/p&gt; &lt;p&gt;Decided to try llama.cpp again, but the vulkan version. And it worked!!!&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; gives you a clean and extremely competent web-ui. Also provides an API endpoint (including an OpenAI compatible one). llama.cpp comes with a million other tools and is extremely tunable. You do not have to wait for other dependent applications to expose this functionality.&lt;/p&gt; &lt;p&gt;llama.cpp is all you need.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s-i-e-v-e"&gt; /u/s-i-e-v-e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T11:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43ziq</id>
    <title>The new king? M3 Ultra, 80 Core GPU, 512GB Memory</title>
    <updated>2025-03-05T14:13:32+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt; &lt;img alt="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" src="https://preview.redd.it/jkhal4p0qvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cb3ce2fdbe1423c5cf740e8f17c9c8df2f9e7b2" title="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. With 512GB of memory a world of possibilities opens up. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkhal4p0qvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3zxwn</id>
    <title>Are we ready!</title>
    <updated>2025-03-05T10:16:41+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt; &lt;img alt="Are we ready!" src="https://preview.redd.it/m0ktikjrjume1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dc8037f70763ba02e1ed164ff1654c69921dfd" title="Are we ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m0ktikjrjume1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T10:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43us5</id>
    <title>Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory</title>
    <updated>2025-03-05T14:07:13+00:00</updated>
    <author>
      <name>/u/iCruiser7</name>
      <uri>https://old.reddit.com/user/iCruiser7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt; &lt;img alt="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" src="https://external-preview.redd.it/IUc-sq0jBjlLBbxyREexc_Ijkq_kHcRXYNu-Mr7u5LI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cb29351c6e2bde66e4d208acbdf5c007acd170" title="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iCruiser7"&gt; /u/iCruiser7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b1t9</id>
    <title>QwQ-32B released, equivalent or surpassing full Deepseek-R1!</title>
    <updated>2025-03-05T19:08:01+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt; &lt;img alt="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" src="https://external-preview.redd.it/GjWMsqQ0sjAo2i1u3zMKBVF8QJTEurDWKLmSNIhLwOE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37675ad756a9c5a85511da4e75709d13466b2af3" title="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1897361654763151544"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4az6k</id>
    <title>Qwen/QwQ-32B · Hugging Face</title>
    <updated>2025-03-05T19:05:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt; &lt;img alt="Qwen/QwQ-32B · Hugging Face" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Qwen/QwQ-32B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:05:05+00:00</published>
  </entry>
</feed>
