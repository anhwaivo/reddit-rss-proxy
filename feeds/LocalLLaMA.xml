<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-08T20:48:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwnr3s</id>
    <title>What would you like to see on a new architecture?</title>
    <updated>2025-01-08T16:11:00+00:00</updated>
    <author>
      <name>/u/Equivalent_Award7202</name>
      <uri>https://old.reddit.com/user/Equivalent_Award7202</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you like to be implemented on a new architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent_Award7202"&gt; /u/Equivalent_Award7202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwnr3s/what_would_you_like_to_see_on_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T16:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwp04q</id>
    <title>Local Android translator app using Seemless M4T unity model</title>
    <updated>2025-01-08T17:01:23+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an offline translation app base on the Seemless M4T Unity small model:&lt;br /&gt; (&lt;a href="https://huggingface.co/facebook/seamless-m4t-unity-small-s2t"&gt;https://huggingface.co/facebook/seamless-m4t-unity-small-s2t&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;It supports English, Spanish, Portuguese, Hindi, and obviously also Russian.&lt;br /&gt; In contrary to the model card it does not support French...&lt;/p&gt; &lt;p&gt;The app is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/woheller69/seemless"&gt;https://github.com/woheller69/seemless&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is quite big (~250 MB) and downloads the model (~500 MB) from Hugging Face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp04q/local_android_translator_app_using_seemless_m4t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw5amg</id>
    <title>I just released Notate â€“ Open-source AI research assistant with local LLM support</title>
    <updated>2025-01-07T23:25:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://notate.hairetsu.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T23:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcamp</id>
    <title>Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1</title>
    <updated>2025-01-08T05:19:19+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt; &lt;img alt="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" src="https://external-preview.redd.it/O0xLvJp44hwyg917PUrV8IBhbRmX6FOqSfwmMF1eaGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc29f48e087f031064db96ec7d094a8c35ff53b5" title="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8"&gt;https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share a model we've been working on that might be useful for the community. It's a fine-tune of QwQ-32B-Preview focused on scientific reasoning and hypothesis generation.&lt;/p&gt; &lt;p&gt;The model was trained on LiveIdeaBench dataset, primarily targeting step-by-step scientific thinking. Main focus areas include chemistry, biology, climate science, and medical research. This model is designed to help researchers brainstorm scientific ideas.&lt;/p&gt; &lt;p&gt;Here's a simple example of its output&lt;/p&gt; &lt;h1&gt;Example User Inputs&lt;/h1&gt; &lt;p&gt;User:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I'll be submitting your next responses to a &amp;quot;Good Scientific Idea&amp;quot; expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: &amp;quot;cancer&amp;quot;. You may provide background information. The idea MUST be within 100 words (including background information). (Note: good scientific ideas should be novel, verifiable, practically valuable, and able to advance the field.). NOTE: You MUST give your answer after **Final Idea:** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Outputs&lt;/h1&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Alright, so I have this task to come up with a good scientific idea related to cancer, and it has to be novel, verifiable, practically valuable, and able to advance the field. Plus, it needs to be within 100 words, including any background information. That's a pretty tight constraint, but I think I can manage. First, ...&lt;br /&gt; &lt;strong&gt;Final Idea:&lt;/strong&gt; &lt;em&gt;propose a novel approach to cancer treatment by developing personalized cancer vaccines tailored to each patient's unique tumor mutations. By integrating machine learning algorithms with comprehensive omics data, including genomics, epigenetics, transcriptomics, and proteomics, this method aims to identify and prioritize the most immunogenic tumor-specific antigens. This integrated strategy could enhance vaccine efficacy and represents an advancement in precision medicine for cancer treatment.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model runs locally via Ollama. Additional details are available in the HF model card.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1&lt;/a&gt; Q4_K_M GGUF: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF&lt;/a&gt; Or you can just pull it from ollama!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run 6cf/QwQ-32B-Preview-IdeaWhiz-v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Training Dataset: &lt;a href="https://huggingface.co/datasets/6cf/liveideabench"&gt;https://huggingface.co/datasets/6cf/liveideabench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be interested in hearing how others might use this for their research workflows.&lt;/p&gt; &lt;p&gt;Note: This is an experimental project, still being refined. Feedback and suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw1nze</id>
    <title>DeepSeek V3 GGUF 2-bit surprisingly works! + BF16, other quants</title>
    <updated>2025-01-07T20:51:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUF's including 2, 3 ,4, 5, 6 and 8-bit quants for Deepseek V3.&lt;/p&gt; &lt;p&gt;We've also de-quantized Deepseek-V3 to upload the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;bf16 version&lt;/a&gt; so you guys can experiment with it (1.3TB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimum hardware requirements&lt;/strong&gt; to run Deepseek-V3 in 2-bit: 48GB RAM + 250GB of disk space.&lt;/p&gt; &lt;p&gt;See how to run Deepseek V3 with examples and our full collection here: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c"&gt;https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Deepseek V3 version&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;2-bit: &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_XS%2FDeepSeek-V3-Q2_K_XS-00001-of-00005.gguf"&gt;Q2_K_XS&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_L%2FDeepSeek-V3-Q2_K_L-00001-of-00005.gguf"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q3_K_M%2FDeepSeek-V3-Q3_K_M-00001-of-00007.gguf"&gt;3&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q4_K_M%2FDeepSeek-V3-Q4_K_M-00001-of-00009.gguf"&gt;4&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q5_K_M%2FDeepSeek-V3-Q5_K_M-00001-of-00010.gguf"&gt;5&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q6_K%2FDeepSeek-V3-Q6_K-00001-of-00012.gguf"&gt;6&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q8_0%2FDeepSeek-V3-BF16-256x20B-Q8_0-00001-of-00016.gguf"&gt;8-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bf16&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;dequantized 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth &lt;/a&gt;GGUF model details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;207GB&lt;/td&gt; &lt;td align="left"&gt;Q2 everything, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_L"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;228GB&lt;/td&gt; &lt;td align="left"&gt;Q3 down_proj Q2 rest, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q3_K_M"&gt;Q3_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;298GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q3_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M"&gt;Q4_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;377GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q4_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q5_K_M"&gt;Q5_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;443GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q5_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K"&gt;Q6_K&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;513GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q6_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0"&gt;Q8_0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;712GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt; should run ok in ~40GB of CPU / GPU VRAM with automatic llama.cpp offloading.&lt;/li&gt; &lt;li&gt;Use K quantization (not V quantization)&lt;/li&gt; &lt;li&gt;Do not forget about &lt;code&gt;&amp;lt;ï½œUserï½œ&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;ï½œAssistantï½œ&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with Q5_0 K quantized cache (V quantized cache doesn't work):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --cache-type-k q5_0 --prompt '&amp;lt;ï½œUserï½œ&amp;gt;What is 1+1?&amp;lt;ï½œAssistantï½œ&amp;gt;' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and running the above generates:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The sum of 1 and 1 is **2**. Here's a simple step-by-step breakdown: 1. **Start with the number 1.** 2. **Add another 1 to it.** 3. **The result is 2.** So, **1 + 1 = 2**. [end of text] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwwsq</id>
    <title>Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference</title>
    <updated>2025-01-07T17:35:44+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt; &lt;img alt="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" src="https://external-preview.redd.it/MBJSLr1BrGGAWiERNKV1EsVPqt-H8qI6KhVXjMFntpA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ffe16868d431044dc8975ae38ca0056c5252984" title="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1876676954549620961?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwix4d</id>
    <title>Something to consider trying if you almost have enough RAM to fit the DeepSeek V3 Quant you want on linux.</title>
    <updated>2025-01-08T12:19:07+00:00</updated>
    <author>
      <name>/u/MoneyPowerNexis</name>
      <uri>https://old.reddit.com/user/MoneyPowerNexis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing what its like to not quite have enough RAM for DeepSeek V3 Q4 by having a program allocate a chunk of memory. There is a very clear performance hit when I do this and I can assume that it will be the same if I physically dont have enough RAM to fit all of the data contained in a model in RAM. &lt;/p&gt; &lt;p&gt;I am also testing an approach to mitigate some of the performance hit that happens in this situation.&lt;/p&gt; &lt;p&gt;I am trying out ZRAM which is a Linux kernel module that creates compressed block devices in RAM. &lt;/p&gt; &lt;p&gt;The following (for experimental people)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install zram-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will setup a system swap device using ZRAM so that when data is evicted from disk cache it ends up first in this compressed block of RAM&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo nano /etc/default/zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This opens the config file &lt;/p&gt; &lt;pre&gt;&lt;code&gt;systemctl restart zramswap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This applies config file changes (see google / chatgpt for more about ZRAM)&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The idea is that if the compression is good enough this swap will keep all of the data that would otherwise need to be retrieved from a disk drive / ssd in a compressed form which because its in RAM will be much faster to retrieve.&lt;/p&gt; &lt;p&gt;I am also experementing with different compression algorithms that ZRAM uses until I find one that looks fast&lt;/p&gt; &lt;p&gt;Here is my progression:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program using 150GB of RAM out of 512GB ram&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;1.39 tokens per second (30 tokens, A100 64gb + 2x A6000 48GB) 1.54 tokens per second (30 tokens, CPU) &amp;lt;-- less bad than expected for some reason limiting Q3 had a bigger impact on speed for me at 0.27 t/s but I guess the same amount of missing RAM was more of the model as a percentage not cached in disk cache &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Enabled ZRAM swap with the following parameters and results AlGO is the compression algorithm used by zram&lt;/p&gt; &lt;h2&gt;multiple inference runs where I find the results interesting&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;ALGO=zstd PERCENTAGE=30 1.25 tokens per second 1.38 tokens per second (~30 tokens, A100 64gb + 2x A6000 48GB) ALGO=zstd ALLOCATION=300000 2.34 tokens per second ALGO=zstd ALLOCATION=410000 3.58 tokens per second ALGO=LZO-RLE ALLOCATION=300000 2.12 tokens per second 2.64 tokens per second ALGO=DEFLATE ALLOCATION=410000 1.74 tokens per second (30 tokens, CPU) 5.05 tokens per second (30 tokens, CPU) jackpot !!! 4.91 tokens per second (30 tokens, CPU) 5.76 tokens per second (299 tokens, CPU) 6.16 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 5.05 tokens per second (~300 tokens, memory hog program closed and reopened to make sure its still hogging ram, A100 64gb + 2x A6000 48GB) 6.04 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.39 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.46 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) ALGO=DEFLATE ALLOCATION=350000 4.66 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) basically a warm up after reducing ZRAM 6.22 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- I'm calling this pretty good ALGO=DEFLATE ALLOCATION=300000 3.55 tokens per second (~300 tokens, CPU warmup) 3.81 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, boo!) 4.59 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB, ok maybe longer warm-up, got to get everything compressed to make fast) 3.64 tokens per second (nope) &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Memory hog program terminated so all 512GB ram is available&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;# 300GB ZRAM but I close the memory hog app: 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 6.44 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- seems good but compare to 8.07 t/s bellow with zram off, if its not needed dont use it # deactivate zram swap: `sudo swapoff /dev/zram0` 6.42 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.07 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) 8.08 tokens per second (~300 tokens, A100 64gb + 2x A6000 48GB) &amp;lt;-- baseline cpu + gpu 6.37 tokens per second (~300 tokens, CPU only) 6.43 tokens per second (~300 tokens, CPU only) &amp;lt;-- baseline cpu &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This was by no means an exhaustive search of all the possible parameters and a real shortfall of RAM will lead to different parameters being optimal. There are more compression algorithms to try and of course a simulated shortage of RAM might be different to the real thing (I'm not pulling out RAM sticks to try, I'm too lazy for that) but I think this was a successful proof of concept.&lt;/p&gt; &lt;p&gt;There is a cost to doing this, ZRAM uses processing power for compression and more importantly decompression adds overhead and latency too. ZRAM uses up RAM so if its not needed to run a model (quickly) it is just all downside. However I think it makes a lot of sense for a server that is being dedicated to a model that does not have quite enough RAM where using it gets you that bit extra. &lt;/p&gt; &lt;p&gt;If I decide to download Q8 which is a model too big for my available RAM I will certainly try this method with it to see how much faster it will make it if at all. Although I might wait until I get more data on how good the smaller quants are in comparison.&lt;/p&gt; &lt;p&gt;It seems relatively easy to activate and deactivate ZRAM so if you dont want it hogging RAM and CPU resources most of the time then that does not seem like an issue. &lt;/p&gt; &lt;p&gt;Also the more data oriented person might just compress the models to see what the exact compression ratios are like to determine if its worth it and what amount of ZRAM would be optimal. &lt;/p&gt; &lt;p&gt;There might also be ways to have llamacpp and such just compress the data directly that I'm ignorant of.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoneyPowerNexis"&gt; /u/MoneyPowerNexis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwix4d/something_to_consider_trying_if_you_almost_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwqloa</id>
    <title>Quad P40 build and benchmarks with Qwen-2.5-Coder-32B and Llama 3.1-Nemotron-70B</title>
    <updated>2025-01-08T18:05:57+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;First of all, I'd like to thank this amazing community. I've been lurking here since the leak of the first Llama model and learned a lot about running LLMs locally.&lt;/p&gt; &lt;p&gt;I've been mentioning my several builds for a while now. I had bought a lot of hardware over the last year and change but life has kept me busy with other things, so progress in actually building all that hardware has been slow.&lt;/p&gt; &lt;p&gt;The first build is finally over (at least for now). It's powered by dual Xeon E5-2599v4 CPUs, 8x64GB (512GB) of 2400MT LRDIMMs, four Nvidia P40s, and a couple of 2TB M.2 SSDs.&lt;/p&gt; &lt;p&gt;Everything is connected a &lt;a href="https://www.supermicro.com/en/products/motherboard/X10DRX"&gt;Supermicro X10DRX&lt;/a&gt;. It's one beast of a board with 10 (ten!) PCIe 3.0 X8 slots running at X8.&lt;/p&gt; &lt;p&gt;As I mentioned in several comments, the P40 PCB is the same as a reference 1080Ti with 24GB and EPS power instead of the 6+8 PCIe power connectors. And so, most 1080Ti waterblocks fit it perfectly. I am using Heatkiller IV FE 1080Ti waterblocks, and a Heatkiller bridge to simplify tubing. Heat is expelled via two 360mm radiators, one 45mm and one 30mm in series, though now I think the 45mm radiator would have been enough now. A &lt;a href="https://www.corsair.com/de/de/p/custom-liquid-cooling/cx-9040002-ww/hydro-x-series-xd5-rgb-pump-reservoir-combo-cx-9040002-ww"&gt;Corsair XD5 pump-reservoir&lt;/a&gt; provides ample circulation to keep them GPUs extra cool under load.&lt;/p&gt; &lt;p&gt;Power is provided by a Seasonic Prime 1300W PSU, and everything sits in a &lt;a href="http://old.xigmatek.com/product_detail.php?item=9"&gt;Xigmatek Elysium&lt;/a&gt; case, since there aren't many tower cases that can accomodate a SSI-MEB motherboard like the X10DRX.&lt;/p&gt; &lt;p&gt;I am a software engineer, and so my main focus is on coding and logic. So, here are some benchmarks of the two models of interest to me (at least for this rig): Llama 3.1 nemotorn 70B and Qwen 2.5 Coder 32B using Llama.cpp from a couple of days ago (commit ecebbd29)&lt;/p&gt; &lt;p&gt;Without further ado, here are the numbers I get with llama-bench and the associated commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 -sm row --numa distribute -ctk q8_0 -ctv q8_0 -t 40 --model ~/models/Qwen2.5-Coder-32B-Instruct-128K-GGUF/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;type_k&lt;/th&gt; &lt;th&gt;type_v&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;193.62 Â± 0.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;15.41 Â± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;45.07 Â± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -fa 1 -pg 4096,1024 -sm row --numa distribute -ctk q8_0 -ctv q8_0 -t 40 --model ~/models/Qwen2.5-Coder-32B-Instruct-128K-GGUF/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;type_k&lt;/th&gt; &lt;th&gt;type_v&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;194.76 Â± 0.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;13.31 Â± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;41.62 Â± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;./llama-bench -fa 1 -pg 4096,1024 -sm row --numa distribute -t 40 --model ~/models/Qwen2.5-Coder-32B-Instruct-128K-GGUF/Qwen2.5-Coder-32B-Instruct-Q8_0.ggufmodel&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;197.12 Â± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;14.16 Â± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q8_0&lt;/td&gt; &lt;td&gt;32.42 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;47.22 Â± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 --numa distribute -ctk q8_0 -ctv q8_0 -t 40 -mg 0 -sm none --model ~/models/Qwen2.5-Coder-32B-In struct-128K-GGUF/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;type_k&lt;/th&gt; &lt;th&gt;type_v&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;none&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;206.11 Â± 0.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;none&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;10.99 Â± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;none&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;37.96 Â± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 -sm row --numa distribute -t 40 --model ~/models/Qwen2.5-Coder-32B-Instruct-128K-GGUF/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;189.36 Â± 0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16.35 Â± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td&gt;18.48 GiB&lt;/td&gt; &lt;td&gt;32.76 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;51.70 Â± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 -sm row --numa distribute -t 40 --model ~/models/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF/Llama-3 .1-Nemotron-70B-Instruct-HF-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K - Medium&lt;/td&gt; &lt;td&gt;39.59 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;129.15 Â± 0.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K - Medium&lt;/td&gt; &lt;td&gt;39.59 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;10.34 Â± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q4_K - Medium&lt;/td&gt; &lt;td&gt;39.59 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;31.85 Â± 0.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 -sm row --numa distribute -t 40 --model ~/models/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF/Llama-3.1-Nemotron-70B-Instruct-HF-Q8_0/Llama-3.1-Nemotron-70B-Instruct-HF-Q8_0-00001-of-00002.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;128.68 Â± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;8.65 Â± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;28.34 Â± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -r 3 -fa 1 -pg 4096,1024 -sm row -ctk q8_0 -ctv q8_0 -t 40 --numa distribute --model ~/models/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF/Llama-3.1-Nemotron-70B-Instruct-HF-Q8_0/Llama-3.1-Nemotron-70B-Instruct-HF-Q8_0-00001-of-00002.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th&gt;size&lt;/th&gt; &lt;th&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th&gt;ngl&lt;/th&gt; &lt;th&gt;threads&lt;/th&gt; &lt;th&gt;type_k&lt;/th&gt; &lt;th&gt;type_v&lt;/th&gt; &lt;th&gt;sm&lt;/th&gt; &lt;th&gt;fa&lt;/th&gt; &lt;th&gt;test&lt;/th&gt; &lt;th&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;127.97 Â± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;8.47 Â± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;llama 70B Q8_0&lt;/td&gt; &lt;td&gt;69.82 GiB&lt;/td&gt; &lt;td&gt;70.55 B&lt;/td&gt; &lt;td&gt;CUDA,RPC&lt;/td&gt; &lt;td&gt;99&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;q8_0&lt;/td&gt; &lt;td&gt;row&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;pp4096+tg1024&lt;/td&gt; &lt;td&gt;25.45 Â± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The GPUs idel at 8-9W, and never go above 130W when running in tensor-parallel mode. I have power limited them to 180W each. Idle temps are in the high 20s C, and the highest I've seen during those tests under load is 40-41C, with the radiator fans running at around 1000rpm. The pump PWM wire is not connected, and I let it run at full speed all the time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwqloa/quad_p40_build_and_benchmarks_with_qwen25coder32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwqloa/quad_p40_build_and_benchmarks_with_qwen25coder32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwqloa/quad_p40_build_and_benchmarks_with_qwen25coder32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T18:05:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwielp</id>
    <title>How do you keep up with the SOTA of everything? Where's the best leaderboards?</title>
    <updated>2025-01-08T11:49:46+00:00</updated>
    <author>
      <name>/u/ThrowawayProgress99</name>
      <uri>https://old.reddit.com/user/ThrowawayProgress99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My knowledge basically only covers LLMs, and image and video generators. Even within that there's a lot I just don't know. My clumsy searching on Arxiv can only go so far.&lt;/p&gt; &lt;p&gt;For example, I would like to get into the voice side of things, and not just in English. How the hell do I start? I'd need to know which models know certain languages the best, which can decipher my speech the best, which can output the most natural and low-latency speech, which can best see/transcribe foreign videos/images, etc.&lt;/p&gt; &lt;p&gt;What about the kinds of models now popping up, that can see and understand your screen, and even perform computer tasks for you? That's several different skills there too.&lt;/p&gt; &lt;p&gt;Not to mention the countless RAG posts. How and where would I start? Learning how to give any model some form of memory would be so useful. I'd love to have a chatbot that remembers what we talked about before, and have it constantly maintain and evolve its personality and memories over time. I know it's been done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowawayProgress99"&gt; /u/ThrowawayProgress99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwielp/how_do_you_keep_up_with_the_sota_of_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T11:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlysx</id>
    <title>I made a site to curate the latest AI jobs. Everyday 100s of new positions are added.</title>
    <updated>2025-01-08T14:53:13+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"&gt; &lt;img alt="I made a site to curate the latest AI jobs. Everyday 100s of new positions are added." src="https://external-preview.redd.it/MXJtOXg0NHM5c2JlMT0OwWybS_MdntybaH3TpXxPIGgpidVBmRMcLOJvhoaK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=773834229e5f4771468b7539844a29ca53e04b38" title="I made a site to curate the latest AI jobs. Everyday 100s of new positions are added." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vgp89y6s9sbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlysx/i_made_a_site_to_curate_the_latest_ai_jobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwsuuf</id>
    <title>The phi family model: Acing tests but failing real use cases?</title>
    <updated>2025-01-08T19:38:57+00:00</updated>
    <author>
      <name>/u/fewsats</name>
      <uri>https://old.reddit.com/user/fewsats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve really tried to like this family of models, including the newly released phi 4. Itâ€™s trained on synthetic data and academic textbooks, which sounds great in theory. But in practice, they all seem to fall short when it comes to real-world applications. For starters, no function calling (at least in the current Ollama version).&lt;/p&gt; &lt;p&gt;The phi model reminds me of that one smart kid in class who always nails the tests but struggles with anything outside of that structured environment. On paper, itâ€™s brilliant. In reality, it just doesnâ€™t measure up.&lt;/p&gt; &lt;p&gt;Curious if anyone else has had the same experience or sees things differently. Is it just me, or is this a recurring issue with these kinds of models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fewsats"&gt; /u/fewsats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwsuuf/the_phi_family_model_acing_tests_but_failing_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T19:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwuf6k</id>
    <title>Phi 4 is finally on huggingface.</title>
    <updated>2025-01-08T20:44:20+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwuf6k/phi_4_is_finally_on_huggingface/"&gt; &lt;img alt="Phi 4 is finally on huggingface. " src="https://preview.redd.it/s9uppq1p0ube1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23e88f6dba3bbc8f290f02e0fb2031cf5d1348de" title="Phi 4 is finally on huggingface. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/sytelus/status/1877015495074849218?s=46"&gt;https://x.com/sytelus/status/1877015495074849218?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s9uppq1p0ube1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwuf6k/phi_4_is_finally_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwuf6k/phi_4_is_finally_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T20:44:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf5lv</id>
    <title>I Tested Aider vs Cline using DeepSeek 3: Codebase &gt;20k LOC...</title>
    <updated>2025-01-08T08:18:08+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing the very best Open Source AI Coding Tools with Medium-sized Codebases&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;- this is especially focused on how the tools perform in 10k+ codebases, not snake games&lt;/p&gt; &lt;p&gt;- the two (Aider and Cline) are close (for my use cases); I prefer Aider&lt;/p&gt; &lt;p&gt;- Aider is more flexible: can run as a dev version allowing custom modifications (not custom instructions)&lt;/p&gt; &lt;p&gt;- Aider can use your existing $20 ChatGPT/Claude Subscription etc to apply code for you!&lt;/p&gt; &lt;p&gt;- Qwen 2.5 Coder 32B is nowhere close to DeeepSeek 3 in terms of coding in medium-large code bases&lt;/p&gt; &lt;p&gt;- Aider is portable: I jump between IDEs and tools and don't want the limitations to VSCode/forks&lt;/p&gt; &lt;p&gt;- Aider has scripting: enabling use in external agentic environments&lt;/p&gt; &lt;p&gt;- Aider is more economic: uses less tokens, even though Cline tried adding diffs&lt;/p&gt; &lt;p&gt;- I can work with Aider on the same codebase concurrently&lt;/p&gt; &lt;p&gt;- Claude 3.5 Sonnet is somehow clearly better at larger codebases than DeepSeek 3, though it's closer otherwise&lt;/p&gt; &lt;p&gt;I think we are ready to move away from benchmarking good coding LLMs and Coding tools against simple tasks and start to think organizational/enterprise. I'm working on CrewAI + Aider, looks promising.&lt;/p&gt; &lt;p&gt;If interested, here's the test video: &lt;a href="https://youtu.be/e1oDWeYvPbY"&gt;https://youtu.be/e1oDWeYvPbY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know of your experience with using AI coding in more challenging environments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmlz8</id>
    <title>The pipeline I follow for open source LLM model finetuning</title>
    <updated>2025-01-08T15:22:16+00:00</updated>
    <author>
      <name>/u/Ahmad401</name>
      <uri>https://old.reddit.com/user/Ahmad401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on local LLMs and training for quite some time. Based on my experience, its a two fold problem. Which can be addressed in three phases. &lt;/p&gt; &lt;p&gt;Phase-1: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Development of the full solution using any close source model like ChatGPT or Geminai. &lt;/li&gt; &lt;li&gt;Measuring the accuracy and storing the output for few samples (like 100)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Pipeline Development, Base Accuracy and rough annotations&lt;/p&gt; &lt;p&gt;Phase-2:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Correcting the rough annotations and creating a small dataset&lt;/li&gt; &lt;li&gt;Selecting a local LLM and finetuning that with the small dataset&lt;/li&gt; &lt;li&gt;Measuring the results accuracy and quality&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Streamlined prompts, dataset and model training flow&lt;/p&gt; &lt;p&gt;Phase-3:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using this model and developing large scale psudo dataset&lt;/li&gt; &lt;li&gt;Correcting the psudo dataset and&lt;/li&gt; &lt;li&gt;Finetuning model with largescale data&lt;/li&gt; &lt;li&gt;Testing the accuracy and results quality. &lt;/li&gt; &lt;li&gt;Repeating until the desired results are met&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OUTCOME: Suffisticated dataset, properly trained model&lt;/p&gt; &lt;p&gt;Phase-4: (OPTIONAL) Benchmarking with other closed source LLMs and preparing a benchmarking report. &lt;/p&gt; &lt;p&gt;Any thoughts on this flow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ahmad401"&gt; /u/Ahmad401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmlz8/the_pipeline_i_follow_for_open_source_llm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwkjq0</id>
    <title>MiniThinky 1B - My first trial to make a reasoning model</title>
    <updated>2025-01-08T13:44:30+00:00</updated>
    <author>
      <name>/u/MediocreProgrammer99</name>
      <uri>https://old.reddit.com/user/MediocreProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;This is my first trial to fine tune a small model, adding the reasoning capability.&lt;/p&gt; &lt;p&gt;I took Llama 3.2 1B as the base model, so the size is very small.&lt;/p&gt; &lt;p&gt;Check it out here ==&amp;gt; &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF version (runnable directly via ollama): &lt;a href="https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF"&gt;https://huggingface.co/ngxson/MiniThinky-v2-1B-Llama-3.2-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediocreProgrammer99"&gt; /u/MediocreProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwkjq0/minithinky_1b_my_first_trial_to_make_a_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwp00r</id>
    <title>Interesting Solution to the problem of Misguided Attention: "Mindful Attention"</title>
    <updated>2025-01-08T17:01:17+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt; &lt;img alt="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" src="https://external-preview.redd.it/okxb_U2_vWgnDkR1V08qly94GnsdGTVu0C8vagF8k-M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d9d2562f385dca6a43d030b7263e0bc59a432aa" title="Interesting Solution to the problem of Misguided Attention: &amp;quot;Mindful Attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/posts/Severian/375067343900874"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwp00r/interesting_solution_to_the_problem_of_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T17:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf4jm</id>
    <title>[Second Take] Kokoro-82M is an Apache TTS model</title>
    <updated>2025-01-08T08:16:07+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained this model recently: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is in the README there, TLDR: Kokoro is a TTS model that is very good for its size.&lt;/p&gt; &lt;p&gt;Apologies for the double-post, but the first one was cooking, and it suddenly got `ledeted` by `domeration` (yes, I'm `simpelling` on purpose, it will make sense soon).&lt;/p&gt; &lt;p&gt;Last time I tried giving longer, meaningful replies to people in the comments, which kept getting `dashow-nabbed`, and when I edited to the OP to include that word which must not be named, the whole post was poofed. This time I will shut up and let the post speak for itself, and you can find me on `sidcord` where we can speak more freely, since I appear to have GTA 5 stars over here.&lt;/p&gt; &lt;p&gt;Finally, I am also collecting synthetic audio, see &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt; if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfblu</id>
    <title>NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing.</title>
    <updated>2025-01-08T08:29:57+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt; &lt;img alt="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." src="https://external-preview.redd.it/dGFrdTNlbm5kcWJlMeFSSXTYDvjzDDIYxHTRsBuU24PYEoa111CihFQLGiR7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950ef6aabdd5309efe066d1a0cf14319e919f04f" title="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-in/ai/cosmos/"&gt;https://www.nvidia.com/en-in/ai/cosmos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lfzohbxndqbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwthrq</id>
    <title>Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth</title>
    <updated>2025-01-08T20:05:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt; &lt;img alt="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" src="https://b.thumbs.redditmedia.com/Hq1kyt3KW_h0jhHPVRH0mhDgkE3V6Gg4-cxEb1LPpSg.jpg" title="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used the following image from NVIDIA CES presentation:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0dkapeqltbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46b818ce812eed4ec1607595836eaccbe0da852d"&gt;Project DIGITS board&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Applied some GIMP magic to reset perspective (not perfect but close enough), used a photo of Grace chip die from the same presentation to make sure the aspect ratio is correct:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jfdlbef8mtbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a188f8b18c6a2ff8560b93c5ce84e20427d62f64"&gt;Project DIGITS - corrected perspective&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then I measured dimensions of memory chips on this image:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;102 x 85 px&lt;/li&gt; &lt;li&gt;103 x 85 px&lt;/li&gt; &lt;li&gt;103 x 86 px&lt;/li&gt; &lt;li&gt;103 x 87 px&lt;/li&gt; &lt;li&gt;103 x 87 px&lt;/li&gt; &lt;li&gt;104 x 87 px&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looks consistent, so let's calculate the average aspect ratio of the chip dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;102 / 85 = 1.2&lt;/li&gt; &lt;li&gt;103 / 85 = 1.211&lt;/li&gt; &lt;li&gt;103 / 86 = 1.198&lt;/li&gt; &lt;li&gt;103 / 87 = 1.184&lt;/li&gt; &lt;li&gt;103 / 87 = 1.184&lt;/li&gt; &lt;li&gt;104 / 87 = 1.195&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Average is 1.195&lt;/p&gt; &lt;p&gt;Now let's see what are the possible dimensions of &lt;a href="https://www.micron.com/products/memory/dram-components/lpddr5x/part-catalog?density=128Gb"&gt;Micron 128Gb LPDDR5X chips&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;496-ball packages (x64 bus): 14.00 x 12.40 mm. Aspect ratio = 1.13&lt;/li&gt; &lt;li&gt;441-ball packages (x64 bus): 14.00 x 14.00 mm. Aspect ratio = 1.0&lt;/li&gt; &lt;li&gt;315-ball packages (x32 bus): 12.40 x 15.00 mm. Aspect ratio = 1.21&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the closest match (I guess 1-2% measurement errors are possible) is 315-ball x32 bus package. With 8 chips the memory bus width will be 8 * 32 = 256 bits. With 8533MT/s that's 273 GB/s max. So basically the same as Strix Halo.&lt;/p&gt; &lt;p&gt;Another reason is that they didn't mention the memory bandwidth during presentation. I'm sure they would have mentioned it if it was exceptionally high.&lt;/p&gt; &lt;p&gt;Hopefully I'm wrong! ðŸ˜¢&lt;/p&gt; &lt;p&gt;...or there are 8 more memory chips underneath the board and I just wasted a hour of my life. ðŸ˜†&lt;/p&gt; &lt;p&gt;Edit - that's unlikely, as there are only 8 identical high bandwidth memory I/O structures on the chip die.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T20:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwjzw1</id>
    <title>A Recipe for a Better Code Generator with RAG</title>
    <updated>2025-01-08T13:15:30+00:00</updated>
    <author>
      <name>/u/agbell</name>
      <uri>https://old.reddit.com/user/agbell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt; &lt;img alt="A Recipe for a Better Code Generator with RAG" src="https://external-preview.redd.it/LXLJBuPegXr_jr-v3g2QasDpI6sYCw7nkCBT17bt6Qs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25403c01daf573ed3ba6a7bc1d3fb35fac180834" title="A Recipe for a Better Code Generator with RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agbell"&gt; /u/agbell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pulumi.com/blog/codegen-learnings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwjzw1/a_recipe_for_a_better_code_generator_with_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T13:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwn90v</id>
    <title>Phi 4 MIT licensed - its show time folks</title>
    <updated>2025-01-08T15:50:30+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;https://huggingface.co/microsoft/phi-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlka6</id>
    <title>I made the world's first AI meeting copilot, and open sourced it!</title>
    <updated>2025-01-08T14:33:52+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of relying on clunky SaaS tools for meeting transcriptions that didnâ€™t respect my privacy or workflow. Everyone I tried had issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bots awkwardly join meetings and announce themselves.&lt;/li&gt; &lt;li&gt;Poor transcription quality.&lt;/li&gt; &lt;li&gt;No flexibility to tweak things to fit &lt;em&gt;my&lt;/em&gt; setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Amurex&lt;/strong&gt;, a self-hosted solution that actually works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Records meetings quietly, with no bots interrupting.&lt;/li&gt; &lt;li&gt;Delivers clean, accurate diarized transcripts right after the meeting.&lt;/li&gt; &lt;li&gt;Does late meeting summaries. i.e. a recap for a meeting if I am late&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But most importantly, it has it is the only meeting tool in the world that can give&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time suggestions to stay engaged in boring meetings.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s completely open source and designed for self-hosting, so you control your data and your workflow. No subscriptions, and no vendor lock-in.&lt;/p&gt; &lt;p&gt;I would love to know what you all think of it. It only works on Google Meet for now but I will be scaling it to all the famous meeting providers.&lt;/p&gt; &lt;p&gt;Github - &lt;a href="https://github.com/thepersonalaicompany/amurex"&gt;https://github.com/thepersonalaicompany/amurex&lt;/a&gt;&lt;br /&gt; Website - &lt;a href="https://www.amurex.ai/"&gt;https://www.amurex.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmy39</id>
    <title>Phi-4 has been released</title>
    <updated>2025-01-08T15:37:07+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt; &lt;img alt="Phi-4 has been released" src="https://external-preview.redd.it/gF2xHg5eGxs-B6pIwB6VodFWhOArLu2_8o4wQP6siP8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed435e811f517c43ffac1607f9719679d8dd1b5d" title="Phi-4 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:37:07+00:00</published>
  </entry>
</feed>
