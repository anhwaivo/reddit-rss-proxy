<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-18T05:06:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1is4exs</id>
    <title>GROK-3 to come with “Big Brain” and “DeepSearch”</title>
    <updated>2025-02-18T04:32:03+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4exs/grok3_to_come_with_big_brain_and_deepsearch/"&gt; &lt;img alt="GROK-3 to come with “Big Brain” and “DeepSearch”" src="https://preview.redd.it/uulm3yrlstje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd966cca4dca8a532467ddada259838813febabe" title="GROK-3 to come with “Big Brain” and “DeepSearch”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uulm3yrlstje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4exs/grok3_to_come_with_big_brain_and_deepsearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4exs/grok3_to_come_with_big_brain_and_deepsearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:32:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1irklio</id>
    <title>Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)</title>
    <updated>2025-02-17T14:06:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt; &lt;img alt="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" src="https://external-preview.redd.it/phQl7qnxBNwj9zS7K_vfIQhmHll-NjLQgZ0PIR30DxA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=623bc922d387ff0d98a441576fd2df6e097cc4a6" title="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/stepfun-ai/step-audio-67b33accf45735bb21131b0b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4joi</id>
    <title>alibaba mnn released its full multimodal ios app, models fully run local</title>
    <updated>2025-02-18T04:39:29+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt; &lt;img alt="alibaba mnn released its full multimodal ios app, models fully run local" src="https://external-preview.redd.it/nFgqKmZCG_UKkOoGTTwMgu4Lps-UbpzBRzAqIHJJI9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b425d8842d03ac390b8862521ef50395c10fca5b" title="alibaba mnn released its full multimodal ios app, models fully run local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6yfovbv0ttje1.png?width=2010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=310222dc12625a3f0e0382d0bb06a4d44c00d6c8"&gt;https://preview.redd.it/6yfovbv0ttje1.png?width=2010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=310222dc12625a3f0e0382d0bb06a4d44c00d6c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o5napto8ttje1.gif"&gt;https://i.redd.it/o5napto8ttje1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;support text to text， image to text，audio to text&lt;br /&gt; github: &lt;a href="https://github.com/alibaba/MNN/blob/master/apps/iOS/MNNLLMChat/README.md"&gt;https://github.com/alibaba/MNN/blob/master/apps/iOS/MNNLLMChat/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previously released android app: &lt;a href="https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md"&gt;https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk7vq</id>
    <title>What to expect in 2025 for running big LLMs</title>
    <updated>2025-02-17T13:48:12+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy hardware by the end of this year for running local LLMs. Since Deepseek R1 spoiled me and raised my expectations I was thinking about bigger models (32-70B or maybe hard-quantized R1).&lt;/p&gt; &lt;p&gt;Is there any hardware coming soon or a super efficient model, new architecture etc. In 2025 to enable running these models for &amp;lt;3k Euro at 10+ tokens/s?&lt;/p&gt; &lt;p&gt;What I am watching: - Nvidia Digits - AMD AI Max Pro 395&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1ksy</id>
    <title>Optimal Local LLM Setup for a 300-Person Research Institute – Hardware &amp; Software Stack Advice?</title>
    <updated>2025-02-18T02:04:28+00:00</updated>
    <author>
      <name>/u/Standing_Appa8</name>
      <uri>https://old.reddit.com/user/Standing_Appa8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m planning to deploy a local LLM server at my research institute (around 300 people) that can handle various tasks across different departments. I’m particularly interested in both hardware and software stack recommendations to manage the expected traffic efficiently.&lt;/p&gt; &lt;p&gt;I recently came across a high-end setup that featured: - A top-tier CPU (e.g., Ryzen 9 9950x) with a high-quality, upgrade-friendly motherboard&lt;br /&gt; - 256GB DDR5 RAM and fast NVMe storage (2×2TB SN850X)&lt;br /&gt; - Redundant power supply and a custom watercooling loop for reliable, continuous operation&lt;br /&gt; - Dual high-end GPUs (L40S with 48GB VRAM each) for handling large models and multiple concurrent requests&lt;br /&gt; - For lighter workloads, a single GPU like an Nvidia 3090 (24GB) might be enough&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Software: &lt;ul&gt; &lt;li&gt;Using an inference container (like Ollama) with environment variables (e.g., &lt;code&gt;OLLAMA_NUM_PARALLEL=4&lt;/code&gt;) to handle up to 4 concurrent requests, relying on continuous batching&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Web-Ui&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love to get your thoughts on the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Traffic &amp;amp; Concurrency: For roughly 5 concurrent users per 100 people (with each session lasting up to an hour), what would be the best approach to managing traffic? Should I consider a single multi-GPU server, or is a distributed/multi-node setup more effective?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Software Stack Recommendations &lt;/p&gt; &lt;ul&gt; &lt;li&gt;What are your experiences with using inference engines like Ollama versus alternatives such as vLLM?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Are there other software stacks, container orchestration systems, or batching strategies that can help optimize concurrent request handling for diverse tasks?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;How do you manage smart unloading of models and resource allocation when switching tasks on the fly?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights, real-world experiences, or alternative suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance for your help and ideas.&lt;/p&gt; &lt;p&gt;I would like to draw a techniquel diagram for the next meeting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standing_Appa8"&gt; /u/Standing_Appa8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1ksy/optimal_local_llm_setup_for_a_300person_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1ksy/optimal_local_llm_setup_for_a_300person_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1ksy/optimal_local_llm_setup_for_a_300person_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T02:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1is2s3q</id>
    <title>DeepSeek 1.5B on Android</title>
    <updated>2025-02-18T03:05:21+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"&gt; &lt;img alt="DeepSeek 1.5B on Android" src="https://external-preview.redd.it/M29td3JyazRkdGplMREgtlSh22tIg8ofupSivEXbZ27zomojPoV29y7odKjy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=878850029aea6767826fe847b74d4b0b586e60f0" title="DeepSeek 1.5B on Android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently release v0.8.5 of ChatterUI with some minor improvements to the app, including fixed support for DeepSeek-R1 distills and an entirely reworked styling system:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Vali-98/ChatterUI/releases/tag/v0.8.5"&gt;https://github.com/Vali-98/ChatterUI/releases/tag/v0.8.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall, I'd say the responses of the 1.5b and 8b distills are slightly better than the base models, but its still very limited output wise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3zz0ipp4dtje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T03:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvmhv</id>
    <title>What are the odds these are legit?</title>
    <updated>2025-02-17T21:36:53+00:00</updated>
    <author>
      <name>/u/jwil00</name>
      <uri>https://old.reddit.com/user/jwil00</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"&gt; &lt;img alt="What are the odds these are legit?" src="https://b.thumbs.redditmedia.com/nwPSVmSRhkBTlmLfEnbl7r1VJW_spzxnNyBtXwxYWlY.jpg" title="What are the odds these are legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously I’m thinking not great, but the deal was too good to pass up for all that VRAM.&lt;/p&gt; &lt;p&gt;Also, how would I go about confirming their legitimacy once they arrive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwil00"&gt; /u/jwil00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irvmhv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irvmhv/what_are_the_odds_these_are_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T21:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1irkkeo</id>
    <title>Mistral Saba | Mistral AI (Not Open Sourced)</title>
    <updated>2025-02-17T14:05:15+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt; &lt;img alt="Mistral Saba | Mistral AI (Not Open Sourced)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Saba | Mistral AI (Not Open Sourced)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/en/news/mistral-saba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iryolp</id>
    <title>Build LLM ground up</title>
    <updated>2025-02-17T23:46:51+00:00</updated>
    <author>
      <name>/u/meetrais</name>
      <uri>https://old.reddit.com/user/meetrais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;My this GitHub repository contains code for building an LLM from the ground up, step by step.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/meetrais/A-Z-of-Tranformer-Architecture"&gt;https://github.com/meetrais/A-Z-of-Tranformer-Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meetrais"&gt; /u/meetrais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iryolp/build_llm_ground_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iryolp/build_llm_ground_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iryolp/build_llm_ground_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T23:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1irgfkp</id>
    <title>all I said was "hi"</title>
    <updated>2025-02-17T09:55:06+00:00</updated>
    <author>
      <name>/u/CaptTechno</name>
      <uri>https://old.reddit.com/user/CaptTechno</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt; &lt;img alt="all I said was &amp;quot;hi&amp;quot;" src="https://preview.redd.it/16ci5no49oje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de52b259bcf0f525d3c7a4880c7597d54d6e38f4" title="all I said was &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptTechno"&gt; /u/CaptTechno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ci5no49oje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T09:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlv4q</id>
    <title>The Hugging Face NLP course is back with chapters on fine-tuning LLMs</title>
    <updated>2025-02-17T15:05:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt; &lt;img alt="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" src="https://external-preview.redd.it/tQnPN9F6mNbTPcfQVDpQv5OONbx7hdsHVHxXCjSZIqM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfa4dea237900e2d3b867be223ed501cd9b3f4b7" title="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nlp-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T15:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk3r6</id>
    <title>New (linear complexity ) Transformer architecture achieved improved performance</title>
    <updated>2025-02-17T13:42:34+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://robinwu218.github.io/ToST/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwx6q</id>
    <title>DeepSeek-v2.5 dynamic quants anyone?</title>
    <updated>2025-02-17T22:30:13+00:00</updated>
    <author>
      <name>/u/Enturbulated</name>
      <uri>https://old.reddit.com/user/Enturbulated</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The unsloth dynamic quants of DeepSeek-R1 made some waves recently.&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There has been some interest expressed in giving other models the same treatment. Weeks later, not seen much done about it. Maybe I've been looking in the wrong places? Maybe nobody has because DSR1 is particularly amenable to this treatment and there's little real payoff for other models?&lt;/p&gt; &lt;p&gt;Regardless, looking at what other MoE models might benefit, one very easy answer is the DeepSeek v2 model series. Mainly because unsloth's llama.cpp fork for this requires fairly little effort to modify for this use.&lt;/p&gt; &lt;p&gt;So, what the hell.&lt;br /&gt; &lt;a href="https://huggingface.co/Enturbulate/DeepSeek-v2.5-1210-UD-gguf"&gt;https://huggingface.co/Enturbulate/DeepSeek-v2.5-1210-UD-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Five quants posted, iq1_s through iq3_m, ~49GB through ~97GB. imatrix data klepped from bartowski. Thanks!&lt;/p&gt; &lt;p&gt;The quantization strategy is pretty simple-minded, basically just don't let the attention/output layers drop below q4_k. Is this optimal? LOL. Should still perform better than standard llama.cpp low-bit quants.&lt;/p&gt; &lt;p&gt;Anyone want to share thoughts on what other models, if any, might be worth some effort?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Enturbulated"&gt; /u/Enturbulated &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irwx6q/deepseekv25_dynamic_quants_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T22:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlihr</id>
    <title>LLMs already have ads (sort of)</title>
    <updated>2025-02-17T14:50:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt; &lt;img alt="LLMs already have ads (sort of)" src="https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4b56b82708f12907eed5cb9688415ff2947f8a5" title="LLMs already have ads (sort of)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Your AI assistant might already have a built-in corporate bias&lt;/p&gt; &lt;p&gt;I think most of us here wondered how LLMs will map out to a traditional ad-driven business model. The consensus was that LLMs could be used in a similar way by showing bias towards specific products or brands.&lt;/p&gt; &lt;p&gt;There's a paper in ICLR 2025 that shows that it already happens to an extent: &lt;a href="https://openreview.net/forum?id=odjMSBSWRt"&gt;DarkBench: Benchmarking Dark Patterns in Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark of 660 prompts to test for manipulative behaviors in LLMs&lt;/li&gt; &lt;li&gt;one of the main &amp;quot;dark patterns&amp;quot; they found was &lt;strong&gt;brand bias&lt;/strong&gt; - LLMs actively promoting their parent company's products over competitors &lt;ul&gt; &lt;li&gt;Detected in LLMs from OpenAI, Anthropic, Meta, Google, and Mistral&lt;/li&gt; &lt;li&gt;Mistral 8x7B was was the only model showing high manipulation but NO brand bias (french are le cool again)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458"&gt;https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples of the bias categories as identified by authors:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902"&gt;https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full dataset on HF: &lt;a href="https://huggingface.co/datasets/anonymous152311/darkbench"&gt;https://huggingface.co/datasets/anonymous152311/darkbench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwhkz</id>
    <title>Mistral Saba</title>
    <updated>2025-02-17T22:12:21+00:00</updated>
    <author>
      <name>/u/Pleasant-PolarBear</name>
      <uri>https://old.reddit.com/user/Pleasant-PolarBear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"&gt; &lt;img alt="Mistral Saba" src="https://b.thumbs.redditmedia.com/Y-zL84O1MHPzF3H4PHjtXzbGVQMTeskg-HRUs4R7LEk.jpg" title="Mistral Saba" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant-PolarBear"&gt; /u/Pleasant-PolarBear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irwhkz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irwhkz/mistral_saba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T22:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iru2qq</id>
    <title>We added open models support to RA.Aid and need help testing</title>
    <updated>2025-02-17T20:34:58+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"&gt; &lt;img alt="We added open models support to RA.Aid and need help testing" src="https://external-preview.redd.it/aDJudDkyczdmcmplMdTGmhKtGIygTvOfC6af7JgWNHXPN4qj83wOXgAySB8l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=667212a835edd61ecdf20652eb344d105fcc750e" title="We added open models support to RA.Aid and need help testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r5db71s7frje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iru2qq/we_added_open_models_support_to_raaid_and_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4fm6</id>
    <title>My new local inference rig</title>
    <updated>2025-02-18T04:33:05+00:00</updated>
    <author>
      <name>/u/Jackalzaq</name>
      <uri>https://old.reddit.com/user/Jackalzaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"&gt; &lt;img alt="My new local inference rig" src="https://a.thumbs.redditmedia.com/QFvwlttKs0TaTL0gcHjxfR2osnKgT81xH5Ciajzzy78.jpg" title="My new local inference rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Supermicro sys 2048gr trt2 with 8x instinct mi60s with a sysrack enclosure so i dont lose my mind.&lt;/p&gt; &lt;p&gt;R1 1.58bit dynamic quant (671b) runs at around 4-6 tok per second Llama 405b q4km at about 1.5 tok per second&lt;/p&gt; &lt;p&gt;With no cpu offloading my context is around 12k and 8k respectively. Havent tested it with partial cpu offloading yet.&lt;/p&gt; &lt;p&gt;Sound can get up to over 70db when the case is open and stays around 50db when running inference with case closed.&lt;/p&gt; &lt;p&gt;Also using two separate circuits for this build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jackalzaq"&gt; /u/Jackalzaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is4fm6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4geo</id>
    <title>GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1</title>
    <updated>2025-02-18T04:34:22+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"&gt; &lt;img alt="GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1" src="https://preview.redd.it/8dwhr7o0ttje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1647f77aba24121cd40230ddff75d1a85e640b28" title="GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dwhr7o0ttje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhttv</id>
    <title>Zonos, the easy to use, 1.6B, open weight, text-to-speech model that creates new speech or clones voices from 10 second clips</title>
    <updated>2025-02-17T11:31:55+00:00</updated>
    <author>
      <name>/u/SoundHole</name>
      <uri>https://old.reddit.com/user/SoundHole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;hr /&gt; &lt;p&gt;I started experimenting with this model that dropped around a week ago &amp;amp; it performs fantastically, but I haven't seen any posts here about it so thought maybe it's my turn to share.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Zonos runs on as little as 8GB vram &amp;amp; converts any text to audio speech. It can also clone voices using clips between 10 &amp;amp; 30 seconds long. In my limited experience toying with the model, the results are convincing, especially if time is taken curating the samples (I recommend &lt;a href="https://www.ocenaudio.com/en/"&gt;Ocenaudio&lt;/a&gt; for a noob friendly audio editor).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;It is amazingly easy to set up &amp;amp; run via Docker (if you are using Linux. Which you should be. I am, by the way).&lt;/p&gt; &lt;p&gt;EDIT: Someone posted a &lt;a href="https://github.com/sdbds/Zonos-for-windows"&gt;Windows friendly fork&lt;/a&gt; that I absolutely cannot vouch for.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;First, install the singular special dependency:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;apt install -y espeak-ng &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Then, instead of running a uv as the authors suggest, I went with the much simpler &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid#docker-installation"&gt;Docker Installation&lt;/a&gt; instructions, which consists of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloning the repo &lt;/li&gt; &lt;li&gt;Running 'docker compose up' inside the cloned directory&lt;/li&gt; &lt;li&gt;Pointing a browser to &lt;a href="http://0.0.0.0:7860/"&gt;http://0.0.0.0:7860/&lt;/a&gt; for the UI&lt;/li&gt; &lt;li&gt;Don't forget to 'docker compose down' when you're finished&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Oh my goodness, it's brilliant!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The model is here: &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;Zonos Transformer&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;There's also a &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;hybrid model&lt;/a&gt;. I'm not sure what the difference is, there's no elaboration, so, I've only used the transformer myself.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you're using Windows... I'm not sure what to tell you. The authors straight up claim Windows is not currently supported but there's always VM's or whatever whatever. Maybe someone can post a solution.&lt;/p&gt; &lt;p&gt;Hope someone finds this useful or fun!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;EDIT: &lt;a href="https://www.sndup.net/crc4m/"&gt;Here's an example&lt;/a&gt; I quickly whipped up on the default settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoundHole"&gt; /u/SoundHole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T11:31:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpx0b</id>
    <title>Don’t sleep on The Allen Institute for AI (AI2)</title>
    <updated>2025-02-17T17:51:51+00:00</updated>
    <author>
      <name>/u/dontbanana</name>
      <uri>https://old.reddit.com/user/dontbanana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Allen Institute says its open-source model can beat DeepSeek&lt;/p&gt; &lt;p&gt;“The same tricks: AI2’s models use a novel reinforcement learning technique—training by way of “rewards” and “punishments” for right and wrong outputs—in which the model is taught to solve math or other problems with verifiable answers. DeepSeek used similar reinforcement learning techniques to train its models on reasoning tasks.&lt;/p&gt; &lt;p&gt;“It is pretty much, I would even argue, identical,” Hajishirzi said. “It is very simple… we had it in this paper in late November and DeepSeek came after us. Someone was asking me, ‘Did they actually copy what you did?’ I said, ‘I don’t know. It was so close that each team could come up with this independently.’ So, I don’t know, but it’s open research. A lot of these ideas could be shared.””&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dontbanana"&gt; /u/dontbanana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.emergingtechbrew.com/stories/2025/02/07/allen-institute-open-source-model-deepseek?mbcid=38624075.320719&amp;amp;mblid=76a9d29d5c33&amp;amp;mid=4bf97fa50758e4f9907627b7deaa5807&amp;amp;utm_campaign=etb&amp;amp;utm_medium=newsletter&amp;amp;utm_source=morning_brew"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpozr</id>
    <title>Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!</title>
    <updated>2025-02-17T17:43:11+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt; &lt;img alt="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" src="https://external-preview.redd.it/R6NtBwOFUehmfI110Qr1QSx4QJoALZS5zC4GvG9AcZo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ceb3ec7bb36d8267e1328a5ec597a5541141024" title="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-36B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1eht</id>
    <title>ClosedAI Next Open Source</title>
    <updated>2025-02-18T01:55:55+00:00</updated>
    <author>
      <name>/u/cabsterman</name>
      <uri>https://old.reddit.com/user/cabsterman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"&gt; &lt;img alt="ClosedAI Next Open Source" src="https://preview.redd.it/grv77lpq0tje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fd9c4adfa555517144d41c5b320ff875075d9bf" title="ClosedAI Next Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/sama/status/1891667332105109653"&gt;https://x.com/sama/status/1891667332105109653&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cabsterman"&gt; /u/cabsterman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/grv77lpq0tje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iry4lu</id>
    <title>How can I optimize my 1.000.000B MoE Reasoning LLM?</title>
    <updated>2025-02-17T23:21:26+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, my mum built this LLM for me called Brain, it has a weird architecture that resembles MoE but its called MoL (Mixture of Lobes), it has around 1 000 000B parameters (synapses) but it's not performing that well on MMLU pro, it gives me a lot of errors with complicated tasks, and I'm struggling to activate the frontal &lt;del&gt;Expert&lt;/del&gt; lobe, it also hallucinates 1/3 of the time, especially at night. It might be some hardware issue since I had no money for an RTX 5090 and I'm instead running it on frozen food and coke. At least it is truly multimodal since it works well with audio and images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T23:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1p2o</id>
    <title>The normies have failed us</title>
    <updated>2025-02-18T02:10:22+00:00</updated>
    <author>
      <name>/u/RenoHadreas</name>
      <uri>https://old.reddit.com/user/RenoHadreas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt; &lt;img alt="The normies have failed us" src="https://preview.redd.it/fosxvznb3tje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38f047fe1dccff4127d9e6709f4812f4bce14d3d" title="The normies have failed us" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenoHadreas"&gt; /u/RenoHadreas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fosxvznb3tje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T02:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1f37</id>
    <title>Sam Altman's poll on open sourcing a model..</title>
    <updated>2025-02-18T01:56:43+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1f37/sam_altmans_poll_on_open_sourcing_a_model/"&gt; &lt;img alt="Sam Altman's poll on open sourcing a model.." src="https://preview.redd.it/dug7nt8n0tje1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30650d5ea5e7314921ac37102332f6c1aae8d6c5" title="Sam Altman's poll on open sourcing a model.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dug7nt8n0tje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1f37/sam_altmans_poll_on_open_sourcing_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1f37/sam_altmans_poll_on_open_sourcing_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T01:56:43+00:00</published>
  </entry>
</feed>
