<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-02T21:21:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1if3lq1</id>
    <title>Sam Altman acknowledges R1</title>
    <updated>2025-02-01T10:31:35+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt; &lt;img alt="Sam Altman acknowledges R1" src="https://preview.redd.it/ot5nsk399ige1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca17a8d86fa20881ff4876577c465ae2c733d9" title="Sam Altman acknowledges R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Straight from the horses mouth. Without R1, or bigger picture open source competitive models, we wouldn’t be seeing this level of acknowledgement from OpenAI. &lt;/p&gt; &lt;p&gt;This highlights the importance of having open models, not only that, but open models that actively compete and put pressure on closed models. &lt;/p&gt; &lt;p&gt;R1 for me feels like a real &lt;em&gt;hard takeoff&lt;/em&gt; moment. &lt;/p&gt; &lt;p&gt;No longer can OpenAI or other closed companies dictate the rate of release. &lt;/p&gt; &lt;p&gt;No longer do we have to get the scraps of what they decide to give us. &lt;/p&gt; &lt;p&gt;Now they have to actively compete in an open market.&lt;/p&gt; &lt;p&gt;No moat. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Source: &lt;a href="https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC"&gt;https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ot5nsk399ige1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffgj4</id>
    <title>DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)</title>
    <updated>2025-02-01T20:24:46+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt; &lt;img alt="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" src="https://external-preview.redd.it/gMJZu1czNWIsX2vol0q37qYGLTI_zKgwHfEyO-m9Uqw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48df94f11b5f4243cdde43be4517d1e3d09e3712" title="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wKZHoGlllu4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig5ycw</id>
    <title>New Docker Guide for R2R's (Reason-to-Retrieve) local AI system</title>
    <updated>2025-02-02T19:54:23+00:00</updated>
    <author>
      <name>/u/docsoc1</name>
      <uri>https://old.reddit.com/user/docsoc1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I just put together a quick beginner’s guide for &lt;a href="https://r2r-docs.sciphi.ai/introduction"&gt;&lt;strong&gt;R2R&lt;/strong&gt;&lt;/a&gt; — an all-in-one open source AI Retrieval-Augmented Generation system that’s easy to self-host and super flexible for a range of use cases. R2R lets you ingest documents (PDFs, images, audio, JSON, etc.) into a local or cloud-based knowledge store, and then query them using advanced hybrid or graph-based search. It even supports multi-step “agentic” reasoning if you want more powerful question answering, coding hints, or domain-specific Q&amp;amp;A on your private data.&lt;/p&gt; &lt;p&gt;I’ve included some references and commands below for anyone new to Docker or Docker Swarm. If you have any questions, feel free to ask!&lt;/p&gt; &lt;h1&gt;Link-List&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Service&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Owners Website&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://sciphi.ai/"&gt;https://sciphi.ai/&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/SciPhi-AI/R2R"&gt;https://github.com/SciPhi-AI/R2R&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Docker &amp;amp; Full Installation Guide&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://r2r-docs.sciphi.ai/self-hosting/installation/full/docker"&gt;Self-Hosting (Docker)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Quickstart Docs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://r2r-docs.sciphi.ai/self-hosting/quickstart"&gt;R2R Quickstart&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Basic Setup Snippet&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install the CLI &amp;amp; Python SDK -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;pip install r2r &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Launch R2R with Docker&lt;/strong&gt;(This command pulls all necessary images and starts the R2R stack — including Postgres/pgvector and the Hatchet ingestion service.)&lt;/p&gt; &lt;p&gt;export OPENAI_API_KEY=sk-...&lt;/p&gt; &lt;p&gt;r2r serve --docker --full &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Verify It’s Running&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open a browser and go to: &lt;a href="http://localhost:7272/v3/health"&gt;&lt;code&gt;http://localhost:7272/v3/health&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You should see: &lt;code&gt;{&amp;quot;results&amp;quot;:{&amp;quot;response&amp;quot;:&amp;quot;ok&amp;quot;}}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Optional&lt;/strong&gt;: &lt;/p&gt; &lt;p&gt;For local LLM inference, you can try the &lt;code&gt;--config-name=full_local_llm&lt;/code&gt; option and run with Ollama or another local LLM provider.&lt;/p&gt; &lt;p&gt;After that, you’ll have a self-hosted system ready to index and query your documents with advanced retrieval. You can also spin up the web apps at &lt;a href="http://localhost:7273"&gt;&lt;code&gt;http://localhost:7273&lt;/code&gt;&lt;/a&gt; and &lt;a href="http://localhost:7274"&gt;&lt;code&gt;http://localhost:7274&lt;/code&gt;&lt;/a&gt; depending on your chosen config.&lt;/p&gt; &lt;h1&gt;Screenshots / Demo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Search &amp;amp; RAG&lt;/strong&gt;: Quickly run &lt;code&gt;r2r retrieval rag --query=&amp;quot;What is X?&amp;quot;&lt;/code&gt; from the CLI to test out the retrieval.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic RAG&lt;/strong&gt;: For multi-step reasoning, &lt;code&gt;r2r retrieval rawr --query=&amp;quot;Explain X to me like I’m 5&amp;quot;&lt;/code&gt; takes advantage of the built-in reasoning agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope you guys enjoy my work! I’m here to help with any questions, feedback, or configuration tips. Let me know if you try R2R or have any recommendations for improvements.&lt;/p&gt; &lt;p&gt;Happy self-hosting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/docsoc1"&gt; /u/docsoc1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig5ycw/new_docker_guide_for_r2rs_reasontoretrieve_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig5ycw/new_docker_guide_for_r2rs_reasontoretrieve_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig5ycw/new_docker_guide_for_r2rs_reasontoretrieve_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T19:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifrf9t</id>
    <title>Deepseek R1 just told me to fist a frozen orange, for real. (This model is AMAZING)</title>
    <updated>2025-02-02T06:45:50+00:00</updated>
    <author>
      <name>/u/IversusAI</name>
      <uri>https://old.reddit.com/user/IversusAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Open WebUI and Deepseek R1 through Open Router to build my own healbot to help heal from sugar and wheat addiction. I was talking to the model, &lt;strong&gt;which is AMAZING&lt;/strong&gt; no joke and I was trying to make it to 10:00pm (when the store closes) and it was giving me help and suggestions to get through.&lt;/p&gt; &lt;p&gt;Note: My system prompt does NOT have anything in it about being explicit. It just asks the model to help me recover and how I want it to act (kind, supportive, etc).&lt;/p&gt; &lt;h3&gt;I had just asked it to help me get to 10:00pm:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/5Y97e8x.jpeg"&gt;https://i.imgur.com/5Y97e8x.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Yeah, there will be no frozen orange fisting, mkay?&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/LAVYIPM.jpeg"&gt;https://i.imgur.com/LAVYIPM.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;LOLOLOL:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/c8ss1p4.jpeg"&gt;https://i.imgur.com/c8ss1p4.jpeg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S.: I did make it to 10pm and the cravings eased. :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IversusAI"&gt; /u/IversusAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T06:45:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7q8n</id>
    <title>Which one is the best open-source model for coding/SWE-tasks?</title>
    <updated>2025-02-02T21:07:12+00:00</updated>
    <author>
      <name>/u/fazkan</name>
      <uri>https://old.reddit.com/user/fazkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last time I checked llama-coder was the only one specialized. Also which benchmarks are relevant here. SWE-bench is the only one that I know of. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fazkan"&gt; /u/fazkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffboy</id>
    <title>SmolVLM fully open source</title>
    <updated>2025-02-01T20:19:01+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt; &lt;img alt="SmolVLM fully open source" src="https://external-preview.redd.it/RpBd16Y386MrSYjhSF5aL1O5cjq2V0xWVKGs2JQsIl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9476c8b4dd1bf85443ac42ac9be87b98d3ff2e1e" title="SmolVLM fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/andimarafioti/status/1885341684134978035"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ift9ik</id>
    <title>Andrew Ng: DeepSeek-R1 and the Future of Generative AI</title>
    <updated>2025-02-02T08:59:00+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hpcwire.com/off-the-wire/andrew-ng-deepseek-r1-and-the-future-of-generative-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ift9ik/andrew_ng_deepseekr1_and_the_future_of_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ift9ik/andrew_ng_deepseekr1_and_the_future_of_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T08:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifltll</id>
    <title>I tested 11 popular local LLM's against my instruction-heavy game/application</title>
    <updated>2025-02-02T01:23:20+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;I have a few applications with some relatively large system prompts for how to handle requests. A lot of them use very strict JSON-formatting. I've scripted benchmarks for them going through a series of real use-case inputs and outputs and here's what I found&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;A dungeon-master scenario. The LLM first plays the role of the dungeon master, being fed state and inventory and then needing to take a user action/decision - reporting the output. The LLM is then responsible for reading over its own response and updating state and inventory JSON, quantity, locations, notes, descriptions, etc based on the content of the story. There are A LOT of rules involved, including of course actually successfully interacting with structured data. Successful models will both be able to advance the story in a very sane way given the long script of inputs/responses (I review afterwards) and track both state and inventory in the desired format.&lt;/p&gt; &lt;h1&gt;Rules&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;32b or less. Llama 3.3 70b performs this task superbly, but i want something that will feasibly run well on GPUs a regular consumer owns. I'm considering that 32gb of high bandwidth memory or VRAM or less.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;no API-only models&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;all quants are Q6. I tested Q8's but results were identical &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;context window of tests accommodates smaller models in that any test that goes over is thrown out&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;temperature is within the model author's recommended range, leaning slightly towards less-creative outputs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;instruct versions unless otherwise specified&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results (best to worst)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Phi4 14b&lt;/strong&gt; - Best by far. Not as smart as some of the others on this list, but it nails the response format instructions and rules 100% of the time. Being 14b its naturally very fast.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 2 22b&lt;/strong&gt; - Best balance. Extremely smart and superb at the interpretation and problem solving portion of the task. Will occasionally fail on JSON output but rarely&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen 32b Instruct&lt;/strong&gt; - this model was probably the smartest of them all. If handed a complex scenario, it would come up with what I considered the best logical solution, however it was pretty poor at JSON and rule-following&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 3 24b&lt;/strong&gt; - this one disappointed me. It's very clever and smart, but compared to the older Mistral Small 2, it's much weaker at instructon following. It could only track state for a short time before it would start deleting or forgetting items and events. Good at JSON format though.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-R1-Distill 32b&lt;/strong&gt; - smart(er) than Qwen 32b instruct but would completely flop on instruction following every 2-3 sequences. Amazing at interpreting state and story, but fell flat on its face with instructions and JSON.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral-Nemo 12b&lt;/strong&gt; - I like this model a lot. It punches higher than its benchmarks consistently and it will get through a number of sequences just fine, but it eventually hallucinates and returns either nonsense JSON, breaks rules, or loses track of state.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Falcon 3 10b&lt;/strong&gt; - Extremelt fast, shockingly smart, but would reliably produce a totally hallucinated output and content every few sequences&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Llama 3.1 8b&lt;/strong&gt; - follows instructions well, but hallucinated JSON formatting and contents far too often to be usable &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Codestral 22b&lt;/strong&gt; - a coding model!? for this? Well yeah - it actually nails the JSON 100% of the time, - but the story/content generation and understanding of actions and their impact on state were terrible. It also would inevitably enter a loop of nonsense output&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-Coder 32b&lt;/strong&gt; - exactly the same as Codestral, just with even worse writing. I love this model &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Nous-Hermes 3 8b&lt;/strong&gt; - slightly worse than regular Llama3.1 8b. Generated far more interesting (better written?) text in sections that allowed it though. This model to me is always &amp;quot;Llama 3.1 that went to art school instead of STEM&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(bonus) &lt;strong&gt;Llama 3.2 3b&lt;/strong&gt; - runs at lightspeed, I want this to be the future of local LLMs - but it's not a fair fight for the little guy. It goes off the rails or fails to follow instructions&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Phi4 14b is the best &lt;em&gt;so far&lt;/em&gt;. It just follows instructions well. But it's not as creative or natural in writing as Llama-based models, nor is it as intelligent or clever as Qwen or Mistral. It's the best at this test, there is no denying it, but i don't particularly enjoy its content compared to the flavor and intelligence of the other models tested. Mistral-Nemo 12b getting close to following instructions and struggling sug&lt;/p&gt; &lt;p&gt;&lt;strong&gt;if you have any other models you'd like to test this against, please mention them!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7ruy</id>
    <title>A bunch of LLMs scheduled to come at end of January were cancelled / delayed</title>
    <updated>2025-02-02T21:09:06+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all ix-nixed by deepseek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/patience_cave/status/1886122517359886745"&gt;https://x.com/patience_cave/status/1886122517359886745&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;gemini 2 - don’t even think about it&lt;/li&gt; &lt;li&gt;grok 3 - welcome to elon timelines&lt;/li&gt; &lt;li&gt;o3 - an unforeseen problem occurred&lt;/li&gt; &lt;li&gt;opus 3.5 - long gone&lt;/li&gt; &lt;li&gt;llama 4 - won’t be sota (sorry zuck)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifmiuu</id>
    <title>Open WebUI Coder Overhaul is now live on GitHub for testing!</title>
    <updated>2025-02-02T02:00:14+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt; &lt;img alt="Open WebUI Coder Overhaul is now live on GitHub for testing!" src="https://external-preview.redd.it/pnWZlhrdj8zYupK7N1vmH0H-SivyUx9-OTH-kjV-R7g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf5d066eb9e2c66c37f101ed9464a98b1e14a000" title="Open WebUI Coder Overhaul is now live on GitHub for testing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Some of you may be familiar with the project I've been working on for the past couple of weeks here that essentially overhauls the OpenWebUI artifacts system and makes it closer to ChatGPT's Canvas or Claude Artifacts. Well, I just published the code and it's available for testing! I really would love some help from people who have real world use cases for this and have them submit issues, pull requests, or feature requests on GitHub!&lt;/p&gt; &lt;p&gt;Here is a brief breakdown on the features:&lt;/p&gt; &lt;p&gt;A side code editor similar to ChatGPT and Claude, supporting a LOT of coding languages. You can cycle through all code blocks in a chat.&lt;/p&gt; &lt;p&gt;A design view mode that lets you see HTML (now with typescript styles included by default) and also React components&lt;/p&gt; &lt;p&gt;A difference viewer that shows you what changed in a code block if an LLM made changes&lt;/p&gt; &lt;p&gt;Code blocks will be shown as attachments in the regular chat while the editor is opened, like Claude.&lt;/p&gt; &lt;p&gt;I hope you all enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T02:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig05tf</id>
    <title>Beginner Walkthrough to Install LLMs on Windows</title>
    <updated>2025-02-02T15:52:10+00:00</updated>
    <author>
      <name>/u/rdmDgnrtd</name>
      <uri>https://old.reddit.com/user/rdmDgnrtd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About six months ago I started a concerted effort to revisit my initial skepticism of LLMs and really try to understand how to get value out of them. As I went through my learning curve, I realized that a lot of the content I was reading either presupposed knowledge I didn't have, or was not easy to follow because of guidelines geared towards using Linux or MacOS. I've been writing the guide I wish I had when I started, which I keep updating as new development happen and as I explore things further. I hope this can help newcomers, feedback welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.oliviertravers.com/running-llms-locally-the-getting-started-windows-stack/"&gt;https://www.oliviertravers.com/running-llms-locally-the-getting-started-windows-stack/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rdmDgnrtd"&gt; /u/rdmDgnrtd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T15:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifi0qu</id>
    <title>Missouri Senator Josh Hawley proposes a ban on Chinese AI models</title>
    <updated>2025-02-01T22:19:39+00:00</updated>
    <author>
      <name>/u/InquisitiveInque</name>
      <uri>https://old.reddit.com/user/InquisitiveInque</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InquisitiveInque"&gt; /u/InquisitiveInque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T22:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifuiiu</id>
    <title>What does your current model lineup look like? Heres mine</title>
    <updated>2025-02-02T10:31:12+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"&gt; &lt;img alt="What does your current model lineup look like? Heres mine" src="https://preview.redd.it/jz2j4geydpge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dc81856dffe6a71d07c12209cea3e879ccb588c" title="What does your current model lineup look like? Heres mine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jz2j4geydpge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T10:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifjuh8</id>
    <title>Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap.</title>
    <updated>2025-02-01T23:45:19+00:00</updated>
    <author>
      <name>/u/convalytics</name>
      <uri>https://old.reddit.com/user/convalytics</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt; &lt;img alt="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." src="https://preview.redd.it/sb0m382v6mge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d527da27d452ede8dca689a709e00f143244cadb" title="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After many reboots and fiddling with blacklisting noveau/nouveau, it's finally working! &lt;/p&gt; &lt;p&gt;36GB of vram goodness and 64GB of system ram. &lt;/p&gt; &lt;p&gt;Planning to install ollama, open-webui and n8n. Any more recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/convalytics"&gt; /u/convalytics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb0m382v6mge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T23:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqagd</id>
    <title>R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible.</title>
    <updated>2025-02-02T05:33:40+00:00</updated>
    <author>
      <name>/u/redditisunproductive</name>
      <uri>https://old.reddit.com/user/redditisunproductive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt; &lt;img alt="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." src="https://external-preview.redd.it/sqLJ5r2pSW7H_l-8ii2E6-qgsVr8VlF7vTFjMAJ9Xb0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1afa3c2905a1dc6f720fe43b945c8b51870613b7" title="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditisunproductive"&gt; /u/redditisunproductive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vectara/hallucination-leaderboard"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T05:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3m32</id>
    <title>Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API</title>
    <updated>2025-02-02T18:17:46+00:00</updated>
    <author>
      <name>/u/HIVVIH</name>
      <uri>https://old.reddit.com/user/HIVVIH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt; &lt;img alt="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" src="https://preview.redd.it/mwax9fnbprge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e20eb50542a043d90fbbd09b00d1b8f8ffea5e4" title="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HIVVIH"&gt; /u/HIVVIH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mwax9fnbprge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyu5b</id>
    <title>Would open weighting GPT 3.5 be interesting for us at all?</title>
    <updated>2025-02-02T14:52:07+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, please explain if the community would care about such legacy model release? Would the same apply to 4o released in like 2 years?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4g1u</id>
    <title>Medtator, local Llama research medical helper</title>
    <updated>2025-02-02T18:51:52+00:00</updated>
    <author>
      <name>/u/RaiRamz</name>
      <uri>https://old.reddit.com/user/RaiRamz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt; &lt;img alt="Medtator, local Llama research medical helper" src="https://preview.redd.it/g7qcnt3fvrge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0da08cfc3f47f01ce1481a781c7b65ab51fcc7f" title="Medtator, local Llama research medical helper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a Medical AI Chatbot Using Llama 3 and PubMed&lt;/p&gt; &lt;p&gt;Hey everyone, I’ve been working on a project that combines Llama 3 with PubMed to create an AI-powered medical research assistant. Meet Medtator—a chatbot that retrieves, summarizes, and explains medical literature from PubMed in an accessible way.&lt;/p&gt; &lt;p&gt;How It Works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;User asks a medical question in natural language.&lt;/li&gt; &lt;li&gt;Keyword extraction (via a transformer-based pipeline) identifies relevant biomedical terms. &lt;/li&gt; &lt;li&gt;PubTator API fetches PubMed articles, ranking the most relevant studies.&lt;/li&gt; &lt;li&gt;Llama 3 generates concise answers using the retrieved context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;Searching PubMed is a pain. If you’re a medical student, researcher, or just a nerd like me, you know how frustrating it is to sift through thousands of articles. Medtator helps find relevant papers quickly and provides AI-assisted summaries to save time.&lt;/p&gt; &lt;p&gt;Challenges &amp;amp; Learnings: • Keyword extraction is tricky—LLMs sometimes miss the right biomedical terms. • Context window limitations force careful article selection to avoid hallucination. • Fine-tuning vs. prompt engineering—so far, smart prompting works surprisingly well.&lt;/p&gt; &lt;p&gt;Still improving it, but I’d love to hear your thoughts on optimizing Llama 3 for retrieval-augmented generation (RAG) tasks. Would love to hear any feedback/recommendations:). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaiRamz"&gt; /u/RaiRamz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7qcnt3fvrge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifm2df</id>
    <title>DeepSeek R1 misinformation is getting out of hand</title>
    <updated>2025-02-02T01:36:11+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9"&gt;https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1 is a &lt;strong&gt;7B parameter language model&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the official Google Cloud blog post? WTF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyzvv</id>
    <title>Mistral Small 3 24b is the first model under 70b I’ve seen pass the “apple” test (even using Q4).</title>
    <updated>2025-02-02T14:59:50+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put all the Deepseek-R1 distills through the “apple” benchmark last week and only 70b passed the “Write 10 sentences that end with the word “apple” “ test, getting all 10 out of10 sentences correct.&lt;/p&gt; &lt;p&gt;I tested a slew of other newer open source models (all the major ones, Qwen, Phi-, Llama, Gemma, Command-R, etc) as well, but no model under 70b has ever managed to succeed in getting all 10 right….until Mistral Small 3 24b came along. It is the first and only model under 70b parameters that I’ve found that could pass this test. Congrats Mistral Team!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifw36r</id>
    <title>R1 is cool, but Mistral 3 Small is the boring workhorse I’m actually excited to fine-tune and deploy</title>
    <updated>2025-02-02T12:21:07+00:00</updated>
    <author>
      <name>/u/logan-diamond</name>
      <uri>https://old.reddit.com/user/logan-diamond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As soon as you use it, you realize it's not meant to be fun. It's a masterfully designed bland base model with very thoughtful trade-offs, especially for one-offs. Unless qwen replies soon, I think it might frequently replace both qwen 14b &amp;amp; 32b. &lt;/p&gt; &lt;p&gt;In 2024 I don't know how many times I read &amp;quot;... is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of...&amp;quot;. &lt;/p&gt; &lt;p&gt;Those times are back ☺️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logan-diamond"&gt; /u/logan-diamond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T12:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifsb1m</id>
    <title>Is the UK about to ban running LLMs locally?</title>
    <updated>2025-02-02T07:48:06+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The UK government is targetting the use of AI to generate illegal imagery, which of course is a good thing, but the wording seems like any kind of AI tool run locally can be considered illegal, as it has the *potential* of generating questionable content. Here's a quote from the news:&lt;/p&gt; &lt;p&gt;&amp;quot;The Home Office says that, to better protect children, the UK will be the first country in the world to make it illegal to possess, create or distribute AI tools designed to create child sexual abuse material (CSAM), with a punishment of up to five years in prison.&amp;quot; They also mention something about manuals that teach others how to use AI for these purposes.&lt;/p&gt; &lt;p&gt;It seems to me that any uncensored LLM run locally can be used to generate illegal content, whether the user wants to or not, and therefore could be prosecuted under this law. Or am I reading this incorrectly?&lt;/p&gt; &lt;p&gt;And is this a blueprint for how other countries, and big tech, can force people to use (and pay for) the big online AI services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T07:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ify8f9</id>
    <title>How national security advisors evaluate tech companies</title>
    <updated>2025-02-02T14:22:33+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"&gt; &lt;img alt="How national security advisors evaluate tech companies" src="https://preview.redd.it/h1mbjvbdjqge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8a53ed64a3981fb5114d597cba9461646d11938" title="How national security advisors evaluate tech companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just realized I should have added tiktok. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1mbjvbdjqge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:22:33+00:00</published>
  </entry>
</feed>
