<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-22T08:24:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iujafd</id>
    <title>Best LLMs!? (Focus: Best &amp; 7B-32B) 02/21/2025</title>
    <updated>2025-02-21T05:16:02+00:00</updated>
    <author>
      <name>/u/DeadlyHydra8630</name>
      <uri>https://old.reddit.com/user/DeadlyHydra8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I am fairly new to this space and this is my first post here so go easy on me 😅&lt;/p&gt; &lt;pre&gt;&lt;code&gt;For those who are also new! What does this 7B, 14B, 32B parameters even mean? - It represents the number of trainable weights in the model, which determine how much data it can learn and process. - Larger models can capture more complex patterns but require more compute, memory, and data, while smaller models can be faster and more efficient. What do I need to run Local Models? - Ideally you'd want the most VRAM GPU possible allowing you to run bigger models - Though if you have a laptop with a NPU that's also great! - If you do not have a GPU focus on trying to use smaller models 7B and lower! - (Reference the Chart below) How do I run a Local Model? - Theres various guides online - I personally like using LMStudio it has a nice interface - I also use Ollama &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Quick Guide!&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: This chart could have issues, please correct me! Take it with a grain of salt&lt;/p&gt; &lt;p&gt;You can run models as big as you want on whatever device you want; I'm not here to push some &amp;quot;corporate upsell.&amp;quot;&lt;/p&gt; &lt;p&gt;Note: For Android, Smolchat and Pocketpal are great apps to download models from Huggingface&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device Type&lt;/th&gt; &lt;th align="left"&gt;VRAM/RAM&lt;/th&gt; &lt;th align="left"&gt;Recommended Bit Precision&lt;/th&gt; &lt;th align="left"&gt;Max LLM Parameters (Approx.)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Smartphones&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Low-end phones&lt;/td&gt; &lt;td align="left"&gt;4 GB RAM&lt;/td&gt; &lt;td align="left"&gt;2 bit to 4-bit&lt;/td&gt; &lt;td align="left"&gt;~1-2 billion&lt;/td&gt; &lt;td align="left"&gt;For basic tasks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range phones&lt;/td&gt; &lt;td align="left"&gt;6-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;2-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Good balance of performance and model size.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end phones&lt;/td&gt; &lt;td align="left"&gt;12 GB RAM&lt;/td&gt; &lt;td align="left"&gt;2-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~6 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;x86 Laptops&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Integrated GPU (e.g., Intel Iris)&lt;/td&gt; &lt;td align="left"&gt;8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;2-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~4 billion&lt;/td&gt; &lt;td align="left"&gt;Suitable for smaller to medium-sized models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gaming Laptops (e.g., RTX 3050)&lt;/td&gt; &lt;td align="left"&gt;4-6 GB VRAM + RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~4-14 billion&lt;/td&gt; &lt;td align="left"&gt;Seems crazy ik but we aim for model size that runs smoothly and responsively&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end Laptops (e.g., RTX 3060)&lt;/td&gt; &lt;td align="left"&gt;8-12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~4-14 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models, especially with 16-bit for higher quality.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ARM Devices&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Raspberry Pi 4&lt;/td&gt; &lt;td align="left"&gt;4-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Best for experimentation and smaller models due to memory constraints.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apple M1/M2 (Unified Memory)&lt;/td&gt; &lt;td align="left"&gt;8-24 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~4-12 billion&lt;/td&gt; &lt;td align="left"&gt;Unified memory allows for larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU Computers&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range GPU (e.g., RTX 4070)&lt;/td&gt; &lt;td align="left"&gt;12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~7-32 billion&lt;/td&gt; &lt;td align="left"&gt;Good for general LLM tasks and development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end GPU (e.g., RTX 3090)&lt;/td&gt; &lt;td align="left"&gt;24 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~14-32 billion&lt;/td&gt; &lt;td align="left"&gt;Big boi territory!&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Server GPU (e.g., A100)&lt;/td&gt; &lt;td align="left"&gt;40-80 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;16-bit to 32-bit&lt;/td&gt; &lt;td align="left"&gt;~20-40 billion&lt;/td&gt; &lt;td align="left"&gt;For the largest models and research.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The point of this post is to essentially find and keep updating this post with the best new models most people can actually use.&lt;/p&gt; &lt;p&gt;While sure the 70B, 405B, 671B and Closed sources models are incredible, some of us don't have the facilities for those huge models and don't want to give away our data 🙃&lt;/p&gt; &lt;p&gt;I will put up what &lt;strong&gt;I believe&lt;/strong&gt; are the best models for each of these categories &lt;strong&gt;CURRENTLY&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Please, please, please, those who are much much more knowledgeable, let me know what models I should put if I am missing any great models or categories I should include!)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I cannot find RRD2.5 for the life of me on HuggingFace.&lt;/p&gt; &lt;p&gt;I will have benchmarks, so those are more definitive. some other stuff will be subjective I will also have links to the repo (I'm also including links; I am no evil man but don't trust strangers on the world wide web)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; {Parameter}: {Model} - {Score}&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;MMLU-Pro (language comprehension and reasoning across diverse domains):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best:&lt;/em&gt; &lt;em&gt;DeepSeek-R1 - 0.84&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/bartowski/QwQ-32B-Preview-GGUF"&gt;QwQ-32B-Preview&lt;/a&gt; - 0.7097&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Phi-4&lt;/a&gt; - 0.704&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF"&gt;Qwen2.5-7B-Instruct&lt;/a&gt; - 0.4724&lt;br /&gt; ------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Math:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Gemini-2.0-Flash-exp - 0.8638&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.8053&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.6788&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5803&lt;/p&gt; &lt;p&gt;Note: DeepSeek's Distilled variations are also great if not better!&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Coding (conceptual, debugging, implementation, optimization):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: OpenAI O1 - 0.981 (148/148)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-32B Coder&lt;/a&gt; - 0.817&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral Small 3&lt;/a&gt; - 0.692&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF"&gt;Qwen2.5-Coder-14B-Instruct&lt;/a&gt; - 0.6707&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"&gt;Llama3.1-8B Instruct&lt;/a&gt; - 0.385&lt;/p&gt; &lt;p&gt;HM:&lt;br /&gt; 32B: &lt;a href="https://huggingface.co/waldie/DeepSeek-R1-Distill-Qwen-32B-4bpw-h6-exl2"&gt;DeepSeek-R1-Distill&lt;/a&gt; - (148/148)&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/THUDM/codegeex4-all-9b"&gt;CodeGeeX4-All&lt;/a&gt; - (146/148)&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Creative Writing:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Arena Creative Writing:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1422, OpenAI 4o - 1420&lt;/em&gt;&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; &lt;strong&gt;-&lt;/strong&gt; 1244&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1199&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1178&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EQ Bench (Emotional Intelligence Benchmarks for LLMs):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: DeepSeek-R1 - 87.11&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;9B:&lt;/em&gt; &lt;a href="https://huggingface.co/ifable/gemma-2-Ifable-9B"&gt;gemma-2-Ifable-9B&lt;/a&gt; &lt;em&gt;- 84.59&lt;/em&gt;&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Longer Query (&amp;gt;= 500 tokens)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1425, Gemini-2.0-Pro/Flash-Thinking-Exp - 1399/1395&lt;/em&gt;&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1264&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1261&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; - 1239&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt; - 1233&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Heathcare/Medical (USMLE, AIIMS &amp;amp; NEET PG, College/Profession level quesions)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(8B) Best Avg.&lt;/em&gt;: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 90.01&lt;/p&gt; &lt;p&gt;(8B) Best USMLE, AIIMS &amp;amp; NEET PG: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 81.07&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Business\&lt;/em&gt;&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.8137&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7567&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7085&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.5539&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5412&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Economics\&lt;/em&gt;&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.859&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7725&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7310&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.6552&lt;/p&gt; &lt;p&gt;Note*: Both of these are based on the benchmarked scores; some online LLMs aren't tested, particularly DeepSeek-R1 and OpenAI o1-mini. So if you plan to use online LLMs you can choose to Claude-3.5-Sonnet or DeepSeek-R1 (which scores better overall)&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard"&gt;https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard"&gt;https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/?leaderboard"&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/math-word-problem-solving-on-math"&gt;https://paperswithcode.com/sota/math-word-problem-solving-on-math&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/code-generation-on-humaneval"&gt;https://paperswithcode.com/sota/code-generation-on-humaneval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeadlyHydra8630"&gt; /u/DeadlyHydra8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iutvpe</id>
    <title>New SOTA on OpenAI's SimpleQA</title>
    <updated>2025-02-21T15:44:08+00:00</updated>
    <author>
      <name>/u/pcamiz</name>
      <uri>https://old.reddit.com/user/pcamiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;French lab beats Perplexity on SimpleQA &lt;a href="https://www.linkup.so/blog/linkup-establishes-sota-performance-on-simpleqa"&gt;https://www.linkup.so/blog/linkup-establishes-sota-performance-on-simpleqa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apparently can be plugged to Llama to improve factuality by a lot. Will be trying it out this weekend. LMK if you integrate it as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcamiz"&gt; /u/pcamiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iutvpe/new_sota_on_openais_simpleqa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iutvpe/new_sota_on_openais_simpleqa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iutvpe/new_sota_on_openais_simpleqa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T15:44:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu19zy</id>
    <title>2025 is an AI madhouse</title>
    <updated>2025-02-20T15:36:23+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt; &lt;img alt="2025 is an AI madhouse" src="https://preview.redd.it/ferhsryxcbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1632f508c8f4f33f22d5753531a2d6bc7a1ca3" title="2025 is an AI madhouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025 is straight-up wild for AI development. Just last year, it was mostly ChatGPT, Claude, and Gemini running the show. &lt;/p&gt; &lt;p&gt;Now? We’ve got an AI battle royale with everyone jumping in Deepseek, Kimi, Meta, Perplexity, Elon’s Grok&lt;/p&gt; &lt;p&gt;With all these options, the real question is: which one are you actually using daily?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ferhsryxcbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T15:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iunu3j</id>
    <title>What's with the too-good-to-be-true cheap GPUs from China on ebay lately? Obviously scammy, but strangely they stay up.</title>
    <updated>2025-02-21T10:27:32+00:00</updated>
    <author>
      <name>/u/Massive_Robot_Cactus</name>
      <uri>https://old.reddit.com/user/Massive_Robot_Cactus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've seen a lot of cheap A100, H100, etc being posted lately on ebay, like $856 for a 40GB pci-e A100. All coming from China, with cloned photos and fresh seller accounts...classic scam material. But they're not coming down so quickly.&lt;/p&gt; &lt;p&gt;Has anyone actually tried to purchase one of these to see what happens? Very much these seem too good to be true, but I'm wondering how the scam works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive_Robot_Cactus"&gt; /u/Massive_Robot_Cactus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T10:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iva9w5</id>
    <title>DeepSeek 671B inference speed vs 70B and 32B</title>
    <updated>2025-02-22T04:14:00+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was originally thinking 671B would perform similar to a 37B model, (if it fits in vram)&lt;br /&gt; In practice it's about 1/2 that speed, a little slower than 70B.&lt;/p&gt; &lt;p&gt;Is this all down to lack of MOE optimizations or is there more to the equation than just 37B?&lt;br /&gt; I'm not disappointed, just genuinely curious.&lt;/p&gt; &lt;p&gt;At a hardware level I do have 128MB's of Cache across my 8 3090's&lt;br /&gt; That cache would be less effective on a 140GB model vs a 16GB model,&lt;br /&gt; But I imagine that only accounts for tiny fraction of the performance difference. &lt;/p&gt; &lt;p&gt;For the numbers I'm seeing: &lt;/p&gt; &lt;p&gt;DeepSeek R1 IQ1-S:&lt;br /&gt; prompt eval time = 5229.69 ms / 967 tokens ( 5.41 ms per token, &lt;strong&gt;184.91 tokens per second&lt;/strong&gt;)&lt;br /&gt; eval time = 110508.74 ms / 1809 tokens ( 61.09 ms per token, &lt;strong&gt;16.37 tokens per second&lt;/strong&gt;) &lt;/p&gt; &lt;p&gt;Llama 70b IQ1-M:&lt;br /&gt; prompt eval time = 2086.46 ms / 981 tokens ( 2.13 ms per token, &lt;strong&gt;470.17 tokens per second&lt;/strong&gt;)&lt;br /&gt; eval time = 81099.67 ms / 1612 tokens ( 50.31 ms per token, &lt;strong&gt;19.88 tokens per second&lt;/strong&gt;) &lt;/p&gt; &lt;p&gt;Qwen2.5 32B IQ2-XXS:&lt;br /&gt; prompt eval time = 1159.91 ms / 989 tokens ( 1.17 ms per token, &lt;strong&gt;852.65 tokens per second&lt;/strong&gt;)&lt;br /&gt; eval time = 50623.16 ms / 1644 tokens ( 30.79 ms per token, &lt;strong&gt;32.48 tokens per second&lt;/strong&gt;) &lt;/p&gt; &lt;p&gt;*I should add I can run 70b way faster than 19T/s, but I'm limiting myself to llapa.cpp with the same settings that work for DeepSeek to keep it as fair as possible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva9w5/deepseek_671b_inference_speed_vs_70b_and_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva9w5/deepseek_671b_inference_speed_vs_70b_and_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iva9w5/deepseek_671b_inference_speed_vs_70b_and_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T04:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuut9n</id>
    <title>Quad GPU setup</title>
    <updated>2025-02-21T16:23:14+00:00</updated>
    <author>
      <name>/u/outsider787</name>
      <uri>https://old.reddit.com/user/outsider787</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuut9n/quad_gpu_setup/"&gt; &lt;img alt="Quad GPU setup" src="https://b.thumbs.redditmedia.com/5lmUdrb6pszzusF6Vnd-M4HTLRH7g1a-eHvWdqRx_AM.jpg" title="Quad GPU setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone mentioned that there's not many quad gpu rigs posted, so here's mine.&lt;/p&gt; &lt;p&gt;Running 4 X RTX A5000 GPUs, on a x399 motherboard and a Threadripper 1950x CPU.&lt;br /&gt; All powered by a 1300W EVGA PSU.&lt;/p&gt; &lt;p&gt;The GPUs are using x16 pcie riser cables to connect to the mobo.&lt;/p&gt; &lt;p&gt;The case is custom designed and 3d printed. (let me know if you want the design, and I can post it)&lt;br /&gt; Can fit 8 GPUs. Currently only 4 are populated.&lt;/p&gt; &lt;p&gt;Running inference on 70b q8 models gets me around 10 tokens/s &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eeg9r1jrpike1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=414597e191990bea2a4a7f6622fe039bb6d177a1"&gt;https://preview.redd.it/eeg9r1jrpike1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=414597e191990bea2a4a7f6622fe039bb6d177a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/953aefbspike1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3073d712f178a775b7391e6ad45f3d92a8524ccc"&gt;https://preview.redd.it/953aefbspike1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3073d712f178a775b7391e6ad45f3d92a8524ccc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/outsider787"&gt; /u/outsider787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuut9n/quad_gpu_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuut9n/quad_gpu_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuut9n/quad_gpu_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T16:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv5q8x</id>
    <title>AlexBefest's CardProjector 24B v1 - A model created to generate character cards in ST format</title>
    <updated>2025-02-22T00:01:35+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv5q8x/alexbefests_cardprojector_24b_v1_a_model_created/"&gt; &lt;img alt="AlexBefest's CardProjector 24B v1 - A model created to generate character cards in ST format" src="https://b.thumbs.redditmedia.com/f5WQwCcAXVCG0qhDJLtVOcrb9MauDUqXAgEsPqO-Zes.jpg" title="AlexBefest's CardProjector 24B v1 - A model created to generate character cards in ST format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: CardProjector 24B v1&lt;/p&gt; &lt;p&gt;Model URL: &lt;a href="https://huggingface.co/AlexBefest/CardProjector-24B-v1"&gt;https://huggingface.co/AlexBefest/CardProjector-24B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;About the model: CardProjector-24B-v1 is a specialized language model derived from Mistral-Small-24B-Instruct-2501, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; in the &lt;strong&gt;chara_card_v2&lt;/strong&gt; specification. This model is designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;p&gt;Usage example in the screenshots&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ju8u3feyzkke1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bd8d4169eb4a69b5e0b407af846e840d88ea3d9"&gt;https://preview.redd.it/ju8u3feyzkke1.png?width=2348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bd8d4169eb4a69b5e0b407af846e840d88ea3d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/awduoysyzkke1.png?width=3610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae219e9ae451fb80639b0863cb56501be411307e"&gt;https://preview.redd.it/awduoysyzkke1.png?width=3610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae219e9ae451fb80639b0863cb56501be411307e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2a0eq76zzkke1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d125b12047fcae4b560539ba8afee83600a55af"&gt;https://preview.redd.it/2a0eq76zzkke1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d125b12047fcae4b560539ba8afee83600a55af&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv5q8x/alexbefests_cardprojector_24b_v1_a_model_created/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv5q8x/alexbefests_cardprojector_24b_v1_a_model_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv5q8x/alexbefests_cardprojector_24b_v1_a_model_created/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T00:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv9r63</id>
    <title>What's the SoTA for CPU-only RAG?</title>
    <updated>2025-02-22T03:44:35+00:00</updated>
    <author>
      <name>/u/EternityForest</name>
      <uri>https://old.reddit.com/user/EternityForest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing around with a few of the options out there, but the vast majority of projects seem to be pretty high performance.&lt;/p&gt; &lt;p&gt;The two that seem the most interesting so far are Ragatouille and this project here: &lt;a href="https://huggingface.co/sentence-transformers/static-retrieval-mrl-en-v1"&gt;https://huggingface.co/sentence-transformers/static-retrieval-mrl-en-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was able to get it to answer questions about 80% of the time in about 10s(wiikipedia zim file builtin search, narrow down articles with embeddings on the titles, embed every sentence with the article title prepended, take the top few matches, append the question and pass the whole thing to Smollmv2, then to distillbert for a more concise answer if needed) but I'm sure there's got to be something way better than my hacky Python script, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EternityForest"&gt; /u/EternityForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv9r63/whats_the_sota_for_cpuonly_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv9r63/whats_the_sota_for_cpuonly_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv9r63/whats_the_sota_for_cpuonly_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T03:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuzyqe</id>
    <title>Are there any open-source alternatives to Google's new AI co-scientist?</title>
    <updated>2025-02-21T19:53:21+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just read about Google's new &lt;a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/"&gt;AI co-scientist&lt;/a&gt; system built on Gemini 2.0. It's a multi-agent system designed to help researchers generate novel hypotheses and accelerate scientific discoveries. The system seems pretty powerful - they claim it's already helped with drug repurposing for leukemia, target discovery for liver fibrosis, and explaining mechanisms of antimicrobial resistance.&lt;/p&gt; &lt;p&gt;While this sounds impressive, I'd much prefer to use an open-source solution (that I can run locally) for my own research. Ideally something that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can operate as a multi-agent system with different specialized roles&lt;/li&gt; &lt;li&gt;Can parse and understand scientific literature&lt;/li&gt; &lt;li&gt;Can generate novel hypotheses and experimental approaches&lt;/li&gt; &lt;li&gt;Can be run without sending sensitive research data to third parties&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Does anything like this exist in the open-source LLM ecosystem yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuzyqe/are_there_any_opensource_alternatives_to_googles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuzyqe/are_there_any_opensource_alternatives_to_googles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuzyqe/are_there_any_opensource_alternatives_to_googles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T19:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv668x</id>
    <title>Local Models vs. Cloud Giants: Are We Witnessing the True Democratization of AI?</title>
    <updated>2025-02-22T00:22:15+00:00</updated>
    <author>
      <name>/u/pawsforeducation</name>
      <uri>https://old.reddit.com/user/pawsforeducation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last month, I heard someone generated a fully custom chatbot for their small business, on a 4-year-old gaming laptop, while avoiding $20k/year in GPT-4 API fees. No data leaks, no throttling, no &amp;quot;content policy&amp;quot; debates. It got me thinking: Is running AI locally finally shifting power away from Big Tech… or just creating a new kind of tech priesthood?&lt;/p&gt; &lt;p&gt;Observations from the Trenches&lt;/p&gt; &lt;p&gt;The Good:&lt;/p&gt; &lt;p&gt;Privacy Wins: No more wondering if your journal entries/medical queries/business ideas are training corporate models.&lt;/p&gt; &lt;p&gt;Cost Chaos: Cloud APIs charge per token, but my RTX 4090 runs 13B models indefinitely for the price of a Netflix subscription.&lt;/p&gt; &lt;p&gt;Offline Superpowers: Got stranded without internet last week? My fine-tuned LLaMA helped debug code while my phone was a brick.&lt;/p&gt; &lt;p&gt;The Ugly:&lt;/p&gt; &lt;p&gt;Hardware Hunger: VRAM requirements feel like a tax on the poor. $2k GPUs shouldn’t be the entry ticket to &amp;quot;democratized&amp;quot; AI.&lt;/p&gt; &lt;p&gt;Tuning Trench Warfare: Spent 12 hours last weekend trying to quantize a model without nuking its IQ. Why isn’t this easier?&lt;/p&gt; &lt;p&gt;The Open-Source Mirage: Even &amp;quot;uncensored&amp;quot; models inherit biases from their training data. Freedom ≠ neutrality.&lt;/p&gt; &lt;p&gt;Real-World Experiments I’m Seeing&lt;/p&gt; &lt;p&gt;A researcher using local models to analyze sensitive mental health data (no ethics board red tape).&lt;/p&gt; &lt;p&gt;Indie game studios generating NPC dialogue on device to dodge copyright strikes from cloud providers.&lt;/p&gt; &lt;p&gt;Teachers running history tutors on Raspberry Pis for schools with no IT budget.&lt;/p&gt; &lt;p&gt;Where do local models actually OUTPERFORM cloud AI right now, and where’s the hype falling flat? Is the ‘democratization’ narrative just coping for those who can’t afford GPT-4 Turbo… or the foundation of a real revolution?”&lt;/p&gt; &lt;p&gt;Curious to hear your war stories. What’s shocked you most about running AI locally? (And if you’ve built something wild with LLaMA, slide into my DMs, I’ll trade you GPU optimization tips.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pawsforeducation"&gt; /u/pawsforeducation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv668x/local_models_vs_cloud_giants_are_we_witnessing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv668x/local_models_vs_cloud_giants_are_we_witnessing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv668x/local_models_vs_cloud_giants_are_we_witnessing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T00:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivd36k</id>
    <title>What system are we using for Local Llamas at home?</title>
    <updated>2025-02-22T07:08:09+00:00</updated>
    <author>
      <name>/u/derjanni</name>
      <uri>https://old.reddit.com/user/derjanni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it would be nice for all of us to get an overview of the current systems used by our community for Local Llama usage at home. Not just will it give us all an overview of where the community stands, but also allows Open Source developers among us to get an insight into what environments need to be considered for future tooling.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ivd36k"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derjanni"&gt; /u/derjanni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivd36k/what_system_are_we_using_for_local_llamas_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivd36k/what_system_are_we_using_for_local_llamas_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivd36k/what_system_are_we_using_for_local_llamas_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T07:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivc6vv</id>
    <title>llama.cpp benchmark on A100</title>
    <updated>2025-02-22T06:09:11+00:00</updated>
    <author>
      <name>/u/databasehead</name>
      <uri>https://old.reddit.com/user/databasehead</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama-bench is giving me around 25tps for tg and around 550 pp with a 80gb A100 running llama3.3:70-q4_K_M. Same card and llama3.1:8b is around 125tps tg (pp through the roof). I have to check, but iirc I installed nvidia driver 565.xx.x, cuda 12.6 update 2, cuda-toolkit 12.6, ubuntu 22.04lts, with linux kernel 6.5.0-27, default gcc 12.3.0, glibc 2.35. llama.cpp compile with cuda architecture 80 which is correct for A100. Wondering if anyone has any ideas about speeding up my single A100 80g with llama3.3:70b q4_K_M?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/databasehead"&gt; /u/databasehead &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivc6vv/llamacpp_benchmark_on_a100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivc6vv/llamacpp_benchmark_on_a100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivc6vv/llamacpp_benchmark_on_a100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T06:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivdkoe</id>
    <title>What are the best uncensored/unfiltered small models(up to 22B) for philosophical conversation/brainstorming?</title>
    <updated>2025-02-22T07:42:12+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The models I tried act unnecessarily like morality police which kills the purpose of philosophical debates. what models would you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivdkoe/what_are_the_best_uncensoredunfiltered_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T07:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iupq6h</id>
    <title>Have we hit a scaling wall in base models? (non reasoning)</title>
    <updated>2025-02-21T12:27:39+00:00</updated>
    <author>
      <name>/u/CH1997H</name>
      <uri>https://old.reddit.com/user/CH1997H</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet&lt;/p&gt; &lt;p&gt;Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the &amp;quot;scaling laws&amp;quot; where the chart just says &amp;quot;line goes up&amp;quot;)&lt;/p&gt; &lt;p&gt;Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling&lt;/p&gt; &lt;p&gt;It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CH1997H"&gt; /u/CH1997H &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T12:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv2wyn</id>
    <title>List of permissively-licensed foundation models with up to 360M parameters for practicing fine-tuning</title>
    <updated>2025-02-21T21:57:03+00:00</updated>
    <author>
      <name>/u/Felladrin</name>
      <uri>https://old.reddit.com/user/Felladrin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all!&lt;/p&gt; &lt;p&gt;I wanted to share this list containing models that are small enough for quick fine-tuning but smart enough for checking how the fine-tuning dataset affects them:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Felladrin/foundation-text-generation-models-below-360m-parameters-659ad9f2e1604bf996b90251"&gt;Hugging Face Collection: Foundation Text-Generation Models Below 360M Parameters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm always looking for new models for this list, so if you know of a permissively-licensed foundation model that is not there yet, please link it in a comment.&lt;/p&gt; &lt;p&gt;Tip: For first-time tuners, an easy way to start, on Mac/Linux/Windows, is using &lt;a href="https://huggingface.co/docs/autotrain"&gt;Hugging Face's AutoTrain&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Bonus: Those models run even on a browser of mobile devices on a single-CPU core, so you can also use them in web applications later!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Felladrin"&gt; /u/Felladrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv2wyn/list_of_permissivelylicensed_foundation_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv2wyn/list_of_permissivelylicensed_foundation_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv2wyn/list_of_permissivelylicensed_foundation_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T21:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iva6j2</id>
    <title>There's also the new ROG Flow Z13 (2025) with 128GB LPDDR5X on board for $2,799</title>
    <updated>2025-02-22T04:08:43+00:00</updated>
    <author>
      <name>/u/ultrapcb</name>
      <uri>https://old.reddit.com/user/ultrapcb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The mem bus is still at 256bit and a M4 Pro or whatever is faster but 128gb vram at this price doesn't sound too bad or not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ultrapcb"&gt; /u/ultrapcb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iva6j2/theres_also_the_new_rog_flow_z13_2025_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T04:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iulq4o</id>
    <title>We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE</title>
    <updated>2025-02-21T07:56:29+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt; &lt;img alt="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" src="https://external-preview.redd.it/dzQ4N25sbTM1Z2tlMVF8vLuY1I7D-30miO4pvAdRk1TFvpSr9DfFmva9zHJp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b308a1599c976871d72ed516a3f26b46303ad50a" title="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vkth2pm35gke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T07:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv0z5w</id>
    <title>What would you do with 96GB of VRAM (quad 3090 setup)</title>
    <updated>2025-02-21T20:35:58+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for inspiration. Mostly curious about ways to get an LLM to learn a code base and become a coding mate I can discuss stuff with about the code base (coding style, bug hunting, new features, refactoring)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv0z5w/what_would_you_do_with_96gb_of_vram_quad_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T20:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujig7</id>
    <title>Deepseek will publish 5 open source repos next week.</title>
    <updated>2025-02-21T05:29:06+00:00</updated>
    <author>
      <name>/u/WashWarm8360</name>
      <uri>https://old.reddit.com/user/WashWarm8360</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt; &lt;img alt="Deepseek will publish 5 open source repos next week." src="https://preview.redd.it/rdzshzfihfke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e7b8ca74fce95adc4de966cdb0f09467f784c54" title="Deepseek will publish 5 open source repos next week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WashWarm8360"&gt; /u/WashWarm8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rdzshzfihfke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iur927</id>
    <title>I tested Grok 3 against Deepseek r1 on my personal benchmark. Here's what I found out</title>
    <updated>2025-02-21T13:46:16+00:00</updated>
    <author>
      <name>/u/goddamnit_1</name>
      <uri>https://old.reddit.com/user/goddamnit_1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, the Grok 3 is here. And as a Whale user, I wanted to know if it's as big a deal as they are making out to be.&lt;/p&gt; &lt;p&gt;Though I know it's unfair for Deepseek r1 to compare with Grok 3 which was trained on 100k h100 behemoth cluster.&lt;/p&gt; &lt;p&gt;But I was curious about how much better Grok 3 is compared to Deepseek r1. So, I tested them on my personal set of questions on reasoning, mathematics, coding, and writing.&lt;/p&gt; &lt;p&gt;Here are my observations.&lt;/p&gt; &lt;h1&gt;Reasoning and Mathematics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 and Deepseek r1 are practically neck-and-neck in these categories.&lt;/li&gt; &lt;li&gt;Both models handle complex reasoning problems and mathematics with ease. Choosing one over the other here doesn't seem to make much of a difference.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 leads in this category. Its code quality, accuracy, and overall answers are simply better than Deepseek r1's.&lt;/li&gt; &lt;li&gt;Deepseek r1 isn't bad, but it doesn't come close to Grok 3. If coding is your primary use case, Grok 3 is the clear winner.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Writing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both models are equally better for creative writing, but I personally prefer Grok 3’s responses.&lt;/li&gt; &lt;li&gt;For my use case, which involves technical stuff, I liked the Grok 3 better. Deepseek has its own uniqueness; I can't get enough of its autistic nature.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Who Should Use Which Model?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 is the better option if you're focused on coding.&lt;/li&gt; &lt;li&gt;For reasoning and math, you can't go wrong with either model. They're equally capable.&lt;/li&gt; &lt;li&gt;If technical writing is your priority, Grok 3 seems slightly better than Deepseek r1 for my personal use cases, for schizo talks, no one can beat Deepseek r1.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a detailed analysis,&lt;a href="https://composio.dev/blog/grok-3-vs-deepseek-r1/"&gt; Grok 3 vs Deepseek r1&lt;/a&gt;, for a more detailed breakdown, including specific examples and test cases.&lt;/p&gt; &lt;p&gt;What are your experiences with the new Grok 3? Did you find the model useful for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goddamnit_1"&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T13:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv45vg</id>
    <title>AMD Strix Halo 128GB performance on deepseek r1 70B Q8</title>
    <updated>2025-02-21T22:51:13+00:00</updated>
    <author>
      <name>/u/hardware_bro</name>
      <uri>https://old.reddit.com/user/hardware_bro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw a review on douying for Chinese mini PC AXB35-2 prototype with AI MAX+ pro 395 and 128GB memory. Running deepseek r1 Q8 on LM studio 0.3.9 with 2k context on windows, no flash attention, the reviewer said it is about 3token/sec.&lt;/p&gt; &lt;p&gt;source: douying id 141zhf666, posted on Feb 13.&lt;/p&gt; &lt;p&gt;For comparison: I have macbook pro m4 MAX 40core GPU 128GB, running LM studio 0.3.10, running deepseek r1 70B distilled Q8 with 2k context, no flash attention or k, v cache. 5.46tok/sec&lt;/p&gt; &lt;p&gt;Update test the mac using MLX instead of GGUF format:&lt;/p&gt; &lt;p&gt;Using MLX Deepseek R1 distill Llama-70B 8bit.&lt;/p&gt; &lt;p&gt;2k context, output 1140tokens at 6.29 tok/sec.&lt;/p&gt; &lt;p&gt;8k context, output 1365 tokens at 5.59 tok/sec&lt;/p&gt; &lt;p&gt;13k max context, output 1437 tokens at 6.31 tok/sec, 1.1% context full&lt;/p&gt; &lt;p&gt;13k max context, output 1437 tokens at 6.36 tok/sec, 1.4% context full&lt;/p&gt; &lt;p&gt;13k max context, output 3422 tokens at 5.86 tok/sec, 3.7% context full&lt;/p&gt; &lt;p&gt;13k max context, output 1624 tokens at 5.62 tok/sec, 4.6% context full&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hardware_bro"&gt; /u/hardware_bro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45vg/amd_strix_halo_128gb_performance_on_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T22:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv45hh</id>
    <title>You can now do function calling with DeepSeek R1</title>
    <updated>2025-02-21T22:50:42+00:00</updated>
    <author>
      <name>/u/ido-pluto</name>
      <uri>https://old.reddit.com/user/ido-pluto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"&gt; &lt;img alt="You can now do function calling with DeepSeek R1" src="https://external-preview.redd.it/3lluEZUQskcTLR8tskEWHfqfXh4UA47uxDjeSpFlBzc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=344959dbe67223a6bb5d5f24659e31e96a57a19c" title="You can now do function calling with DeepSeek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ido-pluto"&gt; /u/ido-pluto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://node-llama-cpp.withcat.ai/blog/v3.6-deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv45hh/you_can_now_do_function_calling_with_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T22:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv6zou</id>
    <title>Ovis2 34B ~ 1B - Multi-modal LLMs from Alibaba International Digital Commerce Group</title>
    <updated>2025-02-22T01:19:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on qwen2.5 series, they covered all sizes from 1B to 32B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/AIDC-AI/ovis2-67ab36c7e497429034874464"&gt;https://huggingface.co/collections/AIDC-AI/ovis2-67ab36c7e497429034874464&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We are pleased to announce the release of Ovis2, our latest advancement in multi-modal large language models (MLLMs). Ovis2 inherits the innovative architectural design of the Ovis series, aimed at structurally aligning visual and textual embeddings. As the successor to Ovis1.6, Ovis2 incorporates significant improvements in both dataset curation and training methodologies.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv6zou/ovis2_34b_1b_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T01:19:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iui6nk</id>
    <title>Starting next week, DeepSeek will open-source 5 repos</title>
    <updated>2025-02-21T04:13:54+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt; &lt;img alt="Starting next week, DeepSeek will open-source 5 repos" src="https://preview.redd.it/syeh0rmm3fke1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3667b8c2ba5c4d6506f21080ba3334e6724119" title="Starting next week, DeepSeek will open-source 5 repos" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syeh0rmm3fke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv72vu</id>
    <title>Are there any LLMs with less than 1m parameters?</title>
    <updated>2025-02-22T01:24:28+00:00</updated>
    <author>
      <name>/u/UselessSoftware</name>
      <uri>https://old.reddit.com/user/UselessSoftware</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that's a weird request and the model would be useless, but I'm doing a proof-of-concept port of llama2.c to DOS and I want a model that can fit inside 640 KB of RAM.&lt;/p&gt; &lt;p&gt;Anything like a 256K or 128K model?&lt;/p&gt; &lt;p&gt;I want to get LLM inferencing working on the original PC. 😆&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UselessSoftware"&gt; /u/UselessSoftware &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T01:24:28+00:00</published>
  </entry>
</feed>
