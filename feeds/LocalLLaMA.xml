<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-19T13:49:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jefu92</id>
    <title>NVIDIA Enters The AI PC Realm With DGX Spark &amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory</title>
    <updated>2025-03-18T21:01:48+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"&gt; &lt;img alt="NVIDIA Enters The AI PC Realm With DGX Spark &amp;amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory" src="https://external-preview.redd.it/5AEU0ER6vvoF_Vwmm5Rb5FZlbnmQmmesAV1c3JbTGmg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74d56d71efdf31cc89b6b1f1afb29f86801be909" title="NVIDIA Enters The AI PC Realm With DGX Spark &amp;amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-enters-ai-pc-realm-dgx-spark-dgx-station-desktops-72-core-grace-cpu-blackwell-gpus-up-to-784-gb-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1je58r5</id>
    <title>Wen GGUFs?</title>
    <updated>2025-03-18T13:42:29+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt; &lt;img alt="Wen GGUFs?" src="https://preview.redd.it/vv2vg9xbcgpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fce563f7a834755ce5916e4567c3e30f30949f6" title="Wen GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vv2vg9xbcgpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T13:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedlum</id>
    <title>DGX Sparks / Nvidia Digits</title>
    <updated>2025-03-18T19:31:07+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt; &lt;img alt="DGX Sparks / Nvidia Digits" src="https://preview.redd.it/4ydasblh2ipe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22bbb5dc941a9764664463ef883da62ce8b80a06" title="DGX Sparks / Nvidia Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have now official Digits/DGX Sparks specs&lt;/p&gt; &lt;p&gt;|| || |Architecture|NVIDIA Grace Blackwell| |GPU|Blackwell Architecture| |CPU|20 core Arm, 10 Cortex-X925 + 10 Cortex-A725 Arm| |CUDA Cores|Blackwell Generation| |Tensor Cores|5th Generation| |RT Cores|4th Generation| |&lt;sup&gt;1&lt;/sup&gt;Tensor Performance |1000 AI TOPS| |System Memory|128 GB LPDDR5x, unified system memory| |Memory Interface|256-bit| |Memory Bandwidth|273 GB/s| |Storage|1 or 4 TB NVME.M2 with self-encryption| |USB|4x USB 4 TypeC (up to 40Gb/s)| |Ethernet|1x RJ-45 connector 10 GbE| |NIC|ConnectX-7 Smart NIC| |Wi-Fi|WiFi 7| |Bluetooth|BT 5.3 w/LE| |Audio-output|HDMI multichannel audio output| |Power Consumption|170W| |Display Connectors|1x HDMI 2.1a| |NVENC | NVDEC|1x | 1x| |OS|&lt;sup&gt;™&lt;/sup&gt; NVIDIA DGX OS| |System Dimensions|150 mm L x 150 mm W x 50.5 mm H| |System Weight|1.2 kg|&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4ydasblh2ipe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jee2b2</id>
    <title>NVIDIA DGX Spark (Project DIGITS) Specs Are Out</title>
    <updated>2025-03-18T19:49:56+00:00</updated>
    <author>
      <name>/u/spectrography</name>
      <uri>https://old.reddit.com/user/spectrography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Memory bandwidth: 273 GB/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spectrography"&gt; /u/spectrography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jewrar</id>
    <title>Dockerfile for deploying Qwen QwQ 32B on A10Gs , L4s or L40S</title>
    <updated>2025-03-19T13:18:07+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Adding a Dockerfile here that can be used to deploy Qwen on any machine which has a combined GPU RAM of ~80GBs. The below Dockerfile is for multi-GPU L4 instances as L4s are the cheapest ones on AWS, feel free to make changes to try it on L40S, A10Gs, A100s etc. Soon will follow up with metrics around single request tokens / sec and throughput.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Dockerfile for Qwen QwQ 32B FROM vllm/vllm-openai:latest # Enable HF Hub Transfer for faster downloads ENV HF_HUB_ENABLE_HF_TRANSFER 1 # Expose port 80 EXPOSE 80 # Entrypoint with API key ENTRYPOINT [&amp;quot;python3&amp;quot;, &amp;quot;-m&amp;quot;, &amp;quot;vllm.entrypoints.openai.api_server&amp;quot;, \ # name of the model &amp;quot;--model&amp;quot;, &amp;quot;Qwen/QwQ-32B&amp;quot;, \ # set the data type to bfloat16 - requires ~1400GB GPU memory &amp;quot;--dtype&amp;quot;, &amp;quot;bfloat16&amp;quot;, \ &amp;quot;--trust-remote-code&amp;quot;, \ # below runs the model on 4 GPUs &amp;quot;--tensor-parallel-size&amp;quot;,&amp;quot;4&amp;quot;, \ # Maximum number of tokens, can lead to OOM if overestimated &amp;quot;--max-model-len&amp;quot;, &amp;quot;8192&amp;quot;, \ # Port on which to run the vLLM server &amp;quot;--port&amp;quot;, &amp;quot;80&amp;quot;, \ # CPU offload in GB. Need this as 8 H100s are not sufficient &amp;quot;--cpu-offload-gb&amp;quot;, &amp;quot;80&amp;quot;, \ &amp;quot;--gpu-memory-utilization&amp;quot;, &amp;quot;0.95&amp;quot;, \ # API key for authentication to the server stored in Tensorfuse secrets &amp;quot;--api-key&amp;quot;, &amp;quot;${VLLM_API_KEY}&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can use the following commands to build and run the above Dockerfile.&lt;/p&gt; &lt;p&gt;&lt;code&gt;docker build -t qwen-qwq-32b .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;followed by&lt;/p&gt; &lt;p&gt;&lt;code&gt;docker run --gpus all --shm-size=2g -p 80:80 -e VLLM_API_KEY=YOUR_API_KEY qwen-qwq-32b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Originally posted here: -&lt;br /&gt; &lt;a href="https://tensorfuse.io/docs/guides/reasoning/qwen_qwq"&gt;https://tensorfuse.io/docs/guides/reasoning/qwen_qwq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewrar/dockerfile_for_deploying_qwen_qwq_32b_on_a10gs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewrar/dockerfile_for_deploying_qwen_qwq_32b_on_a10gs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jewrar/dockerfile_for_deploying_qwen_qwq_32b_on_a10gs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jehxiw</id>
    <title>LLAMA 4 in April?!?!?!?</title>
    <updated>2025-03-18T22:31:24+00:00</updated>
    <author>
      <name>/u/Sea_Anywhere896</name>
      <uri>https://old.reddit.com/user/Sea_Anywhere896</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt; &lt;img alt="LLAMA 4 in April?!?!?!?" src="https://external-preview.redd.it/uRHwEOU98rM_55MIJyA8g2IjSi4Ibl9Ab1kLsdGuLI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af8b7574c0a4b26360d529db34c1b06ffcafcc" title="LLAMA 4 in April?!?!?!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zalw2xetbipe1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=530f772a86a3ec93733a34053e7d66f91df342a0"&gt;https://preview.redd.it/zalw2xetbipe1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=530f772a86a3ec93733a34053e7d66f91df342a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google did similar thing with Gemma 3, so... llama 4 soon?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.llama.com/"&gt;https://www.llama.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Anywhere896"&gt; /u/Sea_Anywhere896 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T22:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeh199</id>
    <title>Gemma 3 27B and Mistral Small 3.1 LiveBench results</title>
    <updated>2025-03-18T21:52:48+00:00</updated>
    <author>
      <name>/u/Vivid_Dot_6405</name>
      <uri>https://old.reddit.com/user/Vivid_Dot_6405</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"&gt; &lt;img alt="Gemma 3 27B and Mistral Small 3.1 LiveBench results" src="https://preview.redd.it/ss640j7rripe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5889de41f1225137aeb588b78ba1420f488711e8" title="Gemma 3 27B and Mistral Small 3.1 LiveBench results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Dot_6405"&gt; /u/Vivid_Dot_6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ss640j7rripe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeddoy</id>
    <title>bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF</title>
    <updated>2025-03-18T19:22:10+00:00</updated>
    <author>
      <name>/u/nicklauzon</name>
      <uri>https://old.reddit.com/user/nicklauzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The man, the myth, the legend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicklauzon"&gt; /u/nicklauzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jef8pr</id>
    <title>Llama-3.3-Nemotron-Super-49B-v1 benchmarks</title>
    <updated>2025-03-18T20:37:26+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"&gt; &lt;img alt="Llama-3.3-Nemotron-Super-49B-v1 benchmarks" src="https://preview.redd.it/9mswvzt3eipe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24e550f4997f92ee036e88cce665df61a6cfcb83" title="Llama-3.3-Nemotron-Super-49B-v1 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mswvzt3eipe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T20:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jex61b</id>
    <title>If "The Model is the Product" article is true, a lot of AI companies are doomed</title>
    <updated>2025-03-19T13:38:25+00:00</updated>
    <author>
      <name>/u/bttf88</name>
      <uri>https://old.reddit.com/user/bttf88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear the community's thoughts on this blog post that was near the top of Hacker News yesterday. Unsurprisingly, it got voted down, because I think it's news that not many YC founders want to hear.&lt;/p&gt; &lt;p&gt;I think the argument holds a lot of merit. Basically, major AI Labs like OpenAI and Anthropic are clearly moving towards training their models for Agentic purposes using RL. OpenAI's DeepResearch is one example, Claude Code is another. The models are learning how to select and leverage tools as part of their training - eating away at the complexities of application layer.&lt;/p&gt; &lt;p&gt;If this continues, the application layer that many AI companies today are inhabiting will end up competing with the major AI Labs themselves. The article quotes the VP of AI @ DataBricks predicting that all closed model labs will shut down their APIs within the next 2 -3 years. Wild thought but not totally implausible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vintagedata.org/blog/posts/model-is-the-product"&gt;https://vintagedata.org/blog/posts/model-is-the-product&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bttf88"&gt; /u/bttf88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedtdx</id>
    <title>NVIDIA RTX PRO 6000 "Blackwell" Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM</title>
    <updated>2025-03-18T19:39:33+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" src="https://external-preview.redd.it/5BAv36T_l_eU_QSz_Xpf7tieC4_Jhv4Dc1Rz9SQmcqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=164c15c74e9b11e253100c5a08166759fbfab7a1" title="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-launch-flagship-gb202-gpu-24k-cores-96-gb-600w-tdp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jew0pk</id>
    <title>Is RTX 50xx series intentionally locked for compute / AI ?</title>
    <updated>2025-03-19T12:40:05+00:00</updated>
    <author>
      <name>/u/EmilPi</name>
      <uri>https://old.reddit.com/user/EmilPi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.videocardbenchmark.net/directCompute.html"&gt;https://www.videocardbenchmark.net/directCompute.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this chart, all 50xx cards are below their 40xx counterparts. And in overall gamers-targeted benchmark &lt;a href="https://www.videocardbenchmark.net/high_end_gpus.html"&gt;https://www.videocardbenchmark.net/high_end_gpus.html&lt;/a&gt; 50xx has just a small edge over 40xx.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmilPi"&gt; /u/EmilPi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jew0pk/is_rtx_50xx_series_intentionally_locked_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jej4s5</id>
    <title>Uncensored Gemma 3</title>
    <updated>2025-03-18T23:24:33+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B"&gt;https://huggingface.co/soob3123/amoral-gemma3-12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just finetuned this gemma 3 a day ago. Havent gotten it to refuse to anything yet. &lt;/p&gt; &lt;p&gt;Please feel free to give me feedback! This is my first finetuned model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T23:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedy17</id>
    <title>Nvidia digits specs released and renamed to DGX Spark</title>
    <updated>2025-03-18T19:44:57+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt; Memory Bandwidth 273 GB/s&lt;/p&gt; &lt;p&gt;Much cheaper for running 70gb - 200 gb models than a 5090. Cost $3K according to nVidia. Previously nVidia claimed availability in May 2025. Will be interesting tps versus &lt;a href="https://frame.work/desktop"&gt;https://frame.work/desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1je8axe</id>
    <title>I'm not one for dumb tests but this is a funny first impression</title>
    <updated>2025-03-18T15:56:57+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt; &lt;img alt="I'm not one for dumb tests but this is a funny first impression" src="https://preview.redd.it/s5k3j9z70hpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7562163bfe7eb6adc1234ea41f7c18eca73fb49c" title="I'm not one for dumb tests but this is a funny first impression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5k3j9z70hpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jewjr8</id>
    <title>Sonnet 3.7 Max – Max Spending, Max Regret</title>
    <updated>2025-03-19T13:07:33+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sonnet 3.7 Max, thinking I'd max out my workflow.&lt;/p&gt; &lt;p&gt;Turns out, I also maxed out my budget and my anxiety levels.&lt;/p&gt; &lt;p&gt;Max is gambling:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The cost?&lt;/strong&gt; High.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The guarantee?&lt;/strong&gt; Only that you’ll have extra troubleshooting to do.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jewjr8/sonnet_37_max_max_spending_max_regret/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T13:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jes9za</id>
    <title>Nemotron-Super-49B - Just MIGHT be a killer for creative writing. (24gb Vram)</title>
    <updated>2025-03-19T08:28:10+00:00</updated>
    <author>
      <name>/u/Majestical-psyche</name>
      <uri>https://old.reddit.com/user/Majestical-psyche</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24 GB Vram, with IQ3 XXS (for 16k context, you can use XS for 8k)&lt;/p&gt; &lt;p&gt;I'm not sure if I got lucky or not, I usally don't post until I know it's good. BUT, luck or not - its creative potiental is there! And it's VERY creative and smart on my first try using it. And, it has really good context recall. Uncencored for NSFW stories too?&lt;/p&gt; &lt;p&gt;Ime, The new: Qwen, Mistral small, Gemma 3 are all dry and not creative, and not smart for stories... &lt;/p&gt; &lt;p&gt;I'm posting this because I would like feed back on your experince with this model for creative writing.&lt;/p&gt; &lt;p&gt;What is your experince like?&lt;/p&gt; &lt;p&gt;Thank you, my favorite community. ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestical-psyche"&gt; /u/Majestical-psyche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jes9za/nemotronsuper49b_just_might_be_a_killer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T08:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeczzz</id>
    <title>New reasoning model from NVIDIA</title>
    <updated>2025-03-18T19:07:23+00:00</updated>
    <author>
      <name>/u/mapestree</name>
      <uri>https://old.reddit.com/user/mapestree</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt; &lt;img alt="New reasoning model from NVIDIA" src="https://external-preview.redd.it/S69zjX2lDQklr7v9YvMlzRoANZizsM0E74iOfCibG0E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2980af0c07cf4d3c4c52a935239567d59c9b3be3" title="New reasoning model from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapestree"&gt; /u/mapestree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/5kluqad.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeqxvq</id>
    <title>Meta releases new model: VGGT (Visual Geometry Grounded Transformer.)</title>
    <updated>2025-03-19T06:43:01+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://vgg-t.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeqxvq/meta_releases_new_model_vggt_visual_geometry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeqxvq/meta_releases_new_model_vggt_visual_geometry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T06:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1je6ns1</id>
    <title>Meta talks about us and open source source AI for over 1 Billion downloads</title>
    <updated>2025-03-18T14:46:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt; &lt;img alt="Meta talks about us and open source source AI for over 1 Billion downloads" src="https://preview.redd.it/gcql3piongpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b8393e9781f3853aac114d10af307ef017ca59" title="Meta talks about us and open source source AI for over 1 Billion downloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gcql3piongpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T14:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevseg</id>
    <title>New Multiview 3D Model by Stability AI</title>
    <updated>2025-03-19T12:27:23+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"&gt; &lt;img alt="New Multiview 3D Model by Stability AI" src="https://external-preview.redd.it/MjJ0cmhwMHUzbnBlMU3CkfuJ2R9TEEddx2aHL95I0ePLV3sEuU-hTYUsrCez.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0250d00697c362c042930fc3be758b09733fea28" title="New Multiview 3D Model by Stability AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This multi-view diffusion model transforms 2D images into immersive 3D videos with realistic depth and perspective—without complex reconstruction or scene-specific optimization.&lt;/p&gt; &lt;p&gt;The model generates 3D videos from a single input image or up to 32, following user-defined camera trajectories as well as 14 other dynamic camera paths, including 360°, Lemniscate, Spiral, Dolly Zoom, Move, Pan, and Roll.&lt;/p&gt; &lt;p&gt;Stable Virtual Camera is currently in research preview. &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://stability.ai/news/introducing-stable-virtual-camera-multi-view-video-generation-with-3d-camera-control"&gt;https://stability.ai/news/introducing-stable-virtual-camera-multi-view-video-generation-with-3d-camera-control&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://stable-virtual-camera.github.io/"&gt;https://stable-virtual-camera.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://stability.ai/s/stable-virtual-camera.pdf"&gt;https://stability.ai/s/stable-virtual-camera.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model weights: &lt;a href="https://huggingface.co/stabilityai/stable-virtual-camera"&gt;https://huggingface.co/stabilityai/stable-virtual-camera&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Stability-AI/stable-virtual-camera"&gt;https://github.com/Stability-AI/stable-virtual-camera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/clw6m2au3npe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevseg/new_multiview_3d_model_by_stability_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeod23</id>
    <title>Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!</title>
    <updated>2025-03-19T03:49:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt; &lt;img alt="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" src="https://b.thumbs.redditmedia.com/XFSJpkliPR8uY1jFq2k76gGRqvjuRsWCKVnvxNMCY1M.jpg" title="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jeod23"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T03:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jevzm3</id>
    <title>only the real ones remember</title>
    <updated>2025-03-19T12:38:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt; &lt;img alt="only the real ones remember" src="https://preview.redd.it/dh21r5dq5npe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5558750be400389e9a0376174765e8479016507" title="only the real ones remember" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh21r5dq5npe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T12:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jes8ue</id>
    <title>Llama4 is probably coming next month, multi modal, long context</title>
    <updated>2025-03-19T08:25:40+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt; &lt;img alt="Llama4 is probably coming next month, multi modal, long context" src="https://external-preview.redd.it/rBTq4xgcNZ3Fw4CkfyZwhQC8tEjpI_nR5A7MTEmNxNA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2e5f0e1f34849faacd7b3f87b98e5ad732879a6" title="Llama4 is probably coming next month, multi modal, long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6"&gt;https://preview.redd.it/cblvrkrcwlpe1.png?width=1677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a2c29ccbad996630f7cddc95d39a35ba9ef3fb6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW"&gt;https://www.meta.com/blog/connect-2025-llamacon-save-the-date/?srsltid=AfmBOoqvpQ6A0__ic3TrgNRj_RoGpBKWSnRmGFO_-RbGs5bZ7ntliloW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably ~1M context, multi modal&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T08:25:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jev3fl</id>
    <title>A man can dream</title>
    <updated>2025-03-19T11:47:24+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt; &lt;img alt="A man can dream" src="https://preview.redd.it/cw3hsv4mwmpe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e23762b65bf659739163a3e09585431a44e8b5" title="A man can dream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cw3hsv4mwmpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T11:47:24+00:00</published>
  </entry>
</feed>
