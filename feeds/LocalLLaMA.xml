<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-03T13:42:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l1wdlj</id>
    <title>llama4:maverick vs qwen3:235b</title>
    <updated>2025-06-02T22:58:01+00:00</updated>
    <author>
      <name>/u/M3GaPrincess</name>
      <uri>https://old.reddit.com/user/M3GaPrincess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. Which do like best and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/M3GaPrincess"&gt; /u/M3GaPrincess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wdlj/llama4maverick_vs_qwen3235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wdlj/llama4maverick_vs_qwen3235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wdlj/llama4maverick_vs_qwen3235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T22:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l23rrg</id>
    <title>Did anyone that ordered the GMK X2 from Amazon get it yet?</title>
    <updated>2025-06-03T05:18:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I've read elsewhere, GMK is reportedly giving priority to orders made directly on their website. So Amazon orders get the leftovers. Has anyone gotten a X2 ordered off of Amazon?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l23rrg/did_anyone_that_ordered_the_gmk_x2_from_amazon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l23rrg/did_anyone_that_ordered_the_gmk_x2_from_amazon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l23rrg/did_anyone_that_ordered_the_gmk_x2_from_amazon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T05:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2bmnt</id>
    <title>My setup for managing multiple LLM APIs + local models with a unified interface</title>
    <updated>2025-06-03T13:16:18+00:00</updated>
    <author>
      <name>/u/vivi541</name>
      <uri>https://old.reddit.com/user/vivi541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Wanted to share something I've been using for the past few months that's made my LLM workflow way smoother.&lt;/p&gt; &lt;p&gt;I was getting tired of juggling API keys for OpenAI, Anthropic, Groq, and a few other providers, plus constantly switching between different interfaces and keeping track of token costs across all of them. Started looking for a way to centralize everything.&lt;/p&gt; &lt;p&gt;Found this combo of Open WebUI + LiteLLM that's been pretty solid: &lt;a href="https://github.com/g1ibby/homellm"&gt;https://github.com/g1ibby/homellm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What I like about it:&lt;/p&gt; &lt;p&gt;- Single ChatGPT-style interface for everything&lt;/p&gt; &lt;p&gt;- All my API usage and costs in one dashboard (finally know how much I'm actually spending!)&lt;/p&gt; &lt;p&gt;- Super easy to connect tools like Aider - just point them to one endpoint instead of managing keys everywhere&lt;/p&gt; &lt;p&gt;- Can tunnel in my local Ollama server or other self-hosted models, so everything lives in the same interface&lt;/p&gt; &lt;p&gt;It's just Docker Compose, so pretty straightforward if you have a VPS lying around. Takes about 10 minutes to get running.&lt;/p&gt; &lt;p&gt;Anyone else using something similar? Always curious how others are handling the multi-provider chaos. The local + cloud hybrid approach has been working really well for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vivi541"&gt; /u/vivi541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bmnt/my_setup_for_managing_multiple_llm_apis_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bmnt/my_setup_for_managing_multiple_llm_apis_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bmnt/my_setup_for_managing_multiple_llm_apis_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T13:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2btw0</id>
    <title>Its my first PC build , I need help. Is this enough to run LLM locally !</title>
    <updated>2025-06-03T13:25:06+00:00</updated>
    <author>
      <name>/u/Series-Curious</name>
      <uri>https://old.reddit.com/user/Series-Curious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://pcpricetracker.in/b/s/74ff4d5d-5825-4841-8bbc-dd6851a52ca6"&gt;PCPriceTracker Build&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Selection&lt;/th&gt; &lt;th align="left"&gt;Source&lt;/th&gt; &lt;th align="right"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Processor&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/3746a9dcc20314ac958396bdb9187b91"&gt;Amd Ryzen 5 7600 Gaming Desktop Processor (100-100001015BOX)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Computech Store&lt;/td&gt; &lt;td align="right"&gt;17894&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/d7f43854d69a1e61a11fb26743f2f5ec"&gt;Gigabyte B650M D3HP AX AM5 Micro ATX Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Computech Store&lt;/td&gt; &lt;td align="right"&gt;11489&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Graphic Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/c545e552075fe3e651809b8be6408e40"&gt;ASUS Dual RTX 3060 V2 OC Edition 12GB GDDR6 192-Bit LHR Graphics card with DLSS AI Rendering&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Easyshoppi&lt;/td&gt; &lt;td align="right"&gt;24000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/64f479a5d14300f31a573cc68f74efca"&gt;DeepCool PM750D Series Non-Modular 80 PLUS Gold Power Supply R-PM750D-FA0B-UK&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Clarion&lt;/td&gt; &lt;td align="right"&gt;6425&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Cabinet&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/3cd87944bb1c42f61d840c038cb71902"&gt;DEEPCOOL MATREXX 40 ESSENTIAL MICRO-ATX CABINET (DP-MATX-MATREXX40)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Elitehubs&lt;/td&gt; &lt;td align="right"&gt;2999&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/75c35088b3d989be4a9d0563ce1a04d4"&gt;Acer BL-9BWWA-446 Desktop Ram HT200 Series 32GB (16GBx2) DDR5 7200MHz (Silver)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Computech Store&lt;/td&gt; &lt;td align="right"&gt;13099&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Additional Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hard drive&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SSD drive&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpricetracker.in/products/18b6a7101ba29bb3a122d87462123217"&gt;Acer Predator GM7000 1TB M.2 NVMe Gen4 Internal SSD (BL.9BWWR.105)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Variety Online&lt;/td&gt; &lt;td align="right"&gt;7257&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Additional SSD&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Monitor&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Additional Monitor&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Keyboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mouse&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Headset&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Case Fans&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Grand Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;INR 83163&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Series-Curious"&gt; /u/Series-Curious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2btw0/its_my_first_pc_build_i_need_help_is_this_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2btw0/its_my_first_pc_build_i_need_help_is_this_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2btw0/its_my_first_pc_build_i_need_help_is_this_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T13:25:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1j94p</id>
    <title>NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly</title>
    <updated>2025-06-02T14:20:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly" src="https://external-preview.redd.it/CZ499DlxtUi8-a0hH-i2iuvuqGABLEdCAAN2p00rlA0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64ca9b6f5516529ad31d61311b7ab6293c4d549c" title="NVIDIA RTX PRO 6000 Unlocks GB202's Full Performance In Gaming: Beats GeForce RTX 5090 Convincingly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-beats-geforce-rtx-5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1j94p/nvidia_rtx_pro_6000_unlocks_gb202s_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T14:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l19yud</id>
    <title>IQ1_Smol_Boi</title>
    <updated>2025-06-02T05:26:51+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt; &lt;img alt="IQ1_Smol_Boi" src="https://preview.redd.it/9u1teeqt4g4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab7030e08d75ad0063051230aa543c0b2a3ac7f" title="IQ1_Smol_Boi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks asked me for an R1-0528 quant that might fit on 128GiB RAM + 24GB VRAM. I didn't think it was possible, but turns out my new smol boi &lt;code&gt;IQ1_S_R4&lt;/code&gt; is 131GiB and actually runs okay (ik_llama.cpp fork only), and has perplexity lower &amp;quot;better&amp;quot; than &lt;code&gt;Qwen3-235B-A22B-Q8_0&lt;/code&gt; which is almost twice the size! Not sure that means it is better, but kinda surprising to me.&lt;/p&gt; &lt;p&gt;Unsloth's newest smol boi is an odd &lt;code&gt;UD-TQ1_0&lt;/code&gt; weighing in at 151GiB. The &lt;code&gt;TQ1_0&lt;/code&gt; quant is a 1.6875 bpw quant types for TriLMs and BitNet b1.58 models. However, if you open up the side-bar on the modelcard it doesn't actually have any TQ1_0 layers/tensors and is mostly a mix of IQN_S and such. So not sure what is going on there or if it was a mistake. It does at least run from what I can tell, though I didn't try inferencing with it. They do have an &lt;code&gt;IQ1_S&lt;/code&gt; as well, but it seems rather larger given their recipe though I've heard folks have had success with it.&lt;/p&gt; &lt;p&gt;Bartowski's smol boi &lt;code&gt;IQ1_M&lt;/code&gt; is the next smallest I've seen at about 138GiB and seems to work okay in my limited testing. Surprising how these quants can still run at such low bit rates!&lt;/p&gt; &lt;p&gt;Anyway, I wouldn't recommend these smol bois if you have enough RAM+VRAM to fit a more optimized larger quant, but if at least there are some options &amp;quot;For the desperate&amp;quot; haha...&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9u1teeqt4g4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T05:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l23d09</id>
    <title>Do small reasoning/CoT models get stuck in long thinking loops more often?</title>
    <updated>2025-06-03T04:53:49+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;As the title suggests, I've noticed small reasoning models tend to think a lot, sometimes they don't stop.&lt;/p&gt; &lt;p&gt;QwQ-32B, DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-0528-Qwen3-8B. &lt;/p&gt; &lt;p&gt;Larger models tend to not get stuck as often. Could it be because of short context windows? Or am I imagining it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l23d09/do_small_reasoningcot_models_get_stuck_in_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l23d09/do_small_reasoningcot_models_get_stuck_in_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l23d09/do_small_reasoningcot_models_get_stuck_in_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T04:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l219ol</id>
    <title>OSS implementation of OpenAI's vector search tool?</title>
    <updated>2025-06-03T02:57:02+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Is there a library that implements OpenAI's vector search?&lt;/p&gt; &lt;p&gt;Something where you can create vector stores, add files (pdf, docx, md) to the vector stores and then search these vector store for a certain query.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l219ol/oss_implementation_of_openais_vector_search_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l219ol/oss_implementation_of_openais_vector_search_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l219ol/oss_implementation_of_openais_vector_search_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T02:57:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1wnsz</id>
    <title>Why use thinking model ?</title>
    <updated>2025-06-02T23:10:34+00:00</updated>
    <author>
      <name>/u/Empty_Object_9299</name>
      <uri>https://old.reddit.com/user/Empty_Object_9299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm relatively new to using models. I've experimented with some that have a &amp;quot;thinking&amp;quot; feature, but I'm finding the delay quite frustrating – a minute to generate a response feels excessive. &lt;/p&gt; &lt;p&gt;I understand these models are popular, so I'm curious what I might be missing in terms of their benefits or how to best utilize them. &lt;/p&gt; &lt;p&gt;Any insights would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Object_9299"&gt; /u/Empty_Object_9299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wnsz/why_use_thinking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wnsz/why_use_thinking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1wnsz/why_use_thinking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T23:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1q3dk</id>
    <title>Which programming languages do LLMs struggle with the most, and why?</title>
    <updated>2025-06-02T18:45:36+00:00</updated>
    <author>
      <name>/u/alozowski</name>
      <uri>https://old.reddit.com/user/alozowski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that LLMs do well with Python, which is quite obvious, but often make mistakes in other languages. I can't test every language myself, so can you share, which languages have you seen them struggle with, and what went wrong?&lt;/p&gt; &lt;p&gt;For context: I want to test LLMs on various &amp;quot;hard&amp;quot; languages &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alozowski"&gt; /u/alozowski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1q3dk/which_programming_languages_do_llms_struggle_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1q3dk/which_programming_languages_do_llms_struggle_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1q3dk/which_programming_languages_do_llms_struggle_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T18:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2agpu</id>
    <title>Attention by Hand - Practice attention mechanism on an interactive webpage</title>
    <updated>2025-06-03T12:21:40+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt; &lt;img alt="Attention by Hand - Practice attention mechanism on an interactive webpage" src="https://external-preview.redd.it/nhMEOcQ2pY2cWvmMC1a4Ya8l-ZpFkuu1hArRGS_70Jo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d72bc65c67e8fa81fbd23e548bba69e1a0bb3e8" title="Attention by Hand - Practice attention mechanism on an interactive webpage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/fmji9oswfp4f1.gif"&gt;https://i.redd.it/fmji9oswfp4f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this: &lt;a href="https://vizuara-ai-learning-lab.vercel.app/"&gt;https://vizuara-ai-learning-lab.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nuts-And-Bolts-AI is an interactive web environment where you can practice AI concepts by writing down matrix multiplications. &lt;/p&gt; &lt;p&gt;(1) Let’s take the attention mechanism in language models as an example. &lt;/p&gt; &lt;p&gt;(2) Using Nuts-And-Bolts-AI, you can actively engage with the step-by-step calculation of the scaled dot-product attention mechanism. &lt;/p&gt; &lt;p&gt;(3) Users can input values and work through each matrix operation (Q, K, V, scores, softmax, weighted sum) manually within a guided, interactive environment. &lt;/p&gt; &lt;p&gt;Eventually, we will add several modules on this website: &lt;/p&gt; &lt;p&gt;- Neural Networks from scratch&lt;/p&gt; &lt;p&gt;- CNNs from scratch&lt;/p&gt; &lt;p&gt;- RNNs from scratch&lt;/p&gt; &lt;p&gt;- Diffusion from scratch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1e6ic</id>
    <title>Ignore the hype - AI companies still have no moat</title>
    <updated>2025-06-02T10:06:26+00:00</updated>
    <author>
      <name>/u/No_Tea2273</name>
      <uri>https://old.reddit.com/user/No_Tea2273</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt; &lt;img alt="Ignore the hype - AI companies still have no moat" src="https://external-preview.redd.it/TawOSRI4o3WDthoH5zp4cL7vlpQPtqKfMqXniUZMdX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc884e61e4fbae7ba821197e1b5320440aaab413" title="Ignore the hype - AI companies still have no moat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An article I wrote a while back, I think &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; still wins&lt;/p&gt; &lt;p&gt;The basis of it is that Every single AI tool – has an open source alternative, every. single. one – so programming wise, for a new company to implement these features is not a matter of development complexity but a matter of getting the biggest audience &lt;/p&gt; &lt;p&gt;Everything has an open source versioned alternative right now&lt;/p&gt; &lt;p&gt;Take for example&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Tea2273"&gt; /u/No_Tea2273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://river.berlin/blog/there-is-still-no-moat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T10:06:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1pgv9</id>
    <title>latest llama.cpp (b5576) + DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf successful VScode + MCP running</title>
    <updated>2025-06-02T18:21:35+00:00</updated>
    <author>
      <name>/u/tyoyvr-2222</name>
      <uri>https://old.reddit.com/user/tyoyvr-2222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1pgv9/latest_llamacpp_b5576_deepseekr10528qwen38bq8/"&gt; &lt;img alt="latest llama.cpp (b5576) + DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf successful VScode + MCP running" src="https://external-preview.redd.it/sYVW4X6gc-B0EvqWe0QNA3QGRk5e7XKmzrtKFGnT66k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9cb9655f58f9405afdc373583acebb28d62dac9" title="latest llama.cpp (b5576) + DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf successful VScode + MCP running" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b5576"&gt;Release b5576 · ggml-org/llama.cpp&lt;/a&gt; and try to use MCP tools with folllowing environment:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF"&gt;DeepSeek-R1-0528-Qwen3-8B-Q8_0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;VS code&lt;/li&gt; &lt;li&gt;Cline&lt;/li&gt; &lt;li&gt;MCP tools like mcp_server_time, filesystem, MS playwright&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Got application error before b5576 previously, but all tools can run smoothly now.&lt;br /&gt; It took longer time to &amp;quot;think&amp;quot; compared with &lt;a href="https://huggingface.co/unsloth/Devstral-Small-2505-GGUF"&gt;Devstral-Small-2505-GGUF&lt;/a&gt;&lt;br /&gt; Anyway, it is a good model with less VRAM if want to try local development.&lt;/p&gt; &lt;p&gt;my Win11 batch file for reference, adjust based on your own environment:&lt;br /&gt; ```TEXT&lt;br /&gt; SET LLAMA_CPP_PATH=G:\ai\llama.cpp&lt;br /&gt; SET PATH=%LLAMA_CPP_PATH%\build\bin\Release\;%PATH%&lt;br /&gt; SET LLAMA_ARG_HOST=0.0.0.0&lt;br /&gt; SET LLAMA_ARG_PORT=8080&lt;br /&gt; SET LLAMA_ARG_JINJA=true&lt;br /&gt; SET LLAMA_ARG_FLASH_ATTN=true&lt;br /&gt; SET LLAMA_ARG_CACHE_TYPE_K=q8_0&lt;br /&gt; SET LLAMA_ARG_CACHE_TYPE_V=q8_0&lt;br /&gt; SET LLAMA_ARG_N_GPU_LAYERS=65&lt;br /&gt; SET LLAMA_ARG_CTX_SIZE=131072&lt;br /&gt; SET LLAMA_ARG_SWA_FULL=true&lt;br /&gt; SET LLAMA_ARG_MODEL=models\deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf&lt;br /&gt; llama-server.exe --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 --repeat-penalty 1.1&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/262hbrj02k4f1.png?width=1011&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d9d0a799cc96053b4b255429c2a7e4b85a995ce"&gt;https://preview.redd.it/262hbrj02k4f1.png?width=1011&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d9d0a799cc96053b4b255429c2a7e4b85a995ce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tyoyvr-2222"&gt; /u/tyoyvr-2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1pgv9/latest_llamacpp_b5576_deepseekr10528qwen38bq8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1pgv9/latest_llamacpp_b5576_deepseekr10528qwen38bq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1pgv9/latest_llamacpp_b5576_deepseekr10528qwen38bq8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T18:21:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1mb6y</id>
    <title>PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion</title>
    <updated>2025-06-02T16:20:20+00:00</updated>
    <author>
      <name>/u/SandSalt8370</name>
      <uri>https://old.reddit.com/user/SandSalt8370</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"&gt; &lt;img alt="PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion" src="https://external-preview.redd.it/w-YzJC8yYFljokN1sVgB95jsZmNJotgMItgN5CbyhjY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d65ddc626651c2d4f886345c5637afbe5263791e" title="PlayAI's Latest Diffusion-based Speech Editing Model: PlayDiffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PlayAI open-sourced a new Speech Editing model today that allows for precise &amp;amp; clean speech editing. A huge step up from traditional autoregressive models that aren't designed for this task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandSalt8370"&gt; /u/SandSalt8370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/playht/playdiffusion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1mb6y/playais_latest_diffusionbased_speech_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T16:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l25oyk</id>
    <title>What happened to the fused/merged models?</title>
    <updated>2025-06-03T07:23:10+00:00</updated>
    <author>
      <name>/u/Su1tz</name>
      <uri>https://old.reddit.com/user/Su1tz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember back when QwQ-32 first came out there was a FuseO1 thing with SkyT1. Are there any newer models like this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Su1tz"&gt; /u/Su1tz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l25oyk/what_happened_to_the_fusedmerged_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l25oyk/what_happened_to_the_fusedmerged_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l25oyk/what_happened_to_the_fusedmerged_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T07:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l20f2h</id>
    <title>LLM an engine</title>
    <updated>2025-06-03T02:13:46+00:00</updated>
    <author>
      <name>/u/localremote762</name>
      <uri>https://old.reddit.com/user/localremote762</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can’t help but feel like the LLM, ollama, deep seek, openAI, Claude, are all engines sitting on a stand. Yes we see the raw power it puts out when sitting on an engine stand, but we can’t quite conceptually figure out the “body” of the automobile. The car changed the world, but not without first the engine. &lt;/p&gt; &lt;p&gt;I’ve been exploring mcp, rag and other context servers and from what I can see, they all suck. ChatGPTs memory does the best job, but when programming, remembering that I always have a set of includes, or use a specific theme, they all do a terrible job. &lt;/p&gt; &lt;p&gt;Please anyone correct me if I’m wrong, but it feels like we have all this raw power just waiting to be unleashed, and I can only tap into the raw power when I’m in an isolated context window, not on the open road. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/localremote762"&gt; /u/localremote762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l20f2h/llm_an_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l20f2h/llm_an_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l20f2h/llm_an_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T02:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1jyld</id>
    <title>Smallest LLM you tried that's legit</title>
    <updated>2025-06-02T14:48:26+00:00</updated>
    <author>
      <name>/u/Remarkable-Law9287</name>
      <uri>https://old.reddit.com/user/Remarkable-Law9287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what's the smallest LLM you've used that gives proper text, not just random gibberish? &lt;/p&gt; &lt;p&gt;I've tried qwen2.5:0.5B.it works pretty well for me, actually quite good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Law9287"&gt; /u/Remarkable-Law9287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1jyld/smallest_llm_you_tried_thats_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1t75j</id>
    <title>ZorkGPT: Open source AI agent that plays the classic text adventure game Zork</title>
    <updated>2025-06-02T20:46:12+00:00</updated>
    <author>
      <name>/u/stickystyle</name>
      <uri>https://old.reddit.com/user/stickystyle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an AI system that plays Zork (the classic, and very hard 1977 text adventure game) using multiple open-source LLMs working together.&lt;/p&gt; &lt;p&gt;The system uses separate models for different tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent model decides what actions to take&lt;/li&gt; &lt;li&gt;Critic model evaluates those actions before execution&lt;/li&gt; &lt;li&gt;Extractor model parses game text into structured data&lt;/li&gt; &lt;li&gt;Strategy generator learns from experience to improve over time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unlike the other Pokemon gaming projects, this focuses on using open source models. I had initially wanted to limit the project to models that I can run locally on my MacMini, but that proved to be fruitless after many thousands of turns. I also don't have the cash resources to runs this on Gemini or Claude (like how can those guys afford that??). The AI builds a map as it explores, maintains memory of what it's learned, and continuously updates its strategy.&lt;/p&gt; &lt;p&gt;The live viewer shows real-time data of the AI's reasoning process, current game state, learned strategies, and a visual map of discovered locations. You can watch it play live at &lt;a href="https://zorkgpt.com"&gt;https://zorkgpt.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project code: &lt;a href="https://github.com/stickystyle/ZorkGPT"&gt;https://github.com/stickystyle/ZorkGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanted to share something I've been playing with after work that I thought this audience would find neat. I just wiped its memory this morning and started a fresh &amp;quot;no-touch&amp;quot; run, so let's see how it goes :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stickystyle"&gt; /u/stickystyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1t75j/zorkgpt_open_source_ai_agent_that_plays_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1t75j/zorkgpt_open_source_ai_agent_that_plays_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1t75j/zorkgpt_open_source_ai_agent_that_plays_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T20:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1rb18</id>
    <title>I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!</title>
    <updated>2025-06-02T19:32:00+00:00</updated>
    <author>
      <name>/u/carlrobertoh</name>
      <uri>https://old.reddit.com/user/carlrobertoh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"&gt; &lt;img alt="I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!" src="https://external-preview.redd.it/MXRsZjNqNWZmazRmMXGOM2N51B2QnCZhafa7NKplGti0671pTg7o1NRLqsqm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f73f62c3f1d72a7c956c94ca78e646d7393252" title="I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing a coding assistant for JetBrains IDEs called &lt;strong&gt;ProxyAI&lt;/strong&gt; (previously CodeGPT), and I wanted to experiment with an idea where LLM is instructed to produce diffs as opposed to regular code blocks, which ProxyAI then applies directly to your project.&lt;/p&gt; &lt;p&gt;I was fairly skeptical about this at first, but after going back-and-forth with the initial version and getting it where I wanted it to be, it simply started to amaze me. The model began generating paths and diffs for files it had never seen before and somehow these &amp;quot;hallucinations&amp;quot; were correct (this mostly happened with modifications to build files that typically need a fixed path).&lt;/p&gt; &lt;p&gt;What really surprised me was how natural the workflow became. You just describe what you want changed, and the diffs appear in &lt;em&gt;near real-time&lt;/em&gt;, almost always with the correct diff patch - can't praise enough how good it feels for &lt;strong&gt;quick iterations&lt;/strong&gt;! In most cases, it takes &lt;em&gt;less than a minute&lt;/em&gt; for the LLM to make edits across many different files. When smaller models mess up (which happens fairly often), there's a simple retry mechanism that usually gets it right on the second attempt - fairly similar logic to Cursor's Fast Apply.&lt;/p&gt; &lt;p&gt;This whole functionality is &lt;strong&gt;free&lt;/strong&gt;, &lt;strong&gt;open-source&lt;/strong&gt;, and available for &lt;strong&gt;every model and provider&lt;/strong&gt;, regardless of tool calling capabilities. &lt;strong&gt;No vendor lock-in&lt;/strong&gt;, &lt;strong&gt;no premium features&lt;/strong&gt; - just plug in your API key or connect to a local model and give it a go!&lt;/p&gt; &lt;p&gt;For me, this feels much more intuitive than the typical &lt;em&gt;&amp;quot;switch to edit mode&amp;quot;&lt;/em&gt; dance that most AI coding tools require. I'd definitely encourage you to give it a try and let me know what you think, or what the current solution lacks. Always looking to improve!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tryproxy.io/"&gt;https://www.tryproxy.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Best regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlrobertoh"&gt; /u/carlrobertoh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zcq3wk5ffk4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T19:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2bgc1</id>
    <title>Semantic Search PoC for Hugging Face – Now with Parameter Size Filters (0-1B to 70B+)</title>
    <updated>2025-06-03T13:08:16+00:00</updated>
    <author>
      <name>/u/dvanstrien</name>
      <uri>https://old.reddit.com/user/dvanstrien</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! &lt;/p&gt; &lt;p&gt;I’ve recently updated my prototype semantic search for Hugging Face Space, which makes it easier to discover models not only via semantic search but also by &lt;strong&gt;parameter size&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;There are currently over 1.5 million models on the Hub, and finding the right one can be a challenge. &lt;/p&gt; &lt;p&gt;This PoC helps you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Semantic search using the &lt;strong&gt;summaries&lt;/strong&gt; generated by a small LLM (&lt;a href="https://huggingface.co/davanstrien/Smol-Hub-tldr"&gt;https://huggingface.co/davanstrien/Smol-Hub-tldr&lt;/a&gt;) &lt;/li&gt; &lt;li&gt;Filter models by &lt;strong&gt;parameter size&lt;/strong&gt;, from 0-1B all the way to 70B+&lt;/li&gt; &lt;li&gt;It also allows you to find similar models/datasets. For datasets in particular, I've found this can be a nice way to find a bunch of datasets super quickly. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it here: &lt;a href="https://huggingface.co/spaces/librarian-bots/huggingface-semantic-search"&gt;https://huggingface.co/spaces/librarian-bots/huggingface-semantic-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FWIW, for this Space, I also tried a different approach to developing it. Basically, I did the backend API dev myself (since I'm familiar enough with that kind of dev work for it to be quick), but vibe coded the frontend using the OpenAPI Specification for the backed as context for the LLM). Seems to work quite well (at least the front end is better than anything I would do on my own...) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvanstrien"&gt; /u/dvanstrien &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T13:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2afie</id>
    <title>PipesHub - Open Source Enterprise Search Platform(Generative-AI Powered)</title>
    <updated>2025-06-03T12:20:03+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I’m excited to share something we’ve been building for the past few months – &lt;strong&gt;PipesHub&lt;/strong&gt;, a fully open-source Enterprise Search Platform.&lt;/p&gt; &lt;p&gt;In short, PipesHub is your &lt;strong&gt;customizable, scalable, enterprise-grade RAG platform&lt;/strong&gt; for everything from intelligent search to building agentic apps — all powered by your own models and data.&lt;/p&gt; &lt;p&gt;We also connect with tools like Google Workspace, Slack, Notion and more — so your team can quickly find answers and trained on &lt;em&gt;your&lt;/em&gt; company’s internal knowledge.&lt;/p&gt; &lt;p&gt;You can run also it locally and use any AI Model out of the box including Ollama.&lt;br /&gt; &lt;strong&gt;We’re looking for early feedback&lt;/strong&gt;, so if this sounds useful (or if you’re just curious), we’d love for you to check it out and tell us what you think!&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2b83p</id>
    <title>Vision Language Models are Biased</title>
    <updated>2025-06-03T12:58:13+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://vlmsarebiased.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2820t</id>
    <title>nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face</title>
    <updated>2025-06-03T10:06:22+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt; &lt;img alt="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" src="https://external-preview.redd.it/RP_o1NnFnVgqmDAj8haRnOnwD5ZnZcjaUEqHghtS6ig.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbe8cb87d4ec3c680868083410ff0f4da7c3636d" title="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T10:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1qqdx</id>
    <title>At the airport people watching while I run models locally:</title>
    <updated>2025-06-02T19:10:02+00:00</updated>
    <author>
      <name>/u/Current-Ticket4214</name>
      <uri>https://old.reddit.com/user/Current-Ticket4214</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt; &lt;img alt="At the airport people watching while I run models locally:" src="https://preview.redd.it/55ab38z0ck4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee34cc7e6232ae1fc31a5076b1cc4064bd66305d" title="At the airport people watching while I run models locally:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Ticket4214"&gt; /u/Current-Ticket4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/55ab38z0ck4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T19:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l27g8d</id>
    <title>Google opensources DeepSearch stack</title>
    <updated>2025-06-03T09:25:47+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt; &lt;img alt="Google opensources DeepSearch stack" src="https://external-preview.redd.it/jtUtL7EqwS5bMEk8XfF81tFd6n1MgnQyQL0hQG-jzRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0633532a1f9e627adaa6246fd5a299a809f2654" title="Google opensources DeepSearch stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it's not evident if this is the exact same stack they use in the Gemini user app, it sure looks very promising! Seems to work with Gemini and Google Search. Maybe this can be adapted for any local model and SearXNG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T09:25:47+00:00</published>
  </entry>
</feed>
