<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-17T07:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k0c40c</id>
    <title>We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed</title>
    <updated>2025-04-16T04:38:13+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt; &lt;img alt="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" src="https://external-preview.redd.it/OTVoem9nbmRsNHZlMRZyoyYKNpzPJZZUnGrUtyeCYi3ToyFLi7JPjGL-ftCw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b12cd479c2024bd0aed4acb204f01a7a4780624" title="We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Menlo Research again, and today we’d like to introduce a new paper from our team related to search.&lt;/p&gt; &lt;p&gt;Have you ever felt that when searching on Google, &lt;strong&gt;you know for sure there’s no way you’ll get the result you want on the first try&lt;/strong&gt; (you’re already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea.&lt;/p&gt; &lt;p&gt;We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model &amp;quot;work harder&amp;quot; and be more diligent, it could actually perform better.&lt;/p&gt; &lt;p&gt;Normally when training LLMs, repetitive actions are something people want to avoid, because they’re thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of &lt;strong&gt;46%&lt;/strong&gt;, compared to just &lt;strong&gt;20%&lt;/strong&gt; from a baseline model trained the same way. So that gives us some evidence that &lt;strong&gt;Repetition is not hallucination.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfect for research use cases.&lt;/p&gt; &lt;p&gt;Attached a demo in the clip.&lt;/p&gt; &lt;p&gt;(The beginning has a little meme to bring you some laughs 😄 - Trust me ReZero is Retry and Zero from Deepseek-zero)&lt;/p&gt; &lt;p&gt;Links to the paper/data below:&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2504.11001"&gt;https://arxiv.org/abs/2504.11001&lt;/a&gt;&lt;br /&gt; huggingface: &lt;a href="https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404"&gt;https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404&lt;/a&gt;&lt;br /&gt; github: &lt;a href="https://github.com/menloresearch/ReZero"&gt;https://github.com/menloresearch/ReZero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; As much as we want to make this model perfect, we are well aware of its limitations, specifically about training set and a bit poor design choice of reward functions. However we decided to release the model anyway, because it's better for the community to have access and play with it (also our time budget for this research is already up).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x9c46kt8l4ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T04:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0iu5z</id>
    <title>Announcing RealHarm: A Collection of Real-World Language Model Application Failure</title>
    <updated>2025-04-16T12:10:26+00:00</updated>
    <author>
      <name>/u/chef1957</name>
      <uri>https://old.reddit.com/user/chef1957</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm David from&lt;a href="https://giskard.ai"&gt; Giskard&lt;/a&gt;, and we work on securing Agents.&lt;/p&gt; &lt;p&gt;Today, we are announcing &lt;strong&gt;RealHarm&lt;/strong&gt;: a dataset of &lt;em&gt;real-world&lt;/em&gt; problematic interactions with &lt;strong&gt;AI agents&lt;/strong&gt;, drawn from publicly reported incidents.&lt;/p&gt; &lt;p&gt;Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different—and much messier.&lt;/p&gt; &lt;p&gt;With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of &lt;em&gt;deployers&lt;/em&gt;—the companies or teams actually shipping LLMs—and we found some surprising results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reputational damage&lt;/strong&gt; was the most common organizational harm.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Misinformation and hallucination&lt;/strong&gt; were the most frequent hazards&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art guardrails have failed&lt;/strong&gt; to catch many of the incidents. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope this dataset can help researchers, developers, and product teams better understand, test, and prevent real-world harms.&lt;/p&gt; &lt;p&gt;The paper and dataset: &lt;a href="https://realharm.giskard.ai/"&gt;https://realharm.giskard.ai/&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We'd love feedback, questions, or suggestions—especially if you're deploying LLMs and have real harmful scenarios.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chef1957"&gt; /u/chef1957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T12:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0odhq</id>
    <title>KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title>
    <updated>2025-04-16T16:16:39+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"&gt; &lt;img alt="KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say..." src="https://external-preview.redd.it/E0QrtLGdAenlhx0dgrRxQhYXEHxRQVilnk0OkkkKL-M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7a4d66aac5a95d9f7e7eefde94e0ec3332c0946" title="KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/py5Tvae.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T16:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k10rtg</id>
    <title>Tried OpenAI Codex and it sucked 👎</title>
    <updated>2025-04-17T01:13:45+00:00</updated>
    <author>
      <name>/u/itzco1993</name>
      <uri>https://old.reddit.com/user/itzco1993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released today the Claude Code competitor, called Codex (will add link in comments).&lt;/p&gt; &lt;p&gt;Just tried it but failed miserable to do a simple task, first it was not even able to detect the language the codebase was in and then it failed due to context window exceeded.&lt;/p&gt; &lt;p&gt;Has anyone tried it? Results?&lt;/p&gt; &lt;p&gt;Looks promising mainly because code is open source compared to anthropic's claude code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itzco1993"&gt; /u/itzco1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10rtg/tried_openai_codex_and_it_sucked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10rtg/tried_openai_codex_and_it_sucked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k10rtg/tried_openai_codex_and_it_sucked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T01:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0r9pi</id>
    <title>Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title>
    <updated>2025-04-16T18:12:44+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When running the llama.cpp WebUI with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \ --seed 42 \ --mlock \ --n-gpu-layers -1 \ --ctx-size 8096 \ --port 10000 \ --temp 1.0 \ --top-k 64 \ --top-p 0.95 \ --min-p 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality.&lt;/p&gt; &lt;p&gt;For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T18:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0qbme</id>
    <title>o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!</title>
    <updated>2025-04-16T17:34:46+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"&gt; &lt;img alt="o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!" src="https://preview.redd.it/0p5ymcc7g8ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d96f80ad111301c2dfe2b713b55f0121905d377" title="o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0p5ymcc7g8ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:34:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0qisr</id>
    <title>OpenAI introduces codex: a lightweight coding agent that runs in your terminal</title>
    <updated>2025-04-16T17:42:48+00:00</updated>
    <author>
      <name>/u/MorroWtje</name>
      <uri>https://old.reddit.com/user/MorroWtje</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"&gt; &lt;img alt="OpenAI introduces codex: a lightweight coding agent that runs in your terminal" src="https://external-preview.redd.it/L2s8FUcxTxmbnY9A3xFNDqLsOD-NZikx1UTncO36YW4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9814fd7577317ca58f6bc696ee800e0ebe489eab" title="OpenAI introduces codex: a lightweight coding agent that runs in your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorroWtje"&gt; /u/MorroWtje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0kape</id>
    <title>Price vs LiveBench Performance of non-reasoning LLMs</title>
    <updated>2025-04-16T13:22:09+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"&gt; &lt;img alt="Price vs LiveBench Performance of non-reasoning LLMs" src="https://preview.redd.it/eiojps9w67ve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d129a188635d4a6845ab6a526591d280f4cd4c30" title="Price vs LiveBench Performance of non-reasoning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eiojps9w67ve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T13:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0h641</id>
    <title>Droidrun is now Open Source</title>
    <updated>2025-04-16T10:32:33+00:00</updated>
    <author>
      <name>/u/Sleyn7</name>
      <uri>https://old.reddit.com/user/Sleyn7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"&gt; &lt;img alt="Droidrun is now Open Source" src="https://preview.redd.it/9zbo1emvc6ve1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9650416ab69f13bb7190fc4810e3ec5984d6be6d" title="Droidrun is now Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, Wow! Just a couple of days ago, I posted here about Droidrun and the response was incredible – we had over 900 people sign up for the waitlist! Thank you all so much for the interest and feedback.&lt;/p&gt; &lt;p&gt;Well, the wait is over! We're thrilled to announce that the Droidrun framework is now public and open-source on GitHub!&lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks again for your support. Let's keep on running&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sleyn7"&gt; /u/Sleyn7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9zbo1emvc6ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T10:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0p3h0</id>
    <title>Results of Ollama Leakage</title>
    <updated>2025-04-16T16:46:35+00:00</updated>
    <author>
      <name>/u/zxbsmk</name>
      <uri>https://old.reddit.com/user/zxbsmk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"&gt; &lt;img alt="Results of Ollama Leakage" src="https://preview.redd.it/kl4bv7ne78ve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52549e31655556f832850c261393e3623b27e4f3" title="Results of Ollama Leakage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many servers still seem to be missing basic security.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.freeollama.com/"&gt;https://www.freeollama.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxbsmk"&gt; /u/zxbsmk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kl4bv7ne78ve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0xc46</id>
    <title>A fast, native desktop UI for transcribing audio and video using Whisper</title>
    <updated>2025-04-16T22:27:11+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since my last post, I've added several new features such as batch processing (multiple files at once) and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A fast, native desktop UI for transcribing audio and video using Whisper — built entirely in modern C++ and Qt. I’ll be regularly updating it with more features.&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Supports translation for 100+ languages (not models ending in &lt;code&gt;.en&lt;/code&gt; like &lt;code&gt;medium.en&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt; — drag in multiple files, select several at once, or use &amp;quot;Open With&amp;quot; on multiple items; they'll run one-by-one automatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installer handles everything&lt;/strong&gt; — downloads dependencies, compiles and optimizes Whisper for your system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully C++ implementation&lt;/strong&gt; — no Python, no scripts, no CLI fuss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU acceleration via Vulkan&lt;/strong&gt; — runs fast on AMD, Intel, or NVIDIA.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drag &amp;amp; drop&lt;/strong&gt;, &lt;strong&gt;Open With&lt;/strong&gt;, or click &amp;quot;Open File&amp;quot; — multiple ways to load media.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-converts&lt;/strong&gt; to &lt;code&gt;.mp3&lt;/code&gt; if needed using FFmpeg.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dropdown menus&lt;/strong&gt; to pick model (e.g. &lt;code&gt;tiny&lt;/code&gt;, &lt;code&gt;medium-en&lt;/code&gt;, &lt;code&gt;large-v3&lt;/code&gt;) and language (e.g. &lt;code&gt;en&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Textbox for extra Whisper arguments&lt;/strong&gt; if you want advanced control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-downloads missing models&lt;/strong&gt; from Hugging Face.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time console output&lt;/strong&gt; while transcription is running.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transcript opens in Notepad&lt;/strong&gt; when finished.&lt;/li&gt; &lt;li&gt;Choose between &lt;code&gt;.txt&lt;/code&gt; &lt;strong&gt;and/or&lt;/strong&gt; &lt;code&gt;.srt&lt;/code&gt; &lt;strong&gt;output&lt;/strong&gt; (with timestamps!).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Windows 10 or later&lt;/li&gt; &lt;li&gt;AMD, Intel, or NVIDIA Graphics Card with Vulkan support (almost all modern GPUs including Integrated Graphics)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Download the latest installer from the Releases page.&lt;/li&gt; &lt;li&gt;Run the app — that’s it.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Credits&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;whisper.cpp&lt;/code&gt; by Georgi Gerganov&lt;/li&gt; &lt;li&gt;FFmpeg builds by &lt;a href="http://Gyan.dev"&gt;Gyan.dev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Built with Qt&lt;/li&gt; &lt;li&gt;Installer created with Inno Setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’ve ever wanted a &lt;strong&gt;simple, native app&lt;/strong&gt; for Whisper that runs fast and handles everything for you — give this a try.&lt;/p&gt; &lt;p&gt;Let me know what you think, I’m actively improving it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/blob/main/preview.png"&gt;preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xc46/a_fast_native_desktop_ui_for_transcribing_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T22:27:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k13tz6</id>
    <title>Fun fact: Google also has a project called Codex</title>
    <updated>2025-04-17T03:57:41+00:00</updated>
    <author>
      <name>/u/Cheap_Ship6400</name>
      <uri>https://old.reddit.com/user/Cheap_Ship6400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/google/codex"&gt;https://github.com/google/codex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it's for dnn-based data compression&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Ship6400"&gt; /u/Cheap_Ship6400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k13tz6/fun_fact_google_also_has_a_project_called_codex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k13tz6/fun_fact_google_also_has_a_project_called_codex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k13tz6/fun_fact_google_also_has_a_project_called_codex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T03:57:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k142n9</id>
    <title>What is the latest gossip on a Qwen 3 release date?</title>
    <updated>2025-04-17T04:11:19+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am suffering from the wait.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k142n9/what_is_the_latest_gossip_on_a_qwen_3_release_date/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k142n9/what_is_the_latest_gossip_on_a_qwen_3_release_date/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k142n9/what_is_the_latest_gossip_on_a_qwen_3_release_date/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T04:11:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k13tui</id>
    <title>[2504.12285] BitNet b1.58 2B4T Technical Report</title>
    <updated>2025-04-17T03:57:28+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Abstract&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3&gt;Notables:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;They used activation functions that are compatible with activation sparsity, which means a more efficient version can be created with this base in the future.&lt;/li&gt; &lt;li&gt;trained on publicly available data (Not Phi's proprietary dataset.)&lt;/li&gt; &lt;li&gt;GPU implementation: (Ladder/Bitblas) &lt;a href="https://github.com/microsoft/BitBLAS"&gt;https://github.com/microsoft/BitBLAS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;BitNet b1.58 2B4T employs squared ReLU. This choice is motivated by its potential to improve model sparsity and computational characteristics within the 1-bit context: &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The pre-training corpus comprised a mixture of publicly available text and code datasets, including large web crawls like DCLM (Li et al., 2024b,) and educational web pages like FineWeb-EDU (Penedo et al.,, 2024). To enhance mathematical reasoning abilities, we also incorporated synthetically generated mathematical data. The data presentation strategy aligned with the two-stage training: the bulk of general web data was processed during Stage 1, while higher-quality curated datasets were emphasized during the Stage 2 cooldown phase, coinciding with the reduced learning rate&lt;/p&gt; &lt;p&gt;The SFT phase utilized a diverse collection of publicly available instruction-following and conversational datasets. These included, but were not limited to, WildChat (Zhao et al.,, 2024), LMSYS-Chat-1M (Zheng et al.,, 2024), WizardLM Evol-Instruct (Xu et al., 2024a,), and SlimOrca &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.12285"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k13tui/250412285_bitnet_b158_2b4t_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k13tui/250412285_bitnet_b158_2b4t_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T03:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k14pyg</id>
    <title>Back to Local: What’s your experience with Llama 4</title>
    <updated>2025-04-17T04:51:00+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of news and discussion recently about closed-source API-only models recently (which is understandable), but let’s pivot back to local models.&lt;/p&gt; &lt;p&gt;What’s your recent experience with Llama 4? I actually find it quite great, better than 3.3 70B, and it’s really optimized for CPU inference. Also if it’s fits in the unified memory of your Mac it just speeds along!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14pyg/back_to_local_whats_your_experience_with_llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14pyg/back_to_local_whats_your_experience_with_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k14pyg/back_to_local_whats_your_experience_with_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T04:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0xszu</id>
    <title>OpenAI in talks to buy Windsurf for about $3 billion, Bloomberg News reports</title>
    <updated>2025-04-16T22:48:42+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xszu/openai_in_talks_to_buy_windsurf_for_about_3/"&gt; &lt;img alt="OpenAI in talks to buy Windsurf for about $3 billion, Bloomberg News reports" src="https://external-preview.redd.it/5O6asQHry3_JNEHfIMxM55mrnO4LZZq2kY7qqwHxrAA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b23e82dd4151b83d586d57c3f35457898dba501" title="OpenAI in talks to buy Windsurf for about $3 billion, Bloomberg News reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/technology/artificial-intelligence/openai-talks-buy-windsurf-about-3-billion-bloomberg-news-reports-2025-04-16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xszu/openai_in_talks_to_buy_windsurf_for_about_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0xszu/openai_in_talks_to_buy_windsurf_for_about_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T22:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0pnvl</id>
    <title>OpenAI Introducing OpenAI o3 and o4-mini</title>
    <updated>2025-04-16T17:08:52+00:00</updated>
    <author>
      <name>/u/stocksavvy_ai</name>
      <uri>https://old.reddit.com/user/stocksavvy_ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, OpenAI releasing OpenAI &lt;strong&gt;o3&lt;/strong&gt; and &lt;strong&gt;o4-mini,&lt;/strong&gt; the latest o-series of models trained to think for longer before responding. These are the smartest models they've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stocksavvy_ai"&gt; /u/stocksavvy_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/introducing-o3-and-o4-mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T17:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k15ah1</id>
    <title>We fought SB-1047; the same is happening in New York and now is a good time to voice opposition to the RAISE Act</title>
    <updated>2025-04-17T05:28:19+00:00</updated>
    <author>
      <name>/u/Suitable-Listen355</name>
      <uri>https://old.reddit.com/user/Suitable-Listen355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been lurking &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; for a while, and remember how the community reacted when lawmakers in California attempted to pass SB-1047, an anti-open weights piece of legislation that would punish derivative models and make the creators of open-weights models liable for so much that open-weights models would be legally barely viable. Some links to posts from the anti-SB-1047 era: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1es87fm/right_now_is_a_good_time_for_californians_to_tell/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1es87fm/right_now_is_a_good_time_for_californians_to_tell/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1cxqtrv/california_senate_passes_sb1047/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1cxqtrv/california_senate_passes_sb1047/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fkfkth/quick_reminder_sb_1047_hasnt_been_signed_into_law/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1fkfkth/quick_reminder_sb_1047_hasnt_been_signed_into_law/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thankfully, Governor Gavin Newsom vetoed the bill, and the opposition of the open-source community was heard. However, there is now a similar threat in the state of New York: the RAISE Act (A.6453). &lt;/p&gt; &lt;p&gt;The RAISE Act, like SB-1047, imposes state laws that affect models everywhere. Although it does not go as far as the SB-1047, it still should be in principle opposed that a single jurisdiction can be disruptive in a general model release. Outside of that initial consideration, I have listed things I find particularly problematic with the act and its impact on AI development: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;The act imposes a rule if a model is trained with over $5m of resources, a third-party auditor must be hired to audit its compliance. &lt;/li&gt; &lt;li&gt;In addition, even before you cross the $5m threshold, if you &lt;strong&gt;plan&lt;/strong&gt; to train a model that would qualify you as a large developer, you must implement and publish a safety protocol (minus some detail requirements) and send a redacted copy to the AG before training begins. &lt;/li&gt; &lt;li&gt;You may &lt;strong&gt;not&lt;/strong&gt; deploy a frontier model if it poses an “unreasonable risk” of causing critical harm (e.g. planning a mass attack or enabling a bioweapon). &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;First off, it is not at all clear what constitutes an &amp;quot;unreasonable risk&amp;quot;. Something like planning a mass attack is probably possible with prompt engineering on current frontier models with search capabilities already, and the potential liability implications for this &amp;quot;unreasonable risk&amp;quot; provision can stifle development. The issues I have with third-party audits is that many of these audit groups are themselves invested in the &amp;quot;AI safety&amp;quot; bubble. Rules that exist even before one starts training are also a dangerous precedent and set the precedent to far more regulatory hurdles in the future. Even if this act is not as egregious as SB-1047, it is of my opinion that this is a dangerous precedent to be passed into state law and hopefully federal legislation that is pro-development and preempts state laws like these is passed. (Although that's just one of my pipe dreams, the chance of such federal legislation is probably low, considering the Trump admin is thinking of banning DeepSeek right now). &lt;/p&gt; &lt;p&gt;The representative behind SB-1047 is Alex Bores of the 73rd District of New York and if you are in New York, I encourage you to contact your local representative in the New York State Assembly to oppose it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Listen355"&gt; /u/Suitable-Listen355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k15ah1/we_fought_sb1047_the_same_is_happening_in_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k15ah1/we_fought_sb1047_the_same_is_happening_in_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k15ah1/we_fought_sb1047_the_same_is_happening_in_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T05:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0tkca</id>
    <title>Massive 5000 tokens per second on 2x3090</title>
    <updated>2025-04-16T19:47:07+00:00</updated>
    <author>
      <name>/u/woozzz123</name>
      <uri>https://old.reddit.com/user/woozzz123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt; &lt;img alt="Massive 5000 tokens per second on 2x3090" src="https://b.thumbs.redditmedia.com/Kqc4r4j1pvS-lOt8Ugi0fd-cS_ZlQgpSRkB-O5FUESc.jpg" title="Massive 5000 tokens per second on 2x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For research purposes I need to process huge amounts of data as quickly as possible.&lt;/p&gt; &lt;h1&gt;The model&lt;/h1&gt; &lt;p&gt;Did testing across models, and it came to be that Qwen2.5-7B is &amp;quot;just good enough&amp;quot;. Bigger ones are better but slower. The two tests which were indicative were MMLU-pro (language understanding) and BBH (a bunch of tasks &lt;a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark%5C_tasks/keywords%5C_to%5C_tasks.md#summary-table"&gt;https://github.com/google/BIG-bench/blob/main/bigbench/benchmark\_tasks/keywords\_to\_tasks.md#summary-table&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mcb690qly8ve1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f"&gt;https://preview.redd.it/mcb690qly8ve1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Intuitively, you can see that the jumps in performance gets smaller and smaller the bigger the models you pick.&lt;/p&gt; &lt;h1&gt;Processing engine&lt;/h1&gt; &lt;p&gt;There will be lots of small queries, so vLLM makes sense, but I used Aphrodite engine due to tests with speculative decoding.&lt;/p&gt; &lt;h1&gt;Model Quantization&lt;/h1&gt; &lt;p&gt;Now, with 2x 3090's theres plenty of VRAM, so there shouldn't be any issue running it, however I was thinking of perhaps a larger KV cache or whatever might increase processing speed. It indeed did, on a test dataset of randomly selected documents, these were the results;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Prompt throughput t/s&lt;/th&gt; &lt;th align="left"&gt;Generation throughput t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Unquantized&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AWQ / GPTQ&lt;/td&gt; &lt;td align="left"&gt;1300&lt;/td&gt; &lt;td align="left"&gt;400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;W4A16-G128 / W8A8&lt;/td&gt; &lt;td align="left"&gt;2000&lt;/td&gt; &lt;td align="left"&gt;500&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Performance of AWQ / GTPQ and W4A16-G128 was very similar in terms of MMLU &amp;amp; BBH, however W8A8 was clearly superior (using llm_eval);&lt;/p&gt; &lt;p&gt;&lt;code&gt;lm_eval --model vllm \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model_args YOUR_MODEL,add_bos_token=true \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--tasks TASKHERE \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--num_fewshot 3 for BBH, 5 for MMLU_PRO\&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--batch_size 'auto'&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So, I continued with the W8A8&lt;/p&gt; &lt;h1&gt;Speculative Decoding&lt;/h1&gt; &lt;p&gt;Unfortunately, 7B has a different tokenizer than the smaller models, so I cannot use 0.5, 1.5 or 3B as draft model. Aphrodite supports speculative decoding through ngram, but this rougly halves performance &lt;a href="https://aphrodite.pygmalion.chat/spec-decoding/ngram/"&gt;https://aphrodite.pygmalion.chat/spec-decoding/ngram/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Final optimizations&lt;/h1&gt; &lt;p&gt;Here's the command to run an OpenAI REST API:&lt;/p&gt; &lt;p&gt;&lt;code&gt;aphrodite run ./Qwen2.5-7B-Instruct_W8A8_custom --port 8000 -tp 2 --max_seq_len 8192 --max_model_len 8192 --max_num_seqs 32 --tensor-parallel-size 2 --gpu-memory-utilization 0.75&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Note the parameter &amp;quot;&lt;code&gt;max_num_seqs&lt;/code&gt;&amp;quot; , this is the number of concurrent requests in a batch, how many requests the GPU processes at the same time. I did some benchmarking on my test set and got this results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;max_num_seqs&lt;/th&gt; &lt;th align="left"&gt;ingest t/s&lt;/th&gt; &lt;th align="left"&gt;generate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;3000&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;2500&lt;/td&gt; &lt;td align="left"&gt;750&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They fluctuate so these are a ballpark, but the difference is clear if you run it. I chose the 32 one. Running things then in &amp;quot;production&amp;quot;:&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pe7vam5q29ve1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91cd4c10ab713481d093c43cd83ad4d160be6fa5"&gt;https://preview.redd.it/pe7vam5q29ve1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91cd4c10ab713481d093c43cd83ad4d160be6fa5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4500 t/s ingesting&lt;/p&gt; &lt;p&gt;825 t/s generation&lt;/p&gt; &lt;p&gt;with +- 5k tokens context.&lt;/p&gt; &lt;p&gt;I think even higher numbers are possible, perhaps quantized KV, better grouping of documents so KV cache gets used more? Smaller context size. However, this speed is sufficient for me, so no more finetuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woozzz123"&gt; /u/woozzz123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T19:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0mesv</id>
    <title>IBM Granite 3.3 Models</title>
    <updated>2025-04-16T14:54:48+00:00</updated>
    <author>
      <name>/u/suitable_cowboy</name>
      <uri>https://old.reddit.com/user/suitable_cowboy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"&gt; &lt;img alt="IBM Granite 3.3 Models" src="https://external-preview.redd.it/Di-LJPiKH5-hlOr8JOzFQOIzNY3wtbEXkzZj38FaUy4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea64ef12850fb58da12ba852867d09166207a09" title="IBM Granite 3.3 Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras"&gt;Announcement Post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b"&gt;3.3 Speech Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suitable_cowboy"&gt; /u/suitable_cowboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T14:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0z1bk</id>
    <title>Forget DeepSeek R2 or Qwen 3, Llama 2 is clearly our local savior.</title>
    <updated>2025-04-16T23:47:00+00:00</updated>
    <author>
      <name>/u/Cameo10</name>
      <uri>https://old.reddit.com/user/Cameo10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0z1bk/forget_deepseek_r2_or_qwen_3_llama_2_is_clearly/"&gt; &lt;img alt="Forget DeepSeek R2 or Qwen 3, Llama 2 is clearly our local savior." src="https://preview.redd.it/2668luheaave1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f0758fa3651c39cecde93e9b2a9cb77dacb62f" title="Forget DeepSeek R2 or Qwen 3, Llama 2 is clearly our local savior." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No, this is not edited and it is from Artificial Analysis &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cameo10"&gt; /u/Cameo10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2668luheaave1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0z1bk/forget_deepseek_r2_or_qwen_3_llama_2_is_clearly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0z1bk/forget_deepseek_r2_or_qwen_3_llama_2_is_clearly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T23:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0u8ew</id>
    <title>Somebody needs to tell Nvidia to calm down with these new model names.</title>
    <updated>2025-04-16T20:14:27+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"&gt; &lt;img alt="Somebody needs to tell Nvidia to calm down with these new model names." src="https://preview.redd.it/hl0xrywo89ve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba3293f40fb091a49f266882c48318181875c821" title="Somebody needs to tell Nvidia to calm down with these new model names." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hl0xrywo89ve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-16T20:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k14k6a</id>
    <title>JetBrains AI now has local llms integration and is free with unlimited code completions</title>
    <updated>2025-04-17T04:40:53+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"&gt; &lt;img alt="JetBrains AI now has local llms integration and is free with unlimited code completions" src="https://b.thumbs.redditmedia.com/2k-Ribx9nGO9Xazuaac7EOKvTqA2Do41B3wRLWXenuQ.jpg" title="JetBrains AI now has local llms integration and is free with unlimited code completions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.jetbrains.com/rider/whatsnew/"&gt;What's New in Rider&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Rider goes AI&lt;/p&gt; &lt;p&gt;JetBrains AI Assistant has received a major upgrade, making AI-powered development more accessible and efficient. With this release, &lt;strong&gt;AI features are now free in JetBrains IDEs&lt;/strong&gt;, including unlimited code completion, support for local models, and credit-based access to cloud-based features. &lt;a href="https://www.jetbrains.com/ai-ides/buy/"&gt;A new subscription system&lt;/a&gt; makes it easy to scale up with AI Pro and AI Ultimate tiers.&lt;/p&gt; &lt;p&gt;This release introduces major enhancements to boost productivity and reduce repetitive work, including smarter code completion, support for new cloud models like GPT-4.1 (сoming soon), Claude 3.7, and Gemini 2.0, advanced RAG-based context awareness, and a new Edit mode for multi-file edits directly from chat&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k14k6a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T04:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k10yak</id>
    <title>Honest thoughts on the OpenAI release</title>
    <updated>2025-04-17T01:22:49+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"&gt; &lt;img alt="Honest thoughts on the OpenAI release" src="https://preview.redd.it/inywb6g3rave1.gif?width=216&amp;amp;crop=smart&amp;amp;s=01f6b7c9517ca3a073290dfd9cde5f09acbb00d9" title="Honest thoughts on the OpenAI release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay bring it on&lt;/p&gt; &lt;p&gt;o3 and o4-mini:&lt;br /&gt; - We all know full well from many open source research (like DeepseekMath and Deepseek-R1) that if you keep scaling up the RL, it will be better -&amp;gt; OpenAI just scale it up and sell an APIs, there are a few different but so how much better can it get?&lt;br /&gt; - More compute, more performance, well, well, &lt;strong&gt;more tokens?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;codex?&lt;br /&gt; - Github copilot used to be codex&lt;br /&gt; - Acting like there are not like a &lt;strong&gt;tons of things out there: Cline, RooCode, Cursor, Windsurf,...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Worst of all they &lt;strong&gt;are hyping up the community, the open source, local, community, for their commercial interest,&lt;/strong&gt; throwing out vague information about Open and Mug of OpenAI on ollama account etc...&lt;/p&gt; &lt;p&gt;Talking about 4.1 ? coding halulu, delulu yes benchmark is good.&lt;/p&gt; &lt;p&gt;Yeah that's my rant, downvote me if you want. I have been in this thing since 2023, and I find it more and more annoying following these news. It's misleading, it's boring, it has nothing for us to learn about, it has nothing for us to do except for paying for their APIs and maybe contributing to their open source client, which they are doing because they know there is no point just close source software. &lt;/p&gt; &lt;p&gt;This is pointless and sad development of the AI community and AI companies in general, we could be so much better and so much more, accelerating so quickly, yes we are here, paying for one more token and learn nothing &lt;strong&gt;(if you can call scaling RL which we all know is a LEARNING AT ALL).&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/inywb6g3rave1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T01:22:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k12i6l</id>
    <title>Trump administration reportedly considers a US DeepSeek ban</title>
    <updated>2025-04-17T02:44:14+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"&gt; &lt;img alt="Trump administration reportedly considers a US DeepSeek ban" src="https://preview.redd.it/80uc8c906bve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=027d4187d5af43a99f8442134a91d40393c2dc07" title="Trump administration reportedly considers a US DeepSeek ban" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/04/16/trump-administration-reportedly-considers-a-us-deepseek-ban/"&gt;https://techcrunch.com/2025/04/16/trump-administration-reportedly-considers-a-us-deepseek-ban/&lt;/a&gt;&lt;br /&gt; Washington Takes Aim at DeepSeek and Its American Chip Supplier, Nvidia: &lt;a href="https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html"&gt;https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80uc8c906bve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T02:44:14+00:00</published>
  </entry>
</feed>
