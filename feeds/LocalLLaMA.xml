<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-13T15:24:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kkq8q8</id>
    <title>Microsoft Researchers Introduce ARTIST</title>
    <updated>2025-05-12T11:34:55+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt; &lt;img alt="Microsoft Researchers Introduce ARTIST" src="https://preview.redd.it/90acs85p7c0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c24f942d10fedd4f933d6f856346cbfea33433" title="Microsoft Researchers Introduce ARTIST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft Research introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework that combines agentic reasoning, reinforcement learning, and dynamic tool use to enhance LLMs. ARTIST enables models to autonomously decide when, how, and which tools to use during multi-step reasoning, learning robust strategies without step-level supervision. The model improves reasoning and interaction with external environments through integrated tool queries and outputs. Evaluated on challenging math and function-calling benchmarks, ARTIST outperforms top models like GPT-4o, achieving up to 22% gains. It demonstrates emergent agentic behaviors, setting a new standard in generalizable and interpretable problem-solving. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/"&gt;https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: &lt;a href="https://arxiv.org/abs/2505.01441"&gt;https://arxiv.org/abs/2505.01441&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90acs85p7c0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kln4tt</id>
    <title>Has anyone gotten featherless-ai’s Qwerky-QwQ-32B running locally?</title>
    <updated>2025-05-13T14:19:33+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kln4tt/has_anyone_gotten_featherlessais_qwerkyqwq32b/"&gt; &lt;img alt="Has anyone gotten featherless-ai’s Qwerky-QwQ-32B running locally?" src="https://external-preview.redd.it/pu7Btbv0Xni8S0Ui12FJRAYNOBsuAfSaA3OmHuyOI4E.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e3c02cfb091d236b3906ddd94f00f8923794570" title="Has anyone gotten featherless-ai’s Qwerky-QwQ-32B running locally?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They claim “We now have a model far surpassing GPT-3.5 turbo, without QKV attention.”… makes me want to try it. &lt;/p&gt; &lt;p&gt;What are your thoughts on this architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://substack.recursal.ai/p/qwerky-72b-and-32b-training-large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kln4tt/has_anyone_gotten_featherlessais_qwerkyqwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kln4tt/has_anyone_gotten_featherlessais_qwerkyqwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T14:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl2rol</id>
    <title>AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer</title>
    <updated>2025-05-12T20:16:23+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt; &lt;img alt="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" src="https://external-preview.redd.it/_iSjbA70JS45LJP3sdoibj1AlqIDoIJY2LS6cGv_uYs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0de3aa9807b6219594e46420823c44e09e2e57dc" title="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I'm on the team building &lt;strong&gt;AG-UI&lt;/strong&gt;, an open-source, self-hostable, lightweight, event-based protocol for facilitating rich, real-time, agent-user interactivity.&lt;/p&gt; &lt;p&gt;Today, we've released this protocol, and I believe this could help solve a major pain point for those of us building with AI agents.&lt;/p&gt; &lt;h1&gt;The Problem AG-UI Solves&lt;/h1&gt; &lt;p&gt;Most agents today have been backend automators: data migrations, form-fillers, summarizers. They work behind the scenes and are great for many use cases.&lt;/p&gt; &lt;p&gt;But interactive agents, which work alongside users (like Cursor &amp;amp; Windsurf as opposed to Devin), can unlock massive new use-cases for AI agents and bring them to the apps we use every day.&lt;/p&gt; &lt;p&gt;AG-UI aims to make these easy to build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A smooth user-interactive agent requires:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time updates&lt;/li&gt; &lt;li&gt;Tool orchestration&lt;/li&gt; &lt;li&gt;Shared mutable state&lt;/li&gt; &lt;li&gt;Security boundaries&lt;/li&gt; &lt;li&gt;Frontend synchronization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AG-UI unlocks all of this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593"&gt;https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's all built on event-streaming (&lt;strong&gt;HTTP/SSE/webhooks&lt;/strong&gt;) – creating a seamless connection between any AI backend (OpenAI, CrewAI, LangGraph, Mastra, your custom stack) and your frontend.&lt;/p&gt; &lt;p&gt;The magic happens in 5 simple steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Your app sends a request to the agent&lt;/li&gt; &lt;li&gt;Then opens a single event stream connection&lt;/li&gt; &lt;li&gt;The agent sends lightweight event packets as it works&lt;/li&gt; &lt;li&gt;Each event flows to the Frontend in real-time&lt;/li&gt; &lt;li&gt;Your app updates instantly with each new development&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is how we finally break the barrier between AI backends and user–facing applications, enabling agents that collaborate alongside users rather than just performing isolated tasks in the background.&lt;/p&gt; &lt;h1&gt;Who It's For&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Building agents? AG-UI makes them interactive with minimal code&lt;/li&gt; &lt;li&gt;Using frameworks like LangGraph, CrewAI, Mastra, AG2? We're already compatible&lt;/li&gt; &lt;li&gt;Rolling your own solution? AG-UI works without any framework&lt;/li&gt; &lt;li&gt;Building a client? Target the AG-UI protocol for consistent behavior across agents&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Check It Out&lt;/h1&gt; &lt;p&gt;The protocol is open and pretty simple, just 16 standard events. We've got examples and docs at &lt;a href="http://docs.ag-ui.com/"&gt;docs.ag-ui.com&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;p&gt;Check out the AG-UI Protocol GitHub: &lt;a href="https://github.com/ag-ui-protocol/ag-ui"&gt;https://github.com/ag-ui-protocol/ag-ui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release announcement: &lt;a href="https://x.com/CopilotKit/status/1921940427944702001"&gt;https://x.com/CopilotKit/status/1921940427944702001&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pre-release webinar with Mastra: &lt;a href="https://www.youtube.com/watch?v=rnZfEbC-ATE"&gt;https://www.youtube.com/watch?v=rnZfEbC-ATE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What challenges have you faced while building with agents and adding the user-interactive layer?&lt;br /&gt; Would love your thoughts, comments, or questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1klayhz</id>
    <title>What's the best medical model currently?</title>
    <updated>2025-05-13T02:27:43+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had been using llms only for coding and math but I realised that medicine should in theory be easier for an llm. Is there a good benchmark and what is the current best model for medical advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T02:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl7j1z</id>
    <title>New Model: Llama 3.3 70B Magnum Nexus</title>
    <updated>2025-05-12T23:38:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"&gt; &lt;img alt="New Model: Llama 3.3 70B Magnum Nexus" src="https://external-preview.redd.it/2S7d4MDGhIjkJfj7T1VTxqrZdba8wRVPZ7_koopiHT8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82fade8a8551d752841176f9d256122c646f7dcd" title="New Model: Llama 3.3 70B Magnum Nexus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post from &lt;a href="/u/EntropicDisorder"&gt;u/EntropicDisorder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Hey folks! It's Doctor Shotgun here, purveyor of LLM finetunes. You might have seen some of my work on HuggingFace in the past, either independently or as part of Anthracite.&lt;/p&gt; &lt;p&gt;I'm here with yet another creative writing focused finetune. Yes, I know. Llama 3.3 is so last generation in the realm of LLMs, but it's not like we've been getting anything new in the semi-chonker size range recently; no Llama 4 70B, no Qwen 3 72B, and no open-weights Mistral Medium 3.&lt;/p&gt; &lt;p&gt;Using the model stock method, I merged a few separate rsLoRA finetunes I did on L3.3 70B with some variations on the data and hparams, and the result seems overall a bit more stable in terms of handling different prompt formats (with or without prepended character names, with or without prefills).&lt;/p&gt; &lt;p&gt;I've included some SillyTavern presets for those who use that (although feel free to try your own templates too and let me know if something works better!).&lt;/p&gt; &lt;p&gt;Also, I'd like to give an honorable mention to the Doctor-Shotgun/L3.3-70B-Magnum-v5-SFT-Alpha model used as the base for this merge. It's what I'd call the &amp;quot;mad genius&amp;quot; variant. It was my first attempt at using smarter prompt masking, and it has its flaws but boy can it write when it's in its element. I made it public on my HF a while back but never really announced it, so I figured I'd mention it here.&amp;quot;&lt;/p&gt; &lt;p&gt;You can ask him any question!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-Nexus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T23:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kky1sg</id>
    <title>Meta has released an 8B BLT model</title>
    <updated>2025-05-12T17:12:33+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&amp;amp;utm_medium=organic%20social&amp;amp;utm_content=video&amp;amp;utm_campaign=fair"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kll7h3</id>
    <title>Possible to run 27B / 32B models on 20GB VRAM?</title>
    <updated>2025-05-13T12:54:45+00:00</updated>
    <author>
      <name>/u/PlanetMercurial</name>
      <uri>https://old.reddit.com/user/PlanetMercurial</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have RTX3060 12GB and to speed up inference I added another RTX4000 8GB for a total of 20GB VRAM.&lt;br /&gt; I was hoping to run 27B and 32B models that are Q4_K_M quantized (Thus far i have only run GGUF not EXL2).&lt;/p&gt; &lt;p&gt;PCIE Interface info:&lt;br /&gt; RTX3060 - 16x PCIE4.0&lt;br /&gt; RTX4000 - 4x PCIE4.0&lt;/p&gt; &lt;p&gt;From my own rough calculations the reasoning was that 32 Billion parameters means 32GB ram (for a 8bit per weight quant) but if i used 4bit quant it should be half of that so 16GB VRAM and I thought I would use the extra 4GB VRAM for context etc.&lt;/p&gt; &lt;p&gt;But in reality when i tried with Koboldcpp I couldn't get the model to fit into the 20GB VRAM, I reduced the layers to 45 to make it fit and even then i get a measly &lt;code&gt;3 tokens /sec&lt;/code&gt; , I used the following command&lt;/p&gt; &lt;p&gt;&lt;code&gt;.\koboldcpp.exe --usecublas --tensor_split 15 12 --contextsize 4096 --skiplauncher --quiet --flashattention --gpulayers 45 --model C:\DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm probably setting the `tensor_split` wrongly without understanding.&lt;br /&gt; Also gpu layers i have currently set to 45 but the model has a total of 65 layers.&lt;/p&gt; &lt;p&gt;So is this possible at all ? Can i get a token rate of 10 to 20 tokens / sec.&lt;br /&gt; I have read on other posts to move to GPTQ would that model quant use lower VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlanetMercurial"&gt; /u/PlanetMercurial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kll7h3/possible_to_run_27b_32b_models_on_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kll7h3/possible_to_run_27b_32b_models_on_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kll7h3/possible_to_run_27b_32b_models_on_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3rfa</id>
    <title>In your experience and opinion, is Qwen3 32B better than QwQ 32B?</title>
    <updated>2025-05-12T20:55:57+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title, basically.&lt;/p&gt; &lt;p&gt;If you have tried both and used them I would really like to know your answer.&lt;/p&gt; &lt;p&gt;From what I’ve seen Qwen3 32B gives answers with less thinking tokens so I don’t know how that affects performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1klfcu0</id>
    <title>Is anyone actually using local models to code in their regular setups like roo/cline?</title>
    <updated>2025-05-13T06:46:39+00:00</updated>
    <author>
      <name>/u/kms_dev</name>
      <uri>https://old.reddit.com/user/kms_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I've tried, models from 30b onwards start to be useful for local coding. With a 2x 3090 setup, I can squeeze in upto ~100k tokens and those models also go bad beyond 32k tokens occasionally missing the diff format or even forgetting some of the instructions.&lt;/p&gt; &lt;p&gt;So I checked which is cheaper/faster to use with cline, qwen3-32b 8-bit quant vs Gemini 2.5 flash.&lt;/p&gt; &lt;p&gt;Local setup cost per 1M output tokens: &lt;/p&gt; &lt;p&gt;I get about 30-40 tok/s on my 2x3090 setup consuming 700w. So to generate 1M tokens, energy used: 1000000/33/3600×0.7 = 5.9kwh Cost of electricity where I live: $0.18/kwh Total cost per 1M output tokens: $1.06&lt;/p&gt; &lt;p&gt;So local model cost: ~$1/M tokens Gemini 2.5 flash cost: $0.6/M tokens&lt;/p&gt; &lt;p&gt;Is my setup inefficient? Or the cloud models to good?&lt;/p&gt; &lt;p&gt;Is Qwen3 32B better than Gemini 2.5 flash in real world usage?&lt;/p&gt; &lt;p&gt;Cost wise, cloud models are winning if one doesn't mind the privacy concerns.&lt;/p&gt; &lt;p&gt;Is anyone still choosing to use local models for coding despite the increased costs? If so, which models are you using and how?&lt;/p&gt; &lt;p&gt;Ps: I really want to use local models for my coding purposes and couldn't get an effective workflow in place for coding/software development.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kms_dev"&gt; /u/kms_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T06:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1klagiq</id>
    <title>FastVLM: Fast Vision Language Model by Apple</title>
    <updated>2025-05-13T02:02:23+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"&gt; &lt;img alt="FastVLM: Fast Vision Language Model by Apple" src="https://external-preview.redd.it/5nJp9i1DhXBKR2Li4PwUtZTAJ-O6pX5Feq_-crW2MLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eeb74b79dbc85c33f416efb8f75cfa8942c4c025" title="FastVLM: Fast Vision Language Model by Apple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/apple/ml-fastvlm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T02:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1klo80p</id>
    <title>Handling Unhealthy GPU Nodes in EKS Cluster</title>
    <updated>2025-05-13T15:03:16+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klo80p/handling_unhealthy_gpu_nodes_in_eks_cluster/"&gt; &lt;img alt="Handling Unhealthy GPU Nodes in EKS Cluster" src="https://external-preview.redd.it/k14nAjt7tuKIBase6_EFPnPrj_2s-QiEcd1tjSPmar4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73c4c7cdd4d1a9d5c1df94742a1f1355e861caeb" title="Handling Unhealthy GPU Nodes in EKS Cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;If you’re running &lt;strong&gt;GPU workloads on an EKS cluster&lt;/strong&gt;, your nodes can occasionally enter &lt;code&gt;NotReady&lt;/code&gt; states due to issues like network outages, unresponsive kubelets, running privileged commands like &lt;code&gt;nvidia-smi&lt;/code&gt;, or other unknown problems with your container code. These issues can become very expensive, leading to financial losses, production downtime, and reduced user trust.&lt;/p&gt; &lt;p&gt;We recently published a blog about handling unhealthy nodes in EKS clusters using three approaches:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Using a metric-based CloudWatch alarm to send an email notification.&lt;/li&gt; &lt;li&gt;Using a metric-based alarm to trigger an AWS Lambda for automated remediation.&lt;/li&gt; &lt;li&gt;Relying on Karpenter’s Node Auto Repair feature for automated in-cluster healing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below is a table that gives a quick summary of the pros and cons of each method. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hfxutiiadk0f1.png?width=719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b3bdcd9a65b1a8ead3dd45a0230dd7fa5cc0826"&gt;https://preview.redd.it/hfxutiiadk0f1.png?width=719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b3bdcd9a65b1a8ead3dd45a0230dd7fa5cc0826&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://tensorfuse.io/docs/blogs/handling_unhealthy_nodes_in_eks"&gt;Read the blog for detailed explanations along with implementation code&lt;/a&gt;. Let us know your feedback in the thread. Hope this helps you save on your cloud bills!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klo80p/handling_unhealthy_gpu_nodes_in_eks_cluster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klo80p/handling_unhealthy_gpu_nodes_in_eks_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klo80p/handling_unhealthy_gpu_nodes_in_eks_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T15:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1klf6n3</id>
    <title>New tiny model by AllenAI: OLMo-2-1B</title>
    <updated>2025-05-13T06:35:19+00:00</updated>
    <author>
      <name>/u/CattailRed</name>
      <uri>https://old.reddit.com/user/CattailRed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strange that nobody mention &lt;a href="https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct"&gt;OLMo-2-0425-1B-Instruct&lt;/a&gt; yet. Trying it out as a potential candidate of an LLM to live in my tablet. So far I've tested only a little bit and I'm not sure if I shouldn't just use Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;Are there recommended inference parameters for OLMo series? I can't seem to find any.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CattailRed"&gt; /u/CattailRed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T06:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl6l7o</id>
    <title>Qwen3-2.4B-A0.6B MoE</title>
    <updated>2025-05-12T22:55:22+00:00</updated>
    <author>
      <name>/u/suayptalha</name>
      <uri>https://old.reddit.com/user/suayptalha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve released &lt;strong&gt;Arcana&lt;/strong&gt; &lt;strong&gt;Qwen3 2.4B A0.6B&lt;/strong&gt;, a &lt;strong&gt;Mixture of Experts (MoE)&lt;/strong&gt; model with &lt;strong&gt;2.4B parameters&lt;/strong&gt;, optimized for &lt;strong&gt;code&lt;/strong&gt;, &lt;strong&gt;math&lt;/strong&gt;, &lt;strong&gt;medical&lt;/strong&gt; and &lt;strong&gt;instruction following&lt;/strong&gt; tasks. It includes 4 experts (each with 0.6B parameters) for more accurate results and better efficiency.&lt;/p&gt; &lt;p&gt;Model Link: &lt;a href="https://huggingface.co/suayptalha/Arcana-Qwen3-2.4B-A0.6B"&gt;https://huggingface.co/suayptalha/Arcana-Qwen3-2.4B-A0.6B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suayptalha"&gt; /u/suayptalha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T22:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl9qdy</id>
    <title>Why hasn't the new version of each AI chatbot been successful?</title>
    <updated>2025-05-13T01:26:13+00:00</updated>
    <author>
      <name>/u/gutierrezz36</name>
      <uri>https://old.reddit.com/user/gutierrezz36</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ChatGPT: Latest version of GPT4o (the one who sucks up to you) reverted Gemini: Latest version of Gemini Pro 2.5 (05-06) reverted Grok: Latest version (3.5) delayed Meta: Latest version (LLaMa 4) released but unsatisfactory and to top it off lying in benchmarks&lt;/p&gt; &lt;p&gt;What's going on here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gutierrezz36"&gt; /u/gutierrezz36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T01:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kle86h</id>
    <title>Claimify: Extracting high-quality claims from language model outputs</title>
    <updated>2025-05-13T05:33:59+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"&gt; &lt;img alt="Claimify: Extracting high-quality claims from language model outputs" src="https://external-preview.redd.it/z3fQ_LBH0JOwH6gpEM6IxNLU-jf616qbxzrliOEmY8k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c323908306a9b40cda848b5e82798de7a79478f1" title="Claimify: Extracting high-quality claims from language model outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Summary: Dasha Metropolitansky, a Research Data Scientist at Microsoft, explains the Claimify system, which performs claim extraction. She defines a claim as a simple factual statement verifiable as true or false, and extraction as the process of breaking down text into these claims. Claim extraction is crucial for evaluating long-form content generated by language models, particularly for detecting hallucinations and assessing relevance, as it makes it easier to check individual points independently. Claimify works by first breaking text down into sentences, then extracting claims from each sentence with surrounding context for accuracy. The process involves three stages: selection (filtering non-verifiable statements), disambiguation (resolving ambiguous statements using context or flagging them), and decomposition (breaking disambiguated sentences into simple claims). Examples demonstrate that Claimify extracts more comprehensive and specific factual claims compared to a baseline method, capturing details about economic hardship, inflation's impact on currency value, and specific issues like public health crises and contaminated water, thereby unlocking better evaluation capabilities for language model outputs.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog: &lt;a href="https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/"&gt;https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.10855"&gt;https://arxiv.org/abs/2502.10855&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WTs-Ipt0k-M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkrgyl</id>
    <title>Qwen releases official quantized models of Qwen3</title>
    <updated>2025-05-12T12:39:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt; &lt;img alt="Qwen releases official quantized models of Qwen3" src="https://preview.redd.it/ok2e3kp5jc0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32d02567371fef442da1e95968e95dba1cbebc18" title="Qwen releases official quantized models of Qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re officially releasing the quantized models of Qwen3 today!&lt;/p&gt; &lt;p&gt;Now you can deploy Qwen3 via Ollama, LM Studio, SGLang, and vLLM — choose from multiple formats including GGUF, AWQ, and GPTQ for easy local deployment.&lt;/p&gt; &lt;p&gt;Find all models in the Qwen3 collection on Hugging Face.&lt;/p&gt; &lt;p&gt;Hugging Face：&lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ok2e3kp5jc0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkrw0</id>
    <title>final version of Skywork-OR1 (Open Reasoner 1) series of models</title>
    <updated>2025-05-13T12:34:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-OR1-32B"&gt;https://huggingface.co/Skywork/Skywork-OR1-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-OR1-7B"&gt;https://huggingface.co/Skywork/Skywork-OR1-7B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkh4w</id>
    <title>The Hidden Algorithms Powering Your Coding Assistant - How Cursor and Windsurf Work Under the Hood</title>
    <updated>2025-05-13T12:19:30+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just published a deep dive into the algorithms powering AI coding assistants like Cursor and Windsurf. If you've ever wondered how these tools seem to magically understand your code, this one's for you.&lt;/p&gt; &lt;p&gt;In this (free) post, you'll discover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The hidden context system&lt;/strong&gt; that lets AI understand your entire codebase, not just the file you're working on&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The ReAct loop&lt;/strong&gt; that powers decision-making (hint: it's a lot like how humans approach problem-solving)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why multiple specialized models&lt;/strong&gt; work better than one giant model and how they're orchestrated behind the scenes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How real-time adaptation&lt;/strong&gt; happens when you edit code, run tests, or hit errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/diamantai/p/the-hidden-algorithms-powering-your?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;Read the full post here →&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kldquv</id>
    <title>Architecture Review of the new MoE models</title>
    <updated>2025-05-13T05:03:33+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the release of DeepSeek V3, there is a rush of new MoE models. I read their papers and looked at config.json and modeling_*.py files and summarized their data in the following table. Here are some observations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek becomes highly KV cache efficient after introduction of MLA in DeepSeek V2&lt;/li&gt; &lt;li&gt;Qwen's MoE architecture is basically the same as Mixtral but with more experts and more layers.&lt;/li&gt; &lt;li&gt;Llama-4 and DeepSeek are both MoE with shared experts. While Scout has no non-MoE (ie dense) layers, all other models have some dense layers. Maverick even has interleaved&lt;/li&gt; &lt;li&gt;Performance-wise, it seems like Qwen3-235B-A22B &amp;gt; DeepSeek-V3 &amp;gt;&amp;gt; Llama-4-Maverick accordin g to lmarena and livebench. Qwen3 seems to excel in all areas except coding compare to DSV3.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;dense layer#&lt;/th&gt; &lt;th align="left"&gt;MoE layer#&lt;/th&gt; &lt;th align="left"&gt;shared&lt;/th&gt; &lt;th align="left"&gt;active/routed&lt;/th&gt; &lt;th align="left"&gt;Active&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Active%&lt;/th&gt; &lt;th align="left"&gt;fp16 kv@128k&lt;/th&gt; &lt;th align="left"&gt;kv%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-MoE-16B&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.83B&lt;/td&gt; &lt;td align="left"&gt;16.38B&lt;/td&gt; &lt;td align="left"&gt;17.28%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;85.47%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2-Lite&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.66B&lt;/td&gt; &lt;td align="left"&gt;15.71B&lt;/td&gt; &lt;td align="left"&gt;16.93%&lt;/td&gt; &lt;td align="left"&gt;3.8GB&lt;/td&gt; &lt;td align="left"&gt;12.09%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/160&lt;/td&gt; &lt;td align="left"&gt;21.33B&lt;/td&gt; &lt;td align="left"&gt;235.74B&lt;/td&gt; &lt;td align="left"&gt;8.41%&lt;/td&gt; &lt;td align="left"&gt;8.44GB&lt;/td&gt; &lt;td align="left"&gt;1.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/256&lt;/td&gt; &lt;td align="left"&gt;37.45B&lt;/td&gt; &lt;td align="left"&gt;671.03B&lt;/td&gt; &lt;td align="left"&gt;5.58%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;3.34B&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;10.94%&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;19.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;94&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;22.14B&lt;/td&gt; &lt;td align="left"&gt;235.09B&lt;/td&gt; &lt;td align="left"&gt;9.42%&lt;/td&gt; &lt;td align="left"&gt;23.5GB&lt;/td&gt; &lt;td align="left"&gt;4.998%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout-17B-16E&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/16&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;107.77B&lt;/td&gt; &lt;td align="left"&gt;15.93%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;11.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/128&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;400.71B&lt;/td&gt; &lt;td align="left"&gt;4.28%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;2.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x7B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;12.88B&lt;/td&gt; &lt;td align="left"&gt;46.70B&lt;/td&gt; &lt;td align="left"&gt;27.58%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;25.696%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;39.15B&lt;/td&gt; &lt;td align="left"&gt;140.62B&lt;/td&gt; &lt;td align="left"&gt;27.84%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;9.956%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klhvvx</id>
    <title>On the Hugging Face Hub, you can now add Collections within Collections</title>
    <updated>2025-05-13T09:47:28+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"&gt; &lt;img alt="On the Hugging Face Hub, you can now add Collections within Collections" src="https://preview.redd.it/psitfubvri0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=927e4c70666a8537d067a9396b391eba03765991" title="On the Hugging Face Hub, you can now add Collections within Collections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Bertrand Chevrier on X: &lt;a href="https://x.com/kramp/status/1922221760193187939"&gt;https://x.com/kramp/status/1922221760193187939&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/psitfubvri0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T09:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kljute</id>
    <title>Geotracking in Gpus…</title>
    <updated>2025-05-13T11:47:51+00:00</updated>
    <author>
      <name>/u/Ashefromapex</name>
      <uri>https://old.reddit.com/user/Ashefromapex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.pcgamer.com/hardware/graphics-cards/us-senator-announces-a-bill-requiring-geotracking-in-high-end-gpus-to-prevent-the-chinese-government-from-wielding-the-ruinous-power-of-your-nvidia-rtx-4090/"&gt;https://www.pcgamer.com/hardware/graphics-cards/us-senator-announces-a-bill-requiring-geotracking-in-high-end-gpus-to-prevent-the-chinese-government-from-wielding-the-ruinous-power-of-your-nvidia-rtx-4090/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashefromapex"&gt; /u/Ashefromapex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T11:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kli1hf</id>
    <title>AMD Ryzen AI Max+ PRO 395 Linux Benchmarks</title>
    <updated>2025-05-13T09:58:25+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be wrong but it seems to be slower than a 4060ti from an LLM point of view... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-ryzen-ai-max-pro-395/7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kli1hf/amd_ryzen_ai_max_pro_395_linux_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kli1hf/amd_ryzen_ai_max_pro_395_linux_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T09:58:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1klltt4</id>
    <title>The Qwen3 chat template is *still bugged*</title>
    <updated>2025-05-13T13:23:17+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I hope everyone remembers all the twists and turns with the Qwen3 template. First, it was not working at all, then, the Unsloth team fixed the little bug with iterating over the messages. But, alas, it's not over yet!&lt;/p&gt; &lt;p&gt;I had a hint something was wrong when the biggest Qwen3 model available on OpenRouter wouldn't execute a web search twice. But it was only once I started testing my own agent framework that I realized what was wrong.&lt;/p&gt; &lt;p&gt;Qwen3 uses an XML tool calling syntax that the Jinja template transforms into the known OpenAI-compatible structure. But there's a catch. Once you call a tool once, you save that tool call in the chat history. And that tool call entry has:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;tool_calls&amp;quot;: [...] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The problem is, the current template code expects every history item to have a &amp;quot;content&amp;quot; block:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- for message in messages %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + message.content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set content = message.content %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Therefore, whenever you use any OpenAI-compatible client that saves the chat history and you use &lt;em&gt;more than one tool call&lt;/em&gt;, the conversation will become broken and the server will start reporting an error:&lt;/p&gt; &lt;p&gt;&lt;code&gt; got exception: {&amp;quot;code&amp;quot;:500,&amp;quot;message&amp;quot;:&amp;quot;[json.exception.out_of_range.403] key 'content' not found&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;server_error&amp;quot;} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I think the fix is to patch the assistant branch similar to the &amp;quot;forward messages&amp;quot; branch:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- set content = message.content if message.content is not none else '' %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and then to refer to &lt;code&gt;content&lt;/code&gt; instead of &lt;code&gt;message.content&lt;/code&gt; later on. If someone could poke the Unsloth people to fix the template, that would be pretty neat (for now, I hacked my agent's code to always append an empty code block into tool call assistant history messages since I use my own API for whatever reason, but that's not something you can do if you're using standard libraries).&lt;/p&gt; &lt;p&gt;UPDATE: I believe this is the how the corrected template should look like: &lt;code&gt;jinja {%- if tools %} {{- '&amp;lt;|im_start|&amp;gt;system\n' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + '\n\n' }} {%- endif %} {{- &amp;quot;# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags:\n&amp;lt;tools&amp;gt;&amp;quot; }} {%- for tool in tools %} {{- &amp;quot;\n&amp;quot; }} {{- tool | tojson }} {%- endfor %} {{- &amp;quot;\n&amp;lt;/tools&amp;gt;\n\nFor each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags:\n&amp;lt;tool_call&amp;gt;\n{\&amp;quot;name\&amp;quot;: &amp;lt;function-name&amp;gt;, \&amp;quot;arguments\&amp;quot;: &amp;lt;args-json-object&amp;gt;}\n&amp;lt;/tool_call&amp;gt;&amp;lt;|im_end|&amp;gt;\n&amp;quot; }} {%- else %} {%- if messages[0].role == 'system' %} {{- '&amp;lt;|im_start|&amp;gt;system\n' + messages[0].content + '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %} {%- for forward_message in messages %} {%- set index = (messages|length - 1) - loop.index0 %} {%- set message = messages[index] %} {%- set current_content = message.content if message.content is defined and message.content is not none else '' %} {%- set tool_start = '&amp;lt;tool_response&amp;gt;' %} {%- set tool_start_length = tool_start|length %} {%- set start_of_message = current_content[:tool_start_length] %} {%- set tool_end = '&amp;lt;/tool_response&amp;gt;' %} {%- set tool_end_length = tool_end|length %} {%- set start_pos = (current_content|length) - tool_end_length %} {%- if start_pos &amp;lt; 0 %} {%- set start_pos = 0 %} {%- endif %} {%- set end_of_message = current_content[start_pos:] %} {%- if ns.multi_step_tool and message.role == &amp;quot;user&amp;quot; and not(start_of_message == tool_start and end_of_message == tool_end) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} {%- for message in messages %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + message.content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set m_content = message.content if message.content is defined and message.reasoning_content is not none else '' %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is defined and message.reasoning_content is not none %} {%- set reasoning_content = message.reasoning_content %} {%- else %} {%- if '&amp;lt;/think&amp;gt;' in m_content %} {%- set content = (m_content.split('&amp;lt;/think&amp;gt;')|last).lstrip('\n') %} {%- set reasoning_content = (m_content.split('&amp;lt;/think&amp;gt;')|first).rstrip('\n') %} {%- set reasoning_content = (reasoning_content.split('&amp;lt;think&amp;gt;')|last).lstrip('\n') %} {%- endif %} {%- endif %} {%- if loop.index0 &amp;gt; ns.last_query_index %} {%- if loop.last or (not loop.last and (not reasoning_content.trim() == &amp;quot;&amp;quot;)) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n&amp;lt;think&amp;gt;\n' + reasoning_content.strip('\n') + '\n&amp;lt;/think&amp;gt;\n\n' + m_content.lstrip('\n') }} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {%- if (loop.first and m_content) or (not loop.first) %} {{- '\n' }} {%- endif %} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '&amp;lt;tool_call&amp;gt;\n{&amp;quot;name&amp;quot;: &amp;quot;' }} {{- tool_call.name }} {{- '&amp;quot;, &amp;quot;arguments&amp;quot;: ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments | tojson }} {%- endif %} {{- '}\n&amp;lt;/tool_call&amp;gt;' }} {%- endfor %} {%- endif %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- elif message.role == &amp;quot;tool&amp;quot; %} {%- if loop.first or (messages[loop.index0 - 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_start|&amp;gt;user' }} {%- endif %} {{- '\n&amp;lt;tool_response&amp;gt;\n' }} {{- message.content }} {{- '\n&amp;lt;/tool_response&amp;gt;' }} {%- if loop.last or (messages[loop.index0 + 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '&amp;lt;|im_start|&amp;gt;assistant\n' }} {%- if enable_thinking is defined and enable_thinking is false %} {{- '&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;\n\n' }} {%- endif %} {%- endif %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Seems to work correctly, I've made it work with Roo Code using this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T13:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1klh6h4</id>
    <title>Intel Partner Prepares Dual Arc "Battlemage" B580 GPU with 48 GB of VRAM</title>
    <updated>2025-05-13T08:57:55+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt; &lt;img alt="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" src="https://external-preview.redd.it/jpmGpdPWJLe0CTi-snjYV4vMSX3vWCL1VBf0G3Bgwcg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=706cf667fea0a8034c97f841fc9b7a5c0d2e2f28" title="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/336687/intel-partner-prepares-dual-arc-battlemage-b580-gpu-with-48-gb-of-vram"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T08:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkmah</id>
    <title>Qwen3 Technical Report</title>
    <updated>2025-05-13T12:26:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt; &lt;img alt="Qwen3 Technical Report" src="https://preview.redd.it/kku7lzsulj0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8d566f0f7c92d2b0575c613f30a76aafba7a29" title="Qwen3 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Technical Report released.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf"&gt;https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kku7lzsulj0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:26:42+00:00</published>
  </entry>
</feed>
