<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-13T23:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ioqk97</id>
    <title>Is it possible to weight certain tokens more or less than others using llama.cpp?</title>
    <updated>2025-02-13T18:58:52+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the newer models after llama 2 write in a very unnatural way because of the tokenizer. Normal english words are made up of smaller root words, with prefixes and suffixes sometimes affixed on either side. The tokenizer in the more recent models tries to have all words represented as a single token. I think that it makes the models more likely to use a vocabulary that doesn't reflect common speech or text.&lt;/p&gt; &lt;p&gt;My question is if there's a way to weight words differently to counteract this in order to produce more natural and human sounding text?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioqk97/is_it_possible_to_weight_certain_tokens_more_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioqk97/is_it_possible_to_weight_certain_tokens_more_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioqk97/is_it_possible_to_weight_certain_tokens_more_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T18:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iouhbo</id>
    <title>Running Deepseek R1 discussion on level 1 techs</title>
    <updated>2025-02-13T21:45:39+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's a decent discussion on level 1 techs on running Deepseek R1 for anyone who's interested&lt;/p&gt; &lt;p&gt;It dives into some thorough benchmarks for your system to understand how your memory and disks are performing&lt;/p&gt; &lt;p&gt;&lt;a href="https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4"&gt;https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ion4ug</id>
    <title>i built a free, open-source video transcription tool alternative to happyscribe</title>
    <updated>2025-02-13T16:35:09+00:00</updated>
    <author>
      <name>/u/ShakaLaka_Around</name>
      <uri>https://old.reddit.com/user/ShakaLaka_Around</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks,&lt;/p&gt; &lt;p&gt;after spending months building a video transcription service and failing to turn it into a viable business, I decided to open-source the entire thing. It's called halfway, and it might be useful for anyone needing reliable video/audio transcription.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast transcription of any audio/video file&lt;/li&gt; &lt;li&gt;Speaker detection/diarization&lt;/li&gt; &lt;li&gt;Clean, minimal editor interface&lt;/li&gt; &lt;li&gt;Export to SRT, VTT, CSV, TXT, JSON, PDF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you'll need your own AssemblyAI API key to run it, but they offer a free tier with 50$ of transcription. more models &amp;amp; ollama will be supported in the near future.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="http://github.com/moaljumaa/halfwayml_open"&gt;github.com/moaljumaa/halfwayml_open&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hope it solves a problem for any of you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShakaLaka_Around"&gt; /u/ShakaLaka_Around &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion4ug/i_built_a_free_opensource_video_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion4ug/i_built_a_free_opensource_video_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ion4ug/i_built_a_free_opensource_video_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T16:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1io811j</id>
    <title>Who builds PCs that can handle 70B local LLMs?</title>
    <updated>2025-02-13T01:48:52+00:00</updated>
    <author>
      <name>/u/Moist-Mongoose4467</name>
      <uri>https://old.reddit.com/user/Moist-Mongoose4467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are only a few videos on YouTube that show folks buying old server hardware and cobbling together affordable PCs with a bunch of cores, RAM, and GPU RAM. Is there a company or person that does that for a living (or side hustle)? I don't have $10,000 to $50,000 for a home server with multiple high-end GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist-Mongoose4467"&gt; /u/Moist-Mongoose4467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T01:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ionmvn</id>
    <title>Which LLMs are best at low-latency translation? (tl;dr LLama often beats Sonnet and 4o, Gemma 9b is surprisingly OK)</title>
    <updated>2025-02-13T16:56:09+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/llm_translation_comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ionmvn/which_llms_are_best_at_lowlatency_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ionmvn/which_llms_are_best_at_lowlatency_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T16:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohbij</id>
    <title>I built a knowledge management system that enables you to connect knowledge to any RAG</title>
    <updated>2025-02-13T11:48:33+00:00</updated>
    <author>
      <name>/u/Outside-Project-1451</name>
      <uri>https://old.reddit.com/user/Outside-Project-1451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce Simba ‚Äì an open-source solution I developed to simplify managing and leveraging knowledge in Retrieval-Augmented Generation (RAG) systems.&lt;/p&gt; &lt;p&gt;In simple terms, Simba enables you to structure and connect a knowledge base (Word, PDF, PowerPoint documents, etc.) to any chatbot.&lt;/p&gt; &lt;p&gt;üîç Why Simba?&lt;/p&gt; &lt;p&gt;While working on AI projects, I frequently encountered challenges such as:&lt;/p&gt; &lt;p&gt;üìÇ Handling long, complex documents (including tables, images, multiple sections‚Ä¶)&lt;/p&gt; &lt;p&gt;üîé Indexing and structuring information for effective retrieval&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Controlling the sources that a chatbot uses&lt;/p&gt; &lt;p&gt;Simba addresses these issues with:&lt;/p&gt; &lt;p&gt;‚úÖ Advanced parsing that automatically structures documents using state-of-the-art algorithms&lt;/p&gt; &lt;p&gt;‚úÖ An intuitive interface to visualize, modify, and organize data chunks&lt;/p&gt; &lt;p&gt;‚úÖ Precise knowledge control to include or exclude sources as needed&lt;/p&gt; &lt;p&gt;‚úÖ A flexible architecture allowing you to choose your LLMs, vector databases, chunking strategies, and parsers&lt;/p&gt; &lt;p&gt;üìå When to Use Simba?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For long and complex documents (tables, images, multiple sections‚Ä¶)&lt;/li&gt; &lt;li&gt;When you need granular control over which sources are included during conversations&lt;/li&gt; &lt;li&gt;When managing data access is critical (permissions and roles ‚Äì a feature coming soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ Who Is Simba For?&lt;/p&gt; &lt;p&gt;Simba is crafted for developers aiming to integrate a structured knowledge base into their RAG systems.&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Although the project is still evolving and doesn‚Äôt yet cover every planned feature, it‚Äôs on track to become a powerful tool for the community.&lt;/p&gt; &lt;p&gt;üí° Feedback Is a Gift!&lt;/p&gt; &lt;p&gt;The magic of open source lies in collaboration. If you encounter bugs, unclear areas, or simply have suggestions, please share your feedback. You can propose improvements, bug fixes, or new features directly on GitHub.&lt;/p&gt; &lt;p&gt;Check out the repository here: &lt;a href="https://github.com/GitHamza0206/simba"&gt;https://github.com/GitHamza0206/simba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚≠ê Simba is nearing 100 stars on GitHub, and the goal is to reach 1000 stars within the next 2 months! If you appreciate the project, please give it a star ‚≠ê ‚Äì your support means a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Project-1451"&gt; /u/Outside-Project-1451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioovam</id>
    <title>How to safely connect cloud server to home GPU server</title>
    <updated>2025-02-13T17:47:48+00:00</updated>
    <author>
      <name>/u/zabirauf</name>
      <uri>https://old.reddit.com/user/zabirauf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioovam/how_to_safely_connect_cloud_server_to_home_gpu/"&gt; &lt;img alt="How to safely connect cloud server to home GPU server" src="https://external-preview.redd.it/4lBsEIVimqlFdpMpLp_zPLjjTGcwnGZuPxQdaWN6nlY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=232d0c5c3f9f3a8f73cf6c17523eed512ddb9611" title="How to safely connect cloud server to home GPU server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put together a small site (mostly for my own use) to convert content into Markdown. It needed GPU power for docling, but I wasn‚Äôt keen on paying for cloud GPUs. Instead, I used my home GPU server and a cloud VM. This post shows how I tunnel requests back to my local rig using Tailscale and Docker‚Äîskipping expensive cloud compute. All ports stay hidden, keeping the setup secure and wallet-friendly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zabirauf"&gt; /u/zabirauf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://zohaib.me/safely-connect-cloud-server-to-home-gpu-server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioovam/how_to_safely_connect_cloud_server_to_home_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioovam/how_to_safely_connect_cloud_server_to_home_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T17:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iouc3l</id>
    <title>What's the best (in your opinion) 2025 paper so far?</title>
    <updated>2025-02-13T21:39:18+00:00</updated>
    <author>
      <name>/u/blueredscreen</name>
      <uri>https://old.reddit.com/user/blueredscreen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe something that was hidden behind the shadows of all the other news.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blueredscreen"&gt; /u/blueredscreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouc3l/whats_the_best_in_your_opinion_2025_paper_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouc3l/whats_the_best_in_your_opinion_2025_paper_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iouc3l/whats_the_best_in_your_opinion_2025_paper_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofe4w</id>
    <title>[update] aiaio: simple, lightweight ui with more features now</title>
    <updated>2025-02-13T09:30:25+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt; &lt;img alt="[update] aiaio: simple, lightweight ui with more features now" src="https://external-preview.redd.it/eWJ1dWZtaTlsdmllMTTMNvywGLfHKtiMdeeDDuKKJ-xtwCq_lpvrE6nUhuq6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f5f1683ed07dbea3c137ac3ecb29bfaf68079ce" title="[update] aiaio: simple, lightweight ui with more features now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1bduxmi9lvie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iokuej</id>
    <title>From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub</title>
    <updated>2025-02-13T14:56:32+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iokuej/from_chunks_to_blocks_accelerating_uploads_and/"&gt; &lt;img alt="From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub" src="https://external-preview.redd.it/9bVq3MHrLE5rqWVwUmEaBGsVL9u8ztrFqCUNFgUp-_A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fc91f4cbf069d2ea7656e31720b4d8ca601b7a8" title="From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/from-chunks-to-blocks"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iokuej/from_chunks_to_blocks_accelerating_uploads_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iokuej/from_chunks_to_blocks_accelerating_uploads_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T14:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iovdog</id>
    <title>Debugging a chatbot using simple VSCode breakpoints</title>
    <updated>2025-02-13T22:24:59+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iovdog/debugging_a_chatbot_using_simple_vscode/"&gt; &lt;img alt="Debugging a chatbot using simple VSCode breakpoints" src="https://external-preview.redd.it/NmRlcXU1b2ZmemllMd9ugdNfahK8ttlbZaRK6vHPvkJlvww_CIJN8nkPyI_L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3388f0a577cf813c1606989a84870cef71335b1a" title="Debugging a chatbot using simple VSCode breakpoints" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes you just need to hit pause and step over &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbkgwftffzie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iovdog/debugging_a_chatbot_using_simple_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iovdog/debugging_a_chatbot_using_simple_vscode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T22:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3hn2</id>
    <title>NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models.</title>
    <updated>2025-02-12T22:19:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt; &lt;img alt="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." src="https://preview.redd.it/95ysyjzs8sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846630231480ed6a71d97aeaed4938ab9b5cc355" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95ysyjzs8sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T22:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ion5at</id>
    <title>i built a free, open-source video transcription tool alternative to happyscribe</title>
    <updated>2025-02-13T16:35:41+00:00</updated>
    <author>
      <name>/u/ShakaLaka_Around</name>
      <uri>https://old.reddit.com/user/ShakaLaka_Around</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks,&lt;/p&gt; &lt;p&gt;after spending months building a video transcription service and failing to turn it into a viable business, I decided to open-source the entire thing. It's called halfway, and it might be useful for anyone needing reliable video/audio transcription.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast transcription of any audio/video file&lt;/li&gt; &lt;li&gt;Speaker detection/diarization&lt;/li&gt; &lt;li&gt;Clean, minimal editor interface&lt;/li&gt; &lt;li&gt;Export to SRT, VTT, CSV, TXT, JSON, PDF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you'll need your own AssemblyAI API key to run it, but they offer a free tier with 50$ of transcription. more models &amp;amp; ollama will be supported in the near future.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="http://github.com/moaljumaa/halfwayml_open"&gt;github.com/moaljumaa/halfwayml_open&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hope it solves a problem for any of you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShakaLaka_Around"&gt; /u/ShakaLaka_Around &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T16:35:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4s4s</id>
    <title>This paper might be a breakthrough Google doesn't know they have</title>
    <updated>2025-02-12T23:14:52+00:00</updated>
    <author>
      <name>/u/Ok-Possibility-5586</name>
      <uri>https://old.reddit.com/user/Ok-Possibility-5586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.03824"&gt;2105.03824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/p&gt; &lt;p&gt;^^^ this paper is from 2022 before LLMs blew up in the public imagination.&lt;/p&gt; &lt;p&gt;If someone is able to replicate this, maybe by training a smaller model and cutting out the layers and splicing into a bigger model (or something else, I'm winging it here) then maybe we get some big speedups. According to the paper (from Google) it's looking at a 90% speedup and memory reduction.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; have you seen this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Possibility-5586"&gt; /u/Ok-Possibility-5586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iof0r2</id>
    <title>When it comes to fine-tuning LLMs, the training dataset isn‚Äôt just a factor‚Äîit‚Äôs the kingmaker.</title>
    <updated>2025-02-13T09:01:31+00:00</updated>
    <author>
      <name>/u/Excellent_Delay_3701</name>
      <uri>https://old.reddit.com/user/Excellent_Delay_3701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let‚Äôs take a look at the current SOTA models‚ÄîLlama 3.x, DeepSeek, Mistral, and others. (Don't forget I am talking about fine-tune for specific tasks, not pre-train)&lt;/p&gt; &lt;p&gt;The real kingmaker for top performance? A meticulously cleaned, balanced, and well-structured dataset. Even if a ‚Äúperfect‚Äù dataset doesn‚Äôt exist, getting as close as possible makes all the difference.&lt;/p&gt; &lt;p&gt;Sure, training variables and hyperparameters impact an LLM‚Äôs performance. But in the end, isn‚Äôt the dataset everything?&lt;/p&gt; &lt;p&gt;If you‚Äôre fine-tuning an LLM or SLM for a specific task and not seeing the results you want after a few iterations, the first place you should look is the dataset. &lt;/p&gt; &lt;p&gt;How many of you changes model architectures, apply something new?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Delay_3701"&gt; /u/Excellent_Delay_3701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can‚Äôt actually see or look close. It can‚Äôt zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iolo5o</id>
    <title>SWE-agent is the new open-source SOTA on SWE-bench Lite. It run locally as well!</title>
    <updated>2025-02-13T15:32:53+00:00</updated>
    <author>
      <name>/u/ofirpress</name>
      <uri>https://old.reddit.com/user/ofirpress</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SWE-agent is an open source software engineering agent that works with any kind of model. Our 1.0 release adds tons of new features: massively parallel runs; cloud-based deployment; extensive configurability with tool bundles; new command line interface &amp;amp; utilities. Completely open-source (MIT), extensive configuration, easy to hack. Since it uses LiteLLM for LM interfacing, you can use it with a local LM: we've used it with Qwen and other community members have used it with Llama. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/swe-agent/swe-agent"&gt;https://github.com/swe-agent/swe-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;SWE-agent is now powered by our new SWE-ReX package, a lightweight, general purpose sandboxed code execution engine that supports local Docker, AWS, Modal deployments &lt;a href="https://github.com/SWE-agent/swe-rex"&gt;https://github.com/SWE-agent/swe-rex&lt;/a&gt;. You can use it to easily build your own agent with code execution from scratch without the hassle of figuring out how to communicate with running docker containers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ofirpress"&gt; /u/ofirpress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T15:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioueat</id>
    <title>MatterGen - eh, let's go ahead and change the world right quick</title>
    <updated>2025-02-13T21:41:58+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Creating novel materials with diffusion models. &lt;/p&gt; &lt;p&gt;Code...&lt;br /&gt; Yes.&lt;br /&gt; &lt;a href="https://github.com/microsoft/mattergen"&gt;https://github.com/microsoft/mattergen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/"&gt;https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:41:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioun7d</id>
    <title>TransformerLab - Generate Datasets and FineTune LLMs on them</title>
    <updated>2025-02-13T21:52:59+00:00</updated>
    <author>
      <name>/u/Firm-Development1953</name>
      <uri>https://old.reddit.com/user/Firm-Development1953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"&gt; &lt;img alt="TransformerLab - Generate Datasets and FineTune LLMs on them" src="https://external-preview.redd.it/enAzdmN2MWQ5emllMZpy0iTD7NNvaDxqshMpw7GdO8fY3vqdzTO6gEvuQwaM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33dd2b3f9422dac66a9b82657ae76ab85a0017c1" title="TransformerLab - Generate Datasets and FineTune LLMs on them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm-Development1953"&gt; /u/Firm-Development1953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xyvsqv1d9zie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioikl0</id>
    <title>Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445</title>
    <updated>2025-02-13T13:03:22+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt; &lt;img alt="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" src="https://preview.redd.it/8u7jixwzmwie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f7ab3742f3fd3c2245bf8eadfbaad2fecacd6ac" title="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8u7jixwzmwie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioj9ly</id>
    <title>Hugging Face just open sourced the free agents course!</title>
    <updated>2025-02-13T13:40:53+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt; &lt;img alt="Hugging Face just open sourced the free agents course!" src="https://external-preview.redd.it/dZG5o7Z-X0P3aHLdfTQ585OGGBpbWn7hxgShkrQ_wfw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c62c131ea4a762f597958b6a4288a1baf7a8d965" title="Hugging Face just open sourced the free agents course!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/agents-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohk4o</id>
    <title>Let's build DeepSeek from Scratch | Taught by MIT PhD graduate</title>
    <updated>2025-02-13T12:03:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt; &lt;img alt="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vjwhw6ticwie1.gif"&gt;https://i.redd.it/vjwhw6ticwie1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us for the 6pm Youtube premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ever since DeepSeek was launched, everyone is focused on: &lt;/p&gt; &lt;p&gt;- Flashy headlines&lt;/p&gt; &lt;p&gt;- Company wars&lt;/p&gt; &lt;p&gt;- Building LLM applications powered by DeepSeek&lt;/p&gt; &lt;p&gt;I very strongly think that students, researchers, engineers and working professionals should focus on the foundations. &lt;/p&gt; &lt;p&gt;The real question we should ask ourselves is: &lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;If you ask this question, you will discover that to make DeepSeek work, there are a number of key ingredients which play a role:&lt;/p&gt; &lt;p&gt;(1) Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;(2) Multi-head Latent Attention (MLA)&lt;/p&gt; &lt;p&gt;(3) Rotary Positional Encodings (RoPE)&lt;/p&gt; &lt;p&gt;(4) Multi-token prediction (MTP)&lt;/p&gt; &lt;p&gt;(5) Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;(6) Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;My aim with the ‚ÄúBuild DeepSeek from Scratch‚Äù playlist is: &lt;/p&gt; &lt;p&gt;- To teach you the mathematical foundations behind all the 6 ingredients above.&lt;/p&gt; &lt;p&gt;- To code all 6 ingredients above, from scratch.&lt;/p&gt; &lt;p&gt;- To assemble these ingredients and to run a ‚Äúmini Deep-Seek‚Äù on your own.&lt;/p&gt; &lt;p&gt;After this, you will among the top 0.1%. of ML/LLM engineers who can build DeepSeek ingredients on their own.&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours. &lt;/p&gt; &lt;p&gt;It will be in-depth. No fluff. Solid content. &lt;/p&gt; &lt;p&gt;Join us for the 6pm premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S: Attached is a small GIF showing the notes we have made. This is just 5-10% of the total amount of notes and material we have prepared for this series!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iou563</id>
    <title>Nous DeepHermes-3 8B</title>
    <updated>2025-02-13T21:30:54+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Introducing DeepHermes-3 Preview, a new LLM that unifies reasoning and intuitive language model capabilities.&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview&lt;/a&gt; GGUF Quants: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DeepHermes 3 is built from the Hermes 3 datamix, with new reasoning data, creating a model that can toggle on and off long chains of thought for improved accuracy at the cost of more test time compute!&lt;/p&gt; &lt;p&gt;This is our first work on reasoning models, and hope our unique approach to user controlled, toggleable reasoning mode furthers our mission of giving those who use DeepHermes more steerability for whatever need they have.&lt;/p&gt; &lt;p&gt;These early benchmarks show extreme improvement in Mathematical reasoning capabilities when enabled, as well as a modest improvement in GPQA (Google Proof Question Answering) benchmarks&lt;/p&gt; &lt;p&gt;As this is an experimental preview, there is much work to discover the full extent of reasoning generalization, quirks or issues, and much more. &lt;/p&gt; &lt;p&gt;We hope the community will help us in exploring the model and new reasoning paradigm on all sorts of tasks and usecases. We looking forward to hearing your feedback on how we can improve the deep reasoning models we make in the future!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;em&gt;FYI, I'm not from Hermes, just copied this message.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iolxnb</id>
    <title>A live look at the ReflectionR1 distillation process‚Ä¶</title>
    <updated>2025-02-13T15:44:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"&gt; &lt;img alt="A live look at the ReflectionR1 distillation process‚Ä¶" src="https://preview.redd.it/e851xee0gxie1.gif?width=216&amp;amp;crop=smart&amp;amp;s=148dc8683c793423d50c77fc3ceaf9b8b4b9d303" title="A live look at the ReflectionR1 distillation process‚Ä¶" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e851xee0gxie1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T15:44:28+00:00</published>
  </entry>
</feed>
