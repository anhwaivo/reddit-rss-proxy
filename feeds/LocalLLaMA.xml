<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-13T20:24:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lz17w8</id>
    <title>Madness, the ignorant's question. Would it be possible to lighten an LLM model?</title>
    <updated>2025-07-13T19:17:46+00:00</updated>
    <author>
      <name>/u/Macestudios32</name>
      <uri>https://old.reddit.com/user/Macestudios32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;Here is a question that has been in my head for some time. Would it be possible to lighten an LLM by removing content? &lt;/p&gt; &lt;p&gt;I know it's a question that for someone really knowledgeable will be crazy and stupid. &lt;/p&gt; &lt;p&gt;The idea would be, if possible, to remove information that is not relevant to the user on a topic. &lt;/p&gt; &lt;p&gt;Let's give an example: let's say we have a 3B model of parameters that needs 10 gigabytes of VRAM and only a graph of 8 gigabytes of VRAM. We could refine the model or distill it to remove information, for example, from sports and the final result would be 2.7 B of parameters. It is a theoretical question and not a real case, the numbers are invented. &lt;/p&gt; &lt;p&gt;Basically, see if there is a technique that allows you to reduce the size of a model (not quantize) by removing content not necessary for its use and thus improving its performance (less size, more layers in GPU) T&lt;/p&gt; &lt;p&gt;hank you very much and a little patience for those of us who ask stupid questions. &lt;/p&gt; &lt;p&gt;Thanks a lot. &lt;/p&gt; &lt;p&gt;Greetings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Macestudios32"&gt; /u/Macestudios32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T19:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz1fjz</id>
    <title>Problems with LocalDocs on GPT4All</title>
    <updated>2025-07-13T19:26:33+00:00</updated>
    <author>
      <name>/u/slrg1968</name>
      <uri>https://old.reddit.com/user/slrg1968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI folks, when I put a simple markdown (.md) file in the local docs folder (it as full permissions) it tries to embed, but never moves off 0% -- im not sure if something is broke or im doing something wrong -- can anyone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slrg1968"&gt; /u/slrg1968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1fjz/problems_with_localdocs_on_gpt4all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1fjz/problems_with_localdocs_on_gpt4all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1fjz/problems_with_localdocs_on_gpt4all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T19:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8fyj</id>
    <title>This whole thing is giving me WizardLM2 vibes.</title>
    <updated>2025-07-12T19:09:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt; &lt;img alt="This whole thing is giving me WizardLM2 vibes." src="https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104" title="This whole thing is giving me WizardLM2 vibes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kn56m7cgshcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvyhq</id>
    <title>Like some help setting up MCP sever for LM studio</title>
    <updated>2025-07-13T15:44:14+00:00</updated>
    <author>
      <name>/u/Night5124</name>
      <uri>https://old.reddit.com/user/Night5124</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys recently LM studio add support for tool use for local running llms. I wanting to add the option for my local running llm to do searching with my default browser for more up to date information. &lt;/p&gt; &lt;p&gt;But I have no clue how I want to keep in contained to the LM studio UI if possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Night5124"&gt; /u/Night5124 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvyhq/like_some_help_setting_up_mcp_sever_for_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvyhq/like_some_help_setting_up_mcp_sever_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvyhq/like_some_help_setting_up_mcp_sever_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:44:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lytioc</id>
    <title>Looking for my next laptop soon</title>
    <updated>2025-07-13T14:00:28+00:00</updated>
    <author>
      <name>/u/robotecnik</name>
      <uri>https://old.reddit.com/user/robotecnik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Soon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I've done... helps.&lt;/p&gt; &lt;p&gt;Sending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.&lt;/p&gt; &lt;p&gt;I am looking at the p16s Gen 4 AMD with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.&lt;/p&gt; &lt;p&gt;The first one will be more affordable than the second one...&lt;/p&gt; &lt;p&gt;Would current big models run normally on a laptop like that P16s?&lt;/p&gt; &lt;p&gt;Thank you all in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robotecnik"&gt; /u/robotecnik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T14:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly42e5</id>
    <title>Interesting info about Kimi K2</title>
    <updated>2025-07-12T16:05:34+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt; &lt;img alt="Interesting info about Kimi K2" src="https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423" title="Interesting info about Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt; &lt;p&gt;Source: @rasbt on X&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/klm2b78lvgcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyyoff</id>
    <title>How to get LLM structured outputs in TS?</title>
    <updated>2025-07-13T17:35:07+00:00</updated>
    <author>
      <name>/u/too_much_lag</name>
      <uri>https://old.reddit.com/user/too_much_lag</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I come from a Python background where I use Pydantic AI a lot, especially for handling structured data and validation. I’m starting a new project in TypeScript and I’m looking for libraries or frameworks that can help me achieve similar functionality, specifically for structured output and data validation.&lt;/p&gt; &lt;p&gt;Does anyone know of any great TypeScript tools that provide a Pydantic AI like experience?&lt;br /&gt; Any resources, recommendations, or example projects would be really appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/too_much_lag"&gt; /u/too_much_lag &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyoff/how_to_get_llm_structured_outputs_in_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyoff/how_to_get_llm_structured_outputs_in_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyoff/how_to_get_llm_structured_outputs_in_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykf92</id>
    <title>What Causes Poor Long-Context Performance?</title>
    <updated>2025-07-13T04:54:44+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href="https://www.databricks.com/blog/long-context-rag-performance-llms"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt; &lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href="https://arxiv.org/pdf/2410.05258"&gt;this paper&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt; &lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykqbu</id>
    <title>SmolLM-3B when asked if it was Peter Griffin</title>
    <updated>2025-07-13T05:12:45+00:00</updated>
    <author>
      <name>/u/Humble_Hovercraft199</name>
      <uri>https://old.reddit.com/user/Humble_Hovercraft199</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt; &lt;img alt="SmolLM-3B when asked if it was Peter Griffin" src="https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74c41be70bfcf32292dc40a30f75326535854875" title="SmolLM-3B when asked if it was Peter Griffin" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing the &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU"&gt;SmolLM3-3B-WebGPU&lt;/a&gt; Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: &amp;quot;Are you peter griffin?&amp;quot;, it just generated a 4000-token list of &amp;quot;Key Takeaways&amp;quot; about its existence:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0"&gt;https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was only able to trigger this behavior on that specific HF Space (Although, it doesn't seem to be a one time thing. I was able to get &lt;em&gt;very&lt;/em&gt; similar responses by asking it the same question again in a new tab, after refreshing). I've since downloaded the model and wasn't able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Hovercraft199"&gt; /u/Humble_Hovercraft199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T05:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyj81f</id>
    <title>Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)</title>
    <updated>2025-07-13T03:47:18+00:00</updated>
    <author>
      <name>/u/mathsTeacher82</name>
      <uri>https://old.reddit.com/user/mathsTeacher82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia. Google Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024. Any open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/vJjgtOcXq8A"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mathsTeacher82"&gt; /u/mathsTeacher82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyy0yi</id>
    <title>dots.llm1 appears to be very sensitive to quantization?</title>
    <updated>2025-07-13T17:08:35+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With 64GB RAM I could run &lt;a href="https://huggingface.co/unsloth/dots.llm1.inst-GGUF"&gt;dots&lt;/a&gt; with &lt;code&gt;mmap&lt;/code&gt; at Q4 with some hiccups (offloading a small part of the model to the SSD). I had &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/comment/n13cnzx/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;mixed feelings&lt;/a&gt; about the model:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I've been playing around with Dots at Q4_K_XL a bit, and it's one of those models that gives me mixed feelings. It's super-impressive at times, one of the best performing models I've ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I upgraded to 128GB RAM and tried dots again at Q5_K_XL, and (unless I did something wrong before) it was noticeable better. I got curious and also tried Q6_K_XL (highest quant I can fit now) and it was even more noticeable better. &lt;/p&gt; &lt;p&gt;I have no mixed feelings anymore. Compared to especially Q4, Q6 feels almost like a new model. It almost always impress me now, it feels very solid and overall powerful. I think this is now my new favorite overall model.&lt;/p&gt; &lt;p&gt;I'm a little surprised that the difference between Q4, Q5 and Q6 is this large. I thought I would only see this sort of quality gap below Q4, starting at Q3. Has anyone else experienced this too with this model, or any other model for that matter?&lt;/p&gt; &lt;p&gt;I can only fit the even larger model Qwen3-235b at Q4, I wonder if the quality difference is also this big at Q5/Q6 here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyybq8</id>
    <title>Jan doesn't show all available GGUF models from Hugging Face</title>
    <updated>2025-07-13T17:20:41+00:00</updated>
    <author>
      <name>/u/SensitiveDisk0</name>
      <uri>https://old.reddit.com/user/SensitiveDisk0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that when using Jan's built-in Hub, the list of available models seems very limited. Even though there are many GGUF models available on Hugging Face (with proper formatting and quantization), they often don't appear in the search results inside Jan.&lt;/p&gt; &lt;p&gt;I can download them manually by downloading them fron Hugging Face, but it would be a lot more convenient if Jan just showed all compatible GGUF models by default. Do you think there a limitation in the Hub search functionality? Is this a known issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveDisk0"&gt; /u/SensitiveDisk0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyybq8/jan_doesnt_show_all_available_gguf_models_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyybq8/jan_doesnt_show_all_available_gguf_models_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyybq8/jan_doesnt_show_all_available_gguf_models_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lysqk7</id>
    <title>[Rumor] Huawei 920 accelerator coming H2 2026</title>
    <updated>2025-07-13T13:23:53+00:00</updated>
    <author>
      <name>/u/44seconds</name>
      <uri>https://old.reddit.com/user/44seconds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So 6 months ago I discussed some information about the at the time not launched &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/"&gt;910C accelerator here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) &lt;a href="https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/"&gt;https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And semianalysis (regarding the 800 tflop bf16 performance) &lt;a href="https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/"&gt;https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.&lt;/p&gt; &lt;p&gt;On the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.&lt;/p&gt; &lt;p&gt;Recently attended a conference where senior Huawei executives verbally discussed their future plans:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They are aiming for a launch of the 920 in H2 2026 or H1 2027 &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A quote near verbatim &amp;quot;the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.&amp;quot; It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44seconds"&gt; /u/44seconds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T13:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvsqv</id>
    <title>Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)</title>
    <updated>2025-07-13T15:37:33+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm releasing a v1.0 of my &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt; &lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how it works and also checkout my &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyaozv</id>
    <title>Moonshot AI just made their moonshot</title>
    <updated>2025-07-12T20:46:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt; &lt;img alt="Moonshot AI just made their moonshot" src="https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd" title="Moonshot AI just made their moonshot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Screenshot: &lt;a href="https://openrouter.ai/moonshotai"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Announcement: &lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95q67pnr9icf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyyhwz</id>
    <title>Never seen fastllm mentioned here, anyone using it? (kimi k2 local)</title>
    <updated>2025-07-13T17:27:42+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of waiting for k2 ggufs and found this guy:&lt;br /&gt; &lt;a href="https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main"&gt;https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a typo in the commands but it seems to work great, and really easy to get going:&lt;br /&gt; pip install ftllm&lt;br /&gt; ftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40&lt;/p&gt; &lt;p&gt;and just like that I'm getting 7-10T/s on my 5090 + DDR5 Xeon machine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz1s8x</id>
    <title>Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones.</title>
    <updated>2025-07-13T19:40:41+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt; &lt;img alt="Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones." src="https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34bb099dc9d99a1928035eecc7f6474b2da46ba7" title="Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI there guys, hoping you're doing fine.&lt;/p&gt; &lt;p&gt;As always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.&lt;/p&gt; &lt;p&gt;As it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.&lt;/p&gt; &lt;p&gt;Also many thanks to ubergarm (&lt;a href="/u/VoidAlchemy"&gt;/u/VoidAlchemy&lt;/a&gt;) for his data on his quants and Q8_0/FP8 baseline!&lt;/p&gt; &lt;p&gt;For the quants that aren't from him, I did run them with the same command he did, with wiki.text.raw:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-perplexity -m 'model_name.gguf' \ -c 512 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -mla 3 -amb 256 -fmoe \ -f wiki.test.raw &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--------------------------&lt;/p&gt; &lt;p&gt;For baselines, we have this data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 0528 Q8: 3.2119&lt;/li&gt; &lt;li&gt;DeepSeek V3 0324 Q8 and q8_cache (important*): 3.2454&lt;/li&gt; &lt;li&gt;DeepSeek V3 0324 Q8 and F16 cache extrapolated*: 3.2443&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;*Based on &lt;a href="https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241&lt;/a&gt;, on R1 0528 at Q8_0, the difference between F16 and Q8_0 cache is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;-ctk fp16&lt;/code&gt; &lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;-ctk q8_0&lt;/code&gt; &lt;code&gt;3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So then, F16 cache is 0.03% better than Q8_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.&lt;/p&gt; &lt;p&gt;Quants tested for R1 0528:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;IQ1_S_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;UD-TQ1_0&lt;/li&gt; &lt;li&gt;IQ2_KT (ubergarm)&lt;/li&gt; &lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q2_K_XL&lt;/li&gt; &lt;li&gt;IQ3_XXS&lt;/li&gt; &lt;li&gt;IQ3_KT (ubergarm)&lt;/li&gt; &lt;li&gt;Q3_K_XL&lt;/li&gt; &lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;q4_0 (pure)&lt;/li&gt; &lt;li&gt;IQ4_KS_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quants tested for V3 0324:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q1_S_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q2_K_XL&lt;/li&gt; &lt;li&gt;IQ3_XXS&lt;/li&gt; &lt;li&gt;Q3_K_XL&lt;/li&gt; &lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ3_K_R4_Pure (ubergarm)&lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;IQ4_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So here we go:&lt;/p&gt; &lt;h1&gt;DeepSeek R1 0528&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb"&gt;R1 0528 comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2_K_XL, Q3_K_XL, IQ4_XS and Q8_0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542"&gt;R1 0528 Q2_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0"&gt;R1 0528 Q3_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1723d977f7c034496eb7a95bed576b6b53572542"&gt;R1 0528 IQ4_XS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a"&gt;R1 0528 Q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So with a table format, it looks like this (ordered by best to worse PPL)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size (GB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;665.3&lt;/td&gt; &lt;td align="left"&gt;8.000&lt;/td&gt; &lt;td align="left"&gt;3.2119&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_KS_R4&lt;/td&gt; &lt;td align="left"&gt;367.8&lt;/td&gt; &lt;td align="left"&gt;4.701&lt;/td&gt; &lt;td align="left"&gt;3.2286&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;333.1&lt;/td&gt; &lt;td align="left"&gt;4.260&lt;/td&gt; &lt;td align="left"&gt;3.2598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q4_0&lt;/td&gt; &lt;td align="left"&gt;352.6&lt;/td&gt; &lt;td align="left"&gt;4.508&lt;/td&gt; &lt;td align="left"&gt;3.2895&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4&lt;/td&gt; &lt;td align="left"&gt;300.9&lt;/td&gt; &lt;td align="left"&gt;3.847&lt;/td&gt; &lt;td align="left"&gt;3.2730&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_KT&lt;/td&gt; &lt;td align="left"&gt;272.5&lt;/td&gt; &lt;td align="left"&gt;3.483&lt;/td&gt; &lt;td align="left"&gt;3.3056&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;275.6&lt;/td&gt; &lt;td align="left"&gt;3.520&lt;/td&gt; &lt;td align="left"&gt;3.3324&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;254.2&lt;/td&gt; &lt;td align="left"&gt;3.250&lt;/td&gt; &lt;td align="left"&gt;3.3805&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_K_R4&lt;/td&gt; &lt;td align="left"&gt;220.0&lt;/td&gt; &lt;td align="left"&gt;2.799&lt;/td&gt; &lt;td align="left"&gt;3.5069&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;233.9&lt;/td&gt; &lt;td align="left"&gt;2.990&lt;/td&gt; &lt;td align="left"&gt;3.6062&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_KT&lt;/td&gt; &lt;td align="left"&gt;196.7&lt;/td&gt; &lt;td align="left"&gt;2.514&lt;/td&gt; &lt;td align="left"&gt;3.6378&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-TQ1_0&lt;/td&gt; &lt;td align="left"&gt;150.8&lt;/td&gt; &lt;td align="left"&gt;1.927&lt;/td&gt; &lt;td align="left"&gt;4.7567&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S_R4&lt;/td&gt; &lt;td align="left"&gt;130.2&lt;/td&gt; &lt;td align="left"&gt;1.664&lt;/td&gt; &lt;td align="left"&gt;4.8805&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;DeepSeek V3 0324&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81"&gt;V3 0324 Comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here Q2_K_XL performs really good, even better than R1 Q2_K_XL. Reason is unkown for now. ALso, IQ3_XXS is not here as it failed the test with nan, also unkown.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357"&gt;V3 0324 Q2_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5389b135a13c86ff471d38540909a7586e2282ff"&gt;V3 0324 Q3_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9"&gt;V3 0324 IQ4_XS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea"&gt;V3 0324 Q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So with a table format, from best to lower PPL:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size (GB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;665.3&lt;/td&gt; &lt;td align="left"&gt;8.000&lt;/td&gt; &lt;td align="left"&gt;3.2454&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_K_R4&lt;/td&gt; &lt;td align="left"&gt;386.2&lt;/td&gt; &lt;td align="left"&gt;4.936&lt;/td&gt; &lt;td align="left"&gt;3.2596&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;333.1&lt;/td&gt; &lt;td align="left"&gt;4.260&lt;/td&gt; &lt;td align="left"&gt;3.2598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4_Pure&lt;/td&gt; &lt;td align="left"&gt;352.5&lt;/td&gt; &lt;td align="left"&gt;4.505&lt;/td&gt; &lt;td align="left"&gt;3.2942&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4&lt;/td&gt; &lt;td align="left"&gt;324.0&lt;/td&gt; &lt;td align="left"&gt;4.141&lt;/td&gt; &lt;td align="left"&gt;3.3193&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;281.5&lt;/td&gt; &lt;td align="left"&gt;3.600&lt;/td&gt; &lt;td align="left"&gt;3.3690&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;233.9&lt;/td&gt; &lt;td align="left"&gt;2.990&lt;/td&gt; &lt;td align="left"&gt;3.5264&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_K_R4&lt;/td&gt; &lt;td align="left"&gt;226.0&lt;/td&gt; &lt;td align="left"&gt;2.889&lt;/td&gt; &lt;td align="left"&gt;3.5614&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S_R4&lt;/td&gt; &lt;td align="left"&gt;130.2&lt;/td&gt; &lt;td align="left"&gt;1.664&lt;/td&gt; &lt;td align="left"&gt;5.1292&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;254.2&lt;/td&gt; &lt;td align="left"&gt;3.250&lt;/td&gt; &lt;td align="left"&gt;NaN (failed)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;-----------------------------------------&lt;/p&gt; &lt;p&gt;Finally, a small comparison between R1 0528 and V3 0324&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779"&gt;https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;p&gt;So that's all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T19:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv750</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:12:40+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae684eb96341704e9ce19cc7a34eb9eaea57f63" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.&lt;/p&gt; &lt;p&gt;The app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt; &lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt; &lt;p&gt;Here's a brief guide to make this work with ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download the app from GitHub: &lt;a href="https://github.com/coreply/coreply"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt; &lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt; &lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href="http://192.168.xxx.xxx"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt; &lt;p&gt;Let me know how's your experience with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyxf1f</id>
    <title>Benchmarking Qwen3 30B and 235B on dual RTX PRO 6000 Blackwell Workstation Edition</title>
    <updated>2025-07-13T16:44:00+00:00</updated>
    <author>
      <name>/u/blackwell_tart</name>
      <uri>https://old.reddit.com/user/blackwell_tart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised in the banana thread. OP delivers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following benchmarks were taken using official Qwen3 models from Huggingface's Qwen repo for consistency:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B BF16 in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B BF16 on a single GPU&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant on a single GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All benchmarking was done with &lt;code&gt;vllm bench throughput ...&lt;/code&gt; using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, 512GB DDR5 5200 MT/s, PCIe 5.0 x16.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 24.04.2&lt;/li&gt; &lt;li&gt;NVidia drivers 575.57.08&lt;/li&gt; &lt;li&gt;CUDA 12.9&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was the magic Torch incantation that got everything working: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Otherwise these instructions worked well despite being for WSL: &lt;a href="https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3"&gt;https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192 Throughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384 Throughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s Total num prompt tokens: 16383966 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024 Throughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096 Throughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024 Throughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096 Throughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackwell_tart"&gt; /u/blackwell_tart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T16:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyw5u2</id>
    <title>Audiobook Creator - v1.4 - Added support for Orpheus along with Kokoro</title>
    <updated>2025-07-13T15:52:39+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm releasing a new version of my &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;audiobook creator app&lt;/a&gt; which now supports Kokoro and Orpheus. This release adds support for &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS&lt;/a&gt; which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS FastAPI Server&lt;/a&gt; repository.&lt;/p&gt; &lt;p&gt;Listen to a sample audiobook generated using this app: &lt;a href="https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus"&gt;https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;App Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Advanced TTS Engine Support&lt;/strong&gt;: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gradio UI App&lt;/strong&gt;: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker Support&lt;/strong&gt;: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emotion Tags Addition&lt;/strong&gt;: Emotion tags which are supported in Orpheus TTS can be added to the book's text intelligently using an LLM to enhance character voice expression.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Identification&lt;/strong&gt;: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Audiobook Narration&lt;/strong&gt;: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Includes progress bars and execution time measurements for efficient monitoring.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Licensed under GPL v3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Checkout the Audiobook Creator Repo here: &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;https://github.com/prakharsr/audiobook-creator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how the audiobooks sound and if you like the app :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvah4</id>
    <title>Tried Kimi K2 for writing and reasoning, and was not impressed.</title>
    <updated>2025-07-13T15:16:28+00:00</updated>
    <author>
      <name>/u/GlompSpark</name>
      <uri>https://old.reddit.com/user/GlompSpark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here's a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character's thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It's very prudish.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlompSpark"&gt; /u/GlompSpark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lylo75</id>
    <title>Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing</title>
    <updated>2025-07-13T06:09:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt; &lt;img alt="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" src="https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg" title="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench responses:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lylo75"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T06:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyy39n</id>
    <title>IndexTTS2, the most realistic and expressive text-to-speech model so far, has leaked their demos ahead of the official launch! And... wow!</title>
    <updated>2025-07-13T17:11:10+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech&lt;/h1&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully local with open weights.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.&lt;/li&gt; &lt;li&gt;Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.&lt;/li&gt; &lt;li&gt;Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.&lt;/li&gt; &lt;li&gt;Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard &amp;quot;free length&amp;quot; mode where it automatically lets the audio become as long as necessary.&lt;/li&gt; &lt;li&gt;Supported text to speech languages that it can output: English and Chinese. Like most models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's a few real-world use cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that's the perfect length for dubbing.&lt;/li&gt; &lt;li&gt;Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.&lt;/li&gt; &lt;li&gt;Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;So how did it leak?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;They have been preparing a website at &lt;a href="https://index-tts2.github.io/"&gt;https://index-tts2.github.io/&lt;/a&gt; which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they've been preparing, along with demo files.&lt;/li&gt; &lt;li&gt;Here's an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: &lt;a href="https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4"&gt;https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The entire presentation page is here: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;To download all demos and watch the HTML presentation locally, you can also &amp;quot;git clone &lt;a href="https://github.com/index-tts/index-tts2.github.io.git"&gt;https://github.com/index-tts/index-tts2.github.io.git&lt;/a&gt;&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual &lt;em&gt;acting!&lt;/em&gt; Bravo, Bilibili, the company behind this research!&lt;/p&gt; &lt;p&gt;They are planning to release it &amp;quot;soon&amp;quot;, and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I'd say it's coming this month or the next.&lt;/p&gt; &lt;p&gt;Their previous model was Apache 2 license, both for the source code and the weights. Let's hope the next model is the same awesome license.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:11:10+00:00</published>
  </entry>
</feed>
