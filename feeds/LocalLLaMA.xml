<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-06T02:44:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j4dk36</id>
    <title>QWQ-32B Out now on Ollama!</title>
    <updated>2025-03-05T20:49:44+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt; &lt;img alt="QWQ-32B Out now on Ollama!" src="https://b.thumbs.redditmedia.com/O181SWrHVmIh9sGbRY12DzYdKUiOWkWGCwaGeUM_eJQ.jpg" title="QWQ-32B Out now on Ollama!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9immyermoxme1.png?width=818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a95127f64fb4e06bf7252b854cbc5cc6d712558"&gt;https://preview.redd.it/9immyermoxme1.png?width=818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a95127f64fb4e06bf7252b854cbc5cc6d712558&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5knz36hooxme1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=377c42ffbc3c7e8175ee5e2b107c770fc30469a6"&gt;https://preview.redd.it/5knz36hooxme1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=377c42ffbc3c7e8175ee5e2b107c770fc30469a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LINK: &lt;a href="https://ollama.com/library/qwq:32b-q8_0"&gt;https://ollama.com/library/qwq:32b-q8_0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk36/qwq32b_out_now_on_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4g8un</id>
    <title>QwQ-32b creative writing is... quite something.</title>
    <updated>2025-03-05T22:37:31+00:00</updated>
    <author>
      <name>/u/olaf4343</name>
      <uri>https://old.reddit.com/user/olaf4343</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;em&gt;The Boss Key and the Demon Lord’s Snack&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prologue: “Ctrl+Alt+Demons, Part 1”&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jake Moreland was good at one thing: disliking it. The fluorescent glare of his cubicle ceiling, the taste of lukewarm coffee, the way his email inbox screamed, “&lt;em&gt;REMINDER: YOU’RE ONLY HERE FOR THE HEALTH INSURANCE.&lt;/em&gt;”&lt;/p&gt; &lt;p&gt;He clicked past an Excel spreadsheet titled &lt;em&gt;Q3 Hashtag Engagement&lt;/em&gt;, secretly checking his home-brew &lt;em&gt;Final Fantasy VII&lt;/em&gt; fanfiction. A Notification™ popped up: &lt;em&gt;Emergency Meeting: “Building a Collaborative Culture.”&lt;/em&gt; Jake’s middle finger summoned a black icon on his toolbar — a cartoon boss’s face winking. Before he could click it, Emily from HR appeared, clutching a poster about “innovation.”&lt;/p&gt; &lt;p&gt;“Jake!” she trilled. “Mic drop culture starts WITH YOU!”&lt;/p&gt; &lt;p&gt;He reflexively hit the icon.&lt;/p&gt; &lt;p&gt;The world exploded into MS Paint aesthetics: cartoon ellipses, aggressively red blood, and a voiceover that roared &lt;em&gt;“Starting New World!”&lt;/em&gt; When the pixels cleared, Jake stood in a field of mossy ferns, clutching his office chair. A pixelated “?” floated above him.&lt;/p&gt; &lt;p&gt;“Okay,” he muttered, “this is the rushed prologue. Cliché power.”&lt;/p&gt; &lt;p&gt;A twig snapped behind him. He turned to see a girl in a velveteen dress, rolling her eyes. “Ugh, another mortal with no sense of dramatic flair. Are we at the bad part where you get eaten by &lt;em&gt;maple syrup golems&lt;/em&gt;, or the even worse part where you rouse the hero armor?”&lt;/p&gt; &lt;p&gt;“Hero armor?” Jake snorted. “You gonna explain why the boss key cost me a raise &lt;em&gt;and&lt;/em&gt; my reality?”&lt;/p&gt; &lt;p&gt;Her lips quirked. “I’m Lucia. Stick around. You’ll pair well with ‘Destiny’ and enough plot twists to clog a font loading screen.” She popped a mint, her fangs glinting in the sun.&lt;/p&gt; &lt;p&gt;“I’m….” Jake hesitated. “I’m an HR casualty. Don’t ask.”&lt;/p&gt; &lt;p&gt;“Ooh, &lt;em&gt;corporate sins&lt;/em&gt; — a spiritual tie! Follow me.” She skipped into the woods, leaving a trail of contempt.&lt;/p&gt; &lt;p&gt;Behind them, a shadow rippled. A cloaked figure’s voice echoed: &lt;em&gt;“Mortal… you bleed hope. I delight.”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;“Perfect,” Jake sighed. “Now I’m in a party of one: sarcastic vampire kid, my indifference, and a sky.”&lt;/p&gt; &lt;p&gt;Lucia glanced back. “You’re the ‘chosen one,’ right? Say something cheesy. I’m pitching my scene.”&lt;/p&gt; &lt;p&gt;“What if I’d rather refill my Trello board?”&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;The prologue sets Jake’s cynical tone while foreshadowing his growth. Lucia’s brittle snobbery hints at deeper loneliness, and the demon’s haunting already adds stakes — all framed through a lens of absurdity. The bond of flawed, bantering heroes begins here, with jokes as their armor and Jake’s unspoken awe of how &lt;em&gt;wild&lt;/em&gt; life could be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olaf4343"&gt; /u/olaf4343 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4g8un/qwq32b_creative_writing_is_quite_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4g8un/qwq32b_creative_writing_is_quite_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4g8un/qwq32b_creative_writing_is_quite_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T22:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j48k5p</id>
    <title>From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!</title>
    <updated>2025-03-05T17:30:01+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"&gt; &lt;img alt="From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!" src="https://external-preview.redd.it/-QMLQ4lTDWvel2mDwd9Tw9u7Idn4ILKomy3yQX099fQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52a3abf47f6d7769d0954522ed244bd266923078" title="From the TabbyAPI team (for exl2), YALS have been released! It is a new GGUF server that's like TabbyAPI, for ease of usage and speeds. Please check the repo and let us know what do you think!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/theroyallab/YALS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j48k5p/from_the_tabbyapi_team_for_exl2_yals_have_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4dk98</id>
    <title>Open-Source Multi-turn Slack Agent with LangGraph + Arcade</title>
    <updated>2025-03-05T20:49:55+00:00</updated>
    <author>
      <name>/u/MostlyGreat</name>
      <uri>https://old.reddit.com/user/MostlyGreat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing the &lt;a href="https://github.com/ArcadeAI/SlackAgent"&gt;source code&lt;/a&gt; for something we built that might save you a ton of headaches as you're trying to build your own agents - a fully functional Slack agent that can handle multi-turn, tool-calling with real auth flows without making you want to throw your laptop out the window. It supports Gmail, Calendar, GitHub, etc.&lt;/p&gt; &lt;p&gt;Here's also a &lt;a href="https://www.loom.com/share/806e82e8d8e5482090bff780d0168278"&gt;quick video demo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What makes this actually useful:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles complex auth flows - OAuth, 2FA, the works (not just toy examples with hardcoded API keys)&lt;/li&gt; &lt;li&gt;Uses end-user credentials - No sketchy bot tokens with permanent access or limited to one just one user&lt;/li&gt; &lt;li&gt;Multi-service support - Seamlessly jumps between GitHub, Google Calendar, etc. with proper token management&lt;/li&gt; &lt;li&gt;Multi-turn conversations - LangGraph orchestration that maintains context through authentication flows&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Real things it can do:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pull data from private GitHub repos (after proper auth)&lt;/li&gt; &lt;li&gt;Post comments as the actual user&lt;/li&gt; &lt;li&gt;Check and create calendar events&lt;/li&gt; &lt;li&gt;Read and manage Gmail&lt;/li&gt; &lt;li&gt;Web search and crawling via SERP and Firecrawl&lt;/li&gt; &lt;li&gt;Maintain conversation context through the entire flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just recorded a demo showing it handling a complete workflow: checking a private PR, commenting on it, checking my calendar, and scheduling a meeting with the PR authors - all with proper auth flows, not fake demos.&lt;/p&gt; &lt;h1&gt;Why we built this:&lt;/h1&gt; &lt;p&gt;We were tired of seeing agent demos where &amp;quot;tool-using&amp;quot; meant calling weather APIs or other toy examples. We wanted to show what's possible when you give agents proper enterprise-grade auth handling.&lt;/p&gt; &lt;p&gt;It's built to be deployed on Modal and only requires Python 3.10+, Poetry, OpenAI and Arcade API keys to get started. The setup process is straightforward and well-documented in the repo.&lt;/p&gt; &lt;h1&gt;All open source:&lt;/h1&gt; &lt;p&gt;Everything is up on GitHub so you can dive into the implementation details, especially how we used LangGraph for orchestration and &lt;a href="http://Arcade.dev"&gt;Arcade.dev&lt;/a&gt; for tool integration.&lt;/p&gt; &lt;p&gt;The repo explains how we solved the hard parts around:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Token management&lt;/li&gt; &lt;li&gt;LangGraph nodes for auth flow orchestration&lt;/li&gt; &lt;li&gt;Handling auth retries and failures&lt;/li&gt; &lt;li&gt;Proper scoping of permissions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the repo: &lt;a href="https://github.com/ArcadeAI/SlackAgent"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy building!&lt;/p&gt; &lt;p&gt;P.S. In testing, one dev gave it access to the &lt;a href="https://docs.arcade.dev/toolkits/entertainment/spotify"&gt;Spotify tools&lt;/a&gt;. Two days later they had a playlist called &amp;quot;Songs to Code Auth Flows To&amp;quot; with suspiciously specific lyrics. 🎵🔐&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MostlyGreat"&gt; /u/MostlyGreat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4dk98/opensource_multiturn_slack_agent_with_langgraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jxyq</id>
    <title>The Reason why open source models should be in the lead.</title>
    <updated>2025-03-06T01:30:21+00:00</updated>
    <author>
      <name>/u/Feisty-Pineapple7879</name>
      <uri>https://old.reddit.com/user/Feisty-Pineapple7879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is doubling down on its application business. Execs have spoken with investors about three classes of future agent launches, ranging from $2K to $20K/month to do tasks like automating coding and PhD-level research:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty-Pineapple7879"&gt; /u/Feisty-Pineapple7879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jxyq/the_reason_why_open_source_models_should_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4hrgt</id>
    <title>Honest question - what is QwQ actually useful for?</title>
    <updated>2025-03-05T23:41:43+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recognizing wholeheartedly that the title may come off as a smidge provocative, I really am genuinely curious if anyone has a real world example of something that QwQ actually does better than its peers at. I got all excited by the updated benchmarks showing what appeared to be a significant gain over the QwQ preview, and after seeing encouraging scores in coding-adjacent tasks I thought a good test would be having it do something I often have R1 do, which is operate in architect mode and create a plan for a change in Aider or Roo. One of the top posts on &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; right now reads &amp;quot;QwQ-32B released, equivalent or surpassing full Deepseek-R1!&amp;quot; &lt;/p&gt; &lt;p&gt;If that's the case, then it should be at least moderately competent at coding given they purport to match full fat R1 on coding benchmarks. So, I asked it to implement python logging in a ~105 line file based on the existing implementation in another 110 line file.&lt;/p&gt; &lt;p&gt;In both cases, it literally couldn't do it. In Roo, it just kept talking in circles and proposing Mermaid diagrams showing how files relate to each other, despite specifically attaching only the two files in question. After it runs around going crazy for too long, Roo actually force stops the model and writes back &lt;em&gt;&amp;quot;Roo Code uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.7 Sonnet for its advanced agentic coding capabilities.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Now, there are always nuances to agentic tools like Roo, so I went straight to the chat interface and fed it an even simpler file and asked it to perform a code review on a 90 line python script that’s already in good shape. In return, I waited ten minutes while it generated &lt;strong&gt;25,000&lt;/strong&gt; tokens in total (combined thinking and actual response) to suggest I implement an exception handler on a single function. Feeding the identical prompt to Claude took roughly 3 seconds to generate 6 useful suggestions with accompanying code change snippets.&lt;/p&gt; &lt;p&gt;So this brings me back to exactly where I was when I deleted QwQ-Preview after a week. What the hell is this thing actually for? What is it good at? I feel like it’s way more useful as a proof of concept than as a practical model for anything but the least performance sensitive possible tasks. So my question is this - can anyone provide an example (prompt and response) where QwQ was able to answer your question or prompt better than qwen2.5:32b (coder or instruct)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4hrgt/honest_question_what_is_qwq_actually_useful_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j46y3s</id>
    <title>The Mac Studio has been benchmarked with Llama 3.1 405B</title>
    <updated>2025-03-05T16:24:26+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"&gt; &lt;img alt="The Mac Studio has been benchmarked with Llama 3.1 405B" src="https://b.thumbs.redditmedia.com/KgXuoJ3u9SQcUqdCeM_SCtqNLq6m2wfX0B_6FzjMdlk.jpg" title="The Mac Studio has been benchmarked with Llama 3.1 405B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's my guess based on HF models size in GGUF format. MaziyarPanahi/Meta-Llama-3.1-405B-Instruct-GGUF Q3_K_S&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j46y3s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j46y3s/the_mac_studio_has_been_benchmarked_with_llama_31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T16:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j47yei</id>
    <title>FULL LEAKED v0 by Vercel System Prompts (100% Real)</title>
    <updated>2025-03-05T17:05:47+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 05/03/2025)&lt;/p&gt; &lt;p&gt;I managed to get the full system prompts from v0 by Vercel. OVER 1.4K LINES.&lt;/p&gt; &lt;p&gt;There is some interesting stuff you should go and check.&lt;/p&gt; &lt;p&gt;This is 100% real, got it by myself. I managed to extract the full prompts with all the tags included, like &amp;lt;thinking&amp;gt;.&lt;/p&gt; &lt;p&gt;And now the question is, what model is v0 using? soon. 👀&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/x1xhlol/v0-system-prompts"&gt;https://github.com/x1xhlol/v0-system-prompts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j47yei/full_leaked_v0_by_vercel_system_prompts_100_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:05:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4914s</id>
    <title>QwQ 32b demo available</title>
    <updated>2025-03-05T17:48:47+00:00</updated>
    <author>
      <name>/u/ryseek</name>
      <uri>https://old.reddit.com/user/ryseek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen uploaded an app, which supposedly uses QwQ 32b &lt;a href="https://huggingface.co/spaces/Qwen/QwQ-32B-Demo"&gt;https://huggingface.co/spaces/Qwen/QwQ-32B-Demo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;No weights released yet. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryseek"&gt; /u/ryseek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4914s/qwq_32b_demo_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4h5s5</id>
    <title>QwQ-32B flappy bird demo bartowski IQ4_XS 32k context 24GB VRAM</title>
    <updated>2025-03-05T23:15:36+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4h5s5/qwq32b_flappy_bird_demo_bartowski_iq4_xs_32k/"&gt; &lt;img alt="QwQ-32B flappy bird demo bartowski IQ4_XS 32k context 24GB VRAM" src="https://external-preview.redd.it/IK7ZkkRrFkkK0PWUmGrBScpAjn61sE2VlPA_MSEgdmU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5584fc4b010244032e33f22b0757d69bc0243216" title="QwQ-32B flappy bird demo bartowski IQ4_XS 32k context 24GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=BtVIMKQfj38"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4h5s5/qwq32b_flappy_bird_demo_bartowski_iq4_xs_32k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4h5s5/qwq32b_flappy_bird_demo_bartowski_iq4_xs_32k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:15:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j47frd</id>
    <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
    <updated>2025-03-05T16:44:39+00:00</updated>
    <author>
      <name>/u/OC2608</name>
      <uri>https://old.reddit.com/user/OC2608</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This TTS method was made using Qwen 2.5. I think it's similar to Llasa. Not sure if already posted.&lt;/p&gt; &lt;p&gt;Hugging Face Space: &lt;a href="https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS"&gt;https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2503.01710"&gt;https://arxiv.org/pdf/2503.01710&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub Repository: &lt;a href="https://github.com/SparkAudio/Spark-TTS"&gt;https://github.com/SparkAudio/Spark-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/SparkAudio/Spark-TTS-0.5B"&gt;https://huggingface.co/SparkAudio/Spark-TTS-0.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demos: &lt;a href="https://sparkaudio.github.io/spark-tts/"&gt;https://sparkaudio.github.io/spark-tts/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OC2608"&gt; /u/OC2608 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j47frd/sparktts_an_efficient_llmbased_texttospeech_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T16:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b8kc</id>
    <title>QwQ 32B-GGUF quants available!</title>
    <updated>2025-03-05T19:15:35+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"&gt; &lt;img alt="QwQ 32B-GGUF quants available!" src="https://external-preview.redd.it/RTxLEWJN9iyy-yoootrctgBomXSk2KFDde9mtFbC1SI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9a36c0f53de7d1bab8eff48da8fe7dc3f2a37c4" title="QwQ 32B-GGUF quants available!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b8kc/qwq_32bgguf_quants_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:15:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jrnl</id>
    <title>AMD new Fully Open Instella 3B model</title>
    <updated>2025-03-06T01:21:49+00:00</updated>
    <author>
      <name>/u/blazerx</name>
      <uri>https://old.reddit.com/user/blazerx</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blazerx"&gt; /u/blazerx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/introducing-instella-3B/README.html#additional-resources"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jrnl/amd_new_fully_open_instella_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jrnl/amd_new_fully_open_instella_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j42py0</id>
    <title>OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp; 20+ Rich Interactions</title>
    <updated>2025-03-05T13:11:17+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt; &lt;img alt="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" src="https://preview.redd.it/jw78717wevme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992ec58ae3641fac0387c04211452b938e040836" title="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw78717wevme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T13:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j44vep</id>
    <title>Mac Studio just got 512GB of memory!</title>
    <updated>2025-03-05T14:55:01+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For $10,499 (in US), you get 512GB of memory and 4TB storage @ 819 GB/s memory bandwidth. This could be enough to run Llama 3.1 405B @ 8 tps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4jpij</id>
    <title>M3 Ultra is a slightly weakened 3090 w/ 512GB</title>
    <updated>2025-03-06T01:19:01+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To conclude, you are getting a slightly weakened 3090 with 512GB at max config as it gets 114.688TFLOPS FP16 vs 142.32TFLOPS FP16 for 3090 and memory bandwidth of 819.2GB/s vs 936GB/s.&lt;/p&gt; &lt;p&gt;The only place I can find about M3 Ultra spec is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/"&gt;https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, it is highly vague about the spec. So I made an educated guess on the exact spec of M3 Ultra based on this article.&lt;/p&gt; &lt;p&gt;To achieve a GPU of 2x performance of M2 Ultra and 2.6x of M1 Ultra, you need to double the shaders per core from 128 to 256. That's what I guess is happening here for such big improvement.&lt;/p&gt; &lt;p&gt;I also made a guesstimate on what a M4 Ultra can be.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M3 Ultra&lt;/th&gt; &lt;th align="left"&gt;M2 Ultra&lt;/th&gt; &lt;th align="left"&gt;M1 Ultra&lt;/th&gt; &lt;th align="left"&gt;M4 Ultra?&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Core&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;76&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Shader&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;td align="left"&gt;9728&lt;/td&gt; &lt;td align="left"&gt;8192&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU GHz&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;1.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU FP16&lt;/td&gt; &lt;td align="left"&gt;114.688&lt;/td&gt; &lt;td align="left"&gt;54.4768&lt;/td&gt; &lt;td align="left"&gt;42.5984&lt;/td&gt; &lt;td align="left"&gt;137.6256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Type&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5&lt;/td&gt; &lt;td align="left"&gt;LPDDR5X&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Speed&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;6400&lt;/td&gt; &lt;td align="left"&gt;8533&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Controller&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM Bandwidth&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;819.2&lt;/td&gt; &lt;td align="left"&gt;1092.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU P-Core&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU GHz&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;td align="left"&gt;3.2&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU FP16&lt;/td&gt; &lt;td align="left"&gt;2.688&lt;/td&gt; &lt;td align="left"&gt;1.792&lt;/td&gt; &lt;td align="left"&gt;1.6384&lt;/td&gt; &lt;td align="left"&gt;3.456&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Apple is likely to be selling it at 10-15k. If 10k, I think it is quite a good deal as its performance is about 4xDIGITS and RAM is much faster. 15k is still not a bad deal either in that perspective.&lt;/p&gt; &lt;p&gt;There is also a possibility that there is no doubling of shader density and Apple is just playing with words. That would be a huge bummer. In that case, it is better to wait for M4 Ultra.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4jpij/m3_ultra_is_a_slightly_weakened_3090_w_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T01:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j417qh</id>
    <title>llama.cpp is all you need</title>
    <updated>2025-03-05T11:45:16+00:00</updated>
    <author>
      <name>/u/s-i-e-v-e</name>
      <uri>https://old.reddit.com/user/s-i-e-v-e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only started paying somewhat serious attention to locally-hosted LLMs earlier this year.&lt;/p&gt; &lt;p&gt;Went with ollama first. Used it for a while. Found out by accident that it is using llama.cpp. Decided to make life difficult by trying to compile the llama.cpp ROCm backend from source on Linux for a somewhat unsupported AMD card. Did not work. Gave up and went back to ollama.&lt;/p&gt; &lt;p&gt;Built a simple story writing helper cli tool for myself based on file includes to simplify lore management. Added ollama API support to it.&lt;/p&gt; &lt;p&gt;ollama randomly started to use CPU for inference while &lt;code&gt;ollama ps&lt;/code&gt; claimed that the GPU was being used. Decided to look for alternatives.&lt;/p&gt; &lt;p&gt;Found koboldcpp. Tried the same ROCm compilation thing. Did not work. Decided to run the regular version. To my surprise, it worked. Found that it was using vulkan. Did this for a couple of weeks.&lt;/p&gt; &lt;p&gt;Decided to try llama.cpp again, but the vulkan version. And it worked!!!&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; gives you a clean and extremely competent web-ui. Also provides an API endpoint (including an OpenAI compatible one). llama.cpp comes with a million other tools and is extremely tunable. You do not have to wait for other dependent applications to expose this functionality.&lt;/p&gt; &lt;p&gt;llama.cpp is all you need.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s-i-e-v-e"&gt; /u/s-i-e-v-e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T11:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4d5fr</id>
    <title>Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎</title>
    <updated>2025-03-05T20:33:03+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt; &lt;img alt="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" src="https://preview.redd.it/ye3dq51qlxme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4849458d901ac4ba1ba6d60edc9a283317fab5bc" title="Saw this “New Mac Studio” on Marketplace for $800 and was like SOLD!! Hyped to try out DeepSeek R1 on it. LFG!! Don’t be jealous 😎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thing is friggin sweet!! Can’t wait to fire it up and load up full DeepSeek 671b on this monster! It does look slightly different than the promotional photos I saw online which is a little concerning, but for $800 🤷‍♂️. They’ve got it mounted in some kind of acrylic case or something, it’s in there pretty good, can’t seem to remove it easily. As soon as I figure out how to plug it up to my monitor, I’ll give you guys a report. Seems to be missing DisplayPort and no HDMI either. Must be some new type of port that I might need an adapter for. That’s what I get for being on the bleeding edge I guess. 🤓&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ye3dq51qlxme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4d5fr/saw_this_new_mac_studio_on_marketplace_for_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T20:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3zxwn</id>
    <title>Are we ready!</title>
    <updated>2025-03-05T10:16:41+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt; &lt;img alt="Are we ready!" src="https://preview.redd.it/m0ktikjrjume1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dc8037f70763ba02e1ed164ff1654c69921dfd" title="Are we ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m0ktikjrjume1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T10:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4bi0g</id>
    <title>brainless Ollama naming about to strike again</title>
    <updated>2025-03-05T19:26:23+00:00</updated>
    <author>
      <name>/u/gpupoor</name>
      <uri>https://old.reddit.com/user/gpupoor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt; &lt;img alt="brainless Ollama naming about to strike again" src="https://preview.redd.it/hnw7tvbo9xme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff02886ac4815c2bc162e157351ae6d100b824d0" title="brainless Ollama naming about to strike again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpupoor"&gt; /u/gpupoor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hnw7tvbo9xme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4bi0g/brainless_ollama_naming_about_to_strike_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43ziq</id>
    <title>The new king? M3 Ultra, 80 Core GPU, 512GB Memory</title>
    <updated>2025-03-05T14:13:32+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt; &lt;img alt="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" src="https://preview.redd.it/jkhal4p0qvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cb3ce2fdbe1423c5cf740e8f17c9c8df2f9e7b2" title="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. With 512GB of memory a world of possibilities opens up. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkhal4p0qvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4gw91</id>
    <title>QwQ-32B seems to get the same quality final answer as R1 while reasoning much more concisely and efficiently</title>
    <updated>2025-03-05T23:04:05+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I will now switch over to using QwQ as my primary reasoning model instead of R1. In all my testing, it gets the same or superior quality answers as R1 does, while having its chain of thought be much more efficient, much more concise, and much more confident. In contrast, R1 feels like a bumbling idiot who happens to be really smart only because he tries every possible solution. And It's not particularly close either, QwQ takes like 4x fewer tokens than R1 on the same problem while both arriving at the same answer.&lt;/p&gt; &lt;p&gt;Adam was right when he said not all CoTs are equal, and in this case, I think Qwen trained their model to be more efficient without degrading quality at all.&lt;/p&gt; &lt;p&gt;But I'm curious to hear what everyone here thinks, because I'm sure others are more experienced than I am.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4gw91/qwq32b_seems_to_get_the_same_quality_final_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T23:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43us5</id>
    <title>Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory</title>
    <updated>2025-03-05T14:07:13+00:00</updated>
    <author>
      <name>/u/iCruiser7</name>
      <uri>https://old.reddit.com/user/iCruiser7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt; &lt;img alt="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" src="https://external-preview.redd.it/IUc-sq0jBjlLBbxyREexc_Ijkq_kHcRXYNu-Mr7u5LI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cb29351c6e2bde66e4d208acbdf5c007acd170" title="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iCruiser7"&gt; /u/iCruiser7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4b1t9</id>
    <title>QwQ-32B released, equivalent or surpassing full Deepseek-R1!</title>
    <updated>2025-03-05T19:08:01+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt; &lt;img alt="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" src="https://external-preview.redd.it/GjWMsqQ0sjAo2i1u3zMKBVF8QJTEurDWKLmSNIhLwOE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37675ad756a9c5a85511da4e75709d13466b2af3" title="QwQ-32B released, equivalent or surpassing full Deepseek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1897361654763151544"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4b1t9/qwq32b_released_equivalent_or_surpassing_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4az6k</id>
    <title>Qwen/QwQ-32B · Hugging Face</title>
    <updated>2025-03-05T19:05:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt; &lt;img alt="Qwen/QwQ-32B · Hugging Face" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Qwen/QwQ-32B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4az6k/qwenqwq32b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T19:05:05+00:00</published>
  </entry>
</feed>
