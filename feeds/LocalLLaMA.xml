<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-09T03:03:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mkxzdg</id>
    <title>Hugging Face AI Sheets, open-source tool to do data work with open and local models</title>
    <updated>2025-08-08T15:17:30+00:00</updated>
    <author>
      <name>/u/dvilasuero</name>
      <uri>https://old.reddit.com/user/dvilasuero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"&gt; &lt;img alt="Hugging Face AI Sheets, open-source tool to do data work with open and local models" src="https://external-preview.redd.it/Z2tyN2NsODBidGhmMYRX1ulV9J5i3yzICa2szL8XEqVGjY7m7LWkytk1RKM3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd101011d77eb093980aa91356b59b188b5e63c" title="Hugging Face AI Sheets, open-source tool to do data work with open and local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I'm one of the authors of the tool. &lt;/p&gt; &lt;p&gt;We've just open sourced it and think it would be cool for this community. You can vibe test 1000s of models on Hugging Face via Inference Providers, and more importantly deploy it and run local models.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/huggingface/aisheets"&gt;https://github.com/huggingface/aisheets&lt;/a&gt;&lt;br /&gt; How to run it with local models: &lt;a href="https://github.com/huggingface/aisheets?tab=readme-ov-file#running-ai-sheets-with-custom-and-local-llms"&gt;https://github.com/huggingface/aisheets?tab=readme-ov-file#running-ai-sheets-with-custom-and-local-llms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvilasuero"&gt; /u/dvilasuero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/st1tql80bthf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:17:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkavhy</id>
    <title>random bar chart made by Qwen3-235B-A22B-2507</title>
    <updated>2025-08-07T20:19:55+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt; &lt;img alt="random bar chart made by Qwen3-235B-A22B-2507" src="https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd853635222d78299767b459957da8a9ae9f30b5" title="random bar chart made by Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rka3lhpnonhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml2t67</id>
    <title>Actual open source local memory with no hidden cloud</title>
    <updated>2025-08-08T18:19:25+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2t67/actual_open_source_local_memory_with_no_hidden/"&gt; &lt;img alt="Actual open source local memory with no hidden cloud" src="https://external-preview.redd.it/UIfRRgdBHdYc2kWUJyryMmJQiUB0JAMTOp6DVSAFHgs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf88371af12d1e36ef88f2ca0690bc5c173ba5cb" title="Actual open source local memory with no hidden cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Just saw a post from MemU and the feedback from the community.&lt;/p&gt; &lt;p&gt;If you want local memory, try cognee.&lt;/p&gt; &lt;p&gt;We store data in local lancedb and kuzu instances for embeddings and graphs.&lt;br /&gt; We just added BAML powered LLM calls that should let you create memory more reliably with Ollama.&lt;/p&gt; &lt;p&gt;Feel free to test it out on our dev branch, should be released Mon/Tue&lt;/p&gt; &lt;p&gt;We also ran the benchmarks and published results today: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25ddbppe7uhf1.png?width=1544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5eb7e09801dc392bbb884fd0e2c5bd4fd55327ff"&gt;Hotpot QA, 30 runs, 24 questions + one run for Graphiti from few months back&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The repo is here and feel free to ask questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/topoteretes/cognee"&gt;https://github.com/topoteretes/cognee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2t67/actual_open_source_local_memory_with_no_hidden/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2t67/actual_open_source_local_memory_with_no_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2t67/actual_open_source_local_memory_with_no_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T18:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkx5zo</id>
    <title>a lightweight voice clone tool, not dependent on ffmpeg, Python, PyTorch, ONNX, just a single executable file</title>
    <updated>2025-08-08T14:46:11+00:00</updated>
    <author>
      <name>/u/Suitable-Patience916</name>
      <uri>https://old.reddit.com/user/Suitable-Patience916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built an &lt;a href="https://github.com/myshell-ai/OpenVoice"&gt;OpenVoice&lt;/a&gt;-based voice cloning tool that requires no installation, just a single executable file (~14M), supporting multiple formats without dependencies on ffmpeg, Python, PyTorch, ONNX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Single-file executable - no installation required&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Independent of FFmpeg, Python, PyTorch, and ONNX&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Support multiple formats (e.g. mp4, mp3, wav)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Offer multiple built-in base speakers: en-au, en-br, en-default, en-india, en-newest, en-us, es, fr, jp, kr, zh&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Support CPU &amp;amp; GPU&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/jingangdidi/voice_clone"&gt;https://github.com/jingangdidi/voice_clone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Patience916"&gt; /u/Suitable-Patience916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke7ef</id>
    <title>120B runs awesome on just 8GB VRAM!</title>
    <updated>2025-08-07T22:32:04+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU (&lt;del&gt;~17T/s&lt;/del&gt; 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt; &lt;p&gt;You can offload just the attention layers to GPU (requiring about 5 to 8GB of VRAM) for fast prefill.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache for the sequence&lt;/li&gt; &lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt; &lt;li&gt;Routing tables&lt;/li&gt; &lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt; &lt;p&gt;This yields an amazing snappy system for a 120B model! Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt; &lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;with 5GB of vram usage!&lt;/p&gt; &lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt; &lt;p&gt;edit: with this latest PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 36 \ #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster. --n-gpu-layers 999 \ #everything else on the GPU, about 8GB -c 0 -fa \ #max context (128k), flash attention --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 94593.62 ms / 12717 tokens ( 7.44 ms per token, 134.44 tokens per second) eval time = 76741.17 ms / 1966 tokens ( 39.03 ms per token, 25.62 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hitting above 25T/s with only 8GB VRAM use!&lt;/p&gt; &lt;p&gt;Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 28 \ --n-gpu-layers 999 \ -c 0 -fa \ --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 78003.66 ms / 12715 tokens ( 6.13 ms per token, 163.01 tokens per second) eval time = 70376.61 ms / 2169 tokens ( 32.45 ms per token, 30.82 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkza1b</id>
    <title>New paper reveals Chain-of-Thought reasoning of LLMs a mirage</title>
    <updated>2025-08-08T16:06:02+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2508.01191"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T16:06:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml52ys</id>
    <title>8.8 - 8.11 = -0.3</title>
    <updated>2025-08-08T19:47:08+00:00</updated>
    <author>
      <name>/u/PhysicsPast8286</name>
      <uri>https://old.reddit.com/user/PhysicsPast8286</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml52ys/88_811_03/"&gt; &lt;img alt="8.8 - 8.11 = -0.3" src="https://b.thumbs.redditmedia.com/q4fymwti1ZNhh_KG_5NeHCvxLc3q6Zc7PmBmK59fp9M.jpg" title="8.8 - 8.11 = -0.3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked a couple of LLMs - both Closed (Claude Sonnet 4, GPT5 and Gemini 2.5 Pro) and Open Source (GLM 4.5, Qwen3-235B-A22B-2507) with a simple question:&lt;/p&gt; &lt;p&gt;&amp;quot;what is 8.8 - 8.11&amp;quot;&lt;/p&gt; &lt;p&gt;Only - Qwen and GLM gave the correct answer. GLM's COT was the best&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/frs5c39unuhf1.png?width=1422&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a9af3f21cea41d3d698e50d65270ecd471a881"&gt;Claude Sonnet 4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hhp5nyldnuhf1.png?width=1305&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=700637e51e0dda8507868658a8abcd9fe417aae9"&gt;GPT5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6jqjd82jnuhf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d80ecbf3209b2de63544b4d8b8a5e1273d7cbb51"&gt;Gemini 2.5 Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ybo3xkzmuhf1.png?width=990&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=620f482b1f11bb72116728e8423d5b0c2c713d9e"&gt;GLM 4.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vmy83u01nuhf1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96700808c03df1e0c24626294ac6c73c74798fc0"&gt;Qwen3-235B-A22B-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm amazed with the results! 🫠&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhysicsPast8286"&gt; /u/PhysicsPast8286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml52ys/88_811_03/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml52ys/88_811_03/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml52ys/88_811_03/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T19:47:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkvks4</id>
    <title>How Attention Sinks Keep Language Models Stable</title>
    <updated>2025-08-08T13:43:01+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hanlab.mit.edu/blog/streamingllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T13:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkp0am</id>
    <title>Granite 3 8B is seriously underrated - still outperforming newer models</title>
    <updated>2025-08-08T07:42:17+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building AI pipelines using the 12 factor agent approach (shoutout to Dex - check out his YouTube talk and GitHub), and I have to say IBM's Granite 3 8B continues to impress me nearly a year after release.This model consistently outperforms newer closed-source options (yes, I talked about GPT-5 mini/nano) on specific tasks. Where it really shines:&lt;/p&gt; &lt;p&gt;Task classification with structured outputs - It's incredibly reliable at categorizing user requests into the right buckets&lt;/p&gt; &lt;p&gt;Keyword generation for search/RAG - Produces solid results for information retrieval&lt;/p&gt; &lt;p&gt;If you haven't tried Granite 3 8B yet, it's worth adding to your toolkit. I still use larger models for the final aggregation and presentation layer, but for these specialized tasks, Granite punches well above its weight class.&lt;/p&gt; &lt;p&gt;Anyone else having similar experiences with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml6vrs</id>
    <title>Is a 2TB DDR5 RAM consumer grade setup worth it or M3 Ultra is better value? Discussion and specs comparison thread!</title>
    <updated>2025-08-08T20:58:40+00:00</updated>
    <author>
      <name>/u/moko990</name>
      <uri>https://old.reddit.com/user/moko990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to build a medium &amp;quot;budget&amp;quot; professional setup for LLMs. LPCAMM2 seems to be a distant dream still. The LPDDR5X are mainly soldered on and capped at 128GB (Ryzen AI 395) and a bandwidth of 256GB/s (theoretically). The only alternative would be a M4 Ultra (512GB at 800 GB/s) but that's also soldered on the chip. There are really no &lt;strong&gt;consumer CPU&lt;/strong&gt; that can rival the Apple offering, as they are all dual-channel.&lt;/p&gt; &lt;p&gt;But recently AMD dropped an interesting alternative, mainly 8 channel CPUs like the Threadripper Pro 9000 WX! &lt;/p&gt; &lt;p&gt;And few days ago, a &lt;a href="https://www.tomshardware.com/pc-components/ram/v-color-announces-2tb-rdimm-kits-for-threadripper-pro-9000-256gb-modules-promise-stability-at-absurdly-high-ram-capacities"&gt;2TB memory kit made the news&lt;/a&gt;, and paired with the Threadripper Pro 9000 WX + &lt;a href="https://www.asrock.com/mb/AMD/WRX90%20WS%20EVO/index.asp#Specification"&gt;AsROCK MB&lt;/a&gt;, we can end up with &lt;strong&gt;2TB of DDR5-6400 with 8ch CPU. That's (410 GB/s) half of that of the M3 Ultra, but 4x the capacity&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Is it worth investing in such budget &amp;quot;professional setup&amp;quot;? &lt;/p&gt; &lt;p&gt;The MB paired with a good GPU (maybe an 9070XT or whatever is AMD's upcoming flagship, might make this setup a beast, and yet technically not a server still. &lt;/p&gt; &lt;p&gt;I will be waiting a bit more to see if any issues with early adopters, but it's a setup I am considering, and given the &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;recent llama.cpp updates&lt;/a&gt; (offloading moe to CPU, and attention layers to GPU), this is might become an amazing home setup. What do you think? &lt;/p&gt; &lt;p&gt;Specs summary table by o4: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Spec / Feature&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Ryzen AI Max+ 395 (Strix Halo)&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Apple M3 Ultra&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Threadripper Pro 9000 WX + RX 9070 XT&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;CPU Cores / Threads&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;16 cores / 32 threads&lt;/td&gt; &lt;td&gt;32 cores (24P + 8E)&lt;/td&gt; &lt;td&gt;Up to 96 cores / 192 threads&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Memory Type&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;LPDDR5X-8000 (soldered)&lt;/td&gt; &lt;td&gt;Unified LPDDR5X (soldered)&lt;/td&gt; &lt;td&gt;DDR5-6400 RDIMM ECC&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Memory Channels&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4 channels&lt;/td&gt; &lt;td&gt;Unified memory&lt;/td&gt; &lt;td&gt;8 channels&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Max Memory Capacity&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;128 GB&lt;/td&gt; &lt;td&gt;512 GB&lt;/td&gt; &lt;td&gt;2 TB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Memory Bandwidth&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~256 GB/s (theoretical)&lt;/td&gt; &lt;td&gt;~820 GB/s&lt;/td&gt; &lt;td&gt;~410 GB/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;NPU / Neural Engine&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50 TOPS (XDNA 2)&lt;/td&gt; &lt;td&gt;32-core Neural Engine&lt;/td&gt; &lt;td&gt;None&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Integrated GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Radeon 8060S (40 CU, RDNA 3.5)&lt;/td&gt; &lt;td&gt;80-core GPU&lt;/td&gt; &lt;td&gt;None (requires discrete GPU)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PCIe Support&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;PCIe 4.0&lt;/td&gt; &lt;td&gt;No PCIe expansion (SoC only) / (or Thunderbolt 5)&lt;/td&gt; &lt;td&gt;128 lanes PCIe 5.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;ECC Memory Support&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;No&lt;/td&gt; &lt;td&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Upgradeable / Modular&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;No (soldered memory + SoC)&lt;/td&gt; &lt;td&gt;No (SoC, unified memory)&lt;/td&gt; &lt;td&gt;Yes (CPU, RAM, GPU all upgradeable)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Estimated Max Power Draw&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~120 W&lt;/td&gt; &lt;td&gt;~270 W&lt;/td&gt; &lt;td&gt;~700–750 W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Recommended PSU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;300 W adapter / SFF PSU&lt;/td&gt; &lt;td&gt;Built-in 480 W internal PSU&lt;/td&gt; &lt;td&gt;1000 - 1200 W ATX PSU (modular, 80 Plus Gold or better)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moko990"&gt; /u/moko990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6vrs/is_a_2tb_ddr5_ram_consumer_grade_setup_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6vrs/is_a_2tb_ddr5_ram_consumer_grade_setup_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6vrs/is_a_2tb_ddr5_ram_consumer_grade_setup_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T20:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml6rx0</id>
    <title>Shoutout to the community from the Lemonade team!</title>
    <updated>2025-08-08T20:54:26+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a great 48 hours with this community, so I wanted to give a shoutout! I was a longtime lurker here before I finally landed a job that would let me participate full time, and I'm really happy it has worked out.&lt;/p&gt; &lt;p&gt;For those who haven't heard of us, Lemonade provides an OpenAI API server that helps you run state-of-the-art llamacpp+Vulkan/ROCm and OnnxRuntimeGenAI LLMs on your GPUs and NPUs.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt; Discord: &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;https://discord.gg/Z3u8tpqQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2 days ago, we posted about adding &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mjgj2x/llamacpprocm7_beta_is_now_supported_on_lemonade/"&gt;ROCm7 support in Lemonade&lt;/a&gt; and the response on the GitHub and Discord has been awesome to say the least. Huge thanks to everyone who has been on there, especially those reporting issues, contributing PRs, doing show-and-tell, and sharing their joy!&lt;/p&gt; &lt;p&gt;Over the past 2 days, the team and community have cranked out a new release, &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.1.2"&gt;v8.1.2&lt;/a&gt;, that solves a bunch of things people have been asking for:&lt;/p&gt; &lt;p&gt;🤖 &lt;a href="https://lemonade-server.ai/docs/server/server_models/#npu"&gt;3 more NPU models&lt;/a&gt; were tested and added to the suggested list thanks to @Looong01 and @henrylearn2rock&lt;/p&gt; &lt;p&gt;🛜 The &lt;code&gt;--host&lt;/code&gt; option lets you access Lemonade LLMs and web ui from across your home network, thanks to @ajh123&lt;/p&gt; &lt;p&gt;📃 Improvements to the &lt;a href="https://lemonade-server.ai/docs/faq/"&gt;FAQ&lt;/a&gt; with the new questions we've been hearing.&lt;/p&gt; &lt;p&gt;🪨 Fixed the most common issues with the ROCm implementation: device detection/selection, and now it's available with .exe installation.&lt;/p&gt; &lt;p&gt;🪳 Squashed 4 other bugs that have been bugging people.&lt;/p&gt; &lt;p&gt;🚀 What's coming next: - We're planning on spending the next few workdays solving more common issues (especially: setting temperature and repetition penalty) and hanging out in the &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;Discord&lt;/a&gt;. - After that, we'll be picking some new big items to tackle, so please give feedback.&lt;/p&gt; &lt;p&gt;If Lemonade has been useful to you, please drop by the &lt;a href="https://discord.gg/Z3u8tpqQ"&gt;Discord&lt;/a&gt; to let us know or give us a star on the &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And thanks again for being so cool, &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6rx0/shoutout_to_the_community_from_the_lemonade_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6rx0/shoutout_to_the_community_from_the_lemonade_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml6rx0/shoutout_to_the_community_from_the_lemonade_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T20:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkon92</id>
    <title>Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China</title>
    <updated>2025-08-08T07:18:22+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt; &lt;img alt="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" src="https://preview.redd.it/u7fdqw6zwqhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3114187ca53c4b97153190460034166fc59eaccc" title="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I started &lt;a href="https://www.designarena.ai/"&gt;my benchmark&lt;/a&gt; just about a month and a half ago, it has been interesting to see just how well the open weight / open source models are competing with their proprietary counterparts when evaluated on how user comparisons of different generations from each model. &lt;/p&gt; &lt;p&gt;Based on the benchmark, Qwen3 Coder, DeepSeek R1-0528, DeepSeek V3-2024, Qwen3 Instruct 2507, and GLM 4.5 could all be considered to be SOTA. I do think this ranking will change slightly though with one of the OS models being pushed out for GPT-5 (which was recently added, so sample size is too small). &lt;/p&gt; &lt;p&gt;That said, it really feels like we're in a golden age of open source models right now. We're also see a good amount of stagnation right now in the improvements being made by the proprietary models. Do you think OS will continue to keep pace? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7fdqw6zwqhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkw4ug</id>
    <title>GLM45 vs GPT-5, Claude Sonnet 4, Gemini 2.5 Pro — live coding test, same prompt</title>
    <updated>2025-08-08T14:05:13+00:00</updated>
    <author>
      <name>/u/darkageofme</name>
      <uri>https://old.reddit.com/user/darkageofme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re running a live benchmark today with &lt;strong&gt;GLM45&lt;/strong&gt; in the mix against three major proprietary LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every model gets the same prompt for each task&lt;/li&gt; &lt;li&gt;Multiple attempts: simple builds, bug fixes, complex projects, and possibly planning tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ll record:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How GLM45 performs on speed and accuracy&lt;/li&gt; &lt;li&gt;Where it matches or beats closed models&lt;/li&gt; &lt;li&gt;Debug handling in a live environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;16:00 UTC / 19:00 EEST&lt;/p&gt; &lt;p&gt;You'll find us here: &lt;a href="https://live.biela.dev"&gt;https://live.biela.dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkageofme"&gt; /u/darkageofme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkq4i4</id>
    <title>Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507</title>
    <updated>2025-08-08T08:55:44+00:00</updated>
    <author>
      <name>/u/acec</name>
      <uri>https://old.reddit.com/user/acec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt; &lt;img alt="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They claim that &amp;quot;On sequences approaching 1M tokens, the system achieves up to a &lt;strong&gt;3× speedup&lt;/strong&gt; compared to standard attention implementations.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acec"&gt; /u/acec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507/commit/3ffd1f50b179e643d839c86df9ffbbefcb0d5018"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T08:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml3k2m</id>
    <title>GLM-4.5 Air Q8 vs GLM-4.5 IQ2_XXS</title>
    <updated>2025-08-08T18:47:59+00:00</updated>
    <author>
      <name>/u/therealAtten</name>
      <uri>https://old.reddit.com/user/therealAtten</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lowest of lows post, but in all seriousness, both quants are virtually the same size:&lt;br /&gt; GLM-4.5 Air Q8 = 117.5 GB&lt;br /&gt; GLM-4.5 IQ2_XXS = 115.8 GB&lt;/p&gt; &lt;p&gt;I can't be the only one with 128 GB RAM having asked that question to themselves. While GLM-4.5 Air Q6_K_XL is downloading, has anyone by any chance tried both quants and can compare their outputs given your use cases? I am so curious to know if there is a sweet spot in the quality attained for a given RAM capacity, that is not necessarily the largest model you can fit... Thank you for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealAtten"&gt; /u/therealAtten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml3k2m/glm45_air_q8_vs_glm45_iq2_xxs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml3k2m/glm45_air_q8_vs_glm45_iq2_xxs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml3k2m/glm45_air_q8_vs_glm45_iq2_xxs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T18:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml4g5w</id>
    <title>So Deepseek R2 coming next week?</title>
    <updated>2025-08-08T19:22:06+00:00</updated>
    <author>
      <name>/u/Beneficial-Yam2425</name>
      <uri>https://old.reddit.com/user/Beneficial-Yam2425</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems to be chatter about that, anyone heard anything?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Yam2425"&gt; /u/Beneficial-Yam2425 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml4g5w/so_deepseek_r2_coming_next_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml4g5w/so_deepseek_r2_coming_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml4g5w/so_deepseek_r2_coming_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T19:22:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkf543</id>
    <title>To all GPT-5 posts</title>
    <updated>2025-08-07T23:11:59+00:00</updated>
    <author>
      <name>/u/Danny_Davitoe</name>
      <uri>https://old.reddit.com/user/Danny_Davitoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt; &lt;img alt="To all GPT-5 posts" src="https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1" title="To all GPT-5 posts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please. I don’t care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny_Davitoe"&gt; /u/Danny_Davitoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v08gwidjohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkowrw</id>
    <title>Llama.cpp just added a major 3x performance boost.</title>
    <updated>2025-08-08T07:35:50+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama cpp just merged the final piece to fully support attention sinks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My prompt processing speed went from 300 to 1300 with a 3090 for the new oss model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkwkcd</id>
    <title>What do you think it will be?</title>
    <updated>2025-08-08T14:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt; &lt;img alt="What do you think it will be?" src="https://preview.redd.it/it28f5ns1thf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e5ae37fbf6696b49a3a1409b98db6c6507247a" title="What do you think it will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/it28f5ns1thf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml846a</id>
    <title>LEANN – Local RAG with 97% smaller index and Claude Code–compatible semantic search</title>
    <updated>2025-08-08T21:48:46+00:00</updated>
    <author>
      <name>/u/Lanky-District9096</name>
      <uri>https://old.reddit.com/user/Lanky-District9096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;We’re building&lt;/strong&gt; &lt;a href="https://github.com/yichuan-w/LEANN"&gt;&lt;strong&gt;LEANN&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;at Berkeley Sky Lab — a local vector index for RAG that’s:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔒 Privacy-first&lt;/li&gt; &lt;li&gt;📦 97% smaller&lt;/li&gt; &lt;li&gt;🧠 Fully compatible with &lt;strong&gt;Claude Code&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and &lt;strong&gt;GPT-OSS&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Run semantic search on your laptop — fast, lightweight, and cloud-free.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;🧠 Why does LEANN matter?&lt;/h1&gt; &lt;p&gt;Most vector databases store &lt;em&gt;everything&lt;/em&gt; — every embedding, every edge — which quickly balloons to &lt;strong&gt;100+ GB&lt;/strong&gt; when indexing emails, chat, and code.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(For example, embedding just 50 GB of text can require over 500 GB of storage.)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;But most queries only touch a tiny slice of the DB. So we asked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why store every single embedding?&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;⚙️ LEANN introduces two ultra-lightweight backends:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;🔍 Graph-only mode&lt;/strong&gt; Stores &lt;em&gt;no embeddings&lt;/em&gt;, just a pruned HNSW graph. Recomputes embeddings on-the-fly using overlapping neighbors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;💡 PQ+Rerank mode&lt;/strong&gt; compresses vectors with PQ and replaces heavy storage with lightweight recomputation over the candidate set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each has different tradeoffs, but both achieve the same goal:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;🧠 &lt;strong&gt;Massive storage savings&lt;/strong&gt; with &lt;em&gt;no meaningful drop&lt;/em&gt; in recall&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; In modern RAG systems — with long contexts and reasoning-heavy models —&lt;br /&gt; &lt;strong&gt;generation&lt;/strong&gt;, not retrieval, is the bottleneck.&lt;br /&gt; So even with slightly slower retrieval, total latency increases by just &lt;strong&gt;~5%&lt;/strong&gt; or less.&lt;/p&gt; &lt;h1&gt;🔍 LEANN supports semantic search over:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;📨 Apple Mail&lt;/li&gt; &lt;li&gt;💾 Filesystem&lt;/li&gt; &lt;li&gt;🕰️ Chrome / Chat history&lt;/li&gt; &lt;li&gt;🧠 Codebase (Claude Code–compatible)&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;LEANN = your personal &lt;strong&gt;Jarvis&lt;/strong&gt;, running locally.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;🔗 Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2506.08276"&gt;https://arxiv.org/abs/2506.08276&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love for you to try it out, give feedback, or ask questions in the repo! 🙌&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-District9096"&gt; /u/Lanky-District9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml846a/leann_local_rag_with_97_smaller_index_and_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml846a/leann_local_rag_with_97_smaller_index_and_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml846a/leann_local_rag_with_97_smaller_index_and_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T21:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml5032</id>
    <title>gpt-oss Bug Fixes + Fine-tuning now in Unsloth</title>
    <updated>2025-08-08T19:43:52+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml5032/gptoss_bug_fixes_finetuning_now_in_unsloth/"&gt; &lt;img alt="gpt-oss Bug Fixes + Fine-tuning now in Unsloth" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="gpt-oss Bug Fixes + Fine-tuning now in Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;&lt;strong&gt;fine-tune gpt-oss-20b for free&lt;/strong&gt; on Colab&lt;/a&gt;-Fine-tuning.ipynb) with &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;. All other training methods/libraries require a minimum of 40GB VRAM, however we managed to fit it in just 14GB VRAM! We also found some issues with differing implementations of the gpt-oss model which can affect inference performance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Jinja chat template has extra newlines, didn't parse thinking sections correctly&lt;/li&gt; &lt;li&gt;Tool calling wasn't rendered correctly due to using tojson and missing strings&lt;/li&gt; &lt;li&gt;Some third party versions seem to miss &lt;code&gt;&amp;lt;|channel|&amp;gt;final&lt;/code&gt; -&amp;gt; this is a must!&lt;/li&gt; &lt;li&gt;For running in float16 machines, you will get NaNs - please use Float32 and Bfloat16 mixed precision!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Below shows the differences in the using the Harmony library (official OpenAI tokenization) and using chat templates:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1w23dzu0muhf1.png?width=2760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=198562075706d7d4e6d708dae6ddb0e5d9437cd7"&gt;https://preview.redd.it/1w23dzu0muhf1.png?width=2760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=198562075706d7d4e6d708dae6ddb0e5d9437cd7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also updated all GGUFs and BF16 versions and provide linearized versions for finetuning and post-training purposes as well!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/gpt-oss-20b-unsloth-bnb-4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-BF16"&gt;https://huggingface.co/unsloth/gpt-oss-20b-BF16&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also some frequently asked questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Why are the quants all the same size?&lt;/strong&gt; I made BF16 versions and tried doing imatrix and converting them to 1bit to no avail - the perplexity was over 10 million and llama.cpp for now doesn't support non multiples of 256 (gpt-oss uses 2880 as the shape)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why does &amp;lt;|channel|&amp;gt;final appear?&lt;/strong&gt; This is intended as is normal!&lt;/li&gt; &lt;li&gt;Optimal settings? &lt;strong&gt;Temperature = 1.0, min_p = 0.0, top_k = disabled, top_p = 1.0&lt;/strong&gt;. &lt;a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune"&gt;See our docs&lt;/a&gt; for more details!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3o1m2cdyluhf1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa12b7fca641303bbbda23fefcff7fc35cb54ad7"&gt;https://preview.redd.it/3o1m2cdyluhf1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa12b7fca641303bbbda23fefcff7fc35cb54ad7&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Free 20B finetuning Colab notebook&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb&lt;/a&gt;-Fine-tuning.ipynb)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MXFP4 inference only notebook&lt;/strong&gt; (shows how to do reasoning mode = low / medium / high): &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb&lt;/a&gt;-Inference.ipynb)&lt;/li&gt; &lt;li&gt;More details on our docs and our blog! &lt;a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml5032/gptoss_bug_fixes_finetuning_now_in_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml5032/gptoss_bug_fixes_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml5032/gptoss_bug_fixes_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T19:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkxmoa</id>
    <title>GLM-4.5 series new models will be open source soon</title>
    <updated>2025-08-08T15:03:54+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt; &lt;img alt="GLM-4.5 series new models will be open source soon" src="https://preview.redd.it/mmvy25c79thf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff0519caa8ad5551e7e553775eb89d38a4aeb2c0" title="GLM-4.5 series new models will be open source soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mmvy25c79thf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkrb18</id>
    <title>🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!</title>
    <updated>2025-08-08T10:11:45+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt; &lt;img alt="🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!" src="https://preview.redd.it/ud233u23trhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9666b41b192bef19c1d95e2dc31745f398def8d7" title="🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!&lt;/p&gt; &lt;p&gt;🔧 Powered by:&lt;/p&gt; &lt;p&gt;• Dual Chunk Attention (DCA) – A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence. &lt;/p&gt; &lt;p&gt;• MInference – Sparse attention that cuts overhead by focusing on key token interactions&lt;/p&gt; &lt;p&gt;💡 These innovations boost both generation quality and inference speed, delivering up to 3× faster performance on near-1M token sequences.&lt;/p&gt; &lt;p&gt;✅ Fully compatible with vLLM and SGLang for efficient deployment.&lt;/p&gt; &lt;p&gt;📄 See the update model cards for how to enable this feature.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ud233u23trhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml77rq</id>
    <title>The LLM world is an illusion of progress</title>
    <updated>2025-08-08T21:11:49+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my previous rant in which I was saying that LLMs were trapped in monolingualism and the assistant paradigm: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[Mini Rant] Are LLMs trapped in English and the assistant paradigms?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To update this: I feel like things evolved toward bilingualism (Chinese and English), while multilingualism is still at the bottom of the benchmarks of popular released LLMs, and generally not in the lesser-known LLMs.&lt;/p&gt; &lt;p&gt;To address what I call the assistant paradigm: it is now more than ever a cluster*ck because everything you'll want to generate a simple chunk of text will try to make tool calls, and to be fair, there is no normalized template used by more than one provider, which complicates things even more. Merging LLMs at this point may be totally magical, hoping that Frankenstein may not come out at the end of the process, lol.&lt;/p&gt; &lt;p&gt;Anyway, here are other points I want to address this time. Working generally in academia has made me pretty critical of these few points, which I think are underrepresented. They may not be the general community view or criteria of choice, but they're mine, and maybe others, so I wanted to share those with you, beloved LocalLlama community. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparing LLMs is a total illusion at this point&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;As highlighted in a recent paper &amp;quot;&lt;a href="http://arxiv.org/abs/2408.04667"&gt;Non-Determinism of Deterministic LLM Settings&lt;/a&gt;&amp;quot;, LLMs configured to be deterministic can still show significant variations in outputs for the same inputs. This makes comparing LLMs a very tricky task.. if not impossible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks are flawed&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I'm aware of the abundance of benchmarks available, but when I look at the most interesting ones for my use cases, like &lt;a href="https://artificialanalysis.ai/evaluations/gpqa-diamond"&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt; &lt;/a&gt;(which only covers physics, biology, and chemistry) or &lt;a href="https://lastexam.ai/"&gt;Humanity's Last Exam (HLE)&lt;/a&gt;, the issues are glaring&lt;/p&gt; &lt;p&gt;HLE is supposed to be a rigorous benchmark, but it has a major flaw: the answers provided by LLMs are evaluated by... another LLM. This introduces bias and makes the results non-reproducible. How can we trust a benchmark where the judge is as fallible as the models being tested? We now know how LLMs are fallible : Research here showed that using LLMs as judges introduces significant biases and reliability issues. These models tend to favor responses that match their own style or position and struggle with detecting hallucinations without external verification &lt;a href="https://www.semanticscholar.org/paper/Humans-or-LLMs-as-the-Judge-A-Study-on-Judgement-Chen-Chen/a28071c63963cc59ba500cd00c140ac08eb5ccb0"&gt;[1]&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.07791"&gt;[2]&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Moreover, my first point stands as is in English, then, to be crude, its assessment of an LLM's skills is only relevant to about 20% of the world's population. It's a step up in difficulty, but far from a neutral or universally applicable benchmark, which then again marketing and the general peep tend to forget. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent era is a clusterf*ck&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The current trend of integrating tool calls into LLM outputs is creating a mess. Calling it simply function calls before agents was better. Then marketing kicked in. Also, there is no standardized template or protocol (MCP? Lol), making it evermore difficult to compare different tool usage by LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Proprietary platforms are the devil&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was a heavy consumer of gemini-2.5-pro 03-26, like.. addicted to it. Then removed in favour of a more code / math oriented model.. which was less better but ok. Then removed in favour of .. etc.&lt;/p&gt; &lt;p&gt;OpenAI just did the same things to consumers worldwide, and they even won't let them chose between models, and the nomenclature is even blurrier than ever .. According to the model sheet, the GPT-5 family consists of six separate models (gpt-5-main, gpt-5-main-mini, gpt-5-thinking, gpt-5-thinking-mini, gpt-5-thinking-nano, gpt-5-thinking-pro). Just.. omg just let your consumers choose. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Internet will implode with slop&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;There's no other considerations here to make other than there is an ever going increase of mess being generated. Dead Internet Theory holds more than ever and the new &lt;a href="https://blog.cloudflare.com/introducing-pay-per-crawl/"&gt;pay-per-crawl from cloudflare &lt;/a&gt;is a new artefact designing how the web space will be consumed. I seriously hope things will get better, but don't know how&lt;/p&gt; &lt;h1&gt;During this journey I've learned to keep it local and build my own benchmarks&lt;/h1&gt; &lt;p&gt;After all these observations, what I've concluded is that the most reliable approach is to keep LLMs local. After having headache on prompting the simplest use case of harmonizing academic texts with the models in the upper leaderboard of LMArena.. I'm finally back to my earlier loves of local LLMs. At least they don't change unexpectedly, and you control their configuration. More importantly, I needed to build my own benchmarks, individually, in which outputs are validated by myself. Public benchmarks have too many limitations and biases. The best approach is to create private, customized benchmarks tailored to our specific use cases. This way, we can ensure our evaluations are relevant, unbiased, and actually meaningful for our work.&lt;/p&gt; &lt;p&gt;This was cowritten with unsloth/Mistral-Small-3.2-24B-Instruct-2506 at Q_8. Thanks for the whole community for driving such a neat technology !&lt;/p&gt; &lt;p&gt;Edit: typos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T21:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml2r4t</id>
    <title>This is awkward</title>
    <updated>2025-08-08T18:17:14+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2r4t/this_is_awkward/"&gt; &lt;img alt="This is awkward" src="https://preview.redd.it/cf9lsdxk7uhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9f133c985c2883302323a855934de90605934f" title="This is awkward" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cf9lsdxk7uhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2r4t/this_is_awkward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml2r4t/this_is_awkward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T18:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
