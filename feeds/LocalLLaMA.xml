<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-08T23:22:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikjhn7</id>
    <title>Why are many SWEs salty about LLM use for coding?</title>
    <updated>2025-02-08T09:20:38+00:00</updated>
    <author>
      <name>/u/delicate_rabbit</name>
      <uri>https://old.reddit.com/user/delicate_rabbit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am SWE, and I'm using LLM on daily basis. It helps immensely. If I give it correct prompts/context it will spit out the methods/logic I need. It will generate complex SQL queries (if I need them) etc, etc. It will explain concepts I am not familiar with. It will even break down complex problems into digestable chunks where I can then form a whole picture of what I wanna do.&lt;/p&gt; &lt;p&gt;If I am unsure about the syntax/how I'd write some code, or hell even if I straight up don't know how to do it, it will give me the result or at least the direction. However I always, always check if it makes sense. I just don't blindly copy whatever it spits out. If it doesn't work, I fine tune it so it does.&lt;/p&gt; &lt;p&gt;So I am not sure why are so many shitting on it? &lt;/p&gt; &lt;p&gt;&amp;quot;You will forget how to do it yourself !&amp;quot; &lt;/p&gt; &lt;p&gt;Sure, the pure syntax/coding skills might get rustier, but if you can rely on it, evaluate the suggestion, so what? To me it is somewhat akin to saying: &amp;quot;your will forget how to create fire with 2 rocks because you are using the lighter!&amp;quot; If I understand what the end result should be does it matter that I used the lighter and know what fire does?&lt;/p&gt; &lt;p&gt;&amp;quot;AI gives me intern level results!&amp;quot;&lt;/p&gt; &lt;p&gt;Have you tried giving it a detailed prompt and context instead of a vague 5 word sentence before getting mad?&lt;/p&gt; &lt;p&gt;At the end of the day it's just a tool right? If you're getting the result, why does it matter how you got there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/delicate_rabbit"&gt; /u/delicate_rabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjhn7/why_are_many_swes_salty_about_llm_use_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikzlpx</id>
    <title>MLX Quants Vs GGUF</title>
    <updated>2025-02-08T22:31:56+00:00</updated>
    <author>
      <name>/u/BalaelGios</name>
      <uri>https://old.reddit.com/user/BalaelGios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm playing around with some MLX models lately (M3 Max), and have noticed the quants seem different I don't usually see qx_k_m or anything like that just literally 4bit/8bit and a few 6bit. And generally they are smaller than their GGUF counterparts. I know MLX is faster and that is definitely evident. &lt;/p&gt; &lt;p&gt;My question is; is an MLX model at the same quant lower quality than the GGUF model and if so how much lower quality are we talking? &lt;/p&gt; &lt;p&gt;While I haven't noticed anything particularly jarring yet, I'm curious to understand the differences, my assumption since discovering MLX is that, if you're on Apple silicone you should be using MLX. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BalaelGios"&gt; /u/BalaelGios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T22:31:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikw2xd</id>
    <title>Best creative local LLM for world building and creative writing? Fitting in 16gb VRAM?</title>
    <updated>2025-02-08T19:57:26+00:00</updated>
    <author>
      <name>/u/No_Expert1801</name>
      <uri>https://old.reddit.com/user/No_Expert1801</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know I mean creative as in not me having to be creative for it but rather it can come up with some great stuff itself. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Expert1801"&gt; /u/No_Expert1801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikw2xd/best_creative_local_llm_for_world_building_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikw2xd/best_creative_local_llm_for_world_building_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikw2xd/best_creative_local_llm_for_world_building_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn3m4</id>
    <title>Qwen in the mac menu bar</title>
    <updated>2025-02-08T13:20:00+00:00</updated>
    <author>
      <name>/u/Zealousideal-Net1385</name>
      <uri>https://old.reddit.com/user/Zealousideal-Net1385</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt; &lt;img alt="Qwen in the mac menu bar" src="https://external-preview.redd.it/noY1h5Okrq7ZANvEw5MMNJGxiH9lTWE3nW0dM83KMT8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e4ce9351cd888be2d92a844b79e910f95cc4e2d" title="Qwen in the mac menu bar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear all, I Developed this app for Mac OS and I need some testers; since I love Qwen family models I’ve developed this app to enrich the productivity, working as a floating window over other apps. Comments are really appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Net1385"&gt; /u/Zealousideal-Net1385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/andreaturchet/Qwen4Mac"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqsal</id>
    <title>Which models do you run locally?</title>
    <updated>2025-02-08T16:14:15+00:00</updated>
    <author>
      <name>/u/santhosh1993</name>
      <uri>https://old.reddit.com/user/santhosh1993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, if you are using a specific model heavily? which factors stood out for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/santhosh1993"&gt; /u/santhosh1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqsal/which_models_do_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikxf0r</id>
    <title>dataset creation with deepseek</title>
    <updated>2025-02-08T20:55:18+00:00</updated>
    <author>
      <name>/u/Eduard_T</name>
      <uri>https://old.reddit.com/user/Eduard_T</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if this is of any help but I've created a small script that takes questions from a CSV file and send them to deepseek API for answers. It outputs the results with separate columns for timestamp, question, thinking traces, answer as CSV, json and txt files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/EdwardDali/dset"&gt;https://github.com/EdwardDali/dset&lt;/a&gt;&lt;/p&gt; &lt;p&gt;are there other tools doing something like this for AI with reasoning? does distillation requires a different type of dataset?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eduard_T"&gt; /u/Eduard_T &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikxf0r/dataset_creation_with_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikxf0r/dataset_creation_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikxf0r/dataset_creation_with_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T20:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iksy75</id>
    <title>In Eleven Labs I can record a real voice performance and convert that performance into a different voice (and accent), is this possible locally yet?</title>
    <updated>2025-02-08T17:45:49+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Eleven Labs is the only service I'm paying for, mainly because of the feature I described in my title, is there an offline local alternative that is able to do this?&lt;/p&gt; &lt;p&gt;So far, I'm able to clone any voice I want, but I can't transfer a real performance. &lt;/p&gt; &lt;p&gt;Is this possible yet locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikek15</id>
    <title>My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload</title>
    <updated>2025-02-08T04:03:51+00:00</updated>
    <author>
      <name>/u/bo_peng</name>
      <uri>https://old.reddit.com/user/bo_peng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt; &lt;img alt="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" src="https://a.thumbs.redditmedia.com/Yk1sCOddUTNzMn5fOag5QoE4yeshLWIZhFYJ14-t3u0.jpg" title="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321"&gt;https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now waiting for 4060ti 16G to arrive. Requires lots of custom code to efficiently utilize this chimera setup :) So stay tuned. I think it can reach 10+ token/s for quantized 671B after optimizations.&lt;/p&gt; &lt;p&gt;You can use &amp;quot;ASUS Hyper M.2 x16 Gen5 Card&amp;quot; to host 4 NVME. And currently you need AMD CPUs to do native x4x4x4x4 bifurcation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bo_peng"&gt; /u/bo_peng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T04:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik98l2</id>
    <title>I haven't seen many quad GPU setups so here is one</title>
    <updated>2025-02-07T23:39:23+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt; &lt;img alt="I haven't seen many quad GPU setups so here is one" src="https://preview.redd.it/lav3msh9zshe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a16836ce930a2602c2271a999be6be9f7ee678b" title="I haven't seen many quad GPU setups so here is one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lav3msh9zshe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrf3</id>
    <title>OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp; also xAI | iLya &amp; elon both were OG cofounders of OpenAI</title>
    <updated>2025-02-08T09:40:24+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt; &lt;img alt="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" src="https://preview.redd.it/wojbv3ufyvhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672d5dd30929a3d9877d2f0489d521aeb4f36140" title="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wojbv3ufyvhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqy5w</id>
    <title>Building an LLM-Optimized Linux Server on a Budget</title>
    <updated>2025-02-08T16:21:16+00:00</updated>
    <author>
      <name>/u/Unprotectedtxt</name>
      <uri>https://old.reddit.com/user/Unprotectedtxt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt; &lt;img alt="Building an LLM-Optimized Linux Server on a Budget" src="https://external-preview.redd.it/9ar9q08cZR3Pnd8Hxka6_rkxXup9YdrkJJw3Y1V7agI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7da33d2c9a8a6d7c6f95f5206eb1b4796158fe2f" title="Building an LLM-Optimized Linux Server on a Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on these benchmarks wouldn’t buying a Mac Studio with 128 GB RAM M2 Ultra 60 or 72 core be far better than traditional dedicated PC builds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unprotectedtxt"&gt; /u/Unprotectedtxt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxblog.io/build-llm-linux-server-on-budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iko5y3</id>
    <title>How do the models always answer in correct English when much of the web has badly written and incorrect English?</title>
    <updated>2025-02-08T14:15:17+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how the training works to achieve this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik76bj</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-02-07T22:08:04+00:00</updated>
    <author>
      <name>/u/Singularity-42</name>
      <uri>https://old.reddit.com/user/Singularity-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/juqrkouwishe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df7513b32ac2840f23b550e538e3f7b2c6c6ca3" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularity-42"&gt; /u/Singularity-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/juqrkouwishe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T22:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikgsl6</id>
    <title>Germany: "We released model equivalent to R1 back in November, no reason to worry"</title>
    <updated>2025-02-08T06:14:46+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt; &lt;img alt="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" src="https://b.thumbs.redditmedia.com/mRAyAl4yIV32IK8PFM79kLhWfUBplZg1xnMZQcGaLLI.jpg" title="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ikgsl6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik162w</id>
    <title>Trump just said “no” DeepSeek does not pose a national security threat at a press conference</title>
    <updated>2025-02-07T17:57:11+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt; &lt;img alt="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" src="https://preview.redd.it/73sost17arhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d90041a44ec176a4b5a3630a47ce33577c957522" title="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73sost17arhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikguu9</id>
    <title>"Meta Torrented over 81 TB of Data Through Anna’s Archive, Despite Few Seeders"</title>
    <updated>2025-02-08T06:18:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyq45</id>
    <title>DeepSeek Gained over 100+ Millions Users in 20 days.</title>
    <updated>2025-02-08T21:52:58+00:00</updated>
    <author>
      <name>/u/blacktiger3654</name>
      <uri>https://old.reddit.com/user/blacktiger3654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since launching DeepSeek R1 on January 20, DeepSeek has gained over 100 million users, with $0 advertising or marketing cost. By February 1, its daily active users surpassed 30 million, making it the fastest application in history to reach this milestone. &lt;/p&gt; &lt;p&gt;Why? I also spend so much time chat with it, the profound answer, is the key reason for me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blacktiger3654"&gt; /u/blacktiger3654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iks9cl</id>
    <title>Notes on OpenAI o3-mini: How good is it compared to r1 and o1?</title>
    <updated>2025-02-08T17:16:46+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We finally have a reasonable reasoning model from OpenAI that has a reasonable cost; it must be Deepseek r1 impact. But anyway, we now have the first family of models from the o3 series. Also, It is the first reasoning model with official function-calling support.&lt;/p&gt; &lt;p&gt;Another interesting thing is that, unlike o1, we can now see the chain of thought (CoT). However, the CoT is not raw like Deepseek r1, but only a summarized version of it, and I am not sure why they are still keeping it under wraps.&lt;/p&gt; &lt;h1&gt;On pricing&lt;/h1&gt; &lt;p&gt;Perhaps the most highlighting aspect of the model is that it’s 15x cheaper than O1 with comparable performance and, in fact, better at times.&lt;/p&gt; &lt;p&gt;The fact that it is cheaper by 2x than even the GPT-4o is even more amusing. Then why do Chatgpt users have limited queries while GPT-4o has unlimited queries?&lt;/p&gt; &lt;p&gt;Did Deepseek force OpenAI to subsidize API costs?&lt;/p&gt; &lt;h1&gt;On performance&lt;/h1&gt; &lt;p&gt;To know if it actually is a better model than r1 and o1, I tested it on my benchmark questions for reasoning, Math, Coding, etc.&lt;/p&gt; &lt;p&gt;Here’s my observation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O3-mini-high is the best available model for reasoning tasks, apart from o1-pro.&lt;/li&gt; &lt;li&gt;For math, o1 and o3-mini-high are on par, a tad bit better than Deepseek r1.&lt;/li&gt; &lt;li&gt;Again, for coding, o3-mini-high felt better in my use cases but can vary from case to case. It is faster, so it is better to work with.&lt;/li&gt; &lt;li&gt;I can’t get over Deepseek r1 for creative writing, well, especially its CoT traces. I wish OpenAI would disclose the raw CoT in the coming models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is actually good, and given the costs, it’s much better than o1. I would’ve loved if they showed us the actual CoT, and I think a lot of people are now more interested in thought patterns than actual responses.&lt;/p&gt; &lt;p&gt;For in-depth analysis, commentary, and remarks on the OpenAI o3-mini and comparison with Deepseek r1, check out this blog post: &lt;a href="https://composio.dev/blog/openai-o3-mini-vs-deepseek-r1/"&gt;On OpenAI o3-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what have been your views and experiences with the o3-mini. How did you like it compared to Deepseek r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikh3vz</id>
    <title>OpenAI is hiding the actual thinking tokens in o3-mini</title>
    <updated>2025-02-08T06:35:07+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt; &lt;img alt="OpenAI is hiding the actual thinking tokens in o3-mini" src="https://external-preview.redd.it/7p74lVWOq7GrqAnrRqOdU-BQFqsEnbk8XozOhsD2YQc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65833b846107b44ca5029ac644441d85516e7126" title="OpenAI is hiding the actual thinking tokens in o3-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Yesterday I discovered that in fact you can not even with the current &amp;quot;thinking&amp;quot; tag on ChatGPT UI have the actual thinking tokens. In a way, the thinking of the model.&lt;/p&gt; &lt;p&gt;How did i know it? By running one single simple prompt about &amp;quot;maze puzzle&amp;quot; over the two models. The sample can be found in this dataset we built at our company. &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The prompt is a question about solving a maze: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 got it done in 483 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vc5mhbqxuhe1.png?width=1690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=611419e379856111070a3094ac3fbe7bac397381"&gt;Deepseek-R1 giving the correct result&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT 53 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yoxgrs11yuhe1.png?width=1706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f106a5f1553b8da728d9ca05dd32ce3702ecd5f"&gt;ChatGPT result also correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprisingly both results are correct but the devil is in the detail. When I inspect Deepseek-R1 thought process, it's very long, with step by step imagination and testing how it would go through the maze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 thinking tokens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/flt2emzlyuhe1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a16c88501392216f8998f3d97277996fac64a18"&gt;Deepseek-R1 thought process is literal pages and pages of thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT reason mode thinking tokens:&lt;/strong&gt;&lt;br /&gt; I will put the entire thinking tokens of ChatGPT here for everyone to inspect because it's much much shorter&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Let's break down the maze format. From what I'm seeing, the maze is made up of rows and columns, ranging from 0 to 4. Each cell is represented by its coordinates in the form &amp;lt;|row-col|&amp;gt;, followed by wall tokens like &amp;lt;|up_down_left_wall|&amp;gt; and &amp;lt;|blank|&amp;gt;. The walls specify which sides of each cell are blocked, and the origin and target tokens mark the start and end points. The way these tokens are structured seems important for determining the possible movements. Let’s work with this! Alright, I need to figure out where the origin and target are in the maze. I will look through the maze row by row to find them. Looking at row 1, it appears that the origin is at (1,3) and the target is at (1,2). We need to navigate from the origin (1,3) to the target (1,2). So now, it's just about predicting and outputting the individual moves one by one, separated by spaces. Let's figure out the path! I've figured out the maze's wall tokens, but before moving, I need to check if there's a direct path from the origin at (1,3) to the target at (1,2). The origin has walls on the up and left sides, according to the &amp;lt;|up_left_wall|&amp;gt; token. The target is to the left of the origin, and the cell next to the origin at (1,2) seems to have a wall on the right side, marked by &amp;lt;|down_right_wall|&amp;gt;. I’m thinking I’ll need to consider these walls carefully before predicting the next move. From what I’m seeing, the origin at (1,3) has walls on the left and up sides, so I can only move right or down. But moving left to reach the target is blocked because of the wall at (1,3). I checked the adjacency between (1,3) and (1,2), and it’s blocked by the walls in both cells. So, to get from (1,3) to (1,2), I'll need to find a detour. Let’s move down from (1,3) to (2,3) and explore. Okay, it looks like a direct route from (1,3) to (1,2) isn’t possible due to the walls blocking access. I checked various alternatives, including moving down and exploring different connections, but many of those led to dead ends. I've landed on a slightly longer route: get from (1,3) through (2,3) to (2,4), then to (3,4), and eventually backtracking westward while checking valid moves. After verifying the path through multiple neighbor checks, this looks like the best possible route. ANSWER: &amp;lt;|down|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|right|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Do you feel something is off about ChatGPT thought -&amp;gt; ANSWER?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very obvious that with the current thought process of ChatGPT reasoning model, &lt;strong&gt;it's glossing over so many details and somehow it can reach the result??&lt;/strong&gt; You basically cannot use the thinking tokens from ChatGPT to train a distilled thinking model because it is just not possible to reach the conclusion with those thinking, these are not actual thinking tokens of the models.&lt;/p&gt; &lt;p&gt;I have some hypothesis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI is only providing &amp;quot;summarized&amp;quot; version of their thinking tokens.&lt;/li&gt; &lt;li&gt;OpenAI has a model to output bullshit thinking tokens to distract everyone from training a distill model, so they can say they provide the community with the actual data, but there is no data.&lt;/li&gt; &lt;li&gt;They don't have &amp;quot;readable&amp;quot; thinking model but it's just an approximation of the &amp;quot;latent&amp;quot; thinking tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With the track record of OpenAI and ChatGPT, I am leaning towards &amp;quot;they are summarize or give bullshit thinking tokens&amp;quot; to the users more than they have more advanced model as option 3. Why? Because when I look at the UI it's obvious that the thought process is not outputting token per token but in chunk, which is either a summary, or a totally different model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;br /&gt; You can't just distill openAI model anymore, so don't assume everyone is distilling their model, &lt;strong&gt;THEY ARE CLOSED AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The full logs of both answers from ChatGPT and Deepseek-R1 can be found here:&lt;/strong&gt; &lt;a href="https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed"&gt;https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The maze dataset we build can be found here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn5fg</id>
    <title>Glyphstral-24b: Symbolic Deductive Reasoning Model</title>
    <updated>2025-02-08T13:22:41+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt; &lt;img alt="Glyphstral-24b: Symbolic Deductive Reasoning Model" src="https://external-preview.redd.it/DghrhJAW-NKneHTJvXZ7IAcBmIpZ_fU36ahUXITL0bM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad5bc295749b594e323b350912e29031d02d474" title="Glyphstral-24b: Symbolic Deductive Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone! &lt;/p&gt; &lt;p&gt;So I've been really obsessed lately with symbolic AI and the potential to improve reasoning and multi-dimensional thinking. I decided to go ahead and see if I could train a model to use a framework I am calling &amp;quot;Glyph Code Logic Flow&amp;quot;. &lt;/p&gt; &lt;p&gt;Essentially, it is a method of structured reasoning using deductive symbolic logic. You can learn more about it here &lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I first tried training Deepeek R1-Qwen-14 and QWQ-32 but their heavily pre-trained reasoning data seemed to conflict with my approach, which makes sense given the different concepts and ways of breaking down the problem.&lt;/p&gt; &lt;p&gt;I opted for Mistral-Small-24b to see the results, and after 7 days of pure training 24hrs a day (all locally using MLX-Dora at 4bit on my Mac M2 128GB). In all, the model trained on about 27mil tokens of my custom GCLF dataset (each example was around 30k tokens, with a total of 4500 examples)&lt;/p&gt; &lt;p&gt;I still need to get the docs and repo together, as I will be releasing it this weekend, but I felt like sharing a quick preview since this unexpectedly worked out awesomely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player"&gt;https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikt9an</id>
    <title>I Built lfind: A Natural Language File Finder Using LLMs</title>
    <updated>2025-02-08T17:58:53+00:00</updated>
    <author>
      <name>/u/Mahrkeenerh1</name>
      <uri>https://old.reddit.com/user/Mahrkeenerh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt; &lt;img alt="I Built lfind: A Natural Language File Finder Using LLMs" src="https://preview.redd.it/rwb26a0yeyhe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=653ea170e4f0049c8cfde86a3d70e9eb14484f48" title="I Built lfind: A Natural Language File Finder Using LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mahrkeenerh1"&gt; /u/Mahrkeenerh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwb26a0yeyhe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikp5ko</id>
    <title>GeForce RTX 5090 fails to topple RTX 4090 in GPU compute benchmark.</title>
    <updated>2025-02-08T15:02:14+00:00</updated>
    <author>
      <name>/u/el0_0le</name>
      <uri>https://old.reddit.com/user/el0_0le</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So uh. Anyone have a good reason to upgrade from 4090 to 5090? &lt;/p&gt; &lt;p&gt;VRAM? Power? Paper specs? Future updates? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el0_0le"&gt; /u/el0_0le &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GeForce-RTX-5090-fails-to-topple-RTX-4090-in-GPU-compute-benchmark-while-RTX-5080-struggles-against-RTX-4070-Ti.958334.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:02:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyclh</id>
    <title>My little setup grows</title>
    <updated>2025-02-08T21:36:13+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt; &lt;img alt="My little setup grows" src="https://preview.redd.it/ivuoew07izhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805b6dbac5a45f4acb9b1e6962f74ce91e1b6aaa" title="My little setup grows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ivuoew07izhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvnfx</id>
    <title>I really need to upgrade</title>
    <updated>2025-02-08T19:38:41+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt; &lt;img alt="I really need to upgrade" src="https://preview.redd.it/eto6oiq8xyhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be19e4af4ac7edf93efb804fef99881270152ec" title="I really need to upgrade" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eto6oiq8xyhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvo8a</id>
    <title>Your next home lab might have 48GB Chinese card😅</title>
    <updated>2025-02-08T19:39:39+00:00</updated>
    <author>
      <name>/u/Redinaj</name>
      <uri>https://old.reddit.com/user/Redinaj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/"&gt;https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Things are accelerating. China might give us all the VRAM we want. 😅😅👍🏼 Hope they don't make it illegal to import. For security sake, of course &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Redinaj"&gt; /u/Redinaj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:39:39+00:00</published>
  </entry>
</feed>
