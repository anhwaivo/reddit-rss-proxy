<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-17T00:26:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ir03mg</id>
    <title>Good Image to video model?</title>
    <updated>2025-02-16T19:21:05+00:00</updated>
    <author>
      <name>/u/Bitter-Good-2540</name>
      <uri>https://old.reddit.com/user/Bitter-Good-2540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to animate some images and photos, what are good models and tools to use with 16gb vram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-Good-2540"&gt; /u/Bitter-Good-2540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir03mg/good_image_to_video_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir03mg/good_image_to_video_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir03mg/good_image_to_video_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T19:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqymp1</id>
    <title>WebNN - Where we are and what's next</title>
    <updated>2025-02-16T18:20:30+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqymp1/webnn_where_we_are_and_whats_next/"&gt; &lt;img alt="WebNN - Where we are and what's next" src="https://external-preview.redd.it/7_8VmEJbDbXFQ-Jjcpqxa1LIANneIsyFa82Am6g1KVU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571127f4401bd5f2fe9bc99db2225fb42fff87bf" title="WebNN - Where we are and what's next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watched &lt;a href="https://www.youtube.com/watch?v=FoYBWzXCsmM&amp;amp;list=PLNYkxOF6rcIAEVKJ98bDkQRkwvO4grhnt&amp;amp;index=5"&gt;this talk&lt;/a&gt; about the WebNN API. Short-but-sweet overview of what to expect with local/on-device AI execution via browser (with CPU/GPU/NPU acceleration).&lt;/p&gt; &lt;p&gt;TL;DR: It's still early days, but it looks pretty exciting. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1dyekjxkjje1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426833da900b9eaea791bd7be40cae3b2b731395"&gt;https://preview.redd.it/o1dyekjxkjje1.png?width=988&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=426833da900b9eaea791bd7be40cae3b2b731395&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They've made a &lt;a href="https://microsoft.github.io/webnn-developer-preview/"&gt;demo page&lt;/a&gt; where you can run ONNX models via WebNN for image generation tasks, speech-to-text, etc. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87r0cakdkjje1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e69ee6c2c86d06f8af4a659433850371fcbb08c6"&gt;https://preview.redd.it/87r0cakdkjje1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e69ee6c2c86d06f8af4a659433850371fcbb08c6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's also this &lt;a href="https://webmachinelearning.github.io/webnn-status/"&gt;cool page&lt;/a&gt; where you can follow a live status of WebNN operator implementation. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wkn5aup3ljje1.png?width=1236&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b446fa18758d28f32111f381fd46e2e63ad884e"&gt;https://preview.redd.it/wkn5aup3ljje1.png?width=1236&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b446fa18758d28f32111f381fd46e2e63ad884e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We haven't looked much into local AI execution via browser (WASM, WebGPU, etc.) at RunLocal, because it feels slightly too early. &lt;/p&gt; &lt;p&gt;But if any of you have been tinkering with that stuff, please share your thoughts/stories about what it's like!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqymp1/webnn_where_we_are_and_whats_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqymp1/webnn_where_we_are_and_whats_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqymp1/webnn_where_we_are_and_whats_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir033d</id>
    <title>What's the best distributed LLM library?</title>
    <updated>2025-02-16T19:20:28+00:00</updated>
    <author>
      <name>/u/ghost_shaba7</name>
      <uri>https://old.reddit.com/user/ghost_shaba7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run some distributed training experiments, I have some promising results on a single machine but when it comes time to scale up I find setting up pipeline parallelism or tensor parallelism over a local network/internet from scratch is a pain in the ass. I have mainly tried pytorch distributed which works ok but is mainly for same network/datacenter setups has very bad fault tolerance and hivemind/petals has better resilience but some missing functionality and the code is very complex.&lt;/p&gt; &lt;p&gt;Know an easy, simple, scalable solution for running experiments across the internet? or even across multiple computers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghost_shaba7"&gt; /u/ghost_shaba7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir033d/whats_the_best_distributed_llm_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir033d/whats_the_best_distributed_llm_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir033d/whats_the_best_distributed_llm_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T19:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4srn</id>
    <title>More preconverted models for the Anemll library</title>
    <updated>2025-02-16T22:41:58+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just converted and uploaded Llama-3.2-1B-Instruct in both 2048 and 3072 context to &lt;a href="https://huggingface.co/collections/alexgusevski/anemll-converted-models-67b1ff3da727ad35ff31efbd"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Wanted to convert bigger models (context and size) but got some wierd errors, might try again next week or when the library gets updated again (0.1.2 doesn't fix my errors I think). Also there are some new models on the &lt;a href="https://huggingface.co/anemll"&gt;Anemll Huggingface&lt;/a&gt; aswell&lt;/p&gt; &lt;p&gt;Lmk if you have some specific llama 1 or 3b model you want to see although its a bit of hit or miss on my mac if I can convert them or not. Or try convert them yourself, its pretty straight forward but takes time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4srn/more_preconverted_models_for_the_anemll_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4srn/more_preconverted_models_for_the_anemll_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4srn/more_preconverted_models_for_the_anemll_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T22:41:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqufw5</id>
    <title>All models on open llm leaderboard re-evaluated with Math-Verify</title>
    <updated>2025-02-16T15:21:27+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We initially discovered the math evaluation issues when inspecting Qwen models, which had unusually low scores compared to the self-reported performance. After the Math-Verify introduction, the scores more than doubled for these models, showcasing previous severe underestimation of performance.&lt;/p&gt; &lt;p&gt;But Qwen models aren’t alone. Another major family affected is DeepSeek. After switching to Math-Verify, DeepSeek models almost tripled their scores! This is because their answers are typically wrapped in boxed (\boxed{}) notations which the old evaluator couldn’t extract.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqufw5/all_models_on_open_llm_leaderboard_reevaluated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqmwsl</id>
    <title>I pay for chatGPT (20 USD), I specifically use the 4o model as a writing editor. For this kind of task, am I better off using a local model instead?</title>
    <updated>2025-02-16T07:21:41+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't use chatGPT for anything else beyond editing my stories, as mentioned in the title, I only use the 4o model, and I tell it to edit my writing (stories) for grammar, and help me figure out better pacing, better approaches to explain a scene. It's like having a personal editor 24/7.&lt;/p&gt; &lt;p&gt;Am I better off using a local model for this kind of task? If so which one? I've got a 8GB RTX 3070 and 32 GB of RAM.&lt;/p&gt; &lt;p&gt;I'm asking since I don't use chatGPT for anything else. I used to use it for coding and used a better model, but I recently quit programming and only need a writer editor :) &lt;/p&gt; &lt;p&gt;Any model suggestions or system prompts are more than welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T07:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqykw5</id>
    <title>Why does Zed's new LLM work great on their IDE and terrible with VS Code's Continue</title>
    <updated>2025-02-16T18:18:25+00:00</updated>
    <author>
      <name>/u/1Blue3Brown</name>
      <uri>https://old.reddit.com/user/1Blue3Brown</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/"&gt; &lt;img alt="Why does Zed's new LLM work great on their IDE and terrible with VS Code's Continue" src="https://external-preview.redd.it/eiPSFu2d70ntwepfMWvAGG83QDICMu1kMOtYRYGJigI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85f0fc3750e907912278426f45f2b3e3f95adc25" title="Why does Zed's new LLM work great on their IDE and terrible with VS Code's Continue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zed recently created their own fine tuned version &lt;code&gt;Qwen2.5-Coder-7B&lt;/code&gt;(&lt;a href="https://zed.dev/blog/edit-prediction"&gt;Read more&lt;/a&gt;) and it works perfectly fine in Zed.&lt;br /&gt; Since the model is open source i tried to download it form &lt;a href="https://huggingface.co/zed-industries/zeta"&gt;Huggingface&lt;/a&gt;(via LM Studio) and using it inside VS Code with &lt;code&gt;Continue&lt;/code&gt; extension. But it works terribly. Suggests absolute nonsense.&lt;br /&gt; Tokens Size: &lt;code&gt;32768&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Model Name - &lt;code&gt;zed-industries_zeta&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Quantization - &lt;code&gt;Q6_K_L&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here's one example. The first screenshot is from the VS Code. I'm obviously trying to set up a pagination in my endpoint, autocomplete suggests to import things in the middle of the file(and things that i have already imported in the beginning of the file)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/smmgmqdnhjje1.png?width=724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7aff73cce9ecfc8da466431d9d37ccae3118a"&gt;VS Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However Zed suggests the correct code right away:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ccg8chdljje1.png?width=781&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a960968e289b302de82dcaae2565feed78769a5a"&gt;Zed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just is just one example, but pretty much every suggestion is like this.&lt;/p&gt; &lt;p&gt;Here's the screenshot of running model in LM Studio. I'm i doing something wrong? Is there a problem with &lt;code&gt;Continue&lt;/code&gt; extension?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iiteyhmkljje1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1fb4956c478f9c04d9e09c2f2dd28e6571af4c2"&gt;https://preview.redd.it/iiteyhmkljje1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1fb4956c478f9c04d9e09c2f2dd28e6571af4c2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Blue3Brown"&gt; /u/1Blue3Brown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir2cdv</id>
    <title>Does anyone else cycle through the different AI products? like chatgpt&gt;deepseek&gt;Qwen?</title>
    <updated>2025-02-16T20:55:35+00:00</updated>
    <author>
      <name>/u/DapperAd2798</name>
      <uri>https://old.reddit.com/user/DapperAd2798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So basically you ask for some new addition or library to your current files or code, then you take the answer first LLM gives and then plug it into DEEKSEEK and ask the same thing but this time check the code given for optimisation and efficiency then do the same thing in Qwen with the output of CHATGPT&amp;gt;DEEPSEEK then plug it all together and go back to Chatgpt ? u will be surprised to find the models sometimes fighting about what they think is best for the task u asked like &amp;quot;add functions to simulate fluidity on 2d Physics body / particles library&amp;quot; or &amp;quot;write some shaders&amp;quot; that do some specific task and how it integrates it into your code &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DapperAd2798"&gt; /u/DapperAd2798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir2cdv/does_anyone_else_cycle_through_the_different_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir2cdv/does_anyone_else_cycle_through_the_different_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir2cdv/does_anyone_else_cycle_through_the_different_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T20:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh3k1</id>
    <title>Meta's Brain-to-Text AI</title>
    <updated>2025-02-16T01:35:00+00:00</updated>
    <author>
      <name>/u/Particular-Sea2005</name>
      <uri>https://old.reddit.com/user/Particular-Sea2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta's groundbreaking research, conducted in collaboration with the Basque Center on Cognition, Brain and Language, marks a significant advancement in non-invasive brain-to-text communication. The study involved 35 healthy volunteers at BCBL, using both magnetoencephalography (MEG) and electroencephalography (EEG) to record brain activity while participants typed sentences[1][2]. Researchers then trained an AI model to reconstruct these sentences solely from the recorded brain signals, achieving up to 80% accuracy in decoding characters from MEG recordings - at least twice the performance of traditional EEG systems[2].&lt;/p&gt; &lt;p&gt;This research builds upon Meta's previous work in decoding image and speech perception from brain activity, now extending to sentence production[1]. The study's success opens new possibilities for non-invasive brain-computer interfaces, potentially aiding in restoring communication for individuals who have lost the ability to speak[2]. However, challenges remain, including the need for further improvements in decoding performance and addressing the practical limitations of MEG technology, which requires subjects to remain still in a magnetically shielded room[1].&lt;/p&gt; &lt;p&gt;Sources [1] Meta announces technology that uses AI and non-invasive magnetic ... &lt;a href="https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/"&gt;https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/&lt;/a&gt; [2] Using AI to decode language from the brain and advance our ... &lt;a href="https://ai.meta.com/blog/brain-ai-research-human-communication/"&gt;https://ai.meta.com/blog/brain-ai-research-human-communication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Sea2005"&gt; /u/Particular-Sea2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T01:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir43w5</id>
    <title>PLA shroud? Check. String supports? Check. 3x3090 on a budget.</title>
    <updated>2025-02-16T22:11:13+00:00</updated>
    <author>
      <name>/u/TyraVex</name>
      <uri>https://old.reddit.com/user/TyraVex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"&gt; &lt;img alt="PLA shroud? Check. String supports? Check. 3x3090 on a budget." src="https://a.thumbs.redditmedia.com/t5TlvfN7T0vbSmWFtVxNbBX-yzNFAYVWNo0jREYVLq4.jpg" title="PLA shroud? Check. String supports? Check. 3x3090 on a budget." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TyraVex"&gt; /u/TyraVex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ir43w5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir43w5/pla_shroud_check_string_supports_check_3x3090_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T22:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4724</id>
    <title>Which draft model for Mistral Small 2501?</title>
    <updated>2025-02-16T22:15:00+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't seem to find one that's compatible according to LMStudio. I'd really like an MLX version, but a GGUF one would work too.&lt;/p&gt; &lt;p&gt;I want to try out Mistral for a bit and see how it performs compared to Qwen 14b, which I'm currently using. Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir4724/which_draft_model_for_mistral_small_2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T22:15:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir08pp</id>
    <title>The scoop on 4060 ti 16gb cards</title>
    <updated>2025-02-16T19:26:59+00:00</updated>
    <author>
      <name>/u/DramaLlamaDad</name>
      <uri>https://old.reddit.com/user/DramaLlamaDad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I searched and saw a number of old posts on 4060's for LLM but nothing recent. I'm wondering why I don't hear more talk about 4060 ti 16gb cards as a budget option. Sure, they won't beat out 3090's but honestly, I've had zero luck finding finding these 3080 or 3090 used deals locally that other people mention. 4060 ti's are in stock and $500, 4 of them gets you to 64GB and not crazy power usage.&lt;/p&gt; &lt;p&gt;Am I missing something? Seems like these guys would be the darling of the local llm world if there wasn't a catch, so what's the catch? Yes, I know they won't perform on par with the other options but again, $$ per GB and tokens per watt, they feel like a decent spot.&lt;/p&gt; &lt;p&gt;So, anyone know why there isn't more chatter about these guys or the upcoming 5060's with 16gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DramaLlamaDad"&gt; /u/DramaLlamaDad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T19:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir6ha6</id>
    <title>DeepSeek-R1 CPU-only performances (671B , Unsloth 2.51bit, UD-Q2_K_XL)</title>
    <updated>2025-02-17T00:00:53+00:00</updated>
    <author>
      <name>/u/smflx</name>
      <uri>https://old.reddit.com/user/smflx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of us here like to run locally DeepSeek R1 (671B, not distill). Thanks to MoE nature of DeepSeek, CPU inference looks promising.&lt;/p&gt; &lt;p&gt;I'm testing on CPUs I have. Not completed yet, but would like to share &amp;amp; hear about other CPUs too.&lt;/p&gt; &lt;p&gt;Xeon w5-3435X has 195GB/s memory bandwidth (measured by stream)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Function Best Rate MB/s Avg time Min time Max time Copy: 195455.5 0.082330 0.081860 0.083633 Scale: 161245.0 0.100906 0.099228 0.105129 Add: 183597.3 0.131566 0.130721 0.137799 Triad: 181895.4 0.132163 0.131944 0.133183 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The active parameter or R1/V2 is 37B. So if Q4 used, theoretically 195 / 37 * 2 = 10.5 tok/s is possible.&lt;/p&gt; &lt;p&gt;Unsloth provided great quantizations from 1.58 ~ 2.51 bit. The generation speed could be more or less. (Actually less yet)&lt;/p&gt; &lt;p&gt;&lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tested both of 1.58 bit &amp;amp; 2.51 bit on few CPUs, now I stick to 2.51 bit. 2.51bit is better quality, surprisingly faster too.&lt;/p&gt; &lt;p&gt;I got 4.86 tok/s with 2.51bit, while 3.27 tok/s with 1.58bit, on Xeon w5-3435X (1570 total tokens). Also, 3.53 tok/s with 2.51bit, while 2.28 tok/s with 1.58bit, on TR pro 5955wx.&lt;/p&gt; &lt;p&gt;It means compute performance of CPU matters too, and slower with 1.58bit. So, use 2.51bit unless you don't have enough RAM. 256G RAM was enough to run 2.51 bit.&lt;/p&gt; &lt;p&gt;I have tested generation speed with llama.cpp using (1) prompt &amp;quot;hi&amp;quot;, and (2) &amp;quot;Write a python program to print the prime numbers under 100&amp;quot;. Number of tokens generated were (1) about 100, (2) 1500~5000.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama.cpp/build/bin/llama-cli --model DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf --cache-type-k q4_0 --threads 16 --prio 2 --temp 0.6 --ctx-size 8192 --seed 3407&lt;/code&gt;&lt;/p&gt; &lt;p&gt;OK, here is Table.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Cores (CCD)&lt;/th&gt; &lt;th align="left"&gt;COPY (GB/s)&lt;/th&gt; &lt;th align="left"&gt;TRIAD (GB/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;hi&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp&amp;quot;coding&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;kTransformer (tok/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;w5-3435X&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;195&lt;/td&gt; &lt;td align="left"&gt;181&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;4.86&lt;/td&gt; &lt;td align="left"&gt;8.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5955wx&lt;/td&gt; &lt;td align="left"&gt;16 (2)&lt;/td&gt; &lt;td align="left"&gt;96&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;4.29&lt;/td&gt; &lt;td align="left"&gt;3.53&lt;/td&gt; &lt;td align="left"&gt;7.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7F32&lt;/td&gt; &lt;td align="left"&gt;8 (8)&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;86&lt;/td&gt; &lt;td align="left"&gt;3.39&lt;/td&gt; &lt;td align="left"&gt;3.24&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9184X&lt;/td&gt; &lt;td align="left"&gt;16 (8)&lt;/td&gt; &lt;td align="left"&gt;298&lt;/td&gt; &lt;td align="left"&gt;261&lt;/td&gt; &lt;td align="left"&gt;7.52&lt;/td&gt; &lt;td align="left"&gt;4.82&lt;/td&gt; &lt;td align="left"&gt;11.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9534&lt;/td&gt; &lt;td align="left"&gt;64 (8)&lt;/td&gt; &lt;td align="left"&gt;351&lt;/td&gt; &lt;td align="left"&gt;276&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;7.26&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6426Y (2P)&lt;/td&gt; &lt;td align="left"&gt;16+16&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;coming...&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I expected a poor performance of 5955wx, because it has only two CCDs. We can see low memory bandwidth in the table. But, not much difference of performance compared to w5-3435X. Perhaps, compute matters too &amp;amp; memory bandwidth is not saturated in Xeon w5-3435X.&lt;/p&gt; &lt;p&gt;I have checked performance of kTransformer too. It's CPU inference with 1 GPU for compute bound process. While is not pure CPU inference, the performance gain is almost 2x. I didn't tested for all CPU yet, you can assume 2x performance to CPU-only llama.cpp.&lt;/p&gt; &lt;p&gt;With kTransformer, GPU was not saturated but CPU was all busy. I guess one 3090 or 4090 will be enough. One downside of kTransformer is that the context length is limited by VRAM.&lt;/p&gt; &lt;p&gt;The blanks are &amp;quot;not tested yet&amp;quot;. It takes time... Well, I'm testing two Genoa CPUs with only one mainboard.&lt;/p&gt; &lt;p&gt;I would like to hear about other CPUs. Maybe, I will update the table.&lt;/p&gt; &lt;p&gt;Note: I will update &amp;quot;how I checked memory bandwidth using stream&amp;quot;, if you want to check with the same setup. I couldn't get the memory bandwidth numbers I have seen here. My test numbers are lower.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smflx"&gt; /u/smflx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T00:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvmba</id>
    <title>The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig</title>
    <updated>2025-02-16T16:14:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt; &lt;img alt="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" src="https://b.thumbs.redditmedia.com/gFgeC1rJT-bzG1Ahait3F528S102s2yDDa6EevYG4iI.jpg" title="The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve wanted to build a quad 3090 server for llama.cpp/Open WebUI for a while now, but massive shrouds really hampered those efforts. There are very few blower style RTX 3090 out there. They typically cost more than RTX 4090. Experimentation with DeepSeek makes the thought of loading all those weights via x1 risers a nightmare. Already suffering with native x1 on CMP 100-210 trying to offload DeepSeek weights to 6 GPUs.&lt;/p&gt; &lt;p&gt;Also thinking with some systems with 7-8 x16 lane support, upto 32gpu on x4 is entirely possible. DeepSeek fp8 fully GPU powered on a ~$30k retail mostly build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iqvmba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqvmba/the_dry_fit_of_oculink_4x4x4x4_for_rtx_3090_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T16:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1gh</id>
    <title>Why we don't use RXs 7600 XT?</title>
    <updated>2025-02-16T09:57:52+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This GPU has probably cheapest VRAM out there. $330 for 16gb is crazy value, but most people use RTXs 3090 which cost ~$700 on a used market and draw significantly more power. I know that RTXs are better for other tasks, but as far as I know, only important thing in running LLMs is VRAM, especially capacity. Or there's something I don't know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp1gh/why_we_dont_use_rxs_7600_xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:57:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1g6k</id>
    <title>Real AGI is an AI that learns on the fly , has long term memory and other features</title>
    <updated>2025-02-16T20:17:20+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People have been defining AGI as an AI that does all tasks a human can do, but even humans can't do every human task at the same level as an average human worker in that field. Instead humans can learn continously and on the fly to do a task and it takes time to be an expert. We shouldn't expect an AGI to do all tasks in the beginning, but rather it has long term and short term memory to learn to do different tasks and remember it for a later time. Unimportant tasks can be forgotten after the task is completed like short term memory. AGI might need to rest like humans to train and consolidate recently learned tasks into long term memory and to improve its abilities. It should be able to learn by itself constantly and discover how to improve itself. Learning to do many tasks is a corollary of being AGI IE having continuous learning abilities and long term memory, not a prerequisite for AGI. In addition, a human has crystal and fluid intelligence, the abilties to solve problems through memorized patterns and adaptive learning from minimal examples. Right now, AI has some crystalized intelligence via trained datasets and applying reasoning through learned examples. But AI's fluid intelligence is weak, their ability to learn and solve problems with minimal examples is weak. Kinesthetic intelligence is an intelligence too, this requires simulation(like Cosmos) and training in the real world to develop...Humans have social intelligence too.. What about AGI having the ability communicating and guiding other ais? Humans have various desires and drives and an instinct to preserve itself and to reproduce... Likely a human, an AGI should have desires. And perhaps the desire for self-preservation and the ability to self-reproduce or produce a newer better copy.(maybe optional) Also, a human has 150 trillion synapses in the size of a grapefruit and portable, whereas AI requires multiple large stationary GPUS. An ideal AGI should be human brain sized or at least no bigger than an Elephant's brain. This AGI size would require neuromorphic memristors or biocomputing or a compact quantum computer or something else. Humans have a brain stem and a secondary nervous system for automated bodily functions and instincts... Perhaps AGI should also have that...&lt;/p&gt; &lt;p&gt;Being an expert at something is not necessary for AGI as majority of humans aren't experts at anything except for a few... Even those who are experts have specialized domain knowledge. AGi should be average in a few fields at the start then learn to become an expert in various fields.&lt;/p&gt; &lt;p&gt;In conclusion, a true AGI should have these features above.( The most important is the ability and the desire to learn on the fly and continuously and have long term memory!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T20:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp2dd</id>
    <title>I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B</title>
    <updated>2025-02-16T09:59:45+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt; &lt;img alt="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" src="https://external-preview.redd.it/dXNvamJ5ZjQ0aGplMYWu7AlJhgav8Lym1d_sC2ZIykYlrs6Ptnxf7yQvwsov.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8bdc25d398763f3bc1abe33281d773d4310de4" title="I made a UI Reasoning model with 7b parameters with only 450 lines of data. UIGEN-T1-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ro798yf44hje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqp2dd/i_made_a_ui_reasoning_model_with_7b_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T09:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqugti</id>
    <title>Kernel refinement via LLM</title>
    <updated>2025-02-16T15:22:39+00:00</updated>
    <author>
      <name>/u/BreakIt-Boris</name>
      <uri>https://old.reddit.com/user/BreakIt-Boris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt; &lt;img alt="Kernel refinement via LLM" src="https://external-preview.redd.it/Ud40tCnkvgrTgsbDjMTILGOg7G9SqNJ-hrdg_SDxvWo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b1994afccbdb6a19230f6779589edce8ca823a5" title="Kernel refinement via LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly haven't seen this mentioned as of yet. Has left me quite speechless tbh. &lt;/p&gt; &lt;p&gt;&amp;quot;This closed-loop approach makes the code generation process better by guiding it in a different way each time. The team found that by letting this process continue for 15 minutes resulted in an improved attention kernel. &lt;/p&gt; &lt;p&gt;A bar chart showing averaged attention kernel speedup on Hopper GPU, compares the speedup of different attention kernel types between two approaches: 'PyTorch API (Flex Attention)' in orange and 'NVIDIA Workflow with DeepSeek-R1' in green. The PyTorch API maintains a baseline of 1x for all kernels, while the NVIDIA Workflow with DeepSeek-R1 achieves speedups of 1.1x for Causal Mask and Document Mask, 1.5x for Relative Positional, 1.6x for Alibi Bias and Full Mask, and 2.1x for Softcap. Figure 3. Performance of automatically generated optimized attention kernels with flex attention This workflow produced numerically correct kernels for 100% of Level-1 problems and 96% of Level-2 problems, as tested by Stanford’s KernelBench benchmark. ‌&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakIt-Boris"&gt; /u/BreakIt-Boris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqugti/kernel_refinement_via_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqysmn</id>
    <title>Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more</title>
    <updated>2025-02-16T18:27:29+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"&gt; &lt;img alt="Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more" src="https://preview.redd.it/lbzg9mvtnjje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62d7841f479d8e3b864893f1c84ad8bff30ec3ca" title="Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The attached image isn't from my setup specifically, but is the exact same issue. I have 3x 3090s in this machine.&lt;/p&gt; &lt;p&gt;Whenever I add the third card, one has this error and completely stops working for inference. It also usually triggers when I load a model.&lt;/p&gt; &lt;p&gt;After shuffling the cards around, I originally thought I had a bad PCIE slot (there are 8 on the Asrock Romed 8 2T) but no matter what I did, no matter what slot I picked or what GPU I had in it, there would always be one that failed. &lt;/p&gt; &lt;p&gt;I tested each GPU independently and it had no issues with any of them, reinstalled Ubuntu server, changed several driver versions etc. Nothing has worked yet. My power supply is 1600W and should have no issues delivering power to each card.&lt;/p&gt; &lt;p&gt;Has anyone had this issue? I'm out of ideas and so is the Internet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbzg9mvtnjje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqysmn/multigpu_rig_shows_err_in_nvidiasmi_when_i_have_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqynut</id>
    <title>Audiobook Creator - Releasing Version 2</title>
    <updated>2025-02-16T18:21:51+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Followup to my original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm releasing a version 2 of my open source project with cool new features !&lt;/p&gt; &lt;p&gt;Checkout sample multi voice audio for a short story : &lt;a href="https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook"&gt;https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Added Key Features&lt;/strong&gt;:&lt;br /&gt; ✅ &lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;br /&gt; ✅ &lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text. Uses calibre for better formatted text and wider compatibility.&lt;br /&gt; ✅ &lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B. Uses ffmpeg for wider format support. &lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Better narration&lt;/strong&gt;: Reads out only the dialogue in a different voice instead of the entire line in that voice. Also, improves single voice narration with a different dialogue voice from the narrator's voice. &lt;/p&gt; &lt;p&gt;✅ Automatically identifies chapters and adds some silence on audio end to mark its ending.&lt;/p&gt; &lt;p&gt;✅ Improved instructions and prompting while running the scripts for better clarity.&lt;/p&gt; &lt;p&gt;Github Repo Link: &lt;a href="https://github.com/prakharsr/audiobook-creator/"&gt;https://github.com/prakharsr/audiobook-creator/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try out the sample M4B audiobook with cover, chapter timestamps and metadata: &lt;a href="https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b"&gt;https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More new features coming soon !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T18:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir362v</id>
    <title>Jankiest &lt;$1000 70B IQ3_M 8192ctx setup ever</title>
    <updated>2025-02-16T21:30:31+00:00</updated>
    <author>
      <name>/u/spokale</name>
      <uri>https://old.reddit.com/user/spokale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt; &lt;img alt="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" src="https://b.thumbs.redditmedia.com/44X6DS_-957GXg-I7vPFOA1akYTRMttqhRfJ-mtdpgk.jpg" title="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokale"&gt; /u/spokale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ir362v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtfy9</id>
    <title>Just a bunch of H100s required</title>
    <updated>2025-02-16T14:33:57+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt; &lt;img alt="Just a bunch of H100s required" src="https://b.thumbs.redditmedia.com/CMhWZRD3a6zl90Wagyddf6mqUWPT3h6kxFjzeLVrOCc.jpg" title="Just a bunch of H100s required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6"&gt;https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv5s0</id>
    <title>Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC.</title>
    <updated>2025-02-16T15:54:42+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt; &lt;img alt="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." src="https://preview.redd.it/asmx7nh0wije1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05a245ed92468ec7ad3869e7776b9c2d8b8e5f63" title="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asmx7nh0wije1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir3rsl</id>
    <title>Inference speed of a 5090.</title>
    <updated>2025-02-16T21:56:45+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've rented the 5090 on vast and ran my benchmarks (I'll probably have to make a new bech test with more current models but I don't want to rerun all benchs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 5090 is &amp;quot;only&amp;quot; 50% faster in inference than the 4090 (a much better gain than it got in gaming)&lt;/p&gt; &lt;p&gt;I've noticed that the inference gains are almost proportional to the ram speed till the speed is &amp;lt;1000 GB/s then the gain is reduced. Probably at 2TB/s the inference become GPU limited while when speed is &amp;lt;1TB it is vram limited.&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:56:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpzpk</id>
    <title>8x RTX 3090 open rig</title>
    <updated>2025-02-16T11:04:58+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt; &lt;img alt="8x RTX 3090 open rig" src="https://preview.redd.it/sx3t2omvghje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8156846c180a3c1bdf1f4c1dceba69bdbf7a6a6" title="8x RTX 3090 open rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The whole length is about 65 cm. Two PSUs 1600W and 2000W 8x RTX 3090, all repasted with copper pads Amd epyc 7th gen 512 gb ram Supermicro mobo&lt;/p&gt; &lt;p&gt;Had to design and 3D print a few things. To raise the GPUs so they wouldn't touch the heatsink of the cpu or PSU. It's not a bug, it's a feature, the airflow is better! Temperatures are maximum at 80C when full load and the fans don't even run full speed.&lt;/p&gt; &lt;p&gt;4 cards connected with risers and 4 with oculink. So far the oculink connection is better, but I am not sure if it's optimal. Only pcie 4x connection to each. &lt;/p&gt; &lt;p&gt;Maybe SlimSAS for all of them would be better? &lt;/p&gt; &lt;p&gt;It runs 70B models very fast. Training is very slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sx3t2omvghje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T11:04:58+00:00</published>
  </entry>
</feed>
