<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-06T04:07:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1js335l</id>
    <title>Karamaru - An "Edo period" LLM trained on 17th-19th century japanese literature.</title>
    <updated>2025-04-05T13:09:39+00:00</updated>
    <author>
      <name>/u/nomad_lw</name>
      <uri>https://old.reddit.com/user/nomad_lw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt; &lt;img alt="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this a few days ago where a researcher from Sakana AI continually pretrained a Llama-3 Elyza 8B model on classical japanese literature. &lt;/p&gt; &lt;p&gt;What's cool about is that it builds towards an idea that's been brewing on my mind and evidently a lot of other people here,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A model that's able to be a Time-travelling subject matter expert. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Researcher's tweet: &lt;a href="https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19"&gt;https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomad_lw"&gt; /u/nomad_lw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/karamaru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsfou2</id>
    <title>Llama 4 is out and I'm disappointed</title>
    <updated>2025-04-05T22:40:27+00:00</updated>
    <author>
      <name>/u/kaizoku156</name>
      <uri>https://old.reddit.com/user/kaizoku156</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"&gt; &lt;img alt="Llama 4 is out and I'm disappointed" src="https://preview.redd.it/njtxgkmpg3te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51458acf99f28f812ac17fc8cd5e71aeaafea899" title="Llama 4 is out and I'm disappointed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;maverick costs 2-3x of gemini 2.0 flash on open router, scout costs just as much as 2.0 flash and is worse. deepseek r2 is coming, qwen 3 is coming as well, and 2.5 flash would likely beat everything in value for money and it'll come out in next couple of weeks max. I'm a little.... disappointed, all this and the release isn't even locally runnable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaizoku156"&gt; /u/kaizoku156 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njtxgkmpg3te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfou2/llama_4_is_out_and_im_disappointed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T22:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0g38</id>
    <title>Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order</title>
    <updated>2025-04-05T10:27:38+00:00</updated>
    <author>
      <name>/u/Marcuss2</name>
      <uri>https://old.reddit.com/user/Marcuss2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt; &lt;img alt="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" src="https://external-preview.redd.it/bKa6_zcgR56sbwG-Cqtu_jN8tcBni2YVCOOrskJ4IzI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d47a097527e5e5e57498d3ac6192eb2f6741fe55" title="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marcuss2"&gt; /u/Marcuss2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T10:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbjmm</id>
    <title>Llama reasoning soon and llama 4 behemoth</title>
    <updated>2025-04-05T19:31:35+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"&gt; &lt;img alt="Llama reasoning soon and llama 4 behemoth" src="https://preview.redd.it/m1tookk0j2te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=346d6c2a99dd5be293d57eccf5ee3cc161e4faea" title="Llama reasoning soon and llama 4 behemoth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1tookk0j2te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbjr3</id>
    <title>Turn local and private repos into prompts in one click with the gitingest VS Code Extension!</title>
    <updated>2025-04-05T19:31:44+00:00</updated>
    <author>
      <name>/u/Sanjuwa</name>
      <uri>https://old.reddit.com/user/Sanjuwa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"&gt; &lt;img alt="Turn local and private repos into prompts in one click with the gitingest VS Code Extension!" src="https://external-preview.redd.it/Zml5d2NtOGdpMnRlMVdzmEgr56-P0X2MdMr8l29_GfTj5L1NSLkdzC5Bz-eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08af159dd0608d37536754708bb572a540be16a2" title="Turn local and private repos into prompts in one click with the gitingest VS Code Extension!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;First of thanks to &lt;a href="/u/MrCyclopede"&gt;u/MrCyclopede&lt;/a&gt; for amazing work !!&lt;/p&gt; &lt;p&gt;Initially, I converted the his original Python code to TypeScript and then built the extension.&lt;/p&gt; &lt;p&gt;It's simple to use.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open the Command Palette (&lt;code&gt;Ctrl+Shift+P&lt;/code&gt; or &lt;code&gt;Cmd+Shift+P&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Type &amp;quot;Gitingest&amp;quot; to see available commands: &lt;ul&gt; &lt;li&gt;&lt;code&gt;Gitingest: Ingest Local Directory&lt;/code&gt;: Analyze a local directory&lt;/li&gt; &lt;li&gt;&lt;code&gt;Gitingest: Ingest Git Repository&lt;/code&gt;: Analyze a remote Git repository&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the prompts to select a directory or enter a repository URL&lt;/li&gt; &lt;li&gt;View the results in a new text document&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d love for you to check it out and share your feedback:&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lakpahana/export-to-llm-gitingest"&gt;https://github.com/lakpahana/export-to-llm-gitingest&lt;/a&gt; ( please give me a 🌟)&lt;br /&gt; Marketplace: &lt;a href="https://marketplace.visualstudio.com/items?itemName=lakpahana.export-to-llm-gitingest"&gt;https://marketplace.visualstudio.com/items?itemName=lakpahana.export-to-llm-gitingest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your thoughts—any feedback or suggestions would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sanjuwa"&gt; /u/Sanjuwa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6s9t5n5gi2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jscjex</id>
    <title>Llama 4 is the first major model hosted on Hugging Face using Xet</title>
    <updated>2025-04-05T20:15:51+00:00</updated>
    <author>
      <name>/u/jsulz</name>
      <uri>https://old.reddit.com/user/jsulz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt; &lt;img alt="Llama 4 is the first major model hosted on Hugging Face using Xet" src="https://external-preview.redd.it/JGJA9bp-fjQd-DW0Up7N-YoOwvacHf3g0ERwmbk7qZg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f2e4f629da6082e9745bb113ccba95ee01d469" title="Llama 4 is the first major model hosted on Hugging Face using Xet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just dropped Llama 4, and the Xet team has been working behind the scenes to make sure it’s fast and accessible for the entire HF community.&lt;/p&gt; &lt;p&gt;Here’s what’s new:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All Llama 4 models on Hugging Face use the Xet backend&lt;/strong&gt; — a chunk-based storage system built for large AI models.&lt;/li&gt; &lt;li&gt;This enabled us to upload &lt;strong&gt;terabyte-scale model weights in record time&lt;/strong&gt;, and it’s already making downloads faster too.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deduplication hits ~25%&lt;/strong&gt; on base models, and we expect to see at least &lt;strong&gt;40%&lt;/strong&gt; for fine-tuned or quantized variants. That means less bandwidth, faster sharing, and smoother collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We built Xet for this moment, to give model builders and users a better way to version, share, and iterate on large models without the Git LFS pain.&lt;/p&gt; &lt;p&gt;Here’s a quick snapshot of the impact on a few select repositories 👇&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oldvpapxu2te1.png?width=1025&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae6c5cc8c3dbbd339e8aba12a634bfa84b8564eb"&gt;https://preview.redd.it/oldvpapxu2te1.png?width=1025&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae6c5cc8c3dbbd339e8aba12a634bfa84b8564eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what models you’re fine-tuning or quantizing from Llama 4. We’re continuing to optimize the storage layer so you can go from “I’ve got weights” to “it’s live on the Hub” faster than ever.&lt;/p&gt; &lt;p&gt;Related blog post: &lt;a href="https://huggingface.co/blog/llama4-release"&gt;https://huggingface.co/blog/llama4-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsulz"&gt; /u/jsulz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T20:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsl37d</id>
    <title>I'm incredibly disappointed with Llama-4</title>
    <updated>2025-04-06T03:32:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt; &lt;img alt="I'm incredibly disappointed with Llama-4" src="https://external-preview.redd.it/b3VzazkxdGp3NHRlMTiXzVylw52_brdFuwA7wsavAEq_X08g0pyKKuMnYACK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88928fe424454ee437c9d4980fe757da729bb781" title="I'm incredibly disappointed with Llama-4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished my KCORES LLM Arena tests, adding Llama-4-Scout &amp;amp; Llama-4-Maverick to the mix.&lt;br /&gt; My conclusion is that they completely surpassed my expectations... in a negative direction.&lt;/p&gt; &lt;p&gt;Llama-4-Maverick, the 402B parameter model, performs roughly on par with Qwen-QwQ-32B in terms of coding ability. Meanwhile, Llama-4-Scout is comparable to something like Grok-2 or Ernie 4.5...&lt;/p&gt; &lt;p&gt;You can just look at the &amp;quot;20 bouncing balls&amp;quot; test... the results are frankly terrible / abysmal.&lt;/p&gt; &lt;p&gt;Considering Llama-4-Maverick is a massive 402B parameters, why wouldn't I just use DeepSeek-V3-0324? Or even Qwen-QwQ-32B would be preferable – while its performance is similar, it's only 32B.&lt;/p&gt; &lt;p&gt;And as for Llama-4-Scout... well... let's just leave it at that / use it if it makes you happy, I guess... Meta, have you truly given up on the coding domain? Did you really just release vaporware?&lt;/p&gt; &lt;p&gt;Of course, its multimodal and long-context capabilities are currently unknown, as this review focuses solely on coding. I'd advise looking at other reviews or forming your own opinion based on actual usage for those aspects. In summary: I strongly advise against using Llama 4 for coding. Perhaps it might be worth trying for long text translation or multimodal tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pou7a1tjw4te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsl37d/im_incredibly_disappointed_with_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T03:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsalxn</id>
    <title>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</title>
    <updated>2025-04-05T18:51:11+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt; &lt;img alt="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" src="https://external-preview.redd.it/5GYklgQz-p1iWSTGvDsKHeD_QUDxP-9vHZQeXTsgRz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f85ee3e9068eb521d7e3ef4dce3cee7c471c03" title="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsadt3</id>
    <title>Llama4 Released</title>
    <updated>2025-04-05T18:41:26+00:00</updated>
    <author>
      <name>/u/latestagecapitalist</name>
      <uri>https://old.reddit.com/user/latestagecapitalist</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/latestagecapitalist"&gt; /u/latestagecapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jseqbs</id>
    <title>Llama 4 scout is not doing well in "write a raytracer" code creativity benchmark</title>
    <updated>2025-04-05T21:54:44+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt; &lt;img alt="Llama 4 scout is not doing well in &amp;quot;write a raytracer&amp;quot; code creativity benchmark" src="https://external-preview.redd.it/c292Ets96l9SIyQ2uIjvrw2seHBUtAYlhc01uwhFHEU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14a26df29ae2a87e5ac7be16d882a5cac76e40c4" title="Llama 4 scout is not doing well in &amp;quot;write a raytracer&amp;quot; code creativity benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"&gt;previously experimented&lt;/a&gt; with a code creativity benchmark where I asked LLMs to write a small python program to create a raytraced image.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;code&gt;Write a raytracer that renders an interesting scene with many colourful lightsources in python. Output a 800x600 image as a png&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I only allowed one shot, no iterative prompting to solve broken code. I think execute the program and evaluate the imagine. It turns out this is a proxy for code creativity.&lt;/p&gt; &lt;p&gt;In the mean time I tested some new models: LLama 4 scout, Gemini 2.5 exp and Quasar Alpha&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ruh9dufe83te1.png?width=1367&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08bd5968b9ecdc3568380e3c3d1a67a30ce3a005"&gt;https://preview.redd.it/ruh9dufe83te1.png?width=1367&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08bd5968b9ecdc3568380e3c3d1a67a30ce3a005&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLama4 scout underwhelms in quality of generated images compared to the others. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/egq5ugj883te1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5132f98a77b707d8353c4478047dc48b9f4c06c"&gt;https://preview.redd.it/egq5ugj883te1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5132f98a77b707d8353c4478047dc48b9f4c06c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, there is some magic sauce in the fine-tuning of DeepSeek V3-0324, Sonnet 3.7 and Gemini 2.5 Pro that makes them create longer and more varied programs. I assume it is a RL step. Really fascinating, as it seems not all labs have caught up on this yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cpldcpu/llmbenchmark"&gt;Repository here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsitob</id>
    <title>Llama 4 Maverick Testing - 400B</title>
    <updated>2025-04-06T01:22:07+00:00</updated>
    <author>
      <name>/u/YakFull8300</name>
      <uri>https://old.reddit.com/user/YakFull8300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have no idea what they did to this model post training but it's not good. The output for writing is genuinely bad (seriously enough with the emojis) and it misquotes everything. Feels like a step back compared to other recent releases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YakFull8300"&gt; /u/YakFull8300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsitob/llama_4_maverick_testing_400b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T01:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbseu</id>
    <title>Llama4 Scout downloading</title>
    <updated>2025-04-05T19:42:45+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"&gt; &lt;img alt="Llama4 Scout downloading" src="https://preview.redd.it/5nx0y06wk2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=895ab95c094f3843276b8881066b3a8eb61a7d34" title="Llama4 Scout downloading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama4 Scout downloading 😁👍&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nx0y06wk2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsafqw</id>
    <title>Llama 4 announced</title>
    <updated>2025-04-05T18:43:42+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://www.llama.com/llama4/"&gt;https://www.llama.com/llama4/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsfm5j</id>
    <title>Potential Llama 4.2 - 7b</title>
    <updated>2025-04-05T22:36:53+00:00</updated>
    <author>
      <name>/u/medcanned</name>
      <uri>https://old.reddit.com/user/medcanned</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release, I got curious and looked around the implementation code of the Llama4 models in transformers and found something interesting:&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = Llama4ForCausalLM.from_pretrained(&amp;quot;meta-llama4/Llama4-2-7b-hf&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Given the type of model, it will be text-only. So, we just have to be patient :)&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/huggingface/transformers/blob/9bfae2486a7b91dc6d4380b7936e0b2b8c1ed708/src/transformers/models/llama4/modeling_llama4.py#L997"&gt;https://github.com/huggingface/transformers/blob/9bfae2486a7b91dc6d4380b7936e0b2b8c1ed708/src/transformers/models/llama4/modeling_llama4.py#L997&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medcanned"&gt; /u/medcanned &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T22:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsgliv</id>
    <title>it looks like Meta's new model's key innovation of "interleaved no-RoPE attention" for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago.</title>
    <updated>2025-04-05T23:24:45+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt; &lt;img alt="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." src="https://preview.redd.it/7dyflct7o3te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9183b8c88d6a952ada033ccc2507a72f82046e45" title="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7dyflct7o3te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T23:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdtew</id>
    <title>Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models</title>
    <updated>2025-04-05T21:12:16+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt; &lt;img alt="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" src="https://external-preview.redd.it/cW9oa2FtZXAwM3RlMZqijIi1GCa_F1Pp7Yxzhw_7Ni36eaah2O36NNbIKvPq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80afd573b70e01bfb67d4f3998b5e0b518af08d" title="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/j7p6nqep03te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbdm8</id>
    <title>Llama 4 benchmarks</title>
    <updated>2025-04-05T19:24:22+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 benchmarks" src="https://preview.redd.it/cl35fq7qh2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff22b91338fb54450168b9339d67ee62bd7a48ee" title="Llama 4 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cl35fq7qh2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdq4p</id>
    <title>Llama 4 Maverick - Python hexagon test failed</title>
    <updated>2025-04-05T21:08:02+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt; &lt;img alt="Llama 4 Maverick - Python hexagon test failed" src="https://b.thumbs.redditmedia.com/32CwgGLDK_Ju5fOPFA4pivpATQq39V8dluXhV3-Prqw.jpg" title="Llama 4 Maverick - Python hexagon test failed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254"&gt;https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;Write a Python program that shows 20 balls bouncing inside a spinning heptagon:&lt;br /&gt; - All balls have the same radius.&lt;br /&gt; - All balls have a number on it from 1 to 20.&lt;br /&gt; - All balls drop from the heptagon center when starting.&lt;br /&gt; - Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d, #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e32, #e17b34, #dd7a56, #db8449, #d66a35&lt;br /&gt; - The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.&lt;br /&gt; - The material of all the balls determines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.&lt;br /&gt; - All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.&lt;br /&gt; - The heptagon is spinning around its center, and the speed of spinning is 360 degrees per 5 seconds.&lt;br /&gt; - The heptagon size should be large enough to contain all the balls.&lt;br /&gt; - Do not use the pygame library; implement collision detection algorithms and collision response etc. by yourself. The following Python libraries are allowed: tkinter, math, numpy, dataclasses, typing, sys.&lt;br /&gt; - All codes should be put in a single Python file.&lt;/p&gt; &lt;p&gt;DeepSeek R1 and Gemini 2.5 Pro do this in one request. Maverick failed in 8 requests&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jscww3</id>
    <title>Gemini 2.5 Pro is better than Llama 4 behemoth on benchmarks</title>
    <updated>2025-04-05T20:32:01+00:00</updated>
    <author>
      <name>/u/Glittering-Bag-4662</name>
      <uri>https://old.reddit.com/user/Glittering-Bag-4662</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Specifically GPQA Diamond and MMLU Pro. Zuck lying out here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Bag-4662"&gt; /u/Glittering-Bag-4662 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T20:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4iy0</id>
    <title>I think I overdid it.</title>
    <updated>2025-04-05T14:21:22+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt; &lt;img alt="I think I overdid it." src="https://preview.redd.it/i5f8b0knz0te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1448cae5bed745aa96ac7b2801a7bf32c07afd26" title="I think I overdid it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5f8b0knz0te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jshwxe</id>
    <title>First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:</title>
    <updated>2025-04-06T00:32:51+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt; &lt;img alt="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:" src="https://preview.redd.it/1zt2gzrq04te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3abfffb312e36148337fcbbdd96100c2f53bd88c" title="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zt2gzrq04te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T00:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsahy4</id>
    <title>Llama 4 is here</title>
    <updated>2025-04-05T18:46:20+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsax3p</id>
    <title>Llama 4 Benchmarks</title>
    <updated>2025-04-05T19:04:21+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 Benchmarks" src="https://preview.redd.it/o2cd1y15e2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01928d53f0ef81a88115f299ef15628aacc38783" title="Llama 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o2cd1y15e2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsabgd</id>
    <title>Meta: Llama4</title>
    <updated>2025-04-05T18:38:40+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt; &lt;img alt="Meta: Llama4" src="https://external-preview.redd.it/cwgFslgMUPL6p26FpnXYan8AI9J3Uz-yA2DZbRx4puk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c2d0eac2996298f7e242609a095f7deafa5ac1" title="Meta: Llama4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama-downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:38:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsampe</id>
    <title>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!</title>
    <updated>2025-04-05T18:52:08+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt; &lt;img alt="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" src="https://external-preview.redd.it/Z3p2aHZudXhiMnRlMYW4H8xHgtzR3pjuficV95KktJ2KVETiew0YUMQL020k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b332bfe887b8dc264280ed80e4cedb70e9cd787" title="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source from his instagram page&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7bgnzhtxb2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:52:08+00:00</published>
  </entry>
</feed>
