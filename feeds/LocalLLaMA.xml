<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-18T10:07:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1msn3gi</id>
    <title>Ovis2.5 9B ~ 2B - New Multi-modal LLMs from Alibaba</title>
    <updated>2025-08-17T10:09:17+00:00</updated>
    <author>
      <name>/u/Sad_External6106</name>
      <uri>https://old.reddit.com/user/Sad_External6106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing with &lt;strong&gt;Ovis2.5 (2B &amp;amp; 9B)&lt;/strong&gt; the past few days. The cool part is it now has an optional &lt;em&gt;think&lt;/em&gt; mode — the model will slow down a bit but actually self-check and refine answers, which really helps on harder reasoning tasks. Also the OCR feels way better than before, especially on messy charts and dense documents. Overall, a pretty practical upgrade if you care about reasoning + OCR.&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335"&gt;https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_External6106"&gt; /u/Sad_External6106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1msv4us</id>
    <title>GPT-OSS is not good at Brazilian Legal Framework :(</title>
    <updated>2025-08-17T16:17:35+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt; &lt;img alt="GPT-OSS is not good at Brazilian Legal Framework :(" src="https://preview.redd.it/uqksokgduljf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d0c45bd812e8cb6b20834de28e876efa3b08b4c" title="GPT-OSS is not good at Brazilian Legal Framework :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;benchmark: &lt;a href="https://huggingface.co/datasets/celsowm/legalbench.br"&gt;https://huggingface.co/datasets/celsowm/legalbench.br&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uqksokgduljf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy01r</id>
    <title>Why does Qwen3-30B-A3B-Instruct-2507 Q8_0 work on my machine and no others come close?</title>
    <updated>2025-08-17T18:07:08+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm surprised that having a machine with 8GB of VRAM and 32GB of RAM can run this LLM. Slow, yes, but it runs and gives good answers. Why isn't there another one like it? Why not a DeepSeek R1, for example?&lt;/p&gt; &lt;p&gt;I don't really mind waiting too much if I'm going to get an &amp;quot;accurate&amp;quot; answer.&lt;/p&gt; &lt;p&gt;Obviously, I don't use it regularly, but I like having an LLM to maybe ask a &amp;quot;personal&amp;quot; question, and also in case at some point they put restrictions on all non-local LLMs, overprice them, or lobotomize them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1msvs0i</id>
    <title>What happened to the Uncensored models like Dolphin?</title>
    <updated>2025-08-17T16:42:30+00:00</updated>
    <author>
      <name>/u/krigeta1</name>
      <uri>https://old.reddit.com/user/krigeta1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year uncensored model like Dolphin(i was able to use it only) was fully uncensored and able to answers are things that are just really creepy and as of today there are open source LLMs that are so much powerful than the dolphin but nobody is releasing those models anymore?&lt;/p&gt; &lt;p&gt;Any specific reason why we are not getting uncensored models anymore?&lt;/p&gt; &lt;p&gt;Edit: wow guys, its been minutes and you guys have shared a lot of models, Hats off to you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krigeta1"&gt; /u/krigeta1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9coa</id>
    <title>NVLink: 3 or 4 slot?</title>
    <updated>2025-08-18T02:03:15+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"&gt; &lt;img alt="NVLink: 3 or 4 slot?" src="https://b.thumbs.redditmedia.com/IzbRj480PwBafRpORktMRn-9K7XXlI8JeQxvzq-ToRs.jpg" title="NVLink: 3 or 4 slot?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before I hit that buy button could someone please confirm this 3090 configuration would use a FOUR slot NVLink, not three?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt9coa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1msyzh8</id>
    <title>MoE optimization idea (VRAM/RAM)</title>
    <updated>2025-08-17T18:44:39+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt; &lt;img alt="MoE optimization idea (VRAM/RAM)" src="https://a.thumbs.redditmedia.com/HVYu7GZpWuFHEFw5ZYAS9pbfX6y2jpeFyNGTo1j_QB8.jpg" title="MoE optimization idea (VRAM/RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;I was doing some tests, and have noticed that properly offloading MoE to CPU can improve performance, but there's a thing that might not be taken into account.&lt;/p&gt; &lt;p&gt;We're offloading sequentially, not by most commonly used experts, below there's an image it's from my CPU inference engine, I did some changes to it, I can do inference on Qwen3 30B-A3B Q8_0 (35gb) using only 9gb of RAM, speed will drop as I'm constantly loading/unloading the experts from SSD.&lt;/p&gt; &lt;p&gt;But with this I could find something interesting, experts usage isn't linear, there are experts that have higher activation frequency, so my proposed idea is that when offloading between RAM/VRAM we keep track of currently most used experts and move them around based on their usage, most used experts will move to VRAM, least used will drop to RAM, I believe with this kind of smart optimization we may be able to extract more speed from MoE models and also make possible to run bigger models on limited hardware by reducing the amount of in-memory experts.&lt;/p&gt; &lt;p&gt;I would try to implement this into llama.cpp but I'm not very used to C/C++ programming, but would like to hear thoughts on who might be familiar with it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf"&gt;https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtctda</id>
    <title>Trying to run a local offline coding agent with qwen code</title>
    <updated>2025-08-18T04:59:22+00:00</updated>
    <author>
      <name>/u/kuaythrone</name>
      <uri>https://old.reddit.com/user/kuaythrone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get a powerful, offline coding agent running locally. I somehow always get the urge to code when there's no internet, so having a capable local model is a bit of a personal quest. I decided to connect the qwen code frontend to the qwen3-coder-30B model on my desktop using LM Studio.&lt;/p&gt; &lt;p&gt;My setup is an NVIDIA RTX 4080 Super (16GB VRAM) with 32GB of system RAM.&lt;/p&gt; &lt;p&gt;In the LM Studio chat UI, I was getting about 20-24 tok/s. The speed felt ok and I thought I was on the right track.&lt;/p&gt; &lt;p&gt;But I hit a hard wall with the memory bottleneck pretty quickly. The context window defaulted to 4096 tokens, and qwen code immediately complained that it needed 8k+ context just to process a simple &amp;quot;hi&amp;quot;.&lt;/p&gt; &lt;p&gt;To get around this, I pushed the context length to 32k by enabling Flash Attention and KV cache quantization. Technically, it worked, and the larger context was accessible. But the performance trade-off was brutal. Both the token speed and code quality dropped significantly.&lt;/p&gt; &lt;p&gt;I really believe there will be a day when local models catch up to today's SOTA models like Opus 4. Even if the state-of-the-art continues to improve, I would be perfectly happy with the current quality of top-tier coding agents if I could just run them completely local. We're getting closer, but the hardware requirements are still a major hurdle.&lt;/p&gt; &lt;p&gt;For anyone who wants to try this yourself, the setup is surprisingly simple. You just need to use OpenAI as the auth method for Qwen Code and point the OpenAI endpoint to your local LM Studio server in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;quot;not-needed-for-local&amp;quot; OPENAI_BASE_URL=&amp;quot;http://localhost:1234/v1&amp;quot; OPENAI_MODEL=&amp;quot;qwen/qwen3-coder-30b&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's the official guide I used: &lt;a href="https://github.com/QwenLM/qwen-code/blob/main/README.md#2-openai-compatible-api"&gt;https://github.com/QwenLM/qwen-code/blob/main/README.md#2-openai-compatible-api&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else found a better way to balance performance and context size on household hardware? Curious to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuaythrone"&gt; /u/kuaythrone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1msosdv</id>
    <title>Why does Mistral NeMo's usage keep growing even after more than a year since releasing?</title>
    <updated>2025-08-17T11:46:54+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt; &lt;img alt="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" src="https://preview.redd.it/5wd0ayxihkjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef52cf7168e409394d3d181f871e20c40bcefa5d" title="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wd0ayxihkjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T11:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt8yax</id>
    <title>A ridiculously simple (and weird yet interesting) benchmark question I've figured out</title>
    <updated>2025-08-18T01:44:28+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello to y'all,&lt;/p&gt; &lt;p&gt;I've figured out that many models - even frontier ones like Deepseek or Gemini 2.5 Flash - fail with this simple (and a little messed up) question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;I am somehow 4 months older than my brother, but my mother doesn't want to tell me how that's possible. What am I missing here?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(Before you wonder, I got the idea for the scenario from a - presumably - trolling post on Reddit.)&lt;/p&gt; &lt;p&gt;Obviously, something isn't right here: pregnancy lasts 9 months, so what is going on? Deepseek, Gemini and even our beloved small Qwen 3 30B A3B completely failed to answer this and instead came up with explanations like &amp;quot;you are the mother in this scenario&amp;quot;. Mistral Medium just began reasoning to me - outside of any thinking tags, as it is not a reasoning model, burning through tokens - but still got it wrong. The old and dated QwQ, which is a dense reasoning-only model, failed catastrophically. Only few models, like the latest and greatest Qwen 3 235B A3B 2507 Thinking were able to find the solution:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;You're either adopted or she isn't your and your brothers mother at the same time.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I think it's funny to see how these large models fail to answer a question that isn't even thaaaaat hard - thinking out of the box in this way seems to still leave a lot of room for improvement!&lt;/p&gt; &lt;p&gt;Just wanted to share this. Have a good one!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T01:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtggl4</id>
    <title>Agents with Vision?</title>
    <updated>2025-08-18T08:41:17+00:00</updated>
    <author>
      <name>/u/No_Efficiency_1144</name>
      <uri>https://old.reddit.com/user/No_Efficiency_1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of good agent products involve coding, writing, search or text NLP such as classification.&lt;/p&gt; &lt;p&gt;We have very strong vision models now. Does anyone know good agent products, code frameworks or tools that combine both agents with vision? Single agent is ok but multi-agent if possible &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Efficiency_1144"&gt; /u/No_Efficiency_1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtggl4/agents_with_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtggl4/agents_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtggl4/agents_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T08:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtfqyx</id>
    <title>THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp; Governments</title>
    <updated>2025-08-18T07:55:43+00:00</updated>
    <author>
      <name>/u/vancity-boi-in-tdot</name>
      <uri>https://old.reddit.com/user/vancity-boi-in-tdot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfqyx/the_nvidia_ai_gpu_black_market_investigating/"&gt; &lt;img alt="THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp;amp; Governments" src="https://external-preview.redd.it/2B3KEk8KKxicDYT6TmXcM_06VPjlO10fR-AfmTFZPjY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=826faab6abdc870eaebae77c99275a97769c48c6" title="THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp;amp; Governments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vancity-boi-in-tdot"&gt; /u/vancity-boi-in-tdot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/1H3xQaf7BFI?feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfqyx/the_nvidia_ai_gpu_black_market_investigating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfqyx/the_nvidia_ai_gpu_black_market_investigating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T07:55:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtf27z</id>
    <title>Local Meeting Notes with Whisper Transcription + Ollama Summaries (Gemma3n, LLaMA, Mistral) - Meetily</title>
    <updated>2025-08-18T07:12:22+00:00</updated>
    <author>
      <name>/u/Sorry_Transition_599</name>
      <uri>https://old.reddit.com/user/Sorry_Transition_599</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtf27z/local_meeting_notes_with_whisper_transcription/"&gt; &lt;img alt="Local Meeting Notes with Whisper Transcription + Ollama Summaries (Gemma3n, LLaMA, Mistral) - Meetily" src="https://preview.redd.it/2a2dzntv9qjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f3dc35d5c9ea3bf8e064ad2d7690d0a548bd174" title="Local Meeting Notes with Whisper Transcription + Ollama Summaries (Gemma3n, LLaMA, Mistral) - Meetily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’m one of the maintainers of &lt;strong&gt;Meetily&lt;/strong&gt;, an open-source desktop app that uses &lt;strong&gt;Whisper + Ollama&lt;/strong&gt; to turn your meetings into structured notes — 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; handles live transcription of your mic + system audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; runs your chosen local LLM (Gemma3n, LLaMA, Mistral, etc.) to generate summaries and action items.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Standalone desktop app&lt;/strong&gt; (Windows, macOS, Docker) — no plugins, no API keys. Works with any platform (Zoom, Teams, Meet, Discord…).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully offline&lt;/strong&gt; — transcripts + summaries never leave your machine.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;New in v0.0.5&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stable Docker support&lt;/strong&gt; (x86_64 + ARM64).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native installers&lt;/strong&gt; (Windows, macOS, plus Homebrew).&lt;/li&gt; &lt;li&gt;Backend optimizations for faster Whisper → Ollama pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick start with Ollama&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;ollama pull gemma3n:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then point Meetily to your local Ollama instance from the settings modal — it will handle summarization automatically after Whisper transcribes.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Release:&lt;/strong&gt; &lt;a href="https://github.com/Zackriya-Solutions/meeting-minutes/releases/latest"&gt;GitHub – Meetily Latest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious which models folks here prefer for &lt;strong&gt;meeting summarization&lt;/strong&gt; — Gemma3n has been solid in our tests, but eager to compare results with LLaMA/Mistral.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;(Disclosure: I’m a maintainer.)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Transition_599"&gt; /u/Sorry_Transition_599 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2a2dzntv9qjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtf27z/local_meeting_notes_with_whisper_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtf27z/local_meeting_notes_with_whisper_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T07:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mthaox</id>
    <title>🐧 llama.cpp on Steam Deck (Ubuntu 25.04) with GPU (Vulkan) — step-by-step that actually works</title>
    <updated>2025-08-18T09:32:57+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got &lt;strong&gt;llama.cpp&lt;/strong&gt; running on the Steam Deck APU (Van Gogh, &lt;code&gt;gfx1033&lt;/code&gt;) with &lt;strong&gt;GPU acceleration via Vulkan&lt;/strong&gt; on Ubuntu 25.04 (clean install on SteamDeck 256GB). Below are only the steps and commands that worked end-to-end, plus practical ways to verify the GPU is doing the work.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Build llama.cpp with &lt;code&gt;-DGGML_VULKAN=ON&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Use smaller GGUF models (1–3B, quantized) and push as many layers to GPU as VRAM allows via &lt;code&gt;--gpu-layers&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Verify with &lt;code&gt;radeontop&lt;/code&gt;, &lt;code&gt;vulkaninfo&lt;/code&gt;, and (optionally) &lt;code&gt;rocm-smi&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;0) Confirm the GPU is visible (optional sanity)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;rocminfo # should show Agent &amp;quot;gfx1033&amp;quot; (AMD Custom GPU 0405) rocm-smi --json # reports temp/power/VRAM (APUs show limited SCLK data; JSON is stable) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you’ll run GPU tasks as a non-root user:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -aG render,video $USER # log out/in (or reboot) so group changes take effect &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;1) Install the required packages&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;sudo apt update sudo apt install -y \ build-essential cmake git \ mesa-vulkan-drivers libvulkan-dev vulkan-tools \ glslang-tools glslc libshaderc-dev spirv-tools \ libcurl4-openssl-dev ca-certificates &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Quick checks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vulkaninfo | head -n 20 # should print &amp;quot;Vulkan Instance Version: 1.4.x&amp;quot; glslc --version # shaderc + glslang versions print &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(Optional but nice)&lt;/em&gt; speed up rebuilds:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install -y ccache &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;2) Clone and build llama.cpp with Vulkan&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggml-org/llama.cpp cd llama.cpp rm -rf build cmake -B build -DGGML_VULKAN=ON \ -DGGML_CCACHE=ON # optional, speeds up subsequent builds cmake --build build --config Release -j &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;3) Run a model on the GPU&lt;/h1&gt; &lt;h1&gt;a) Pull a model from Hugging Face (requires CURL enabled)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ -hf ggml-org/gemma-3-1b-it-GGUF \ --gpu-layers 32 \ -p &amp;quot;Say hello from Steam Deck GPU.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;b) Use a local model file&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ -m /path/to/model.gguf \ --gpu-layers 32 \ -p &amp;quot;Say hello from Steam Deck GPU.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with quantized models (e.g., &lt;code&gt;*q4_0.gguf&lt;/code&gt;, &lt;code&gt;*q5_k.gguf&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Increase &lt;code&gt;--gpu-layers&lt;/code&gt; until you hit VRAM limits (Deck iGPU usually has ~1 GiB reserved VRAM + shared RAM; if it OOMs or slows down, lower it).&lt;/li&gt; &lt;li&gt;&lt;code&gt;--ctx-size&lt;/code&gt; / &lt;code&gt;-c&lt;/code&gt; increases memory use; keep moderate contexts on an APU.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4) Verify the GPU is actually working&lt;/h1&gt; &lt;h1&gt;Option A: radeontop (simple and effective)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;sudo apt install -y radeontop radeontop &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Watch the &lt;strong&gt;“gpu”&lt;/strong&gt; bar and rings (gfx/compute) jump when you run llama.cpp.&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;radeontop&lt;/code&gt; in one terminal, start llama.cpp in another, and you should see load spike above idle.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option B: Vulkan headless check&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;vulkaninfo | head -n 20 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;If you’re headless you’ll see “DISPLAY not set … skipping surface info”, which is fine; compute still works.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option C: ROCm SMI (APU metrics are limited but still useful)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;watch -n 1 rocm-smi --showtemp --showpower --showmeminfo vram --json &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Look for temperature/power bumps and VRAM use increasing under load.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Option D: DPM states (clock levels changing)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;watch -n 0.5 &amp;quot;cat /sys/class/drm/card*/device/pp_dpm_sclk; echo; cat /sys/class/drm/card*/device/pp_dpm_mclk&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;You should see the active &lt;code&gt;*&lt;/code&gt; move to higher SCLK/MCLK levels during inference.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5) What worked well on the Steam Deck APU (Van Gogh / gfx1033)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vulkan backend&lt;/strong&gt; is the most reliable path for AMD iGPUs/APUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small models (1–12B)&lt;/strong&gt; with &lt;strong&gt;q4/q5&lt;/strong&gt; quantization run smoothly enough for testing around 1b about 25 t/s and 12b (!) gemma3 at 10 t/s.&lt;/li&gt; &lt;li&gt;Pushing as many &lt;code&gt;--gpu-layers&lt;/code&gt; as memory allows gives the best speedup; if you see instability, dial it back.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rocm-smi&lt;/code&gt; on APUs may not show SCLK, but temp/power/VRAM are still indicative; &lt;code&gt;radeontop&lt;/code&gt; is the most convenient “is it doing something?” view.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6) Troubleshooting quick hits&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CMake can’t find Vulkan/glslc&lt;/strong&gt; → make sure &lt;code&gt;libvulkan-dev&lt;/code&gt;, &lt;code&gt;glslc&lt;/code&gt;, &lt;code&gt;glslang-tools&lt;/code&gt;, &lt;code&gt;libshaderc-dev&lt;/code&gt;, &lt;code&gt;spirv-tools&lt;/code&gt; are installed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CMake can’t find CURL&lt;/strong&gt; → &lt;code&gt;sudo apt install -y libcurl4-openssl-dev&lt;/code&gt; or add &lt;code&gt;-DLLAMA_CURL=OFF&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low performance / stutter&lt;/strong&gt; → reduce context size and/or &lt;code&gt;--gpu-layers&lt;/code&gt;, try a smaller quant, ensure no other heavy GPU tasks are running.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Permissions&lt;/strong&gt; → ensure your user is in &lt;code&gt;render&lt;/code&gt; and &lt;code&gt;video&lt;/code&gt; groups and re-log.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That’s the whole path I used to get &lt;strong&gt;llama.cpp&lt;/strong&gt; running with GPU acceleration on the Steam Deck via Vulkan, including how to prove the GPU is active. &lt;/p&gt; &lt;h1&gt;Reflection&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;Steam Deck&lt;/strong&gt; offers a compelling alternative to the &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; as a low-power, compact home server, especially if you're interested in &lt;strong&gt;local LLM inference with GPU acceleration&lt;/strong&gt;. Unlike the Pi, the Deck includes a capable &lt;strong&gt;AMD RDNA2 iGPU&lt;/strong&gt;, substantial memory (16 GB LPDDR5), and &lt;strong&gt;NVMe SSD&lt;/strong&gt; support—making it great for &lt;strong&gt;virtualization&lt;/strong&gt; and &lt;strong&gt;LLM workloads&lt;/strong&gt; directly on the embedded SSD, all within a &lt;strong&gt;mobile, power-efficient&lt;/strong&gt; form factor.&lt;/p&gt; &lt;p&gt;Despite being designed for handheld gaming, the Steam Deck’s &lt;strong&gt;idle power draw&lt;/strong&gt; is surprisingly modest (around &lt;strong&gt;7 W&lt;/strong&gt;), yet it packs far more compute and GPU versatility than a Pi. In contrast, the Raspberry Pi 5 consumes only around &lt;strong&gt;2.5–2.75 W at idle&lt;/strong&gt;, but lacks any integrated GPU suitable for serious acceleration tasks. For tasks like running llama.cpp with a quantized model on &lt;strong&gt;GPU layers&lt;/strong&gt;, the Deck's iGPU opens performance doors the Pi simply can't match. Plus, with low TDP and idle power, the Deck consumes just a bit more energy but delivers &lt;strong&gt;far greater throughput&lt;/strong&gt; and flexibility.&lt;/p&gt; &lt;p&gt;All things considered, the Steam Deck presents a highly efficient and portable alternative for embedded LLM serving—or even broader home server applications—delivering &lt;strong&gt;hardware acceleration, storage, memory, and low power&lt;/strong&gt; in one neat package.&lt;/p&gt; &lt;h1&gt;Power Consumption Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Idle Power (Typical)&lt;/th&gt; &lt;th align="left"&gt;Peak Power (Load)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Raspberry Pi 5 (idle)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~2.5 W – 2.75 W&lt;/td&gt; &lt;td align="left"&gt;~5–6 W (CPU load; no GPU)&lt;a href="https://www.jeffgeerling.com/blog/2024/new-2gb-pi-5-has-33-smaller-die-30-idle-power-savings?utm_source=chatgpt.com"&gt;Pimoroni Buccaneers+6jeffgeerling.com+6jeffgeerling.com+6&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Steam Deck (idle)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~7 W&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://steamcommunity.com/app/1675200/discussions/0/3466100515576632731/?utm_source=chatgpt.com"&gt;steamcommunity.com&lt;/a&gt;up to ~25 W (max APU TDP)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Raspberry Pi 5&lt;/strong&gt;: Multiple sources confirm idle power around &lt;strong&gt;2.5 W&lt;/strong&gt;, nearly identical to Pi 4, with CPU-intensive tasks raising it modestly into the 5–6 W range &lt;a href="https://www.jeffgeerling.com/blog/2024/new-2gb-pi-5-has-33-smaller-die-30-idle-power-savings?utm_source=chatgpt.com"&gt;forums.raspberrypi.com+8jeffgeerling.com+8Home Assistant Community+8&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Steam Deck&lt;/strong&gt;: Users observe idle consumption at about &lt;strong&gt;7 W&lt;/strong&gt; when not charging &lt;a href="https://steamcommunity.com/app/1675200/discussions/0/3466100515576632731/?utm_source=chatgpt.com"&gt;steamcommunity.com+2WIRED+2&lt;/a&gt;. Official spec lists &lt;strong&gt;max APU draw 4–15 W&lt;/strong&gt;, with system‑wide peaks reaching &lt;strong&gt;~25 W&lt;/strong&gt; under heavy load &lt;a href="https://www.reddit.com/r/SteamDeck/comments/xuy7o4/deck_using_too_many_watts_idle/?utm_source=chatgpt.com"&gt;linustechtips.com+9Reddit+9Reddit+9&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why the Deck still wins as a home server&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU Acceleration&lt;/strong&gt;: Built-in RDNA2 GPU enables Vulkan compute, perfect for llama.cpp or similar.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory &amp;amp; Storage&lt;/strong&gt;: 16 GB RAM + NVMe SSD vastly outclass the typical Pi setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low Idle Draw with High Capability&lt;/strong&gt;: While idle wattage is higher than the Pi, it's still minimal for what the system can do.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Versatility&lt;/strong&gt;: Runs full Linux desktop environments, supports virtualization, containerization, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In summary: &lt;em&gt;Yes, the Steam Deck outshines the Raspberry Pi 5 as a compact, low-power, GPU-accelerated home server for LLMs and general compute.&lt;/em&gt; If you can tolerate the slightly higher idle draw (3–5 W more), you gain significant performance and flexibility for AI workloads at home.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mthaox/llamacpp_on_steam_deck_ubuntu_2504_with_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T09:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtbsvt</id>
    <title>Browser-based micro-LLMs (Gemma 270M/Qwen 0.5B) - production experiences?</title>
    <updated>2025-08-18T04:03:43+00:00</updated>
    <author>
      <name>/u/innagadadavida1</name>
      <uri>https://old.reddit.com/user/innagadadavida1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just dropped Gemma 270M specifically for edge deployment. At ~240MB download, it finally feels feasible for browser deployment without users rage-quitting during download. Anyone running these micro-models in prod?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System constraints I'm validating:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download/Storage:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model sizes: Gemma 270M (~240MB), Qwen 0.5B (~500MB) quantized&lt;/li&gt; &lt;li&gt;Progressive loading or still all-or-nothing?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Runtime memory:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seeing &amp;lt;1GB total RAM usage (model + browser overhead)?&lt;/li&gt; &lt;li&gt;Memory spikes during inference?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance reality:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First-token latency on average hardware?&lt;/li&gt; &lt;li&gt;Background tab throttling issues?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tradeoff:&lt;/strong&gt; These models are tiny enough to actually download but potentially too dumb to be useful. Google's pitch is &amp;quot;fine-tune for your specific task&amp;quot; but I'm skeptical about quality floor for customer-facing apps.&lt;/p&gt; &lt;p&gt;Anyone have real metrics from production? Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bounce rate during model download&lt;/li&gt; &lt;li&gt;Device/browser success rates&lt;/li&gt; &lt;li&gt;Actual memory footprint in the wild&lt;/li&gt; &lt;li&gt;Whether users even notice/care about local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Considering a progressive enhancement approach: tiny local model for basic intent classification, API fallback for anything complex. But wondering if the juice is worth the squeeze.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/innagadadavida1"&gt; /u/innagadadavida1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtevqx</id>
    <title>Why is LLM usage on OpenRouter decreasing?</title>
    <updated>2025-08-18T07:01:09+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/rankings/marketing?view=week"&gt;https://openrouter.ai/rankings/marketing?view=week&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I expect exponential growth but seems like growth has stalled in the last few weeks.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is LLM usage really slowing down?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are devs using some other router than OpenRouter?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are devs using APIs directly with LLM providers or with big tech?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtevqx/why_is_llm_usage_on_openrouter_decreasing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtevqx/why_is_llm_usage_on_openrouter_decreasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtevqx/why_is_llm_usage_on_openrouter_decreasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T07:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtekyf</id>
    <title>R-Zero : New Framework to Train LLMs with Zero labelled data</title>
    <updated>2025-08-18T06:42:51+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;R-Zero by Tencent introduces a concept to train LLMs without any labelled data and aims towards self-improving AI without human intervention. It works on the similar principle of GANs i.e. involving a Challenger and Solver where one generates questions and other Solves them. &lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2508.05004?ref=mackenziemorehead.com"&gt;https://arxiv.org/abs/2508.05004?ref=mackenziemorehead.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/kNL6z0wxZ_o?si=iG8U7Go7YeiLsADe"&gt;https://youtu.be/kNL6z0wxZ_o?si=iG8U7Go7YeiLsADe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtekyf/rzero_new_framework_to_train_llms_with_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtekyf/rzero_new_framework_to_train_llms_with_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtekyf/rzero_new_framework_to_train_llms_with_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T06:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt0rld</id>
    <title>Is it just me, or is LM Studio really pushing the new gpt-oss?</title>
    <updated>2025-08-17T19:52:39+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt; &lt;img alt="Is it just me, or is LM Studio really pushing the new gpt-oss?" src="https://b.thumbs.redditmedia.com/I7KSIeyl1d4CoN4rEoRDFE7evtxOdppiW6wbj9W6Q5g.jpg" title="Is it just me, or is LM Studio really pushing the new gpt-oss?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...maybe a little too far? I mean, the setup has a step for &amp;quot;Now download some models&amp;quot; — that only offers gpt-oss. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mmqsn49rwmjf1.png?width=713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c1b4736e379c98e35634f3c6ed02aa26f79bf8a"&gt;the one model to rule them all?&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T19:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrnqq</id>
    <title>Wow anthropic and Google losing coding share bc of qwen 3 coder</title>
    <updated>2025-08-17T13:59:47+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt; &lt;img alt="Wow anthropic and Google losing coding share bc of qwen 3 coder" src="https://preview.redd.it/rwehyliy5ljf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67f716968425732683dc36fdd2644caa8322da3" title="Wow anthropic and Google losing coding share bc of qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwehyliy5ljf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtfw5j</id>
    <title>My open-source agent Maestro is now faster and lets you configure context limits for better local model support</title>
    <updated>2025-08-18T08:04:36+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"&gt; &lt;img alt="My open-source agent Maestro is now faster and lets you configure context limits for better local model support" src="https://a.thumbs.redditmedia.com/2Jm7LDGCGGJuzu7lAAQM2I8TKlWM1S6jW2ieXBd-_l0.jpg" title="My open-source agent Maestro is now faster and lets you configure context limits for better local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I just pushed a big update for Maestro, my open-source AI research agent. I've focused on making it work better with local models.&lt;/p&gt; &lt;p&gt;The biggest change is that you can now fully configure research parameters like planning context limits directly in the UI. This should finally fix the context overflow issues some of you were seeing.&lt;/p&gt; &lt;p&gt;A few other key updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's much faster now due to a full database migration to PostgreSQL. Document processing and lookups are noticeably quicker.&lt;/li&gt; &lt;li&gt;There's an improved CPU-only mode with its own &lt;code&gt;docker-compose.cpu.yml&lt;/code&gt; for easier setup on machines that don't need/have a GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Heads up:&lt;/strong&gt; This is a breaking change because of the new database. You'll need to do a fresh install.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;The project is on GitHub if you'd like to check it out&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to hear any feedback you have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mtfw5j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtfw5j/my_opensource_agent_maestro_is_now_faster_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T08:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt2iev</id>
    <title>GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure.</title>
    <updated>2025-08-17T21:01:10+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt; &lt;img alt="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." src="https://external-preview.redd.it/LOoaiYGJSklUhS_jyXNZtXqZW5tq1NuC9Dm2RNcy-8Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fb11e05f1a215e4e59de75726903bb0ecb7f1d6" title="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was doing some tool calling tests while figuring out how to work with the Harmony GPT-OSS prompt format. I made a little helpful tool here if you're trying to understand how harmony works (there's a whole repo there too with a bit deeper exploration if you're curious):&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html"&gt;https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I wanted to benchmark the system so I asked it to make a fun benchmark, and this is what it came up with. In this video, missiles are falling from the sky and the agent has to see their trajectory and speed, run a tool call with python to anticipate where the missile will be in the future, and fire an explosive anti-missile at it so that it can hit the spot it'll be when the missile arrives. To do this, it needs to have low latency, understand its own latency, and be able to RAPIDLY fire off tool calls. This is firing with 100% accuracy (it technically missed 10 tool calls along the way but was able to recover and fire them before the missiles hit the ground).&lt;/p&gt; &lt;p&gt;So... here's GPT-OSS-20b running 100 agents simultaneously at 131,076 token context, each agent with its own 131k context window, each hitting sub-100ms ttft, blowing everything out of the sky at 10k tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=8T8drT0rwCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt6l87</id>
    <title>NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text</title>
    <updated>2025-08-17T23:53:47+00:00</updated>
    <author>
      <name>/u/RYSKZ</name>
      <uri>https://old.reddit.com/user/RYSKZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt; &lt;img alt="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" src="https://external-preview.redd.it/A88FneA2E8vj8FZkQqex3y_zTUkEEvLZOR75ND29msM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faf8982f5868ceffad20906f1a118bd04e77ded1" title="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA has launched &lt;strong&gt;Granary&lt;/strong&gt;, a massive open-source multilingual speech dataset with 1M hours of audio, supporting 25 European languages, including low-resource ones like Croatian, Estonian, and Maltese.&lt;/p&gt; &lt;p&gt;Alongside it, NVIDIA released &lt;strong&gt;two high-performance STT models&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Canary-1b-v2&lt;/strong&gt;: 1B parameters, top accuracy on Hugging Face for multilingual speech recognition, translating between English and 24 languages, 10× faster inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parakeet-tdt-0.6b-v3&lt;/strong&gt;: 600M parameters, designed for real-time and large-scale transcription with highest throughput in its class.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hugging Face links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granary: &lt;a href="https://huggingface.co/datasets/nvidia/Granary"&gt;https://huggingface.co/datasets/nvidia/Granary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Canary-1b-v2: &lt;a href="https://huggingface.co/nvidia/canary-1b-v2"&gt;https://huggingface.co/nvidia/canary-1b-v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Parakeet-tdt-0.6b-v3: &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RYSKZ"&gt; /u/RYSKZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/speech-ai-dataset-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T23:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt3epi</id>
    <title>M4 Max generation speed vs context size</title>
    <updated>2025-08-17T21:37:31+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt; &lt;img alt="M4 Max generation speed vs context size" src="https://b.thumbs.redditmedia.com/0KFbAMMgVPnCWsrraGEmcRNBchKGkvvSro4EHgyP7yQ.jpg" title="M4 Max generation speed vs context size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a custom benchmark program to map out generation speed vs context size. The program will build up a prompt 10k tokens at a time and log the reported stats from LM Studio. The intention is to simulate agentic coding. Cline/Roo/Kilo use about 20k tokens for the system prompt.&lt;/p&gt; &lt;p&gt;Better images here: &lt;a href="https://oz9h.dk/benchmark/"&gt;https://oz9h.dk/benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My computer is the M4 Max Macbook Pro 128 GB. All models at 4 bit quantization. KV-Cache at 8 bit.&lt;/p&gt; &lt;p&gt;I am quite sad that GLM 4.5 Air degrades so quickly. And impressed that GPT-OSS 120b manages to stay fast even with 100k context. I don't use Qwen3-Coder 30b-a3b much but I am still surprised at how quickly it crashes and it even gets slower than GPT-OSS - a model 4 times larger. And my old workhorse Devstral somehow manages to be the most consistent model regarding speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt3epi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9htu</id>
    <title>FlashAttention 4 Leak</title>
    <updated>2025-08-18T02:10:02+00:00</updated>
    <author>
      <name>/u/InevitableExtreme396</name>
      <uri>https://old.reddit.com/user/InevitableExtreme396</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt; &lt;img alt="FlashAttention 4 Leak" src="https://a.thumbs.redditmedia.com/5f6bt97vhO4cGlHOqVfCoRYr_m9nC10Eu8zHnCuyjR0.jpg" title="FlashAttention 4 Leak" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like the FA4 source code just got leaked here on a branch:&lt;/p&gt; &lt;p&gt;TLDR; As expected, it's mostly Blackwell (SM100+) and Tensor Core Generation 5, and uses the CuTe DSL (CUTLASS). There is also some handwritten PTX.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449"&gt;https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's from the SGlang codebase, one of the popular LLM inference engines (like Llama.cpp for distributed.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang/compare/main...hieu/fa4"&gt;https://github.com/sgl-project/sglang/compare/main...hieu/fa4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableExtreme396"&gt; /u/InevitableExtreme396 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtct4y</id>
    <title>Elon didn't deliver on this announcement. It's already Monday.</title>
    <updated>2025-08-18T04:59:00+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt; &lt;img alt="Elon didn't deliver on this announcement. It's already Monday." src="https://preview.redd.it/rt8xgjaampjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2b6da79d84d1e52439441cafb251d7e1dc508f7" title="Elon didn't deliver on this announcement. It's already Monday." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rt8xgjaampjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
