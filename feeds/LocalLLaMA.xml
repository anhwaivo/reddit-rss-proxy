<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-29T03:02:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kxs06b</id>
    <title>Built a Python library for text classification because I got tired of reinventing the wheel</title>
    <updated>2025-05-28T20:43:56+00:00</updated>
    <author>
      <name>/u/Feeling-Remove6386</name>
      <uri>https://old.reddit.com/user/Feeling-Remove6386</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept running into the same problem at work: needing to classify text into custom categories but having to build everything from scratch each time. Sentiment analysis libraries exist, but what if you need to classify customer complaints into &amp;quot;billing&amp;quot;, &amp;quot;technical&amp;quot;, or &amp;quot;feature request&amp;quot;? Or moderate content into your own categories? Oh ok, you can train a BERT model . Good luck with 2 examples per category.&lt;/p&gt; &lt;p&gt;So I built Tagmatic. It's basically a wrapper that lets you define categories with descriptions and examples, then classify any text using LLMs. Yeah, it uses LangChain under the hood (I know, I know), but it handles all the prompt engineering and makes the whole process dead simple.&lt;/p&gt; &lt;p&gt;The interesting part is the voting classifier. Instead of running classification once, you can run it multiple times and use majority voting. Sounds obvious but it actually improves accuracy quite a bit - turns out LLMs can be inconsistent on edge cases, but when you run the same prompt 5 times and take the majority vote, it gets much more reliable.&lt;/p&gt; &lt;p&gt;from tagmatic import Category, CategorySet, Classifier&lt;/p&gt; &lt;p&gt;categories = CategorySet(categories=[&lt;/p&gt; &lt;p&gt;Category(&amp;quot;urgent&amp;quot;, &amp;quot;Needs immediate attention&amp;quot;),&lt;/p&gt; &lt;p&gt;Category(&amp;quot;normal&amp;quot;, &amp;quot;Regular priority&amp;quot;),&lt;/p&gt; &lt;p&gt;Category(&amp;quot;low&amp;quot;, &amp;quot;Can wait&amp;quot;)&lt;/p&gt; &lt;p&gt;])&lt;/p&gt; &lt;p&gt;classifier = Classifier(llm=your_llm, categories=categories)&lt;/p&gt; &lt;p&gt;result = classifier.voting_classify(&amp;quot;Server is down!&amp;quot;, voting_rounds=5)&lt;/p&gt; &lt;p&gt;Works with any LangChain-compatible LLM (OpenAI, Anthropic, local models, whatever). Published it on PyPI as `tagmatic` if anyone wants to try it.&lt;/p&gt; &lt;p&gt;Still pretty new so open to contributions and feedback. Link: [](&lt;a href="https://pypi.org/project/tagmatic/)https://pypi.org/project/tagmatic/"&gt;https://pypi.org/project/tagmatic/)https://pypi.org/project/tagmatic/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else been solving this same problem? Curious how others approach custom text classification.&lt;/p&gt; &lt;p&gt;Oh, consider leaving a star on github :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Sampaio-Vitor/tagmatic"&gt;https://github.com/Sampaio-Vitor/tagmatic&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feeling-Remove6386"&gt; /u/Feeling-Remove6386 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs06b/built_a_python_library_for_text_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs06b/built_a_python_library_for_text_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs06b/built_a_python_library_for_text_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T20:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxxfe5</id>
    <title>GPU consideration: AMD Pro W7800</title>
    <updated>2025-05-29T00:39:23+00:00</updated>
    <author>
      <name>/u/IngwiePhoenix</name>
      <uri>https://old.reddit.com/user/IngwiePhoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently in talks with a distributor to aquire &lt;a href="https://www.aicipc.com/en/productdetail/51394"&gt;this lil' box&lt;/a&gt;. Since about a year or so, I have been going back and forth in trying to aquire the hardware for my own local AI server - and that as a private customer, no business. Just a dude that wants to put LocalAI and OpenWebUI on the home network and go ham with AI stuff. A little silly, and the estimated price for this (4500â‚¬ - no VAT, no shipment...) is insane. But, as it stands, it is currently the only PCIe Gen 5 server I could find that has somewhat adequate mounts for FLFH GPUs. Welp, RIP wallet...&lt;/p&gt; &lt;p&gt;So I have been looking into what GPUs to add into this. I would &lt;em&gt;prefer&lt;/em&gt; to avoid NVIDIA due to the insane pricing left and right. So, I came across the AMD W7800 - two of them fit in the outmost slots, leaving space in the center for whatever else I happen to come across (probably a TensTorrent card to experiment and learn with that).&lt;/p&gt; &lt;p&gt;Has anyone used that particular GPU yet? ROCm should support partitioning, so I should be able to use the entire 96GB of VRAM to host rather large models. But when I went looking for reviews, I only found such for productivity workloads like Blender and whatnot...not for LLM performance (or other workloads like StableDiffusion etc.).&lt;/p&gt; &lt;p&gt;I am only interested in inference (for now?) and running stuff locally and on my own network. After watching my own mother legit put my freaking address into OpenAI, my mind just imploded...&lt;/p&gt; &lt;p&gt;Thank you in advance and kind regards!&lt;/p&gt; &lt;p&gt;PS.: I live in germany - actually aquiring &amp;quot;the good stuff&amp;quot; involved emailing B2B vendors and praying they are willing to sell to a private customer. It is how I got the offer for the AICIPC system and in parallel for an ASRock Rack Ampere Altra bundle...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngwiePhoenix"&gt; /u/IngwiePhoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxfe5/gpu_consideration_amd_pro_w7800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxfe5/gpu_consideration_amd_pro_w7800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxfe5/gpu_consideration_amd_pro_w7800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T00:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxp4hj</id>
    <title>Bored by RLVF? Here comes RLIF</title>
    <updated>2025-05-28T18:49:54+00:00</updated>
    <author>
      <name>/u/Majestic-Explorer315</name>
      <uri>https://old.reddit.com/user/Majestic-Explorer315</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxp4hj/bored_by_rlvf_here_comes_rlif/"&gt; &lt;img alt="Bored by RLVF? Here comes RLIF" src="https://b.thumbs.redditmedia.com/meg_3iuGnrtE1vZafvcXCt4YXpXmGV_Dszrjzqgyiag.jpg" title="Bored by RLVF? Here comes RLIF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning training rests on external rewards or so I thought. But now we got this remarkable paper that shows that the reward is already in the LLM! how can that even be? I always thought there is no way the model can know what it knows and what it does not know.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/khouzxz5mk3f1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=648ffcdf5acda059d37e8edcbe682a4555e2934e"&gt;https://preview.redd.it/khouzxz5mk3f1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=648ffcdf5acda059d37e8edcbe682a4555e2934e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.arxiv.org/pdf/2505.19590"&gt;Learning to Reason without External Rewards&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestic-Explorer315"&gt; /u/Majestic-Explorer315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxp4hj/bored_by_rlvf_here_comes_rlif/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxp4hj/bored_by_rlvf_here_comes_rlif/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxp4hj/bored_by_rlvf_here_comes_rlif/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T18:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxlus4</id>
    <title>Codestral Embed [embedding model specialized for code]</title>
    <updated>2025-05-28T16:40:54+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"&gt; &lt;img alt="Codestral Embed [embedding model specialized for code]" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Codestral Embed [embedding model specialized for code]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/codestral-embed"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxlus4/codestral_embed_embedding_model_specialized_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T16:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxyf0z</id>
    <title>This Eleven labs Competitor sounds better</title>
    <updated>2025-05-29T01:27:47+00:00</updated>
    <author>
      <name>/u/Beautiful-Essay1945</name>
      <uri>https://old.reddit.com/user/Beautiful-Essay1945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxyf0z/this_eleven_labs_competitor_sounds_better/"&gt; &lt;img alt="This Eleven labs Competitor sounds better" src="https://external-preview.redd.it/cXRuaHU1N3BpbTNmMYVo_tEft_a9iH3F4Ou_yJDFtaczwt9ETwiiGfy1hxPV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55a19a7fd589b324327f4c465600bc851e733f4e" title="This Eleven labs Competitor sounds better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Chatterbox tts &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful-Essay1945"&gt; /u/Beautiful-Essay1945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x864437pim3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxyf0z/this_eleven_labs_competitor_sounds_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxyf0z/this_eleven_labs_competitor_sounds_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T01:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxk3lk</id>
    <title>Another reorg for Meta Llama: AGI team created</title>
    <updated>2025-05-28T15:31:29+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which teams are going to get the most GPUs?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.axios.com/2025/05/27/meta-ai-restructure-2025-agi-llama"&gt;https://www.axios.com/2025/05/27/meta-ai-restructure-2025-agi-llama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama team divided into two teams:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The AGI Foundations unit will include the company's &lt;strong&gt;Llama models&lt;/strong&gt;, as well as efforts to improve capabilities in reasoning, multimedia and voice.&lt;/li&gt; &lt;li&gt;The AI products team will be responsible for the Meta AI assistant, Meta's AI Studio and AI features within Facebook, Instagram and WhatsApp.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The company's AI research unit, known as FAIR (Fundamental AI Research), remains separate from the new organizational structure, though one specific team working on multimedia is moving to the new AGI Foundations team.&lt;/p&gt; &lt;p&gt;Meta hopes that splitting a single large organization into smaller teams will speed product development and give the company more flexibility as it adds additional technical leaders.&lt;/p&gt; &lt;p&gt;The company is also &lt;a href="https://www.businessinsider.com/meta-llama-ai-talent-mistral-2025-5"&gt;seeing key talent depart&lt;/a&gt;, including to French rival Mistral, as reported by Business Insider.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxk3lk/another_reorg_for_meta_llama_agi_team_created/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T15:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdcpi</id>
    <title>impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!</title>
    <updated>2025-05-28T10:08:41+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt; &lt;img alt="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" src="https://preview.redd.it/sd06j27qyh3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51a26804948f34a4686a4018dd2e02a67c40a82" title="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sd06j27qyh3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxa788</id>
    <title>Google AI Edge Gallery</title>
    <updated>2025-05-28T06:33:50+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt; &lt;img alt="Google AI Edge Gallery" src="https://preview.redd.it/s6rgmrfawg3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4720f1c95bf832e5eacd2490cf5b69783a79a11b" title="Google AI Edge Gallery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android &lt;em&gt;(available now)&lt;/em&gt; and iOS &lt;em&gt;(coming soon)&lt;/em&gt; devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images, explore prompts, and more!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery?tab=readme-ov-file"&gt;https://github.com/google-ai-edge/gallery?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6rgmrfawg3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxgzd1</id>
    <title>Is there an open source alternative to manus?</title>
    <updated>2025-05-28T13:23:15+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried manus and was surprised how ahead it is of other agents at browsing the web and using files, terminal etc autonomously.&lt;/p&gt; &lt;p&gt;There is no tool I've tried before that comes close to it.&lt;/p&gt; &lt;p&gt;What's the best open source alternative to Manus that you've tried?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxz7yi</id>
    <title>What's the value of paying $20 a month for OpenAI or Anthropic?</title>
    <updated>2025-05-29T02:07:43+00:00</updated>
    <author>
      <name>/u/mainaisakyuhoon</name>
      <uri>https://old.reddit.com/user/mainaisakyuhoon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Iâ€™m new here. &lt;/p&gt; &lt;p&gt;Over the past few weeks, Iâ€™ve been experimenting with local LLMs and honestly, Iâ€™m impressed by what they can do. Right now, Iâ€™m paying $20/month for Raycast AI to access the latest models. But after seeing how well the models run on Open WebUI, Iâ€™m starting to wonder if paying $20/month for Raycast, OpenAI, or Anthropic is really worth it.&lt;/p&gt; &lt;p&gt;Itâ€™s not about the moneyâ€”I can afford itâ€”but Iâ€™m curious if others here subscribe to these providers. Iâ€™m even considering setting up a local server to run models myself. Would love to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mainaisakyuhoon"&gt; /u/mainaisakyuhoon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxz7yi/whats_the_value_of_paying_20_a_month_for_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxz7yi/whats_the_value_of_paying_20_a_month_for_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxz7yi/whats_the_value_of_paying_20_a_month_for_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T02:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxjbb5</id>
    <title>QwQ 32B is Amazing (&amp; Sharing my 131k + Imatrix)</title>
    <updated>2025-05-28T15:00:38+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious what your experience has been with QwQ 32B. I've seen really good takes on QwQ vs Qwen3, but I think they're not comparable. Here's the differences I see and I'd love feedback.&lt;/p&gt; &lt;h1&gt;When To Use Qwen3&lt;/h1&gt; &lt;p&gt;If I had to choose between QwQ 32B versus Qwen3 for daily AI assistant tasks, I'd choose Qwen3. This is because for 99% of general questions or work, Qwen3 is faster, answers just as well, and does amazing. As where QwQ 32B will do just as good, but it'll often over think and spend much longer answering any question.&lt;/p&gt; &lt;h1&gt;When To Use QwQ 32B&lt;/h1&gt; &lt;p&gt;Now for an AI agent or doing orchestration level work, I would choose QwQ all day every day. It's not that Qwen3 is bad, but it cannot handle the same level of semantic orchestration. In fact, ChatGPT 4o can't keep up with what I'm pushing QwQ to do.&lt;/p&gt; &lt;h1&gt;Benchmarks&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix/blob/main/Benchmarks/Simulation%20Fidelity%20Benchmark.md"&gt;Simulation Fidelity Benchmark&lt;/a&gt; is something I created a long time ago. Firstly I love RP based D&amp;amp;D inspired AI simulated games. But, I've always hated how current AI systems makes me the driver, but without any gravity. Anything and everything I say goes, so years ago I made a benchmark that is meant to be a better enforcement of simulated gravity. And as I'd eventually build agents that'd do real world tasks, this test funnily was an amazing benchmark for everything. So I know it's dumb that I use something like this, but it's been a fantastic way for me to gauge the wisdom of an AI model. I've often valued wisdom over intelligence. It's not about an AI knowing a random capital of X country, it's about knowing when to Google the capital of X country. &lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix/tree/main/Benchmarks"&gt;Benchmark Tests&lt;/a&gt; are here. And if more details on inputs or anything are wanted, I'm more than happy to share. My system prompt was counted with GPT 4 token counter (bc I'm lazy) and it was ~6k tokens. Input was ~1.6k. The shown benchmarks was the end results. But I had tests ranging a total of ~16k tokens to ~40k tokens. I don't have the hardware to test further sadly.&lt;/p&gt; &lt;h1&gt;My Experience With QwQ 32B&lt;/h1&gt; &lt;p&gt;So, what am I doing? Why do I like QwQ? Because it's not just emulating a good story, it's remembering many dozens of semantic threads. Did an item get moved? Is the scene changing? Did the last result from context require memory changes? Does the current context provide sufficient information or is the custom RAG database created needed to be called with an optimized query based on meta data tags provided?&lt;/p&gt; &lt;p&gt;Oh I'm just getting started, but I've been pushing QwQ to the absolute edge. Because AI agents whether a dungeon master of a game, creating projects, doing research, or anything else. A single missed step is catastrophic to simulated reality. Missed contexts leads to semantic degradation in time. Because my agents have to consistently alter what it remembers or knows. I have limited context limits, so it must always tell the future version that must run what it must do for the next part of the process.&lt;/p&gt; &lt;p&gt;Qwen3, Gemma, GPT 4o, they do amazing. To a point. But they're trained to be assistants. But QwQ 32B is weird, incredibly weird. The kind of weird I love. It's an agent level battle tactician. I'm allowing my agent to constantly rewrite it's own system prompts (partially), have full access to grab or alter it's own short term and long term memory, and it's not missing a beat.&lt;/p&gt; &lt;p&gt;The perfection is what makes QwQ so very good. Near perfection is required when doing wisdom based AI agent tasks.&lt;/p&gt; &lt;h1&gt;QwQ-32B-Abliterated-131k-GGUF-Yarn-Imatrix&lt;/h1&gt; &lt;p&gt;I've enjoyed QwQ 32B so much that I made my own version. Note, this isn't a fine tune or anything like that, but my own custom GGUF converted version to run on llama.cpp. But I did do the following:&lt;/p&gt; &lt;p&gt;1.) Altered the llama.cpp conversion script to add yarn meta data tags. (TLDR, unlocked the normal 8k precision but can handle ~32k to 131,072 tokens)&lt;/p&gt; &lt;p&gt;2.) Utilized a hybrid FP16 process with all quants with embed, output, all 64 layers (attention/feed forward weights + bias).&lt;/p&gt; &lt;p&gt;3.) Q4 to Q6 were all created with a ~16M token imatrix to make them significantly better and bring the level of precision much closer to Q8. (Q8 excluded, reasons in repo).&lt;/p&gt; &lt;p&gt;The repo is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix"&gt;https://huggingface.co/datasets/magiccodingman/QwQ-32B-abliterated-131k-GGUF-Yarn-Imatrix&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Have You Really Used QwQ?&lt;/h1&gt; &lt;p&gt;I've had a fantastic time with QwQ 32B so far. When I say that Qwen3 and other models can't keep up, I've genuinely tried to put each in an environment to compete on equal footing. It's not that everything else was &amp;quot;bad&amp;quot; it just wasn't as perfect as QwQ. But I'd also love feedback.&lt;/p&gt; &lt;p&gt;I'm more than open to being wrong and hearing why. Is Qwen3 able to hit just as hard? Note I did utilize Qwen3 of all sizes plus think mode.&lt;/p&gt; &lt;p&gt;But I've just been incredibly happy to use QwQ 32B because it's the first model that's open source and something I can run locally that can perform the tasks I want. So far any API based models to do the tasks I wanted would cost ~$1k minimum a month, so it's really amazing to be able to finally run something this good locally.&lt;/p&gt; &lt;p&gt;If I could get just as much power with a faster, more efficient, or smaller model, that'd be amazing. But, I can't find it.&lt;/p&gt; &lt;h1&gt;Q&amp;amp;A&lt;/h1&gt; &lt;p&gt;Just some answers to questions that are relevant:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What's my hardware setup&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Used 2x 3090's with the following llama.cpp settings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--no-mmap --ctx-size 32768 --n-gpu-layers 256 --tensor-split 20,20 --flash-attn &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxjbb5/qwq_32b_is_amazing_sharing_my_131k_imatrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T15:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxubqe</id>
    <title>Ollama now supports streaming responses with tool calling</title>
    <updated>2025-05-28T22:18:32+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxubqe/ollama_now_supports_streaming_responses_with_tool/"&gt; &lt;img alt="Ollama now supports streaming responses with tool calling" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Ollama now supports streaming responses with tool calling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/streaming-tool"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxubqe/ollama_now_supports_streaming_responses_with_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxubqe/ollama_now_supports_streaming_responses_with_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T22:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdm2z</id>
    <title>DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324</title>
    <updated>2025-05-28T10:25:48+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt; &lt;img alt="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" src="https://b.thumbs.redditmedia.com/Dz5hcsX2WLjgLhcJJfOpbDqkyNTpx1Aiw9gp50Wdl_Q.jpg" title="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The official DeepSeek group has issued an announcement claiming an upgrade, possibly a new model similar to the 0324 version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kxdm2z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:25:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxoehp</id>
    <title>New Expressive Open source TTS model</title>
    <updated>2025-05-28T18:21:10+00:00</updated>
    <author>
      <name>/u/manmaynakhashi</name>
      <uri>https://old.reddit.com/user/manmaynakhashi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt; Exaggeration slider let's you control intensity.&lt;/p&gt; &lt;p&gt;model weights: &lt;a href="https://huggingface.co/ResembleAI/chatterbox"&gt;https://huggingface.co/ResembleAI/chatterbox&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hf space: &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;https://huggingface.co/spaces/ResembleAI/Chatterbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/manmaynakhashi"&gt; /u/manmaynakhashi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoehp/new_expressive_open_source_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoehp/new_expressive_open_source_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoehp/new_expressive_open_source_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T18:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxaxw9</id>
    <title>The Economist: "Companies abandon their generative AI projects"</title>
    <updated>2025-05-28T07:23:16+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/P51MQ"&gt;recent article&lt;/a&gt; in the Economist claims that &amp;quot;the share of companies abandoning most of their generative-AI pilot projects has risen to 42%, up from 17% last year.&amp;quot; Apparently companies who invested in generative AI and slashed jobs are now disappointed and they began rehiring humans for roles.&lt;/p&gt; &lt;p&gt;The hype with the generative AI increasingly looks like a &amp;quot;we have a solution, now let's find some problems&amp;quot; scenario. Apart from software developers and graphic designers, I wonder how many professionals actually feel the impact of generative AI in their workplace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T07:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxw6b9</id>
    <title>Nvidia CEO says that Huawei's chip is comparable to Nvidia's H200.</title>
    <updated>2025-05-28T23:40:56+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a interview with Bloomberg today, Jensen came out and said that Huawei's offering is as good as the Nvidia H200. Which kind of surprised me. Both that he just came out and said it and that it's so good. Since I thought it was only as good as the H100. But if anyone knows, Jensen would know.&lt;/p&gt; &lt;p&gt;Update: Here's the interview.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=c-XAL2oYelI"&gt;https://www.youtube.com/watch?v=c-XAL2oYelI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T23:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxybgo</id>
    <title>Deepseek R1.1 aider polyglot score</title>
    <updated>2025-05-29T01:22:54+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1.1 scored the same as claude-opus-4-nothink 70.7% on aider polyglot.&lt;/p&gt; &lt;p&gt;Old R1 was 56.9%&lt;/p&gt; &lt;p&gt;&lt;code&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tmp.benchmarks/2025-05-28-18-57-01--deepseek-r1-0528 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ - dirname: 2025-05-28-18-57-01--deepseek-r1-0528 test_cases: 225 model: deepseek/deepseek-reasoner edit_format: diff commit_hash: 119a44d, 443e210-dirty pass_rate_1: 35.6 pass_rate_2: 70.7 pass_num_1: 80 pass_num_2: 159 percent_cases_well_formed: 90.2 error_outputs: 51 num_malformed_responses: 33 num_with_malformed_responses: 22 user_asks: 111 lazy_comments: 1 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 0 prompt_tokens: 3218121 completion_tokens: 1906344 test_timeouts: 3 total_tests: 225 command: aider --model deepseek/deepseek-reasoner date: 2025-05-28 versions: 0.83.3.dev seconds_per_case: 566.2 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cost came out to $3.05, but this is off time pricing, peak time is $12.20&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T01:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxvaq2</id>
    <title>New Deepseek R1's long context results</title>
    <updated>2025-05-28T23:01:16+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"&gt; &lt;img alt="New Deepseek R1's long context results" src="https://preview.redd.it/n3mjiheosl3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3533edf28a60d70e6e211ebf6d0ad8a8bf58596d" title="New Deepseek R1's long context results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3mjiheosl3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T23:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxmgtr</id>
    <title>DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)</title>
    <updated>2025-05-28T17:04:48+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"&gt; &lt;img alt="DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)" src="https://external-preview.redd.it/dnJvNHd1dzkwazNmMfbq08Ky_kl08uBBBLb2R6rGiFj8hH36RtTI5_C0jZhK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7de07fc0dd217d9dfd656339a5f8ef5292948f" title="DeepSeek-R1-0528 VS claude-4-sonnet (still a demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The heptagon + 20 balls benchmark can no longer measure their capabilities, so I'm preparing to try something new&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4lh915x90k3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxmgtr/deepseekr10528_vs_claude4sonnet_still_a_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxoco5</id>
    <title>Chatterbox TTS 0.5B - Claims to beat eleven labs</title>
    <updated>2025-05-28T18:19:08+00:00</updated>
    <author>
      <name>/u/Du_Hello</name>
      <uri>https://old.reddit.com/user/Du_Hello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"&gt; &lt;img alt="Chatterbox TTS 0.5B - Claims to beat eleven labs" src="https://external-preview.redd.it/dmdxNW5pN3JjazNmMWJmZsSyJYSsSqC3nLOUAsuog5kud_cYJD6JARLf_51k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24532c5a7bb011896aee76e14448d8f94796530b" title="Chatterbox TTS 0.5B - Claims to beat eleven labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Du_Hello"&gt; /u/Du_Hello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i6nfhj7rck3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T18:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxnjrj</id>
    <title>DeepSeek-R1-0528 ðŸ”¥</title>
    <updated>2025-05-28T17:47:44+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxry4x</id>
    <title>New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!</title>
    <updated>2025-05-28T20:41:49+00:00</updated>
    <author>
      <name>/u/Gloomy-Signature297</name>
      <uri>https://old.reddit.com/user/Gloomy-Signature297</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"&gt; &lt;img alt="New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!" src="https://preview.redd.it/51sg1oyu3l3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=754869c742901ed36f078e402c4f0da41133524e" title="New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gloomy-Signature297"&gt; /u/Gloomy-Signature297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/51sg1oyu3l3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T20:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxxmdr</id>
    <title>DeepSeek R1 05 28 Tested. It finally happened. The ONLY model to score 100% on everything I threw at it.</title>
    <updated>2025-05-29T00:48:50+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ladies and gentlemen, It finally happened. &lt;/p&gt; &lt;p&gt;I knew this day was coming. I knew that one day, a model would come along that would be able to score a 100% on every single task I throw at it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4CXkmFbgV28"&gt;https://www.youtube.com/watch?v=4CXkmFbgV28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Past few weeks have been busy - OpenAI 4.1, Gemini 2.5, Claude 4 - They all did very well, but none were able to score a perfect 100% across every single test. DeepSeek R1 05 28 is the FIRST model ever to do this. &lt;/p&gt; &lt;p&gt;And mind you, these aren't impractical tests like you see many folks on youtube doing. Like number of rs in strawberry or write a snake game etc. These are tasks that we actively use in real business applications, and from those, we chose the edge cases on the more complex side of things. &lt;/p&gt; &lt;p&gt;I feel like I am Anton from Ratatouille (if you have seen the movie). I am deeply impressed (pun intended) but also a little bit numb, and having a hard time coming up with the right words. That a free, MIT licensed model from a largely unknown lab until last year has done better than the commercial frontier is wild.&lt;/p&gt; &lt;p&gt;Usually in my videos, I explain the test, and then talk about the mistakes the models are making. But today, since there ARE NO mistakes, I am going to do something different. For each test, i am going to show you a couple of examples of the model's responses - and how hard these questions are, and I hope that gives you a deep sense of appreciation of what a powerful model this is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T00:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxs47i</id>
    <title>DeepSeek: R1 0528 is lethal</title>
    <updated>2025-05-28T20:48:14+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just used DeepSeek: R1 0528 to address several ongoing coding challenges in RooCode. &lt;/p&gt; &lt;p&gt;This model performed exceptionally well, resolving all issues seamlessly. I hit up DeepSeek via OpenRouter, and the results were DAMN impressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T20:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxnggx</id>
    <title>deepseek-ai/DeepSeek-R1-0528</title>
    <updated>2025-05-28T17:44:07+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:44:07+00:00</published>
  </entry>
</feed>
