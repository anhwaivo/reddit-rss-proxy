<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-09T07:37:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1luw2yu</id>
    <title>In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js</title>
    <updated>2025-07-08T18:20:48+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt; &lt;img alt="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" src="https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c30389f676a71708930e865f312a96dcfa3be046" title="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren't any that I saw. &lt;/p&gt; &lt;p&gt;Anyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using &lt;a href="https://github.com/docling-project/docling/discussions/354"&gt;DocTags&lt;/a&gt; &lt;a href="https://arxiv.org/html/2503.11576v1#S3"&gt;(a custom markup language format made by the Docling team from what I've gathered)&lt;/a&gt;, then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.&lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu"&gt;HF Space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;Demo Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zm461kmdzobf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv94fb</id>
    <title>Code for Skywork-R1V3-38B</title>
    <updated>2025-07-09T03:54:57+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv94fb/code_for_skyworkr1v338b/"&gt; &lt;img alt="Code for Skywork-R1V3-38B" src="https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec52a006c597881fe28d0949f7d9f2503e2791c7" title="Code for Skywork-R1V3-38B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv94fb/code_for_skyworkr1v338b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv94fb/code_for_skyworkr1v338b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T03:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv7s0r</id>
    <title>Reimplementing an LLM from Scratch</title>
    <updated>2025-07-09T02:44:06+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently reimplemented Google's open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.&lt;/p&gt; &lt;p&gt;This was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers: - &lt;a href="https://arxiv.org/pdf/2403.08295"&gt;Gemma 1&lt;/a&gt; - &lt;a href="https://arxiv.org/pdf/2408.00118"&gt;Gemma 2&lt;/a&gt; - &lt;a href="https://arxiv.org/pdf/2406.07101"&gt;Gemma 3&lt;/a&gt; (multimodal vision)&lt;/p&gt; &lt;p&gt;This was a purely educational reimplementation.&lt;/p&gt; &lt;p&gt;I also shared this on LinkedIn with more details if you're curious: üîó &lt;a href="https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;amp;utm_medium=member_desktop"&gt;LinkedIn post here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.&lt;/p&gt; &lt;p&gt;Would love any feedback, suggestions, or advice on what model to reimplement next!&lt;/p&gt; &lt;p&gt;Thanks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T02:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv9yhq</id>
    <title>OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well</title>
    <updated>2025-07-09T04:41:43+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"&gt; &lt;img alt="OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well" src="https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16f1fbd9c6cecc901549703403f1a2afe794b563" title="OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this is probably what a lot of us have been looking for. Haven‚Äôt tried it yet but will be downloading shortly. &lt;/p&gt; &lt;p&gt;From their GitHub page:&lt;/p&gt; &lt;p&gt;‚ÄúHow is this different than Claude Code?&lt;/p&gt; &lt;p&gt;It's very similar to Claude Code in terms of capability. Here are the key differences:&lt;/p&gt; &lt;p&gt;100% open source Not coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important. A focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what's possible in the terminal. A client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sst/opencode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T04:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv910v</id>
    <title>State of Foundation Models, 2025 | Innovation Endeavors</title>
    <updated>2025-07-09T03:49:44+00:00</updated>
    <author>
      <name>/u/LeveredRecap</name>
      <uri>https://old.reddit.com/user/LeveredRecap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://macro.com/app/pdf/696626a1-216b-493a-a0dd-181ce51ce327"&gt;&lt;strong&gt;State of Foundation Models, 2025 | Innovation Endeavors&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Generative AI has gone mainstream&lt;/strong&gt; ‚Äì 1 in 8 workers worldwide now uses AI every month, with 90% of that growth happening in the last 6 months. AI-native applications are now well into the billions of annual run rate revenue.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scaling continues across all dimensions&lt;/strong&gt; ‚Äì All technical metrics for models continue to improve &amp;gt;10x year-over-year, including cost, intelligence, context windows, and more. The average duration of human task a model can reliably do is doubling every 7 months.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The economics of foundation models are confusing&lt;/strong&gt; ‚Äì OpenAI &amp;amp; Anthropic are showing truly unprecedented growth, accelerating at $B+ of annual revenue. But, end-to-end training costs for frontier models near $500M, and the typical model becomes obsolete within 3 weeks of launch thanks to competition &amp;amp; open source convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Just like the smartest humans, the smartest AI will ‚Äúthink before it speaks‚Äù&lt;/strong&gt; ‚Äì Reasoning models trained to think before responding likely represent a new scaling law ‚Äî but training them requires significant advances in post-training, including reinforcement learning &amp;amp; reward models. Post-training may become more important than pre-training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI has now infiltrated almost all specialist professions&lt;/strong&gt; ‚Äì From engineers and accountants to designers and lawyers, AI copilots and agents are now tackling high-value tasks in virtually all knowledge worker domains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agents finally work, but we are early in understanding how to build AI products&lt;/strong&gt; ‚Äì Agents have finally hit the mainstream, but design patterns &amp;amp; system architectures for AI products are still extremely early.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚ÄúAI-native‚Äù organizations will look very different&lt;/strong&gt; ‚Äì Flatter teams of capable generalists will become the norm as generative AI lessens the value of specialized skills. Many roles will blur ‚Äî such as product, design, &amp;amp; engineering.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeveredRecap"&gt; /u/LeveredRecap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T03:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurili</id>
    <title>Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)</title>
    <updated>2025-07-08T15:26:41+00:00</updated>
    <author>
      <name>/u/WithoutReason1729</name>
      <uri>https://old.reddit.com/user/WithoutReason1729</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt; &lt;img alt="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" src="https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0bbd88f042ac1925e2d60123ccd65702e90497" title="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WithoutReason1729"&gt; /u/WithoutReason1729 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:26:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1luq8hp</id>
    <title>Skywork/Skywork-R1V3-38B ¬∑ Hugging Face</title>
    <updated>2025-07-08T14:37:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt; &lt;img alt="Skywork/Skywork-R1V3-38B ¬∑ Hugging Face" src="https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef" title="Skywork/Skywork-R1V3-38B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Skywork-R1V3-38B&lt;/strong&gt; is the &lt;strong&gt;latest and most powerful open-source multimodal reasoning model&lt;/strong&gt; in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves &lt;strong&gt;open-source state-of-the-art (SOTA)&lt;/strong&gt; performance across multiple multimodal reasoning benchmarks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#%F0%9F%8C%9F-key-results"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;üåü Key Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MMMU:&lt;/strong&gt; 76.0 ‚Äî &lt;em&gt;Open-source SOTA, approaching human experts (76.2)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;EMMA-Mini(CoT):&lt;/strong&gt; 40.3 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMK12:&lt;/strong&gt; 78.5 ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Physics Reasoning:&lt;/strong&gt; PhyX-MC-TM (52.8), SeePhys (31.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic Reasoning:&lt;/strong&gt; MME-Reasoning (42.8) ‚Äî &lt;em&gt;Beats Claude-4-Sonnet&lt;/em&gt;, VisuLogic (28.5) ‚Äî &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math Benchmarks:&lt;/strong&gt; MathVista (77.1), MathVerse (59.6), MathVision (52.6) ‚Äî &lt;em&gt;Exceptional problem-solving&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T14:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujedm</id>
    <title>Hunyuan-A13B model support has been merged into llama.cpp</title>
    <updated>2025-07-08T08:36:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt; &lt;img alt="Hunyuan-A13B model support has been merged into llama.cpp" src="https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85e0ea0459ffe03d3921b645c9c77dcaf2f99bd" title="Hunyuan-A13B model support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14425"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv85jp</id>
    <title>Day 12/50: Building a Small Language Model from Scratch - Implementing a Simplified Attention Mechanism in Python</title>
    <updated>2025-07-09T03:03:21+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;On Day 11, I gave you a brief introduction to the attention mechanism. Today, we‚Äôre going to implement it from scratch in Python. But before we dive into the code, let‚Äôs quickly revisit what attention is all about.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;What Is Attention? &lt;/h1&gt; &lt;p&gt;&lt;em&gt;Imagine you‚Äôre in a room with five people, and you‚Äôre trying to understand what‚Äôs going on. You don‚Äôt pay equal attention to all five people, you naturally focus more on the person who‚Äôs talking about something relevant.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;That‚Äôs exactly what attention does for LLMs. When reading a sentence, the model ‚Äúpays more attention‚Äù to the words that are important for understanding the context.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let‚Äôs break it down with a simple example and real code!&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Our Example: ‚ÄúCats love cozy windows‚Äù&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here‚Äôs what our made-up word vectors look like:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch inputs = torch.tensor([ [0.10, 0.20, 0.30], # Cats (x¬π) [0.40, 0.50, 0.60], # love (x¬≤) [0.70, 0.80, 0.10], # cozy (x¬≥) [0.90, 0.10, 0.20] # windows (x‚Å¥) ]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Each row is an embedding for a word, just another way of saying, ‚Äúthis is how the model understands the meaning of the word in numbers.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1: Calculating Attention Scores (How Similar Are These Words?)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Let‚Äôs say we want to find out how much attention the word&lt;/em&gt; &lt;strong&gt;&lt;em&gt;‚Äú&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;strong&gt;&lt;em&gt;‚Äù&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(second word) should pay to all the others.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We do that by computing the dot product between the vector for ‚Äúlove‚Äù and the others. The higher the score, the more related they are.&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;query = inputs[1] # Embedding for &amp;quot;love&amp;quot; attn_scores = torch.empty(inputs.shape[0]) for i, x_i in enumerate(inputs): attn_scores[i] = torch.dot(query, x_i) print(attn_scores) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Or, even faster, do it for all words at once using matrix multiplication:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_scores_all = inputs @ inputs.T print(attn_scores_all) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;This gives us a matrix of similarities, each number tells how strongly one word is related to another.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;2: Turning Scores into Meaningful Weights (Using Softmax)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We use the softmax function to do this:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores_all, dim=-1) print(attn_weights) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much ‚Äúlove‚Äù attends to ‚ÄúCats,‚Äù ‚Äúcozy,‚Äù and ‚Äúwindows.‚Äù&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;3: Creating a Context Vector (The Final Mix)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Here‚Äôs the cool part.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Each word‚Äôs final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If ‚Äúlove‚Äù pays 70% attention to ‚ÄúCats‚Äù and 30% to ‚Äúcozy,‚Äù the context vector will be a blend of those two word vectors.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let‚Äôs do it manually for ‚Äúlove‚Äù (row 2):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_weights_love = attn_weights[1] context_vec_love = torch.zeros_like(inputs[0]) for i, x_i in enumerate(inputs): context_vec_love += attn_weights_love[i] * x_i print(context_vec_love) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Or faster, do it for all words at once:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;context_vectors = attn_weights @ inputs print(context_vectors) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Each row now holds a new version of the word that includes information from the whole sentence.&lt;/em&gt; &lt;/p&gt; &lt;h1&gt;Why Does This Matter?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;This mechanism helps LLMs:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Understand context:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It‚Äôs not just ‚Äúwhat‚Äù a word is but how it fits in the sentence.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Be smarter with predictions:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It can now decide that ‚Äúwindows‚Äù is important because ‚Äúcats love cozy windows.‚Äù&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Handle longer sentences:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Attention lets the model scale and stay relevant, even with lots of words.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR &lt;/h1&gt; &lt;p&gt;&lt;em&gt;The attention mechanism in LLMs:&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;em&gt;Calculates how similar each word is to every other word.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Converts those scores into weights (softmax).&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Builds a new vector for each word using those weights (context vector).&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If this helped clarify things, let me know!&lt;/em&gt;.&lt;em&gt;Tomorrow we are going to code the self attention mechanism with key, query and value matrices.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T03:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv1z7b</id>
    <title>Why hasn't RTX Pro 6000 Balckwell significantly shake down the price of older RTX 6000 / RTX 6000 Ada</title>
    <updated>2025-07-08T22:12:27+00:00</updated>
    <author>
      <name>/u/--dany--</name>
      <uri>https://old.reddit.com/user/--dany--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k) and $2k apart for used ($8k - only new, $6k and $4k).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--dany--"&gt; /u/--dany-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvbmje</id>
    <title>I Built a Multi-Agent System to Generate Better Tech Conference Talk Abstracts</title>
    <updated>2025-07-09T06:23:22+00:00</updated>
    <author>
      <name>/u/Creepy-Row970</name>
      <uri>https://old.reddit.com/user/Creepy-Row970</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been speaking at a lot of tech conferences lately, and one thing that never gets easier is &lt;strong&gt;writing a solid talk proposal&lt;/strong&gt;. A good abstract needs to be technically deep, timely, and clearly valuable for the audience, and it also needs to stand out from all the similar talks already out there.&lt;/p&gt; &lt;p&gt;So I built a new multi-agent tool to help with that.&lt;/p&gt; &lt;p&gt;It works in 3 stages:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt; ‚Äì Does deep research on your topic using real-time web search and trend detection, so you know what‚Äôs relevant &lt;em&gt;right now&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vector Database&lt;/strong&gt; ‚Äì Uses Couchbase to semantically match your idea against previous KubeCon talks and avoids duplication.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; ‚Äì Pulls together everything (your input, current research, and related past talks) to generate a unique and actionable abstract you can actually submit.&lt;/p&gt; &lt;p&gt;Under the hood, it uses:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Google ADK for orchestrating the agents&lt;/li&gt; &lt;li&gt;Couchbase for storage + fast vector search&lt;/li&gt; &lt;li&gt;Nebius models (e.g. Qwen) for embeddings and final generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The end result? A tool that helps you write &lt;strong&gt;better, more relevant, and more original conference talk proposals.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs still an early version, but it‚Äôs already helping me iterate ideas much faster.&lt;/p&gt; &lt;p&gt;If you're curious, here's the &lt;a href="https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/conference_talk_abstract_generator"&gt;Full Code&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Would love thoughts or feedback from anyone else working on conference tooling or multi-agent systems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creepy-Row970"&gt; /u/Creepy-Row970 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T06:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lumsd2</id>
    <title>Mac Studio 512GB online!</title>
    <updated>2025-07-08T12:04:20+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just had a $10k Mac Studio arrive. The first thing I installed was LM Studio. I downloaded qwen3-235b-a22b and fired it up. Fantastic performance with a small system prompt. I fired up devstral and tried to use it with Cline (a large system prompt agent) and very quickly discovered limitations. I managed to instruct the poor LLM to load the memory bank but it lacked all the comprehension that I get from google gemini. Next I'm going to try to use devstral in Act mode only and see if I can at least get some tool usage and code generation out of it, but I have serious doubts it will even work. I think a bigger reasoning model is needed for my use cases and this system would just be too slow to accomplish that.&lt;/p&gt; &lt;p&gt;That said, I wanted to share my experiences with the community. If anyone is thinking about buying a mac studio for LLMs, I'm happy to run any sort of use case evaluation for you to help you make your decision. Just comment in here and be sure to upvote if you do so other people see the post and can ask questions too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T12:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1luxu6s</id>
    <title>Any one tried ERNIE-4.5-21B-A3B?</title>
    <updated>2025-07-08T19:27:45+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv9m3j</id>
    <title>MemOS: A Memory OS for AI System</title>
    <updated>2025-07-09T04:22:13+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Website: &lt;a href="https://memos.openmem.net/"&gt;https://memos.openmem.net/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/MemTensor/MemOS"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge [1]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.03724"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T04:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurzqf</id>
    <title>NextCoder - a Microsoft Collection</title>
    <updated>2025-07-08T15:45:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt; &lt;img alt="NextCoder - a Microsoft Collection" src="https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b2179fc5426163403bc73a148e1730509944514" title="NextCoder - a Microsoft Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:45:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1luy32e</id>
    <title>SmolLM3 has day-0 support in MistralRS!</title>
    <updated>2025-07-08T19:37:29+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt; &lt;img alt="SmolLM3 has day-0 support in MistralRS!" src="https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07999e9a892b675527d3e33998c11728e0e28b01" title="SmolLM3 has day-0 support in MistralRS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a &lt;strong&gt;SoTA 3B model&lt;/strong&gt; with hybrid &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;128k context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Hits ‚ö°105 T/s with AFQ4 @ M3 Max.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using MistralRS means that you get&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Builtin MCP client&lt;/li&gt; &lt;li&gt;OpenAI HTTP server&lt;/li&gt; &lt;li&gt;Python &amp;amp; Rust APIs&lt;/li&gt; &lt;li&gt;Full multimodal inference engine (&lt;strong&gt;in&lt;/strong&gt;: image, audio, text in, &lt;strong&gt;out:&lt;/strong&gt; image, audio, text).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super easy to run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What's next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player"&gt;https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwtdr</id>
    <title>NSFW Model image analysis</title>
    <updated>2025-07-08T18:48:31+00:00</updated>
    <author>
      <name>/u/Technical_Whole_947</name>
      <uri>https://old.reddit.com/user/Technical_Whole_947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Whole_947"&gt; /u/Technical_Whole_947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvcb72</id>
    <title>Here is how we beat ChatGPT at classification with 1 dollar in cloud compute</title>
    <updated>2025-07-09T07:07:53+00:00</updated>
    <author>
      <name>/u/iamMess</name>
      <uri>https://old.reddit.com/user/iamMess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.&lt;/p&gt; &lt;p&gt;This tutorial comes in 3 different formats: 1. This LocalLLaMA post - summary and discussion 2. Our blog post - &lt;a href="https://syv.ai/viden/beating-chatgpt-dollar-dream"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt; 3. Our research paper - &lt;a href="https://arxiv.org/abs/2507.00214"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt; &lt;p&gt;What we did:&lt;/p&gt; &lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt; &lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt; &lt;p&gt;Key results: - 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001) - Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%) - Built-in interpretability - model explains its reasoning for every prediction - Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt; &lt;p&gt;The interesting bits:&lt;/p&gt; &lt;p&gt;What worked: - The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification - Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations - Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt; &lt;p&gt;What didn't: - Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes - More computationally expensive than standard fine-tuning - Quality heavily depends on the initial reasoning generator&lt;/p&gt; &lt;p&gt;Technical details: - Base model: Llama-3.2-1B-Instruct (both stages) - Reasoning dataset: &lt;a href="https://huggingface.co/datasets/syvai/reasoning-gen"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts) - Target task: dair-ai/emotion (6 basic emotions) - Training: Axolotl framework on A40 GPU - Reasoning generator model: &lt;a href="https://huggingface.co/syvai/reasoning-gen-1b"&gt;syvai/reasoning-gen-1b&lt;/a&gt; - Datasets: &lt;a href="https://huggingface.co/datasets/syvai/emotion-reasoning"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/syvai/no-emotion-reasoning"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamMess"&gt; /u/iamMess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T07:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lus2yw</id>
    <title>new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B</title>
    <updated>2025-07-08T15:48:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt; &lt;td align="left"&gt;61.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvbzpx</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-09T06:47:06+00:00</updated>
    <author>
      <name>/u/PotatoFormal8751</name>
      <uri>https://old.reddit.com/user/PotatoFormal8751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotatoFormal8751"&gt; /u/PotatoFormal8751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T06:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1luroqh</id>
    <title>NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$</title>
    <updated>2025-07-08T15:33:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt; &lt;img alt="NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$" src="https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c" title="NVIDIA‚Äôs Highly Anticipated ‚ÄúMini-Supercomputer,‚Äù the DGX Spark, Launches This Month ‚Äî Bringing Immense AI Power to Your Hands ‚Äî up to 4000$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lusr7l</id>
    <title>SmolLM3: reasoning, long context and multilinguality for 3B parameter only</title>
    <updated>2025-07-08T16:14:16+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt; &lt;img alt="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" src="https://preview.redd.it/njam3shfcobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281" title="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use! &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/smollm3"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br /&gt; GGUF/ONIX ckpt are being uploaded here: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njam3shfcobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv53nn</id>
    <title>What's local about this?</title>
    <updated>2025-07-09T00:32:32+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt; &lt;img alt="What's local about this?" src="https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1" title="What's local about this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rqrg67unoobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T00:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
</feed>
