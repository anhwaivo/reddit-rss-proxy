<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-02T00:30:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mf3wr0</id>
    <title>Best way to run the Qwen3 30b A3B coder/instruct models for HIGH throughput and/or HIGH context? (on a single 4090)</title>
    <updated>2025-08-01T18:15:00+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for some &amp;quot;best practices&amp;quot; for this new 30B A3B to squeeze the most out of it with my 4090. Normally I'm pretty up to date on this stuff but I'm a month or so behind the times. I'll share where I'm at and hopefully somebody's got some suggestions :).&lt;/p&gt; &lt;p&gt;I'm sitting on 64gb ram/24gb vram (4090). I'm open to running this thing in ik_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I'd like to have the best of all worlds (fast, low latency, high throughput), but I know it's all a bit of a &amp;quot;pick two&amp;quot; situation usually.&lt;/p&gt; &lt;p&gt;I've got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.&lt;/p&gt; &lt;p&gt;With that setup with 16k context:&lt;/p&gt; &lt;p&gt;Overall tokens/sec (single user, single request): 181.30t/s&lt;/p&gt; &lt;p&gt;Mean latency: 2.88s&lt;/p&gt; &lt;p&gt;Mean Time to First Token: 0.046s&lt;/p&gt; &lt;p&gt;Max Batching tokens/s: 2,549.14t/s (100 requests)&lt;/p&gt; &lt;p&gt;That's not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I'm curious what my options are out there because I wouldn't mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I'd -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik_llama want to chime in with some idea of how its performing and how you'r setting it up? &lt;/p&gt; &lt;p&gt;Open to any advice. I'd like to get this thing running as best I can for both a single user AND for batch-use (I'm fine with it being two separate setups, I can run them when needed appropriately).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:15:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mewq1v</id>
    <title>Hugging Face space for anyone who want to try the new Dots OCR</title>
    <updated>2025-08-01T13:37:45+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"&gt; &lt;img alt="Hugging Face space for anyone who want to try the new Dots OCR" src="https://external-preview.redd.it/jdWLa4yLFu9Ou5jfhrAqJqpvnaA7jgwPFox4bV2vTWM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b249c1a963b7199d36b85e3948a0475db9194b46" title="Hugging Face space for anyone who want to try the new Dots OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My initial experiments with the model is very positive, i hope the space is useful for anyone who want to try the model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/MohamedRashad/Dots-OCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mewq1v/hugging_face_space_for_anyone_who_want_to_try_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T13:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1meucvo</id>
    <title>Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct</title>
    <updated>2025-08-01T11:49:43+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt; &lt;img alt="Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct" src="https://b.thumbs.redditmedia.com/VNoO8rEJvUZhkVA7_FI5gY826jnCUY53ZbZFN6EhWxs.jpg" title="Unsloth GGUFs Perplexity Score Comparison | Qwen3-Coder-30B-A3B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b"&gt;https://preview.redd.it/8edgr1plcegf1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a4d35aab872fa2e1ca113eb8f8510212b54d68b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lower PPL = Better&lt;/p&gt; &lt;p&gt;I didn't test q6 and q8 because they can't fit in my 24gb card&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity.exe --model &amp;quot;&amp;quot; --threads 15 --ctx-size 8000 -f wiki.test.raw --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 99 --mlock --parallel 8 --seed 7894 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 --repeat-penalty 1.05 --presence-penalty 1.5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IQ4_XS&lt;br /&gt; 7 experts PPL = 7.6844&lt;br /&gt; default 8 experts PPL = 7.6741&lt;br /&gt; 9 experts PPL = 7.6890&lt;br /&gt; 10 experts PPL = 7.7343&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meucvo/unsloth_ggufs_perplexity_score_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T11:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mes7rc</id>
    <title>Quantize your own GGUFs the same way as your fav Unsloth Dynamic GGUFs</title>
    <updated>2025-08-01T09:48:04+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/electroglyph/quant_clone"&gt;https://github.com/electroglyph/quant_clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.&lt;/p&gt; &lt;p&gt;For instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf&lt;/p&gt; &lt;p&gt;you get:&lt;/p&gt; &lt;p&gt;llama-quantize --imatrix &amp;lt;imatrix_unsloth.dat&amp;gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &amp;lt;input.gguf&amp;gt; &amp;lt;output.gguf&amp;gt; Q8_0&lt;/p&gt; &lt;p&gt;note that the Q8_0 at the end is just to get llama-quantize to do it's thing (F16/F32/COPY doesn't run quantization). all the tensors will be overridden with the actual --tensor-type params&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T09:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepr38</id>
    <title>DocStrange - Open Source Document Data Extractor</title>
    <updated>2025-08-01T07:08:55+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt; &lt;img alt="DocStrange - Open Source Document Data Extractor" src="https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=12643bc505cd05a85286b55a7fff556b82b4872a" title="DocStrange - Open Source Document Data Extractor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor extractor = DocumentExtractor() result = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;) # Get clean markdown for LLM training markdown = result.extract_markdown() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install docstrange docstrange document.pdf --output json --extract-fields title author date &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/docstrange/"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vghke2r1ycgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf08e5</id>
    <title>SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?</title>
    <updated>2025-08-01T15:56:02+00:00</updated>
    <author>
      <name>/u/we_are_mammals</name>
      <uri>https://old.reddit.com/user/we_are_mammals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"&gt; &lt;img alt="SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?" src="https://preview.redd.it/0cq321qc1fgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=162d6f061d05ab906b370e0b7ce08f4a7f85014d" title="SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/we_are_mammals"&gt; /u/we_are_mammals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0cq321qc1fgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T15:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mesi2s</id>
    <title>GLM-4.5-Air running on 64GB Mac Studio(M4)</title>
    <updated>2025-08-01T10:05:19+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt; &lt;img alt="GLM-4.5-Air running on 64GB Mac Studio(M4)" src="https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf" title="GLM-4.5-Air running on 64GB Mac Studio(M4)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/87ng5bmisdgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T10:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0hou</id>
    <title>support for the upcoming hunyuan dense models has been merged into llama.cpp</title>
    <updated>2025-08-01T16:05:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"&gt; &lt;img alt="support for the upcoming hunyuan dense models has been merged into llama.cpp" src="https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f81f256091726e20730924e97a225729e6c971ec" title="support for the upcoming hunyuan dense models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)&lt;/p&gt; &lt;p&gt;bonus: fix hunyuan_moe chat template&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14878"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0i54</id>
    <title>Qwen 30b a3b 2507 instruct as good as Gemma 3 27B!?</title>
    <updated>2025-08-01T16:05:49+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.&lt;/p&gt; &lt;p&gt;Great at general knowledge, but also follows instructions very well.&lt;/p&gt; &lt;p&gt;Please let me know your experiences with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mexa2g</id>
    <title>Heads up to those that downloaded Qwen3 Coder 480B before yesterday</title>
    <updated>2025-08-01T14:00:47+00:00</updated>
    <author>
      <name>/u/VegetaTheGrump</name>
      <uri>https://old.reddit.com/user/VegetaTheGrump</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mentioned in the new, Qwen3 30B download announcement was that 480B's tool calling was fixed and it &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:%7E:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard"&gt;needed to be re-downloaded&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm just posting it so that no one misses it. I'm using LMStudio and it just showed as &amp;quot;downloaded&amp;quot;. It didn't seem to know there was a change.&lt;/p&gt; &lt;p&gt;EDIT: Yes, this only refers to the unsloth versions of 480B. Thank you &lt;a href="/u/MikeRoz"&gt;u/MikeRoz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetaTheGrump"&gt; /u/VegetaTheGrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T14:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfariy</id>
    <title>All local Roo Code and qwen3 coder 30B Q8</title>
    <updated>2025-08-01T22:51:12+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"&gt; &lt;img alt="All local Roo Code and qwen3 coder 30B Q8" src="https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931537c4a1cfa56a0f6a573590137de43c243511" title="All local Roo Code and qwen3 coder 30B Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Roo Code with &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mostlygeek/vibecities"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt; &lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's my llama-swap config: &lt;/p&gt; &lt;p&gt;``` macros: &amp;quot;qwen3-coder-server&amp;quot;: | /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0 --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05 --jinja --swa-full&lt;/p&gt; &lt;p&gt;models: &amp;quot;Q3-30B-CODER-3090&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot; description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot; filters: # enforce recommended params for model strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot; cmd: | ${qwen3-coder-server} --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf --ctx-size 184320 # rebalance layers/context a bit better across dual GPUs --tensor-split 46,54 ```&lt;/p&gt; &lt;p&gt;Roo code MCP settings: &lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;mcpServers&amp;quot;: { &amp;quot;vibecities&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;, &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;, &amp;quot;headers&amp;quot;: { &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot; }, &amp;quot;alwaysAllow&amp;quot;: [ &amp;quot;page_list&amp;quot;, &amp;quot;page_set&amp;quot;, &amp;quot;page_get&amp;quot; ], &amp;quot;disabled&amp;quot;: false } } } &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g5aj1csfjhgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf86rn</id>
    <title>Cold start vLLM in 5 seconds with GPU snapshotting</title>
    <updated>2025-08-01T21:02:42+00:00</updated>
    <author>
      <name>/u/crookedstairs</name>
      <uri>https://old.reddit.com/user/crookedstairs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/"&gt; &lt;img alt="Cold start vLLM in 5 seconds with GPU snapshotting" src="https://b.thumbs.redditmedia.com/9zGmAn5wtWE9ciYDNGxrwYmoTgYDYEOmBf7p4EOrZDY.jpg" title="Cold start vLLM in 5 seconds with GPU snapshotting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU snapshotting is finally a thing! NVIDIA recently released their &lt;a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html"&gt;CUDA checkpoint/restore API&lt;/a&gt; and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.&lt;/p&gt; &lt;p&gt;GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137"&gt;https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More on how GPU snapshotting works plus additional benchmarks in this blog post: &lt;a href="https://modal.com/blog/gpu-mem-snapshots"&gt;https://modal.com/blog/gpu-mem-snapshots&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crookedstairs"&gt; /u/crookedstairs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf4ihq</id>
    <title>Me lately... Anyone else can relate? 😎</title>
    <updated>2025-08-01T18:37:31+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/"&gt; &lt;img alt="Me lately... Anyone else can relate? 😎" src="https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3d8b06b0091af494d702ac39636bf603e600b301" title="Me lately... Anyone else can relate? 😎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer:&lt;/p&gt; &lt;p&gt;No actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rqzixk49cggf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8la7</id>
    <title>Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good</title>
    <updated>2025-08-01T21:19:07+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt; &lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfbw8a</id>
    <title>DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls</title>
    <updated>2025-08-01T23:43:00+00:00</updated>
    <author>
      <name>/u/JAlbrethsen</name>
      <uri>https://old.reddit.com/user/JAlbrethsen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"&gt; &lt;img alt="DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls" src="https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24b8b3213823bb044c73076e1852d1957545a17f" title="DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JAlbrethsen"&gt; /u/JAlbrethsen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T23:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepz8z</id>
    <title>OpenAI OS model info leaked - 120B &amp; 20B will be available</title>
    <updated>2025-08-01T07:23:36+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt; &lt;img alt="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" src="https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68" title="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/08m94pio0dgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf92r1</id>
    <title>MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs</title>
    <updated>2025-08-01T21:38:58+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt; &lt;img alt="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" src="https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg" title="MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt; &lt;h1&gt;Autonomous Research Mode&lt;/h1&gt; &lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt; &lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt; &lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt; &lt;h1&gt;Document Management&lt;/h1&gt; &lt;p&gt;The application is built around a document management system.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mf92r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf3nw4</id>
    <title>I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape</title>
    <updated>2025-08-01T18:05:28+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt; &lt;img alt="I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape" src="https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg" title="I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt; &lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt; &lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt; &lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt; &lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt; &lt;p&gt;You're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt; &lt;p&gt;That's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt; &lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers. You're overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt; &lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt; &lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt; &lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns. Make sure you're on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt; &lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/c2/leaderboard"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reasonscape.com/c2/explorer"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/the-crypt-keeper/reasonscape"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:05:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1meu3jn</id>
    <title>Gemini 2.5 Deep Think mode benchmarks!</title>
    <updated>2025-08-01T11:36:06+00:00</updated>
    <author>
      <name>/u/Beautiful-Essay1945</name>
      <uri>https://old.reddit.com/user/Beautiful-Essay1945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt; &lt;img alt="Gemini 2.5 Deep Think mode benchmarks!" src="https://preview.redd.it/8wnv6pme9egf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=557a01b879fc1bdbf0cc88dc3a91d0b4a7b1c10c" title="Gemini 2.5 Deep Think mode benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful-Essay1945"&gt; /u/Beautiful-Essay1945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8wnv6pme9egf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meu3jn/gemini_25_deep_think_mode_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T11:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepeqh</id>
    <title>The OpenAI Open weight model might be 120B</title>
    <updated>2025-08-01T06:47:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt; &lt;img alt="The OpenAI Open weight model might be 120B" src="https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg" title="The OpenAI Open weight model might be 120B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt; &lt;p&gt;So as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mepeqh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T06:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf0qlf</id>
    <title>Qwen3-235B-A22B-2507 is the top open weights model on lmarena</title>
    <updated>2025-08-01T16:14:40+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmarena_ai/status/1951308670375174457"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T16:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8pdo</id>
    <title>China report the finetune deepseek scientific model 40.44% on HLE</title>
    <updated>2025-08-01T21:23:37+00:00</updated>
    <author>
      <name>/u/Afraid_Hall_2971</name>
      <uri>https://old.reddit.com/user/Afraid_Hall_2971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt; &lt;img alt="China report the finetune deepseek scientific model 40.44% on HLE" src="https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19933808cef3cec6dce268be3e9d5d269f435579" title="China report the finetune deepseek scientific model 40.44% on HLE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hg：&lt;a href="https://huggingface.co/ScienceOne-AI/S1-Base-671B"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Afraid_Hall_2971"&gt; /u/Afraid_Hall_2971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rnyzqia76hgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T21:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf6bkl</id>
    <title>Qwen3-Embedding-0.6B is fast, high quality, and supports up to 32k tokens. Beats OpenAI embeddings on MTEB</title>
    <updated>2025-08-01T19:47:49+00:00</updated>
    <author>
      <name>/u/No_Edge2098</name>
      <uri>https://old.reddit.com/user/No_Edge2098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"&gt;https://huggingface.co/Qwen/Qwen3-Embedding-0.6B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I switched over today. Initially the results seemed poor, but it turns out there was an issue when using Text embedding inference 1.7.2 related to pad tokens. Fixed in 1.7.3 . Depending on what inference tooling you are using there could be a similar issue.&lt;/p&gt; &lt;p&gt;The very fast response time opens up new use cases. Most small embedding models until recently had very small context windows of around 512 tokens and the quality didn't rival the bigger models you could use through openAI or google.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Edge2098"&gt; /u/No_Edge2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf6bkl/qwen3embedding06b_is_fast_high_quality_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf6bkl/qwen3embedding06b_is_fast_high_quality_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf6bkl/qwen3embedding06b_is_fast_high_quality_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T19:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfaigh</id>
    <title>We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)</title>
    <updated>2025-08-01T22:40:00+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Organization&lt;/th&gt; &lt;th align="left"&gt;HuggingFace Link&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Modality&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;dots.ocr&lt;/td&gt; &lt;td align="left"&gt;REDnote Hilab&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;355B-A32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5-Air&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air Base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-Base"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;106B-A12B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;235B-A22B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;480B-A35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;Alibaba - Qwen&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Base&lt;/td&gt; &lt;td align="left"&gt;Moonshot AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T-32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intern S1&lt;/td&gt; &lt;td align="left"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;241B-A22B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;49B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 7B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 14B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenReasoning Nemotron 32B&lt;/td&gt; &lt;td align="left"&gt;Nvidia&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;step3&lt;/td&gt; &lt;td align="left"&gt;StepFun&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/stepfun-ai/step3"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;321B-A38B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt; &lt;td align="left"&gt;IPADS - PowerInfer&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B-A0.6B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X Instruct-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed X PPO-7B&lt;/td&gt; &lt;td align="left"&gt;ByteDance Seed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Machine Translation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Devstral Small 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Small 24B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Small-24B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Voxtral Mini 3B 2507&lt;/td&gt; &lt;td align="left"&gt;Mistral&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Audio-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4.5B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AFM 4.5B Base&lt;/td&gt; &lt;td align="left"&gt;Arcee AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ling lite-1.5 2506&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-lite-1.5-2506"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ming Lite Omni-1.5&lt;/td&gt; &lt;td align="left"&gt;Ant Group - Inclusion AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;20.3B&lt;/td&gt; &lt;td align="left"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 32B 0727&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 4B 0729&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UIGEN X 8B&lt;/td&gt; &lt;td align="left"&gt;Tesslate&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;command a vision 07-2025&lt;/td&gt; &lt;td align="left"&gt;Cohere&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-vision-07-2025"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;112B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT V1 40B&lt;/td&gt; &lt;td align="left"&gt;Kwaipilot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;40B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0.1 2B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE 4.0 32B&lt;/td&gt; &lt;td align="left"&gt;LG AI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;671B-A37B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-405B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;405B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;109B-A17B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cogito v2 preview llama-70B&lt;/td&gt; &lt;td align="left"&gt;Deep Cogito&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 4.0 VL Light&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-4.0-VL-Light"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A.X 3.1&lt;/td&gt; &lt;td align="left"&gt;SK Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/skt/A.X-3.1"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;35B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmOCR 7B 0725&lt;/td&gt; &lt;td align="left"&gt;AllenAI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B-A3B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kanana 1.5v 3B instruct&lt;/td&gt; &lt;td align="left"&gt;Kakao&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;Image-Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 7B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-7B"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;7B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 21B&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-21B"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;21B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Tri 70B preview SFT&lt;/td&gt; &lt;td align="left"&gt;Trillion Labs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;Text-to-Text&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt; &lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt; &lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T22:40:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf3tm9</id>
    <title>The “Leaked” 120 B OpenAI Model is not Trained in FP4</title>
    <updated>2025-08-01T18:11:35+00:00</updated>
    <author>
      <name>/u/badbutt21</name>
      <uri>https://old.reddit.com/user/badbutt21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt; &lt;img alt="The “Leaked” 120 B OpenAI Model is not Trained in FP4" src="https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06" title="The “Leaked” 120 B OpenAI Model is not Trained in FP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badbutt21"&gt; /u/badbutt21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g1yk8r6b8ggf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T18:11:35+00:00</published>
  </entry>
</feed>
