<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-19T05:34:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i4qrcx</id>
    <title>Training tiny LLM on local machine</title>
    <updated>2025-01-19T04:54:17+00:00</updated>
    <author>
      <name>/u/Dark_Knight003</name>
      <uri>https://old.reddit.com/user/Dark_Knight003</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a ML newbie and want to get into core AI. I plan to learn how to build LLMs and hopefully train a tiny LLM on local machine to understand the concepts in depth. However I understand that training LLMs requires massive GPU resources. &lt;/p&gt; &lt;p&gt;I want to know if it is possible to train a tiny LLM from scratch on local Mac with 32gb gpu (unified memory) that gives coherent and related output? I am planning to buy a new Mac hence wanted to understand this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Knight003"&gt; /u/Dark_Knight003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4qrcx/training_tiny_llm_on_local_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T04:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4r1ig</id>
    <title>Why reasoning models might be a huge breakthrough for local inference / training</title>
    <updated>2025-01-19T05:09:52+00:00</updated>
    <author>
      <name>/u/valewolf</name>
      <uri>https://old.reddit.com/user/valewolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently all the speculation I have seen out there suggests that OpenAI's O series of reasoning models still uses a relatively large expensive to train base model. Given the compute resources OpenAI has that is probably unlikely to change.&lt;/p&gt; &lt;p&gt;For those hoping to train and use models locally however it may be possible to extend the same concepts with a very small ~8B open source model that could easily by trained or run on almost any consumer GPU. Probably the amount of test time compute needed / length of reasoning traces needed to achieve good performance may be extreme but still you could probably get there. Because of the low GPU memory requirements of the base model getting a setup running would likely be easy even if its quite slow relatively speaking. &lt;/p&gt; &lt;p&gt;How far could this approach be scaled? could you use a 1B base model? Obviously there is a point where this may stop working but where is that point I wonder.&lt;/p&gt; &lt;p&gt;Anyways, is anyone aware of open source projects that have explored this idea? and assuming its feasible what would some of the major pros / cons be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valewolf"&gt; /u/valewolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4r1ig/why_reasoning_models_might_be_a_huge_breakthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T05:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4d7f3</id>
    <title>Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)</title>
    <updated>2025-01-18T17:57:29+00:00</updated>
    <author>
      <name>/u/s0n1cm0nk3y</name>
      <uri>https://old.reddit.com/user/s0n1cm0nk3y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt; &lt;img alt="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" src="https://b.thumbs.redditmedia.com/te1a0xEnqnHXgzGURJQZ5QsJ1nReFxta_RZWsEDD0xU.jpg" title="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, the money shot: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674"&gt;https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, I'm aware my PERCs are a bit close, I'm brainstorming on that. So the approach I took was following &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hozg2h/comment/m4di1mw/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;advice from FullStackSensei&lt;/a&gt; I acquired a used &lt;a href="https://www.youtube.com/watch?v=AM--NTHFBlI&amp;amp;lc=Ugzrysx0dPl-yrbO1yp4AaABAg.ADCmLC_52VxADCn-nMSdT-"&gt;GTX1080 dell reference card &lt;/a&gt;with issues. As the only things I needed were the fan and cooler, I wasn't too worried about it being for parts. It took some minor modifications, to include using a dremel and an oscillating cutter:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569"&gt;https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but as shown here, the temps are completely manageable, and the fan is barely blowing : &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958"&gt;https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Parts you'll need:&lt;/p&gt; &lt;p&gt;Links omitted to make sure I'm following guideines.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU fan adapter cable (look for &amp;quot;PWM GPU fan adapter cable&amp;quot;)&lt;/li&gt; &lt;li&gt;Thermal pads of varying sizes&lt;/li&gt; &lt;li&gt;PWM Fan Controller (I used the Coolerguys 12v PWM thermostat model)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps anyone having troubles like I was with all the 3d printed fan shrouds and their concern for noise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s0n1cm0nk3y"&gt; /u/s0n1cm0nk3y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:57:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i48dmj</id>
    <title>Has anyone tried anything besides native Python to build Agents?</title>
    <updated>2025-01-18T14:14:04+00:00</updated>
    <author>
      <name>/u/QaeiouX</name>
      <uri>https://old.reddit.com/user/QaeiouX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, it's a very common question around here to ask. Actually I am working a project and have been using simple python to build my agentic workflow. But as it is expanding, I am facing some issues on keeping up with it. I am planning to use some framework and Pydantic AI is on my radar. I am also interested by Bee Agent Framework but, it's written in typescript predominantly. If you have any other suggestions, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QaeiouX"&gt; /u/QaeiouX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T14:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4i1q8</id>
    <title>I made a Huggingface Space to help build commands for OpenVINO model conversion.</title>
    <updated>2025-01-18T21:35:04+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Optimum-CLI-Tool_tool&lt;/h1&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://huggingface.co/spaces/Echo9Zulu/Optimum-CLI-Tool_tool"&gt;Echo9Zulu/Optimum-CLI-Tool_tool&lt;/a&gt;, a Huggingface Space, that &amp;quot;GUI-ifies&amp;quot; selecting conversion parameters with the Optimum-CLI tool for OpenVINO with Gradio. OpenVINO is a Inference engine offering hardware acceleration for Intel CPUs, GPUs, and NPUs. OpenVINO runtime, Optimum-Intel for Transformers and OpenVINO GenAI all require a deceptively simple model export to an Intermediate Representation format.&lt;/p&gt; &lt;p&gt;I made this tool to help me convert models efficently and know others may find it useful for keeping things organized.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Build conversion commands from available parameters.&lt;/li&gt; &lt;li&gt;Respect positional arguments defined by Optimum.&lt;/li&gt; &lt;li&gt;Discussion of advanced usage for OpenVINO with Transformers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: At runtime your command references prebuilt configuration classes which don't support all models. However, if you open a PR the maintainers make it happen. See my issue &lt;a href="https://github.com/huggingface/optimum-intel/issues/1097"&gt;#1097&lt;/a&gt; and their PR &lt;a href="https://github.com/huggingface/optimum-intel/pull/1099"&gt;#1099&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Check your model architecture &lt;a href="https://huggingface.co/docs/optimum/main/en/intel/openvino/models"&gt;here&lt;/a&gt; to spare yourself a traceback.&lt;/p&gt; &lt;h1&gt;OpenVINO, briefly&lt;/h1&gt; &lt;p&gt;OpenVINO is a C++ runtime with Python APIs for managing inference deployments on Intel hardware from 6th gen forward. Using OpenVINO effectively requires new research for each usecase to discover facts about your model's architecture and the hardware you are using to choose the right settings. Choose your device type and use snippets &lt;a href="https://docs.openvino.ai/2024/about-openvino/compatibility-and-support/supported-devices.html"&gt;here&lt;/a&gt; to query device properties based on your hardware. This requires &lt;a href="https://docs.openvino.ai/2024/get-started/install-openvino.html?PACKAGE=OPENVINO_GENAI&amp;amp;VERSION=v_2024_6_0&amp;amp;OP_SYSTEM=LINUX&amp;amp;DISTRIBUTION=PIP"&gt;additional dependencies&lt;/a&gt; outside Optimum but is easier, faster and more reliable than using &lt;a href="https://www.intel.com/content/www/us/en/ark.html"&gt;Intel Ark&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here's an example Pytorch workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Train&lt;/li&gt; &lt;li&gt;OR finetune&lt;/li&gt; &lt;li&gt;OR quantize&lt;/li&gt; &lt;li&gt;Convert to OpenVINO Intermediate Representation&lt;/li&gt; &lt;li&gt;Test code until inference goes zoom&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: Models exported to OpenVINO IR cannot be trained.&lt;/p&gt; &lt;p&gt;Here are some useful resources to learn more about OpenVINO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks"&gt;openvino_notebooks&lt;/a&gt;: This is where the cutting edge stuff lives. Every other resource on the internet save the source code are 'behind' this repo. Most documentation points here and it's really easy to and much of the docs are outdated&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/optimum/main/en/intel/openvino/export"&gt;openvino-export&lt;/a&gt;: Stored here are the parameters used in this tool. It's one to one.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openvino.ai/2024/index.html"&gt;documentation&lt;/a&gt;: I have found that going straight to git can be more useful since non-Intel examples are scarce&lt;/p&gt; &lt;p&gt;Due to low adoption there aren't many projects which fully leverage the features offered by the Intel AI stack. So, starting with OpenVINO through Transformers is a good way to find out if it's even worth going deeper.&lt;/p&gt; &lt;p&gt;Thanks for checking this out and hopefully the tool is useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4on67</id>
    <title>Building Local LLM with code execution? (RAG, Mac Studio(s), Ingestion of various types of data)</title>
    <updated>2025-01-19T03:03:51+00:00</updated>
    <author>
      <name>/u/doofew</name>
      <uri>https://old.reddit.com/user/doofew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re looking to build local LLM(s) for internal use, and a good amount of internal administrative functions (automations).&lt;/p&gt; &lt;p&gt;Hoping to gather feedback for the best course of action as I’m coming across a lot of differing opinions and conflicting data. In addition, if there are any folks who have implemented similar things that do any consulting work (or know companies/consulting who do), I’d love to have a conversation about a potential engagement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Use Case: (Document (PDF generally) classification, sorting and execution to move)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Scanned documents are dropped into an intake directory on a system.&lt;br /&gt; - Scanned documents are classified based on their contents. This classification is: Company, contents and matched to a list of pre-created storage locations. (This data is fed to the classifier - which companies exist, what options for sorting into directory structure under each company are available, etc).&lt;br /&gt; - After the classification, decisions are made on the sorting and those decisions are summarized on what each file is classified as, and what sorting decisions are made. We’re looking for some mechanism that then would elicit a prompt to a human to show for example the first page of each document, how it was classified and the recommendation on sorting.&lt;br /&gt; - If the human agrees, the next step of workflow would execute a move of these files into the respective sorting locations. (Via script and local execution).&lt;/p&gt; &lt;p&gt;With this framework, we have many other tasks that could be performed which would significantly reduce administrative overhead within our companies.&lt;/p&gt; &lt;p&gt;The data is sensitive in nature, so this must be local.&lt;/p&gt; &lt;p&gt;Given that most of these tasks are batch, the speed in which they execute (minutes for example) is not an issue.&lt;/p&gt; &lt;p&gt;It would be great if we could utilize Mac Studio’s (M2 Ultra’s / 192GB or M4 Ultra’s - whenever they come out). If a multi-layered approach with multiple models is required, we’re open to also having GPU based systems in addition for any specialized heavy lifting, or multiple Mac Studios/Mac Mini’s.&lt;/p&gt; &lt;p&gt;In addition to the basic use case above which I believe can be handled in context for the instructions as they are pretty limited. We also have a lot of company data we’d like to ingest.&lt;/p&gt; &lt;p&gt;- Word, Excel, CSV and PDF Documents&lt;br /&gt; - Quickbooks Data (Invoices, Expenses, Contractor Details, etc)&lt;br /&gt; - Email Repository (Think shared mailbox type data, not individual email boxes)&lt;br /&gt; - Time system data (Billing to specific clients, sites, etc)&lt;/p&gt; &lt;p&gt;Some of this data being via documents, others being via SQL or API calls to gather data from cloud based systems, or other internal systems.&lt;/p&gt; &lt;p&gt;Utilizing this ingested data, we’d build other workflows, task automation (with human feedback prior to execution as described above) and general company info query via chat function.&lt;/p&gt; &lt;p&gt;When researching, it seems an agentic RAG technique may be best to recall with fidelity.&lt;/p&gt; &lt;p&gt;I’ve seen a lot of in-house functionality built on AnythingLLM, Openweb UI and others but the seem limited to just dumping documents into a web gui. Not sure if they are extensible enough to build what we are thinking of doing. Any feedback or pointers is much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doofew"&gt; /u/doofew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4on67/building_local_llm_with_code_execution_rag_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4on67/building_local_llm_with_code_execution_rag_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4on67/building_local_llm_with_code_execution_rag_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T03:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4nect</id>
    <title>NxMobileLM-1.5B-SFT: A New Lightweight Language Model</title>
    <updated>2025-01-19T01:57:03+00:00</updated>
    <author>
      <name>/u/Psychological_Oil727</name>
      <uri>https://old.reddit.com/user/Psychological_Oil727</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features:&lt;/p&gt; &lt;p&gt;Compact Size: At just 1.5 billion parameters, NxMobileLM-1.5B-SFT is designed to run efficiently on resource-constrained devices, including mobile and edge hardware.&lt;/p&gt; &lt;p&gt;Multilingual Support: It performs well across a wide range of languages, making it a versatile solution for diverse linguistic applications.&lt;/p&gt; &lt;p&gt;Performance-Optimized: The model to achieve state-of-the-art results when compared with other models of similar size.&lt;br /&gt; &lt;a href="https://huggingface.co/NTQAI/NxMobileLM-1.5B-SFT"&gt;https://huggingface.co/NTQAI/NxMobileLM-1.5B-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Oil727"&gt; /u/Psychological_Oil727 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4nect/nxmobilelm15bsft_a_new_lightweight_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4nect/nxmobilelm15bsft_a_new_lightweight_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4nect/nxmobilelm15bsft_a_new_lightweight_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T01:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4a2by</id>
    <title>Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)</title>
    <updated>2025-01-18T15:36:46+00:00</updated>
    <author>
      <name>/u/Loya_3005</name>
      <uri>https://old.reddit.com/user/Loya_3005</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt; &lt;img alt="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" src="https://b.thumbs.redditmedia.com/QcFzNc573ISWHe0wjV3Yju32wsgz4dgJupoA0RiLPPc.jpg" title="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/n6awgafpurde1.gif"&gt;Nuggt Demo GIF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We all understand that the quality of LLM output depends heavily on the context and prompt provided. For example, asking an LLM to generate a good blog article on a given topic (let's say &lt;em&gt;X&lt;/em&gt;) might result in a generic answer that may or may not meet your expectations. However, if you provide guidelines on how to write a good article and supply the LLM with additional relevant information about the topic, you significantly increase the chances of receiving a response that aligns with your needs.&lt;/p&gt; &lt;p&gt;With this in mind, I wanted to create a workspace that makes it easy to build and manage context for use with LLMs. I imagine there are many of us who might use LLMs in workflows similar to the following:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Let’s say you want to write an elevator pitch for your startup.&lt;br /&gt; &lt;strong&gt;Step 1&lt;/strong&gt;: Research how to write a good elevator pitch, then save the key points as context.&lt;br /&gt; &lt;strong&gt;Step 2&lt;/strong&gt;: Look up examples of effective elevator pitches and add these examples to your context.&lt;br /&gt; &lt;strong&gt;Step 3&lt;/strong&gt;: Pass this curated context to the LLM and ask it to craft an elevator pitch for your startup. Importantly, you expect transparency—ensuring the LLM uses your provided context as intended and shows how it informed the output.&lt;/p&gt; &lt;p&gt;If you find workflows like this appealing, I think you’ll enjoy this tool. Here are its key features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It integrates &lt;strong&gt;Tavily&lt;/strong&gt; and &lt;strong&gt;Firecrawl&lt;/strong&gt; to gather information on any topic from the internet.&lt;/li&gt; &lt;li&gt;You can highlight any important points, right-click, and save them as context.&lt;/li&gt; &lt;li&gt;You can pass this context to the LLM, which will use it to assist with your task. In its responses, the LLM will cite the relevant parts of the context so you can verify how your input was used and even trace it back to the original sources.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My hypothesis is that many of us would benefit from building strong context to complete our tasks. Of course, I could be wrong—perhaps this is just one of my idiosyncrasies, putting so much effort into creating detailed context! Who knows? The only way to find out is to post it here and see what the community thinks.&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback!&lt;/p&gt; &lt;p&gt;Here is the github repo: &lt;a href="https://github.com/shoibloya/nuggt-research"&gt;https://github.com/shoibloya/nuggt-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loya_3005"&gt; /u/Loya_3005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T15:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4poqz</id>
    <title>Best in class Deep Learning models for 48/24GB VRAM</title>
    <updated>2025-01-19T03:58:38+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on my experience and readings of benchmark results and other people's experience, this is a list I come up:&lt;/p&gt; &lt;p&gt;Knowledge (English): Llama-3.3-70B (48GB) / Llama-3_1-Nemotron-51B (24GB)&lt;br /&gt; Knowledge (Japanese): gemma-2-27b-it&lt;br /&gt; Coding: Qwen-2.5-Coder-32B&lt;br /&gt; Long Context RAG: Llama-3.3-70B (48GB) / Phi-3-medium-128k-instruct (24GB)&lt;br /&gt; Math: ???&lt;br /&gt; Reasoning: ???&lt;br /&gt; Writing: ???&lt;br /&gt; Image2Text: Llama-3.2-vision-11b&lt;br /&gt; Text2Image: Flux&lt;br /&gt; Text2Video: Hunyuan&lt;br /&gt; Image2Video: Cosmos-1.0-Diffusion-14B (48GB) / Cosmos-1.0-Diffusion-7B (24GB)&lt;br /&gt; STT: whisper-turbo&lt;br /&gt; TTS: ???&lt;br /&gt; Classification: colbert-ir/colbertv2.0&lt;br /&gt; Vector Search: all-MiniLM-L6-v2 (stella_en_400M_v5?)&lt;/p&gt; &lt;p&gt;Do you have other suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4poqz/best_in_class_deep_learning_models_for_4824gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4poqz/best_in_class_deep_learning_models_for_4824gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4poqz/best_in_class_deep_learning_models_for_4824gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T03:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i46zfr</id>
    <title>Why can't LLMs be re-trained on the go with the conversation for infinite memory?</title>
    <updated>2025-01-18T12:56:46+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just trying to understand the technical limitations and is this something that's considered. &lt;/p&gt; &lt;p&gt;I think the context window should only exist for instructions, while maintaining an infinte memory. This could really put LLMs in the realms of writing a complete book series and effecively changing the world as w e know it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T12:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4kny3</id>
    <title>Can you compensate for weak LLMs with RAG?</title>
    <updated>2025-01-18T23:39:33+00:00</updated>
    <author>
      <name>/u/Top-Coyote-1832</name>
      <uri>https://old.reddit.com/user/Top-Coyote-1832</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I locally work with a NVIDIA GeForce GTX 1660 Ti, which has 6 GB of VRAM. I use local LLMs for a lot a small, respective tasks, and it works great. Q/V Quantization, Flash Attention, and the smaller models (like Microsoft's phi line) really make this work.&lt;/p&gt; &lt;p&gt;I'm starting a more complicated project where I have my LLM generate code for a medium-popularity Python library to accomplish complex tasks. My local LLM hallucinates heavily about this library, and it is not really effective at all.&lt;/p&gt; &lt;p&gt;I'm trying to use RAG on the library's documentation to inform the LLM about the libraries API and I'm even trying agentic reasoning, where I tell the LLM to ask questions that it needs answered. I then use these questions to facilitate RAG.&lt;/p&gt; &lt;p&gt;My solution is beginning to take minutes at a time, and &lt;em&gt;it still sucks&lt;/em&gt;. Obviously I can do this perfect with OpenAI's 4o with no RAG necessary - it just does it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How effective is RAG at helping weak LLMs? Is this a worthless endeavor?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;EDIT: As pointed out, this GPU doesn’t even support Flash Attention. I was tripping&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Coyote-1832"&gt; /u/Top-Coyote-1832 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kny3/can_you_compensate_for_weak_llms_with_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kny3/can_you_compensate_for_weak_llms_with_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kny3/can_you_compensate_for_weak_llms_with_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T23:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4bfpo</id>
    <title>-Nevoria- LLama 3.3 70b</title>
    <updated>2025-01-18T16:39:34+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;TLDR: This is a merge focused on combining storytelling capabilities with detailed scene descriptions, while maintaining a balanced approach to maintain intelligence and useability and reducing positive bias. Currently ranked as the highest 70B on the UGI benchmark!&lt;/p&gt; &lt;p&gt;What went into this?&lt;/p&gt; &lt;p&gt;I took EVA-LLAMA 3.33 for its killer storytelling abilities and mixed it with EURYALE v2.3's detailed scene descriptions. Added Anubis v1 to enhance the prose details, and threw in some Negative_LLAMA to keep it from being too sunshine-and-rainbows. All this sitting on a Nemotron-lorablated base.&lt;/p&gt; &lt;p&gt;Subtracting the lorablated base during merging causes a &amp;quot;weight twisting&amp;quot; effect. If you've played with my previous Astoria models, you'll recognize this approach - it creates some really interesting balance in how the model responds.&lt;/p&gt; &lt;p&gt;As usual my goal is to keep the model Intelligent with a knack for storytelling and RP. &lt;/p&gt; &lt;p&gt;Benchmark Results:&lt;/p&gt; &lt;p&gt;- UGI Score: 56.75 (Currently #1 for 70B models and equal or better than 123b models!)&lt;/p&gt; &lt;p&gt;- Open LLM Average: 43.92% (while not as useful from people training on the questions, still useful)&lt;/p&gt; &lt;p&gt;- Solid scores across the board, especially in IFEval (69.63%) and BBH (56.60%)&lt;/p&gt; &lt;p&gt;Already got some quantized versions available: &lt;/p&gt; &lt;p&gt;Recommended template: LLam@ception by @.konnect&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B"&gt;https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and experiences with it! Your feedback helps make the next one even better. &lt;/p&gt; &lt;p&gt;Happy prompting! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i435so</id>
    <title>KoboldCpp 1.82 - Now supports OuteTTS v0.2+0.3 with speaker voice synthesis and XTTS/OpenAI speech API, TAESD for Flux &amp; SD3, multilingual whisper (plus RAG and WebSearch from v1.81)</title>
    <updated>2025-01-18T08:27:13+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's me Concedo, here again playing how-many-more-API-endpoints-can-koboldcpp-serve. &lt;/p&gt; &lt;p&gt;Today's release brings long awaited TTS support, which works on all versions of OuteTTS GGUFs including the newly released &lt;strong&gt;v0.3 500M and 1B&lt;/strong&gt; models. It also provides XTTS and OpenAI Speech compatible APIs, so it can work as a direct TTS drop-in for existing frontends that use those features. &lt;/p&gt; &lt;p&gt;There are also some pretty cool improvements, as well as many other features, so do check out the release notes if you haven't yet. Last release, we also added WebSearch and a simple browser based RAG, so check that out if you missed it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases"&gt;https://github.com/LostRuins/koboldcpp/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T08:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hyn9</id>
    <title>The Case for Open Models</title>
    <updated>2025-01-18T21:31:18+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt; &lt;img alt="The Case for Open Models" src="https://external-preview.redd.it/9jCHWcqHBL_Q8CbrEzDT6EMKWVd07U_SmrG2Z0RXzj8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf06e18f281752c7c009c87125ee559793b0b851" title="The Case for Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amgadhasan.substack.com/p/the-case-for-open-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4fmvy</id>
    <title>4080 16gb and my old 3070 8gb</title>
    <updated>2025-01-18T19:45:36+00:00</updated>
    <author>
      <name>/u/Glooves</name>
      <uri>https://old.reddit.com/user/Glooves</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt; &lt;img alt="4080 16gb and my old 3070 8gb" src="https://b.thumbs.redditmedia.com/_MB00WAkuuUTDJAJzS8IGuj3FCZdE56GfbQP0ShAflY.jpg" title="4080 16gb and my old 3070 8gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to throw my old 3070 in and an old set of ddr4 to see what happens. Now up to 24 gb of vram and 64 gb of dram with a 12700kf. I was worried about my 750 watt psu but it’s pulling under 400 watts at load and I’ll set some limits just in case. Got 22 tok/sec on gwen 2.5 32b q4_0. I’ll try a 70b later. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glooves"&gt; /u/Glooves &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i4fmvy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h3pp</id>
    <title>What would you do with free access to a 4x H100 server?</title>
    <updated>2025-01-18T20:52:42+00:00</updated>
    <author>
      <name>/u/SquareJordan</name>
      <uri>https://old.reddit.com/user/SquareJordan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long story short I have one in the lab and all that’s being run on it thus far are benchmarks. What should I do with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SquareJordan"&gt; /u/SquareJordan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cfpz</id>
    <title>Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?</title>
    <updated>2025-01-18T17:23:32+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt; &lt;img alt="Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?" src="https://preview.redd.it/tr0h9qvkdsde1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237cce46c19ab8ed30310b8c79fcf688f233dcf2" title="Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tr0h9qvkdsde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i457gp</id>
    <title>Intel should release a 24GB version of the Arc B580</title>
    <updated>2025-01-18T10:58:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt; &lt;img alt="Intel should release a 24GB version of the Arc B580" src="https://external-preview.redd.it/KNNit46prWlA2v7rjsUV6TaIPMXvtB72RAGA4ZyQjNE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddd3f42144ca0c2a05d54cf349b57f74c2e13f0f" title="Intel should release a 24GB version of the Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The B580 is already showing impressive performance for LLM inference, matching the RTX 3060 in Vulkan benchmarks (~36 tokens/sec on Qwen2 7B) while being more power efficient and $50 cheaper. But VRAM is the real bottleneck for running larger models locally.&lt;/p&gt; &lt;p&gt;With Intel's strong XMX matrix performance and the existing clamshell memory design validated in shipping docs, a 24GB variant is technically feasible. This would enable running 13B models quantized to 8-bit (most 13B models need ~14GB), existing models with larger context, etc.&lt;/p&gt; &lt;p&gt;It would have way better price/performance than RTX 4060 Ti 16GB, native Vulkan support without CUDA lock-in and more performance potential if OpenVINO is further optimized.&lt;/p&gt; &lt;p&gt;The regular B580's stellar price/performance ratio shows Intel can be aggressive on pricing. A ~$329 24GB variant would hit a sweet spot for local LLM enthusiasts building inference rigs.&lt;/p&gt; &lt;p&gt;This is Intel's chance to build mind- and marketshare among AI developers and enthusiasts who are tired of CUDA lock-in. They can grow a community around OpenVINO and their AI tooling. Every developer who builds with Intel's stack today helps their ecosystem forward. The MLPerf results show they have the performance - now they just need to get the hardware into developers' hands.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dec 16 '24: &lt;a href="https://www.pcgamer.com/hardware/graphics-cards/shipping-document-suggests-that-a-24-gb-version-of-intels-arc-b580-graphics-card-could-be-heading-to-market-though-not-for-gaming/"&gt;Shipping document suggests that a 24 GB version of Intel's Arc B580 graphics card could be heading to market, though not for gaming&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8"&gt;https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T10:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4l5hb</id>
    <title>What LLM benchmarks actually measure (explained intuitively)</title>
    <updated>2025-01-19T00:03:18+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;1. GPQA (Graduate-Level Google-Proof Q&amp;amp;A Benchmark)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GPQA evaluates LLMs on their ability to answer highly challenging, graduate-level questions in biology, physics, and chemistry. These questions are designed to be &amp;quot;Google-proof,&amp;quot; meaning they require deep, specialized understanding and reasoning that cannot be easily found through a simple internet search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Questions are crafted to be extremely difficult, with experts achieving around 65% accuracy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain Expertise&lt;/strong&gt;: Tests the model's ability to handle complex, domain-specific questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for scalable oversight experiments where AI systems need to provide reliable information beyond human capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. MMLU (Massive Multitask Language Understanding)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMLU assesses the general knowledge and problem-solving abilities of LLMs across 57 subjects, ranging from elementary mathematics to professional fields like law and ethics. It tests both world knowledge and reasoning skills.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Breadth&lt;/strong&gt;: Covers a wide array of topics, making it a comprehensive test of an LLM's understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Granularity&lt;/strong&gt;: Evaluates models in zero-shot and few-shot settings, mimicking real-world scenarios where models must perform with minimal context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt;: Models are scored based on their accuracy in answering multiple-choice questions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. MMLU-Pro&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: An enhanced version of MMLU, MMLU-Pro introduces more challenging, reasoning-focused questions and increases the number of answer choices from four to ten, making the tasks more complex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Increased Complexity&lt;/strong&gt;: More reasoning-intensive questions, reducing the chance of correct answers by random guessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Demonstrates greater stability under varying prompts, with less sensitivity to prompt variations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Drop&lt;/strong&gt;: Causes a significant drop in accuracy compared to MMLU, highlighting its increased difficulty.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. MATH&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: The MATH benchmark evaluates LLMs on their ability to solve complex mathematical problems, ranging from high school to competition-level mathematics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Problem Types&lt;/strong&gt;: Includes algebra, geometry, probability, and calculus problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-by-Step Solutions&lt;/strong&gt;: Each problem comes with a detailed solution, allowing for evaluation of reasoning steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for educational applications where accurate and efficient problem-solving is crucial.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. HumanEval&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HumanEval focuses on the functional correctness of code generated by LLMs. It consists of programming challenges where models must generate code that passes provided unit tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code Generation&lt;/strong&gt;: Tests the model's ability to understand and produce functional code from docstrings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses the pass@k metric, where 'k' different solutions are generated, and the model is considered successful if any solution passes all tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Coding&lt;/strong&gt;: Simulates real-world coding scenarios where multiple attempts might be made to solve a problem.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMMU evaluates multimodal models on tasks requiring college-level subject knowledge and deliberate reasoning across various disciplines, including visual understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal&lt;/strong&gt;: Incorporates text and images, testing models on tasks like understanding diagrams, charts, and other visual formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert-Level&lt;/strong&gt;: Questions are sourced from university-level materials, ensuring high difficulty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Covers six core disciplines with over 183 subfields, providing a broad assessment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;7. MathVista&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MathVista assesses mathematical reasoning in visual contexts, combining challenges from diverse mathematical and graphical tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Context&lt;/strong&gt;: Requires models to understand and reason with visual information alongside mathematical problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Composition&lt;/strong&gt;: Derived from existing datasets and includes new datasets for specific visual reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Gap&lt;/strong&gt;: Highlights the gap between LLM capabilities and human performance in visually intensive mathematical reasoning.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;8. DocVQA (Document Visual Question Answering)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: DocVQA evaluates models on their ability to answer questions based on document images, testing both textual and visual comprehension.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Document Understanding&lt;/strong&gt;: Assesses the model's ability to interpret various document elements like text, tables, and figures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world document analysis tasks where understanding context and layout is crucial.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses metrics like Average Normalized Levenshtein Similarity (ANLS) to measure performance.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;9. HELM (Holistic Evaluation of Language Models)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HELM evaluates LLMs from multiple angles, offering a comprehensive view of their performance. It assesses accuracy, performance across various tasks, and integrates qualitative reviews to capture subtleties in model responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Holistic Approach&lt;/strong&gt;: Uses established datasets to assess accuracy and performance, alongside qualitative reviews for a nuanced understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Analysis&lt;/strong&gt;: Conducts detailed error analysis to identify specific areas where models struggle.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Diversity&lt;/strong&gt;: Covers a wide range of tasks, from text classification to machine translation, providing a broad assessment of model capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;10. GLUE (General Language Understanding Evaluation)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GLUE provides a baseline for evaluating general language understanding capabilities of LLMs. It includes tasks like sentiment analysis, question answering, and textual entailment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Encompasses a variety of NLP tasks, making it a robust benchmark for general language understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Publicly Available&lt;/strong&gt;: Datasets are publicly available, allowing for widespread use and comparison.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;: GLUE maintains a leaderboard where models are ranked based on their performance across its tasks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;11. BIG-Bench Hard (BBH)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: BBH focuses on the limitations and failure modes of LLMs by selecting particularly challenging tasks from the larger BIG-Bench benchmark.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Consists of 23 tasks where no prior model outperformed average human-rater scores, highlighting areas where models fall short.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Focused Evaluation&lt;/strong&gt;: Aims to push the boundaries of model capabilities by concentrating on tasks that are difficult for current models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Relevance&lt;/strong&gt;: Tasks are designed to reflect real-world challenges where models need to demonstrate advanced reasoning and understanding.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;12. MT-Bench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MT-Bench evaluates models' ability to engage in coherent, informative, and engaging conversations, focusing on conversation flow and instruction-following capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Turn&lt;/strong&gt;: Contains 80 questions with follow-up questions, simulating real-world conversational scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge&lt;/strong&gt;: Uses strong LLMs like GPT-4 to assess the quality of model responses, providing an objective evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human Preferences&lt;/strong&gt;: Responses are annotated by graduate students with domain expertise, ensuring relevance and quality.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;13. FinBen&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: FinBen is designed to evaluate LLMs in the financial domain, covering tasks like information extraction, text analysis, question answering, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Domain-Specific&lt;/strong&gt;: Focuses on financial tasks, providing a specialized benchmark for financial applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Broad Task Coverage&lt;/strong&gt;: Includes 36 datasets covering 24 tasks in seven financial domains, offering a comprehensive evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Evaluates models on practical financial tasks, including stock trading, highlighting their utility in financial services.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;14. LegalBench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: LegalBench assesses LLMs' legal reasoning capabilities, using datasets from various legal domains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Legal Reasoning&lt;/strong&gt;: Tests models on tasks requiring legal knowledge and reasoning, crucial for legal applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaborative Development&lt;/strong&gt;: Developed through collaboration, ensuring a wide range of legal tasks are covered.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world legal scenarios where models must interpret and apply legal principles.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T00:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h1qo</id>
    <title>Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant</title>
    <updated>2025-01-18T20:50:12+00:00</updated>
    <author>
      <name>/u/Voidmesmer</name>
      <uri>https://old.reddit.com/user/Voidmesmer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt; &lt;img alt="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" src="https://external-preview.redd.it/dqmU2zN05DBQIRTAk9vSzRwo09lhayQQe9rfQzGoyS8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3af3bcb4c5c2426d61848d63048912843cb91350" title="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Voidmesmer"&gt; /u/Voidmesmer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=EBHwOCoGMKQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4em80</id>
    <title>Interesting article on how DeepSeek has improved the architecture in DeepSeek V2 and V3.</title>
    <updated>2025-01-18T19:00:14+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://epoch.ai"&gt;epoch.ai&lt;/a&gt; has published an interesting article: &lt;a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture"&gt;https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It talks about MLA, MoE innovations and Multi-Token Prediction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4kzgp</id>
    <title>We need to be able to train models on consumer-grade hardware</title>
    <updated>2025-01-18T23:55:33+00:00</updated>
    <author>
      <name>/u/yoop001</name>
      <uri>https://old.reddit.com/user/yoop001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The number of papers being published is off the charts, and there’s no way the big players can implement everything. That means some game-changing ideas might slip through the cracks. But if everyday folks could test out these so-called breakthroughs, we’d be in a position to help the big players spot the real gems worth scaling up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoop001"&gt; /u/yoop001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T23:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4oeg9</id>
    <title>yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program</title>
    <updated>2025-01-19T02:50:53+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt; &lt;img alt="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" src="https://external-preview.redd.it/hezSolCpDQfrkKm4K9-XqPSYxr_22M4CNnAVkJ9xh7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f2bbfcd7007496d377206d2d8a40c8e2afedc2f" title="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bodo-run/yek"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T02:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4awir</id>
    <title>Have you truly replaced paid models(chatgpt, Claude etc) with self hosted ollama or hugging face ?</title>
    <updated>2025-01-18T16:14:58+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with locally hosted setups, but I keep finding myself coming back to ChatGPT for the ease and performance. For those of you who’ve managed to fully switch, do you still use services like ChatGPT occasionally? Do you use both? &lt;/p&gt; &lt;p&gt;Also, what kind of GPU setup is really needed to get that kind of seamless experience? My 16GB VRAM feels pretty inadequate in comparison to what these paid models offer. Would love to hear your thoughts and setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hb2l</id>
    <title>Theory: trying to use newer and more powerful LLMs to sound more human is likely moving in the wrong direction</title>
    <updated>2025-01-18T21:01:51+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like using more powerful LLMs to try to achieve human like speech is probably moving AWAY from the solution rather than towards it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;tl;dr-&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Newer models are more powerful and have larger context, but are heavily trained with outputs from other LLMs. This results in modern models responding far more intelligently than Llama 1 era models, but also loaded with inescapable &amp;quot;slop&amp;quot; and &amp;quot;GPTisms&amp;quot;. My proposal is that by using older Llama 1 era models/fine-tunes like Guanaco 65b (which is primarily human data trained) as a final step &amp;quot;editor&amp;quot; to rewrite what modern LLMs put out, you can get output that sounds more human and has more natural speech patterns. This could be good for articles/emails/papers/etc.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;My thought process is this: what's the difference between Llama 3.3 and Llama 1? Besides technology, I mean. Lets go back in time, to the days when we only had a 2048 token context windows; before things like NTK RoPE scaling, when your only choices were linear RoPE scaling and/or a superHoT model. Forget all the technological differences, and think: what differentiated the models of those days to the models of today?&lt;/p&gt; &lt;p&gt;Synthetic data in the training, right? The models back then were trained on minimal, if any, synthetic data. Even most of the chat models were barely trained on user/assistant type interactions. The training data was mostly human text.&lt;/p&gt; &lt;p&gt;But today? Modern models have tons and tons of &amp;quot;GPTisms&amp;quot; baked into the training data, not even speaking about how bad the instruct tune can be. So of course when you talk to the thing, it's going to sound like what we now expect an AI Assistant to sound like. And the more powerful models we get, the more we're going to see this synthetic data further baked in there.&lt;/p&gt; &lt;p&gt;In my opinion, it makes a lot of sense to go back and try some of these older models for this purpose, such as using a workflow to have the final step use an old Llama 1 model to rewrite the output of the modern LLMs.&lt;/p&gt; &lt;p&gt;Just to test it, I updated one of my Wilmer workflows to use Mistral Small as the thinker and Guanaco 65b as the responder. I had mistral write the initial response to me trying to sound human, and gave Guanaco very simple instructions to take that response and rewrite it to also sound more human. I then did the same using ChatGPT and Llama 3.3 70b.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;em&gt;This was a very small and simple POC test, and the quality of the output could be far higher with proper prompting, but I was just curious what the difference would be.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I asked the question: &lt;strong&gt;&lt;em&gt;&amp;quot;Which do you like better: Java or C#?&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small response:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Well, it depends on what you're looking to do. Both Java and C# have their strengths. For example:&lt;/p&gt; &lt;p&gt;- **Java**: It’s great for building web applications, Android apps, and large-scale enterprise systems. It has a larger community and more libraries available.&lt;/p&gt; &lt;p&gt;- **C#**: It's fantastic for game development (thanks to Unity), building Windows applications, and integrating with other Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think they both have their use cases. Java might be more versatile for certain web projects, but C# can be incredibly efficient for desktop applications and games.&lt;/p&gt; &lt;p&gt;Do you have a specific type of project in mind that you want to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Obviously a very complete and well written answer, but also very indicative that an AI wrote it. Something about the way it responds is familiar to folks who would be using chatgpt and the like.&lt;/p&gt; &lt;p&gt;Now, look at &lt;strong&gt;Guanaco 65b's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I'd say Java and C# both have their pros and cons. Java is great for building websites, Android apps, and huge enterprise systems. But C# is amazing for making games with Unity, building Windows programs, and hooking into other Microsoft services. Personally, I'd use Java for web stuff and C# for desktop apps and games, but it depends on your project.&lt;/p&gt; &lt;p&gt;So, what're you trying to build? I can give you some pointers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The difference is subtle, but see how it feels a bit more... human?&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;ChatGPT o1's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Honestly, it really depends on what you’re trying to accomplish. Both Java and C# have their own strengths:&lt;/p&gt; &lt;p&gt;* Java: Great for web applications, Android apps, and large-scale enterprise systems. It also has a huge community and tons of libraries.&lt;/p&gt; &lt;p&gt;* C#: Awesome for game development (especially with Unity), building Windows apps, and integrating with Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think both are solid choices. Java might be more versatile for certain web-based projects, while C# shines when it comes to desktop applications and gaming.&lt;/p&gt; &lt;p&gt;Any specific type of project you’re looking to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This one remains sounding like an AI assistant. Same instructions and prompt I gave Guanaco.&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;Llama 3.3's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;so you're wondering about c# and java, right? they both have their own things goin for 'em, and honestly it kinda comes down to what you're workin on and what you like. they're actually pretty similar, they're both object-oriented, they both run on virtual machines, c# uses .net and java uses the jvm, and they both got a ton of libraries and a huge community behind 'em.&lt;/p&gt; &lt;p&gt;so like, here's some differences, ok? c# is made by microsoft, so it works super well with windows and all that, and java's open-source and can run on pretty much anything that's got a jvm, so that's linux, mac, android, all that. and syntax-wise, they're a little different too, like c# uses these curly braces to define a block of code and java uses semicolons to end statements and all that.&lt;/p&gt; &lt;p&gt;personally, i'm cool with both, i mean c# is awesome for windows stuff, but java's got that whole cross-platform thing goin on, so yeah… what about you, got a preference or whatever?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Its... well, about what you'd expect lol. Llama also deciding rewriting like a human means no capitalization =D Also, it added some details of its own to the response, but that's ok.&lt;/p&gt; &lt;p&gt;Again, this is a very simplistic POC just to show what I mean, and I'm probably going to spend more time delving into the idea, but I really think that there's still value in these old models, especially when trying to write things that sound more &amp;quot;human&amp;quot;, like maybe article or email writing.&lt;/p&gt; &lt;p&gt;Anyhow, just a thought I'd figure might spark some conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:01:51+00:00</published>
  </entry>
</feed>
