<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-21T11:34:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lfzh05</id>
    <title>Repurposing 800 x RX 580s for LLM inference - 4 months later - learnings</title>
    <updated>2025-06-20T09:14:15+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in March I asked this sub if RX 580s could be used for anything useful in the LLM space and asked for help on how to implemented inference: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Four months later, we've built a fully functioning inference cluster using around 800 RX 580s across 132 rigs. I want to come back and share what worked, what didn’t so that others can learn from our experience. &lt;/p&gt; &lt;h1&gt;what worked&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Vulkan with llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vulkan backend worked on all RX 580s&lt;/li&gt; &lt;li&gt;Required compiling Shaderc manually to get &lt;code&gt;glslc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;llama.cpp built with custom flags for vulkan support and no avx instructions (our cpus on the builds are very old celerons). we tried countless build attempts and this is the best we could do:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CXXFLAGS=&amp;quot;-march=core2 -mtune=generic&amp;quot; cmake .. \ -DLLAMA_BUILD_SERVER=ON \ -DGGML_VULKAN=ON \ -DGGML_NATIVE=OFF \ -DGGML_AVX=OFF -DGGML_AVX2=OFF \ -DGGML_AVX512=OFF -DGGML_AVX_VNNI=OFF \ -DGGML_FMA=OFF -DGGML_F16C=OFF \ -DGGML_AMX_TILE=OFF -DGGML_AMX_INT8=OFF -DGGML_AMX_BF16=OFF \ -DGGML_SSE42=ON \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Per-rig multi-GPU scaling&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each rig runs 6 GPUs and can split small models across multiple kubernetes containers with each GPU's VRAM shared (could only minimally do 1 GPU per container - couldn't split a GPU's VRAM to 2 containers)&lt;/li&gt; &lt;li&gt;Used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm none&lt;/code&gt; for 6 containers for 6 gpus&lt;/li&gt; &lt;li&gt;for bigger contexts we could extend the small model's limits and use more than 1 GPU's VRAM&lt;/li&gt; &lt;li&gt;for bigger models (Qwen3-30B_Q8_0) we used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm layer&lt;/code&gt; and build a recent llama.cpp implementation for reasoning management where you could turn off thinking mode with &lt;code&gt;--reasoning-budget 0&lt;/code&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Load balancing setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built a fastapi load-balancer backend that assigns each user to an available kubernetes pod&lt;/li&gt; &lt;li&gt;Redis tracks current pod load and handle session stickiness &lt;/li&gt; &lt;li&gt;The load-balancer also does prompt cache retention and restoration. biggest challenge here was how to make the llama.cpp servers accept the old prompt caches that weren't 100% in the processed eval format and would get dropped and reinterpreted from the beginning. we found that using &lt;code&gt;--cache-reuse 32&lt;/code&gt; would allow for a margin of error big enough for all the conversation caches to be evaluated instantly&lt;/li&gt; &lt;li&gt;Models respond via streaming SSE, OpenAI-compatible format&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;what didn’t work&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ROCm HIP \ pytorc \ tensorflow inference&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ROCm technically works and tools like &lt;code&gt;rocminfo&lt;/code&gt; and &lt;code&gt;rocm-smi&lt;/code&gt; work but couldn't get a working llama.cpp HIP build&lt;/li&gt; &lt;li&gt;there’s no functional PyTorch backend for Polaris-class gfx803 cards so pytorch didn't work&lt;/li&gt; &lt;li&gt;couldn't get TensorFlow to work with llama.cpp &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;we’re also putting part of our cluster through some live testing. If you want to throw some prompts at it, you can hit it here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.masterchaincorp.com"&gt;https://www.masterchaincorp.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s running Qwen-30B and the frontend is just a basic llama.cpp server webui. nothing fancy so feel free to poke around and help test the setup. feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsb61</id>
    <title>Local Personal Memo AI Assistant</title>
    <updated>2025-06-21T09:11:44+00:00</updated>
    <author>
      <name>/u/nandospc</name>
      <uri>https://old.reddit.com/user/nandospc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning guys!&lt;/p&gt; &lt;p&gt;So, the idea is to create a personal memo ai assistant. The concept is to feed my local llm with notes, thoughts and little Infos, which can then be retrieved by asking for them like a classic chat-ish model, so like a personal and customized &amp;quot;windows recall&amp;quot; function.&lt;/p&gt; &lt;p&gt;At the beginning I thought to use it locally, but I'm not ditching completely the possibility to also use it remotely, so maybe i'd like something that could also do that in the future.&lt;/p&gt; &lt;p&gt;My PC specs are mid tier: 7600x + 2x16 GB 6000/C30 RAM , 6700xt 12gb VRam, around a total of 8tb of storage split in multiple disks (1tb of boot disk + 2tb of additional storage, both as nvmes), just for clarity.&lt;/p&gt; &lt;p&gt;Currently I daily use Win11 24h2 fully upgraded, but i don't mind to make a dual boot with a Linux OS if needed, I'm used to running them by myself and by work related activities (no problem with distros).&lt;/p&gt; &lt;p&gt;So, what tools do you recommend to use to create this project? What could you use?&lt;/p&gt; &lt;p&gt;Thanks in advance :)&lt;/p&gt; &lt;p&gt;Edit: typos and more infos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nandospc"&gt; /u/nandospc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsb61/local_personal_memo_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsb61/local_personal_memo_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsb61/local_personal_memo_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:11:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgtbo8</id>
    <title>LM Studio much faster than Ollama?</title>
    <updated>2025-06-21T10:22:10+00:00</updated>
    <author>
      <name>/u/MonyWony</name>
      <uri>https://old.reddit.com/user/MonyWony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been getting deep into local LLMs recently and I first started out with LM Studio; easy to use, easy to setup, and works right out of the box. Yesterday I decided it was time to venture further and so I set up Ollama and Open WebGUI. Needless to say it is much better than LM Studio in terms of how capable it is. I'm still new to Ollama and Open WebGUI so I forgive me if I sound dense.&lt;/p&gt; &lt;p&gt;But anyways I was trying out Qwen3 8B and I noticed that it was running much slower on WebGUI. Comparing tokens/second I was getting over 35t/s on LM Studio and just shy of 12t/s on WebGUI. I thought nothing much of it since I assumed it was because using WebGUI requires me to have a browser open and I was sure that it was hampering my performance. I was pretty sure that just using Ollama directly through the CMD would be much faster, but when I tried it I got around 16t/s in Ollama CMD, still less than half the speed I was achieving using LM Studio.&lt;/p&gt; &lt;p&gt;I expected Ollama to be much faster than LM Studio but I guess I was incorrect.&lt;/p&gt; &lt;p&gt;Is there something that I'm doing wrong or is there a setting I need to change?&lt;/p&gt; &lt;p&gt;So far I've only tested Qwen3 8B so maybe it's model specific.&lt;/p&gt; &lt;p&gt;Thanks for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MonyWony"&gt; /u/MonyWony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtbo8/lm_studio_much_faster_than_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtbo8/lm_studio_much_faster_than_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtbo8/lm_studio_much_faster_than_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T10:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgtcrb</id>
    <title>Open source tool to fix LLM-generated JSON</title>
    <updated>2025-06-21T10:24:16+00:00</updated>
    <author>
      <name>/u/arthurtakeda</name>
      <uri>https://old.reddit.com/user/arthurtakeda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! Ever since I started using LLMs to generate JSON for my side projects I occasionally get an error and when looking at the logs it’s usually because of some parsing errors.&lt;/p&gt; &lt;p&gt;I’ve built a tool to fix the most common errors I came across: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Markdown Block Extraction: Extracts JSON from ```json code blocks and inline code&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Trailing Content Removal: Removes explanatory text after valid JSON structures&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Quote Fixing: Fixes unescaped quotes inside JSON strings&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Missing Comma Detection: Adds missing commas between array elements and object properties&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s just pure typescript so it’s very lightweight, hope it’s useful!! Any feedbacks are welcome, thinking of building a Python equivalent soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/aotakeda/ai-json-fixer"&gt;https://github.com/aotakeda/ai-json-fixer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arthurtakeda"&gt; /u/arthurtakeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T10:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgd4tq</id>
    <title>Why haven't I tried llama.cpp yet?</title>
    <updated>2025-06-20T19:43:43+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Oh boy, models on llama.cpp are very fast compared to ollama models. I have no GPU. It got Intel Iris XE GPU. llama.cpp models give super-fast replies on my hardware. I will now download other models and try them.&lt;/p&gt; &lt;p&gt;If anyone of you do not have GPU and want to test these models locally, go for llama.cpp. Very easy to setup, has GUI (site to access chats), can set tons of options in the site. I am super impressed with llama.cpp. This is my local LLM manager going forward.&lt;/p&gt; &lt;p&gt;If anyone knows about llama.cpp, can we restrict cpu and memory usage with llama.cpp models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgdi7i</id>
    <title>GMK X2(AMD Max+ 395 w/128GB) second impressions, Linux.</title>
    <updated>2025-06-20T19:59:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a follow up to my post from a couple of days ago. These are the numbers for Linux.&lt;/p&gt; &lt;p&gt;First, there is no memory size limitation with Vulkan under Linux. It sees 96GB of VRAM with another 15GB of GTT(shared memory) so 111GB combined. With Windows, Vulkan only sees 32GB of VRAM. Using shared memory as a workaround I could use up to 79.5GB total. And since shared memory is the same as &amp;quot;VRAM&amp;quot; on this machine, &lt;del&gt;using shared memory is only about 10% slower.&lt;/del&gt; For smaller models it's only about 10%, but as the model size gets bigger it gets slower. I added a run of llama 3.3 at the end. One with dedicated memory and one with shared. I only allocated 512MB to the GPU. After other uses, like the Desktop GUI, there's pretty much nothing left out of the 512MB. So it must be thrashing. Which gets worse and worse the bigger and bigger the model is.&lt;/p&gt; &lt;p&gt;Oh yeah, unlike in Windows, the GTT size can be adjusted easily in Linux. On my other machines, I crank it down to 1M to effectively turn it off. On this machine, I cranked it up to 24GB. Since I only use this machine to run LLMs et al, 8GB is more than enough for the system. Thus the GPU has 120GB. Like with my Mac, I'll probably crank it up even higher. Since some of my Linux machines run just fine on even 256MB. In this case though, cranking down the dedicated RAM and making it run using GTT would give it that variable unified memory thing like on a Mac.&lt;/p&gt; &lt;p&gt;Here are the results for all the models I ran last time. And since there's more memory available under Linux, I added dots at the end. I was kind of surprised by the results. I fully expected Windows to be distinctly faster. It's not. The results are mixed. I would say they are comparable overall.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Max+ Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | pp512 | 923.76 ± 2.45 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | tg128 | 21.22 ± 0.03 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | pp512 @ d5000 | 486.25 ± 1.08 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | RPC,Vulkan | 99 | 0 | tg128 @ d5000 | 12.31 ± 0.04 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 | 667.17 ± 1.43 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 | 20.86 ± 0.08 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | pp512 @ d5000 | 401.13 ± 1.06 | | gemma2 9B Q8_0 | 9.15 GiB | 9.24 B | Vulkan,RPC | 999 | 0 | tg128 @ d5000 | 12.40 ± 0.06 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 | 129.93 ± 0.08 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 | 10.38 ± 0.01 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 97.25 ± 0.04 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 4.70 ± 0.01 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 | 188.07 ± 3.58 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 | 10.95 ± 0.01 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 125.15 ± 0.52 | | gemma2 27B Q5_K - Medium | 18.07 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 3.73 ± 0.03 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 | 318.41 ± 0.71 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 | 7.61 ± 0.00 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 175.32 ± 0.08 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 3.97 ± 0.01 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 | 227.63 ± 1.02 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 | 7.56 ± 0.00 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 141.86 ± 0.29 | | gemma2 27B Q8_0 | 26.94 GiB | 27.23 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 4.01 ± 0.03 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | pp512 | 231.05 ± 0.73 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | tg128 | 6.44 ± 0.00 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | pp512 @ d10000 | 84.68 ± 0.26 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | RPC,Vulkan | 99 | 0 | tg128 @ d10000 | 4.62 ± 0.01 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | Vulkan,RPC | 999 | 0 | pp512 | 185.61 ± 0.32 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | Vulkan,RPC | 999 | 0 | tg128 | 6.45 ± 0.00 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 117.97 ± 0.21 | | qwen2 32B Q8_0 | 32.42 GiB | 32.76 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 4.80 ± 0.00 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ workaround Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | pp512 | 129.15 ± 2.87 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | tg128 | 20.09 ± 0.03 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | pp512 @ d10000 | 75.32 ± 4.54 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | RPC,Vulkan | 999 | 0 | tg128 @ d10000 | 10.68 ± 0.04 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | Vulkan,RPC | 999 | 0 | pp512 | 92.61 ± 0.31 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | Vulkan,RPC | 999 | 0 | tg128 | 20.87 ± 0.01 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 78.35 ± 0.59 | | llama4 17Bx16E (Scout) Q3_K - Medium | 49.47 GiB | 107.77 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 11.21 ± 0.03 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ workaround Windows** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | RPC,Vulkan | 999 | 0 | pp512 | 26.69 ± 0.83 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | RPC,Vulkan | 999 | 0 | tg128 | 12.82 ± 0.02 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | RPC,Vulkan | 999 | 0 | pp512 @ d2000 | 20.66 ± 0.39 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | RPC,Vulkan | 999 | 0 | tg128 @ d2000 | 2.68 ± 0.04 | **Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | Vulkan,RPC | 999 | 0 | pp512 | 20.67 ± 0.01 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | Vulkan,RPC | 999 | 0 | tg128 | 22.92 ± 0.00 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | Vulkan,RPC | 999 | 0 | pp512 @ d2000 | 19.74 ± 0.02 | | deepseek2 236B IQ2_XS - 2.3125 bpw | 63.99 GiB | 235.74 B | Vulkan,RPC | 999 | 0 | tg128 @ d2000 | 3.05 ± 0.00 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | dots1 142B Q4_K - Medium | 87.99 GiB | 142.77 B | Vulkan,RPC | 999 | 0 | pp512 | 30.89 ± 0.05 | | dots1 142B Q4_K - Medium | 87.99 GiB | 142.77 B | Vulkan,RPC | 999 | 0 | tg128 | 20.62 ± 0.01 | | dots1 142B Q4_K - Medium | 87.99 GiB | 142.77 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 28.22 ± 0.43 | | dots1 142B Q4_K - Medium | 87.99 GiB | 142.77 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 2.26 ± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;pre&gt;&lt;code&gt;**Max+ Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | pp512 | 75.28 ± 0.49 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | tg128 | 5.04 ± 0.01 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 52.03 ± 0.10 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 3.73 ± 0.00 | **Max+ shared memory Linux** | model | size | params | backend | ngl | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | pp512 | 36.91 ± 0.01 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | tg128 | 5.01 ± 0.00 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | pp512 @ d10000 | 29.83 ± 0.02 | | llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan,RPC | 999 | 0 | tg128 @ d10000 | 3.66 ± 0.00 | &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:59:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lghy81</id>
    <title>What's your AI coding workflow?</title>
    <updated>2025-06-20T23:13:43+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I tried Cursor for the first time, and “vibe coding” quickly became my hobby.&lt;br /&gt; It’s fun, but I’ve hit plenty of speed bumps:&lt;/p&gt; &lt;p&gt;• Context limits: big projects overflow the window and the AI loses track.&lt;br /&gt; • Shallow planning: the model loves quick fixes but struggles with multi-step goals.&lt;br /&gt; • Edit tools: sometimes they nuke half a script or duplicate code instead of cleanly patching it.&lt;br /&gt; • Unknown languages: if I don’t speak the syntax, I spend more time fixing than coding.&lt;/p&gt; &lt;p&gt;I’ve been experimenting with prompts that force the AI to plan and research before it writes, plus smaller, reviewable diffs. Results are better, but still far from perfect.&lt;/p&gt; &lt;p&gt;So here’s my question to the crowd:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s your AI-coding workflow?&lt;/strong&gt;&lt;br /&gt; What tricks (prompt styles, chain-of-thought guides, external tools, whatever) actually make the process smooth and steady for you?&lt;/p&gt; &lt;p&gt;Looking forward to stealing… uh, learning from your magic!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghy81/whats_your_ai_coding_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghy81/whats_your_ai_coding_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lghy81/whats_your_ai_coding_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T23:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgrcx6</id>
    <title>Query Classifier for RAG - Save your $$$ and users from irrelevant responses</title>
    <updated>2025-06-21T08:05:25+00:00</updated>
    <author>
      <name>/u/ZucchiniCalm4617</name>
      <uri>https://old.reddit.com/user/ZucchiniCalm4617</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAG systems are in fashion these days. So I built a classifier to filter out irrelevant and vague queries so that only relevant queries and context go to your chosen LLM and get you correct response. It earns you User trust, saves $$$, time and improves User experience if you don't go to LLM with the wrong questions and irrelevant context pulled from datastores(vector or otherwise). It has a rule based component and a small language model component. You can change the config.yaml to customise to any domain. For example- I set it up in health domain where only liver related questions go through and everything else gets filtered out. You can set it up for any other domain. For example, if you have documents only for Electric vehicles, you may want all questions on Internal Combustion engines to be funelled out. Check out the GitHub link(&lt;a href="https://github.com/srinivas-sateesh/RAG-query-classifier"&gt;https://github.com/srinivas-sateesh/RAG-query-classifier&lt;/a&gt;) and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZucchiniCalm4617"&gt; /u/ZucchiniCalm4617 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrcx6/query_classifier_for_rag_save_your_and_users_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrcx6/query_classifier_for_rag_save_your_and_users_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrcx6/query_classifier_for_rag_save_your_and_users_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T08:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lghu05</id>
    <title>Kimi Dev 72B is phenomenal</title>
    <updated>2025-06-20T23:08:09+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using alot of coding and general purpose models for Prolog coding. The codebase has gotten pretty large, and the larger it gets the harder it is to debug.&lt;/p&gt; &lt;p&gt;I've been experiencing a bottleneck and failed prolog runs lately, and none of the other coder models were able to pinpoint the issue.&lt;/p&gt; &lt;p&gt;I loaded up Kimi Dev (MLX 8 Bit) and gave it the codebase. It runs pretty slow with 115k context, but after the first run it pinpointed the problem and provided a solution.&lt;/p&gt; &lt;p&gt;Not sure how it performs on other models, but I am deeply impressed. It's very 'thinky' and unsure of itself in the reasoning tokens, but it comes through in the end.&lt;/p&gt; &lt;p&gt;Anyone know what optimal settings are (temp, etc.)? I haven't found an official guide from Kimi or anyone else anywhere.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghu05/kimi_dev_72b_is_phenomenal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghu05/kimi_dev_72b_is_phenomenal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lghu05/kimi_dev_72b_is_phenomenal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T23:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg9s5q</id>
    <title>OpenBuddy R1 0528 Distil into Qwen 32B</title>
    <updated>2025-06-20T17:26:07+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"&gt; &lt;img alt="OpenBuddy R1 0528 Distil into Qwen 32B" src="https://preview.redd.it/lpxeubca848f1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0c797e5af53cfbc1f305db8ab83a70ad2b8308da" title="OpenBuddy R1 0528 Distil into Qwen 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm so impressed with this model for the size. o1 was the first model I found that could one shot tetris with AI, and even other frontier models can still struggle to do it well. And now a 32B model just managed it!&lt;/p&gt; &lt;p&gt;There was one bug - only one line would be cleared at a time. It fixed this easily when I pointed it out.&lt;/p&gt; &lt;p&gt;I doubt it would one shot it every time, but this model is definitely a step up from standard Qwen 32B, which was already pretty good.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT"&gt;https://huggingface.co/OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lpxeubca848f1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T17:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg71aq</id>
    <title>Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica</title>
    <updated>2025-06-20T15:35:34+00:00</updated>
    <author>
      <name>/u/mylittlethrowaway300</name>
      <uri>https://old.reddit.com/user/mylittlethrowaway300</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"&gt; &lt;img alt="Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica" src="https://external-preview.redd.it/LATs33JDlBoRUx0tiKg7DMdY6oXVXFPIYU36DtiY4tQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98b804adca292cc34f817396897e2d3bdcafc87a" title="Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought this was a really well-written article. &lt;/p&gt; &lt;p&gt;I had a thought: do you guys think smaller LLMs will have fewer copyright issues than larger ones? If I train a huge model on text and tell it that &amp;quot;Romeo and Juliet&amp;quot; is a &amp;quot;tragic&amp;quot; story, and also that &amp;quot;Rabbit, Run&amp;quot; by Updike is also a tragic story, the larger LLM training is more likely to retain entire passages. It has the neurons of the NN (the model weights) to store information as rote memorization. &lt;/p&gt; &lt;p&gt;But, if I train a significantly smaller model, there's a higher chance that the training will manage to &amp;quot;extract&amp;quot; the components of each story that are tragic, but not retain the entire text verbatim.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mylittlethrowaway300"&gt; /u/mylittlethrowaway300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/features/2025/06/study-metas-llama-3-1-can-recall-42-percent-of-the-first-harry-potter-book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T15:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgrxkc</id>
    <title>UAE to appoint their National AI system as ministers' council advisory member</title>
    <updated>2025-06-21T08:45:18+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/posts/mohammedbinrashid_%D8%A7%D9%84%D8%A5%D8%AE%D9%88%D8%A9-%D9%88%D8%A7%D9%84%D8%A3%D8%AE%D9%88%D8%A7%D8%AA-%D8%A8%D8%B9%D8%AF-%D8%A7%D9%84%D8%AA%D8%B4%D8%A7%D9%88%D8%B1-%D9%85%D8%B9-%D8%A3%D8%AE%D9%8A-%D8%B1%D8%A6%D9%8A%D8%B3-activity-7341867717781614592-NH8k?utm_source=share&amp;amp;utm_medium=member_android&amp;amp;rcm=ACoAAA_qTHABhZU1hYm_lxYQw_ApFsOUKzigti8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrxkc/uae_to_appoint_their_national_ai_system_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrxkc/uae_to_appoint_their_national_ai_system_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T08:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgt4xd</id>
    <title>Open Source Unsiloed AI Chunker (EF2024)</title>
    <updated>2025-06-21T10:09:46+00:00</updated>
    <author>
      <name>/u/AskInternational6199</name>
      <uri>https://old.reddit.com/user/AskInternational6199</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt4xd/open_source_unsiloed_ai_chunker_ef2024/"&gt; &lt;img alt="Open Source Unsiloed AI Chunker (EF2024)" src="https://external-preview.redd.it/DrfhMWbsS2YNADYSkzRFo8CavEZsmEVw_qCUPCEDiaM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45f6ef66cb03d47e3fe927651ad03965648e6cd3" title="Open Source Unsiloed AI Chunker (EF2024)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey , Unsiloed CTO here!&lt;/p&gt; &lt;p&gt;Unsiloed AI (EF 2024) is backed by Transpose Platform &amp;amp; EF and is currently being used by teams at Fortune 100 companies and multiple Series E+ startups for ingesting multimodal data in the form of PDFs, Excel, PPTs, etc. And, we have now finally open sourced some of the capabilities. Do give it a try!&lt;/p&gt; &lt;p&gt;Also, we are inviting cracked developers to come and contribute to bounties of upto 1000$ on algora. This would be a great way to get noticed for the job openings at Unsiloed.&lt;/p&gt; &lt;p&gt;Bounty Link- &lt;a href="https://algora.io/bounties"&gt;https://algora.io/bounties&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link - &lt;a href="https://github.com/Unsiloed-AI/Unsiloed-chunker"&gt;https://github.com/Unsiloed-AI/Unsiloed-chunker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/utnjqd8y898f1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=358c1dc9afcf97e4368e277ae2f9b9bc252e0a52"&gt;https://preview.redd.it/utnjqd8y898f1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=358c1dc9afcf97e4368e277ae2f9b9bc252e0a52&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskInternational6199"&gt; /u/AskInternational6199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt4xd/open_source_unsiloed_ai_chunker_ef2024/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt4xd/open_source_unsiloed_ai_chunker_ef2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt4xd/open_source_unsiloed_ai_chunker_ef2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T10:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgcbyh</id>
    <title>Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound.</title>
    <updated>2025-06-20T19:09:42+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt; &lt;img alt="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." src="https://a.thumbs.redditmedia.com/smxOsICItFcgpgZ6jwpSvygFZFitUy4PBiwrObgw-D4.jpg" title="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys. I'm reposting as the old post got removed by some reason.&lt;/p&gt; &lt;p&gt;Now it is time to compare LLMs, where these GPUs shine the most.&lt;/p&gt; &lt;p&gt;hardware-software config:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Fedora 41 (Linux), Kernel 6.19&lt;/li&gt; &lt;li&gt;Torch 2.7.1+cu128&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each card was tuned to try to get the highest clock possible, highest VRAM bandwidth and less power consumption.&lt;/p&gt; &lt;p&gt;The benchmark was run on ikllamacpp, as&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-sweep-bench -m '/GUFs/gemma-3-27b-it-Q4_K_M.gguf' -ngl 999 -c 8192 -fa -ub 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The tuning was made on each card, and none was power limited (basically all with the slider maxed for PL)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 5090: &lt;ul&gt; &lt;li&gt;Max clock: 3010 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 1000&lt;/li&gt; &lt;li&gt;Basically an undervolt plus overclock near the 0.9V point (Linux doesn't let you see voltages)&lt;/li&gt; &lt;li&gt;VRAM overclock: +3000Mhz (34 Gbps effective, so about 2.1 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 4090: &lt;ul&gt; &lt;li&gt;Max clock: 2865 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an undervolt+OC about the 0.91V point.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1650Mhz (22.65 Gbps effective, so about 1.15 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 3090: &lt;ul&gt; &lt;li&gt;Max clock: 1905 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 180&lt;/li&gt; &lt;li&gt;This is confirmed, from windows, an UV + OC of 1905Mhz at 0.9V.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (so about 1.08 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX A6000: &lt;ul&gt; &lt;li&gt;Max clock: 1740 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an UV + OC of about 0.8V&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (about 870 GB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For reference: PP (pre processing) is mostly compute bound, and TG (text generation) is bandwidth bound.&lt;/p&gt; &lt;p&gt;I have posted the raw performance metrics on pastebin, as it is a bit hard to make it readable here on reddit, on &lt;a href="https://pastebin.com/g3vjU6jY"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance Summary (N_KV = 0)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;TG Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Power (W)&lt;/th&gt; &lt;th align="left"&gt;PP t/s/W&lt;/th&gt; &lt;th align="left"&gt;TG t/s/W&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;4,641.54&lt;/td&gt; &lt;td align="left"&gt;76.78&lt;/td&gt; &lt;td align="left"&gt;425&lt;/td&gt; &lt;td align="left"&gt;10.92&lt;/td&gt; &lt;td align="left"&gt;0.181&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;3,625.95&lt;/td&gt; &lt;td align="left"&gt;54.38&lt;/td&gt; &lt;td align="left"&gt;375&lt;/td&gt; &lt;td align="left"&gt;9.67&lt;/td&gt; &lt;td align="left"&gt;0.145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1,538.49&lt;/td&gt; &lt;td align="left"&gt;44.78&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;td align="left"&gt;4.27&lt;/td&gt; &lt;td align="left"&gt;0.124&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1,578.69&lt;/td&gt; &lt;td align="left"&gt;38.60&lt;/td&gt; &lt;td align="left"&gt;280&lt;/td&gt; &lt;td align="left"&gt;5.64&lt;/td&gt; &lt;td align="left"&gt;0.138&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Relative Performance (vs RTX 3090 baseline)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed&lt;/th&gt; &lt;th align="left"&gt;TG Speed&lt;/th&gt; &lt;th align="left"&gt;PP Efficiency&lt;/th&gt; &lt;th align="left"&gt;TG Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;3.02x&lt;/td&gt; &lt;td align="left"&gt;1.71x&lt;/td&gt; &lt;td align="left"&gt;2.56x&lt;/td&gt; &lt;td align="left"&gt;1.46x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;2.36x&lt;/td&gt; &lt;td align="left"&gt;1.21x&lt;/td&gt; &lt;td align="left"&gt;2.26x&lt;/td&gt; &lt;td align="left"&gt;1.17x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1.03x&lt;/td&gt; &lt;td align="left"&gt;0.86x&lt;/td&gt; &lt;td align="left"&gt;1.32x&lt;/td&gt; &lt;td align="left"&gt;1.11x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Degradation with Context (N_KV)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Drop (0→6144)&lt;/th&gt; &lt;th align="left"&gt;TG Drop (0→6144)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;-15.7%&lt;/td&gt; &lt;td align="left"&gt;-13.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;-16.3%&lt;/td&gt; &lt;td align="left"&gt;-14.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;-12.7%&lt;/td&gt; &lt;td align="left"&gt;-14.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;-14.1%&lt;/td&gt; &lt;td align="left"&gt;-14.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And some images!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a"&gt;https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949"&gt;https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6"&gt;https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg80cq</id>
    <title>New Mistral Small 3.2</title>
    <updated>2025-06-20T16:14:11+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open weights: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/MistralAI/status/1936093325116781016/photo/1"&gt;https://x.com/MistralAI/status/1936093325116781016/photo/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgs0d3</id>
    <title>RIGEL: An open-source hybrid AI assistant/framework</title>
    <updated>2025-06-21T08:50:46+00:00</updated>
    <author>
      <name>/u/__z3r0_0n3__</name>
      <uri>https://old.reddit.com/user/__z3r0_0n3__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"&gt; &lt;img alt="RIGEL: An open-source hybrid AI assistant/framework" src="https://external-preview.redd.it/YLvcO6GZN90fnKUxGABmFCgN1xACgLSeDvnM0Igr0UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c355b0db52a56986ec016efb4f52e3e845b3734a" title="RIGEL: An open-source hybrid AI assistant/framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Hey all,&lt;/h3&gt; &lt;p&gt;We're building an open-source project at Zerone Labs called RIGEL — a hybrid AI system that acts as both:&lt;/p&gt; &lt;p&gt;a multi-agent assistant, and&lt;/p&gt; &lt;p&gt;a modular control plane for tools and system-level operations.&lt;/p&gt; &lt;p&gt;It's not a typical desktop assistant — instead, it's designed to work as an AI backend for apps, services, or users who want more intelligent interfaces and automation.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-LLM support (local: Ollama / LLaMA.cpp, remote: Groq, etc.)&lt;/li&gt; &lt;li&gt;Tool-calling via a built-in MCP layer (run commands, access files, monitor systems)&lt;/li&gt; &lt;li&gt;D-Bus API integration (Linux) for embedding AI in other apps&lt;/li&gt; &lt;li&gt;Speech (Whisper STT, Piper TTS) optional but local&lt;/li&gt; &lt;li&gt;Memory and partial RAG support (ChromaDB)&lt;/li&gt; &lt;li&gt;Designed for local-first setups, but cloud-extensible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s currently in developer beta. Still rough in places, but usable and actively growing.&lt;/p&gt; &lt;p&gt;We’d appreciate feedback, issues, or thoughts — especially from people building their own agents, platform AIs, or AI-driven control systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__z3r0_0n3__"&gt; /u/__z3r0_0n3__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zerone-Laboratories/RIGEL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T08:50:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgnri0</id>
    <title>What are some AI tools (free or paid) that genuinely helped you get more done — especially the underrated ones not many talk about?</title>
    <updated>2025-06-21T04:17:18+00:00</updated>
    <author>
      <name>/u/Melted_gun</name>
      <uri>https://old.reddit.com/user/Melted_gun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not looking for the obvious ones like ChatGPT or Midjourney — more curious about those lesser-known tools that actually made a difference in your workflow, mindset, or daily routine.&lt;/p&gt; &lt;p&gt;Could be anything — writing, coding, research, time-blocking, design, personal journaling, habit tracking, whatever.&lt;/p&gt; &lt;p&gt;Just trying to find tools that might not be in my radar but could quietly improve things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Melted_gun"&gt; /u/Melted_gun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T04:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgkhdk</id>
    <title>A100 80GB can't serve 10 concurrent users - what am I doing wrong?</title>
    <updated>2025-06-21T01:18:55+00:00</updated>
    <author>
      <name>/u/Creative_Yoghurt25</name>
      <uri>https://old.reddit.com/user/Creative_Yoghurt25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Qwen2.5-14B-AWQ on A100 80GB for voice calls.&lt;/p&gt; &lt;p&gt;People say RTX 4090 serves 10+ users fine. My A100 with 80GB VRAM can't even handle 10 concurrent requests without terrible TTFT (30+ seconds).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current vLLM config:&lt;/strong&gt; &lt;code&gt;yaml --model Qwen/Qwen2.5-14B-Instruct-AWQ --quantization awq_marlin --gpu-memory-utilization 0.95 --max-model-len 12288 --max-num-batched-tokens 4096 --max-num-seqs 64 --enable-chunked-prefill --enable-prefix-caching --block-size 32 --preemption-mode recompute --enforce-eager &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Configs I've tried:&lt;/strong&gt; - &lt;code&gt;max-num-seqs&lt;/code&gt;: 4, 32, 64, 256, 1024 - &lt;code&gt;max-num-batched-tokens&lt;/code&gt;: 2048, 4096, 8192, 16384, 32768 - &lt;code&gt;gpu-memory-utilization&lt;/code&gt;: 0.7, 0.85, 0.9, 0.95 - &lt;code&gt;max-model-len&lt;/code&gt;: 2048 (too small), 4096, 8192, 12288 - Removed limits entirely - still terrible&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; Input is ~6K tokens (big system prompt + conversation history). Output is only ~100 tokens. User messages are small but system prompt is large.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GuideLLM benchmark results:&lt;/strong&gt; - 1 user: 36ms TTFT ✅&lt;br /&gt; - 25 req/s target: Only got 5.34 req/s actual, 30+ second TTFT - Throughput test: 3.4 req/s max, 17+ second TTFT - 10+ concurrent: 30+ second TTFT ❌&lt;/p&gt; &lt;p&gt;Also considering Triton but haven't tried yet.&lt;/p&gt; &lt;p&gt;Need to maintain &amp;lt;500ms TTFT for at least 30 concurrent users. What vLLM config should I use? Is 14B just too big for this workload?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative_Yoghurt25"&gt; /u/Creative_Yoghurt25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T01:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgt6sx</id>
    <title>Dynamic metaprompting in Open WebUI</title>
    <updated>2025-06-21T10:13:16+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt6sx/dynamic_metaprompting_in_open_webui/"&gt; &lt;img alt="Dynamic metaprompting in Open WebUI" src="https://external-preview.redd.it/YmMyNm1sYWw4OThmMcWg2kr7Oe9IfY8fGfsf43KXN8n2ZXafTDS0jzzrXQ6i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4f01a26834a9f7d88489f6eeb1321977ca56000" title="Dynamic metaprompting in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM proxy with OpenAI-compatible API runs a workflow where system prompt is dynamically mixed from a given set of source prompts according to their weight &lt;/li&gt; &lt;li&gt;The ratios are controlled from a specially crafted artifact that talks back to the workflow over websockets&lt;/li&gt; &lt;li&gt;UI allows to pause or slow down the generation for better control&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/promx.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vnmpwmal898f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt6sx/dynamic_metaprompting_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgt6sx/dynamic_metaprompting_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T10:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lghrf9</id>
    <title>If your tools and parameters aren’t too complex, even Qwen1.5 0.5B can handle tool calling with a simple DSL and finetuning.</title>
    <updated>2025-06-20T23:04:41+00:00</updated>
    <author>
      <name>/u/umtksa</name>
      <uri>https://old.reddit.com/user/umtksa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I designed a super minimal syntax like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;TOOL: param1, param2, param3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then fine-tuned Qwen 1.5 0.5B for just &lt;strong&gt;5 epochs&lt;/strong&gt;, and now it can reliably call &lt;strong&gt;all 11 tools&lt;/strong&gt; in my dataset without any issues.&lt;/p&gt; &lt;p&gt;I'm working in Turkish, and before this, I could only get accurate tool calls using much larger models like &lt;strong&gt;Gemma3:12B&lt;/strong&gt;. But this little model now handles it surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; – If your tool names and parameters are relatively simple like mine, just invent a small DSL and fine-tune a base model. Even &lt;strong&gt;Google Colab’s free tier&lt;/strong&gt; is enough.&lt;/p&gt; &lt;p&gt;here is my own dataset that I use to fine tune qwen1.5 &lt;a href="https://huggingface.co/datasets/umtksa/tools"&gt;https://huggingface.co/datasets/umtksa/tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umtksa"&gt; /u/umtksa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T23:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg7vuc</id>
    <title>mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face</title>
    <updated>2025-06-20T16:09:13+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt; &lt;img alt="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" src="https://external-preview.redd.it/3DBqKqgOLKDMFbcrOD5Qa-3M1IIegLfhMX6TTbsgXeU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6eecbfa2b523b92f82faf94cb6ab334696d320" title="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsykj</id>
    <title>AbsenceBench: LLMs can't tell what's missing</title>
    <updated>2025-06-21T09:58:18+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt; &lt;img alt="AbsenceBench: LLMs can't tell what's missing" src="https://b.thumbs.redditmedia.com/PjMEdZcgsAhmRkC952iAQojRviDlyPY_z4tXX2TYqCE.jpg" title="AbsenceBench: LLMs can't tell what's missing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://arxiv.org/pdf/2506.11440"&gt;AbsenceBench paper&lt;/a&gt; establishes a test that's basically Needle In A Haystack (NIAH) in reverse. &lt;a href="https://github.com/harvey-fin/absence-bench"&gt;Code here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The idea is that models score 100% on NIAH tests, thus perfectly identify added tokens that stand out - which is not equal to perfectly reasoning over longer context though - and try that in reverse, with added hints.&lt;/p&gt; &lt;p&gt;They gave the model poetry, number sequences and GitHub PRs, &lt;em&gt;together with&lt;/em&gt; a modified version with removed words or lines, and then asked the model to identify what's missing. A simple program can figure this out with 100% accurracy. The LLMs can't.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285"&gt;https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using around 8k thinking tokens improved the score by 8% on average. Those 8k thinking tokens are quite longer than the average input - just 5k, with almost all tests being shorter than 12k. Thus, this isn't an issue of long context handling, although results get worse with longer context. For some reason the results also got worse when testing with shorter omissions.&lt;/p&gt; &lt;p&gt;The hypothesis is that the attention mechanism can only attend to tokens that exist. Omissions have no tokens, thus there are no tokens to put attention on. They tested this by adding placeholders, which boosted the scores by 20% to 50%.&lt;/p&gt; &lt;p&gt;The NIAH test just tested finding literal matches. Models that didn't score close to 100% were also bad at long context understanding. Yet as we've seen with NoLiMa and fiction.liveBench, getting 100% NIAH score doesn't equal good long context &lt;em&gt;understanding&lt;/em&gt;. This paper only tests literal omissions and not semantic omissions, like incomplete evidence for a conclusion. Thus, like NIAH a model scoring 100% here won't automatically guarantee good long context understanding.&lt;/p&gt; &lt;p&gt;Bonus: They also shared the average reasoning tokens per model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874"&gt;https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsxyw</id>
    <title>Unsloth Dynamic GGUF Quants For Mistral 3.2</title>
    <updated>2025-06-21T09:57:06+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt; &lt;img alt="Unsloth Dynamic GGUF Quants For Mistral 3.2" src="https://external-preview.redd.it/CrtSkHQg7FYlqUCKyAhEr6h8Hgeh7uXu4dg2iLzQFtI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=206fbbf02fe74bed130c7c80f847013da0053f61" title="Unsloth Dynamic GGUF Quants For Mistral 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgg7a1</id>
    <title>Google releases MagentaRT for real time music generation</title>
    <updated>2025-06-20T21:54:20+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Omar from the Gemma team here, to talk about MagentaRT, our new music generation model. It's real-time, with a permissive license, and just has 800 million parameters.&lt;/p&gt; &lt;p&gt;You can find a video demo right here &lt;a href="https://www.youtube.com/watch?v=Ae1Kz2zmh9M"&gt;https://www.youtube.com/watch?v=Ae1Kz2zmh9M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A blog post at &lt;a href="https://magenta.withgoogle.com/magenta-realtime"&gt;https://magenta.withgoogle.com/magenta-realtime&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GitHub repo &lt;a href="https://github.com/magenta/magenta-realtime"&gt;https://github.com/magenta/magenta-realtime&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And our repository #1000 on Hugging Face: &lt;a href="https://huggingface.co/google/magenta-realtime"&gt;https://huggingface.co/google/magenta-realtime&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T21:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lglhll</id>
    <title>Mistral's "minor update"</title>
    <updated>2025-06-21T02:12:10+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt; &lt;img alt="Mistral's &amp;quot;minor update&amp;quot;" src="https://preview.redd.it/rb70qb16v68f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7248b214307a876a51003f595eeeb9564be8245" title="Mistral's &amp;quot;minor update&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rb70qb16v68f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T02:12:10+00:00</published>
  </entry>
</feed>
