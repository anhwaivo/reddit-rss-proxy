<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-09T22:33:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j7h6cm</id>
    <title>What happens when you connect multiple models together over irc? Something like this.</title>
    <updated>2025-03-09T20:43:39+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7h6cm/what_happens_when_you_connect_multiple_models/"&gt; &lt;img alt="What happens when you connect multiple models together over irc? Something like this." src="https://external-preview.redd.it/hr2zHKYKCyXwJpXfn-w24ChBRcuuCyr2-NXriWR_feM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c71bac912f05ec5b56db8e0cced23a59a55b361" title="What happens when you connect multiple models together over irc? Something like this." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/MdtWiOszTBw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7h6cm/what_happens_when_you_connect_multiple_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7h6cm/what_happens_when_you_connect_multiple_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T20:43:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j707rk</id>
    <title>I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude</title>
    <updated>2025-03-09T05:02:27+00:00</updated>
    <author>
      <name>/u/SamchonFramework</name>
      <uri>https://old.reddit.com/user/SamchonFramework</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt; &lt;img alt="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" src="https://external-preview.redd.it/r6RwKjQxXMDR-e5d7t4YxF5tZC1G1HG5fUFlHIeaPcI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57049fdbdeab552ae7b0d02c18658d895181b100" title="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SamchonFramework"&gt; /u/SamchonFramework &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nestia.io/articles/llm-function-calling/i-made-mcp-alternative-solution.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T05:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6xpvt</id>
    <title>How large is your local LLM context?</title>
    <updated>2025-03-09T02:38:39+00:00</updated>
    <author>
      <name>/u/iwinux</name>
      <uri>https://old.reddit.com/user/iwinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm new to this rabbit hole. Never realized context is such a VRAM hog until I loaded my first model (Qwen2.5 Coder 14B Instruct &lt;code&gt;Q4_K_M&lt;/code&gt; GGUF) with LM Studio. On my Mac mini M2 Pro (32GB RAM), increasing context size from 32K to 64K almost eats up all RAM.&lt;/p&gt; &lt;p&gt;So I wonder, do you run LLMs with max context size by default? Or keep it as low as possible?&lt;/p&gt; &lt;p&gt;For my use case (coding, as suggested by the model), I'm already spoiled by Claude / Gemini's huge context size :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iwinux"&gt; /u/iwinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T02:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72c1y</id>
    <title>‘chain of draft’ could cut AI costs by 90%</title>
    <updated>2025-03-09T07:21:14+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72c1y/chain_of_draft_could_cut_ai_costs_by_90/"&gt; &lt;img alt="‘chain of draft’ could cut AI costs by 90%" src="https://external-preview.redd.it/VPaHR1A0XGUXYqEupnzQMib5WY6OUTBwIapwC6B9UMs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eb12e0a785ad29a7788e2ea82678955a0c194dd" title="‘chain of draft’ could cut AI costs by 90%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/less-is-more-how-chain-of-draft-could-cut-ai-costs-by-90-while-improving-performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72c1y/chain_of_draft_could_cut_ai_costs_by_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j72c1y/chain_of_draft_could_cut_ai_costs_by_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T07:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7505c</id>
    <title>simple-computer-use: a lightweight open source Computer Use implementation for Windows and Linux</title>
    <updated>2025-03-09T10:38:44+00:00</updated>
    <author>
      <name>/u/nava_7777</name>
      <uri>https://old.reddit.com/user/nava_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I made &lt;a href="https://github.com/pnmartinez/simple-computer-use"&gt;https://github.com/pnmartinez/simple-computer-use&lt;/a&gt; to solve a&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Nowawadays we can &lt;strong&gt;code with Natural Language with Cursor, Windsurf, or other tools.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;However, &lt;strong&gt;ideas often come up while away from the PC&lt;/strong&gt;, and I find myself &lt;strong&gt;putting my hardware to work for me through TeamViewer&lt;/strong&gt; or similar (which is uncomfortable),&lt;/li&gt; &lt;li&gt;I consider that &lt;strong&gt;voice support&lt;/strong&gt; for these apps like Cursor would be absolutely awesome (some issues already opened in their repo),&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Solution&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I made myself (yet another) &lt;strong&gt;tool to control a desktop GUI with natural language&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Adding a layer for &lt;strong&gt;voice control&lt;/strong&gt; is just the next step.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TODO&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice processing layer&lt;/strong&gt; to send the task comfortably from e.g. a phone to the desktop hardware,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;increase robustness&lt;/strong&gt;: the current implementation is too-heavily realiant on OCR (vision capabilities for icons can be greatly improved with vLLMs, this is just a POC).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to use, give feedback, open issues and PRs. etc.&lt;/p&gt; &lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/29891887/420660898-bdd5bc25-fe88-4105-a3ed-f435f98e4f18.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE1MTY5NTUsIm5iZiI6MTc0MTUxNjY1NSwicGF0aCI6Ii8yOTg5MTg4Ny80MjA2NjA4OTgtYmRkNWJjMjUtZmU4OC00MTA1LWEzZWQtZjQzNWY5OGU0ZjE4LndlYm0_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMwOVQxMDM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00Y2ZjZWYwZDM5N2M1MTI4ZGY4YjdmZTVkNTkxMDJhMGY3MjFkYzk0NjQ1ZDk1OGQ5MGVjMjE2YTU0NTAxMjQ2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fL-o54DMITQUFiS_Er4QtTp-Dy_N6I_ooYLQE-VzG_E"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nava_7777"&gt; /u/nava_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T10:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ckg0</id>
    <title>What is the best framework for running llms locally?</title>
    <updated>2025-03-09T17:23:43+00:00</updated>
    <author>
      <name>/u/BABA_yaaGa</name>
      <uri>https://old.reddit.com/user/BABA_yaaGa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With possible integration with other frameworks for agent building through the api.&lt;/p&gt; &lt;p&gt;My HW setup:&lt;/p&gt; &lt;p&gt;AMD Ryzen 9 3950x CPU 16 gb ram (will add more) 1x rtx 3090 2TB storage &lt;/p&gt; &lt;p&gt;Edit1: I need the best performance possible and also be able to run the quantized models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BABA_yaaGa"&gt; /u/BABA_yaaGa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ckg0/what_is_the_best_framework_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ckg0/what_is_the_best_framework_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ckg0/what_is_the_best_framework_for_running_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T17:23:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nzrk</id>
    <title>New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s</title>
    <updated>2025-03-08T18:51:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt; &lt;img alt="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" src="https://preview.redd.it/wfkxh0q5iine1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63ab35ce1aa6589c56196d048a5e4231e07749f" title="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfkxh0q5iine1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7iaz4</id>
    <title>WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp; Qdrant</title>
    <updated>2025-03-09T21:33:40+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"&gt; &lt;img alt="WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp;amp; Qdrant" src="https://external-preview.redd.it/S9bl2NwR9T_6QLlvwFTmOUTu7C-3tJMOo7MGY2kvTlc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a94f23393f1d8c9c0739f49ff1c6b1de91d3f233" title="WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp;amp; Qdrant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dkruyt/WebRAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T21:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7dedc</id>
    <title>llama.cpp RPC is great! the network is not the bottleneck</title>
    <updated>2025-03-09T18:00:05+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a 2nd rig with my older cards (1 3060 and 3 P40s). My main rig is 6x3090s.&lt;/p&gt; &lt;p&gt;I networked them and used llama.rpc to distributed a model across them. My limits are PCIe3, some slots are x8, the ethernet are 1Gigabit ethernet, and my switch is a 1Gigabit switch as well. I ran different tests to see the performance. Using Qwen2.5-Math-72b. I'm running from my main rig and RPC to the 2nd rig.&lt;/p&gt; &lt;p&gt;Results are below. The number of RPC connection is not the culprit in things slowing down, but how fast the model can crunch it, it's when I put more data on the card that it slows down. This leads me to believe that if my 2nd rig was all 3090s that my performance won't suffer as much either. The data is below, do with it what you will. Money of course is the real bottleneck, my builds are budget builds. cheap dual x99 boards with 10yrs old $5 used CPUs. $15 gigabit switch, etc.&lt;/p&gt; &lt;p&gt;With that said, imagine a future where we have an open weight AGI and assume it's as big as DSR1 or the closed models, you can now begin to crunch what it would take to run one at home if you had the money. ;-) Start saving up.&lt;/p&gt; &lt;p&gt;rig 1&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 0.19 ms / 2 runs ( 0.10 ms per token, 10362.69 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 18283.60 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 11115.47 ms / 40 tokens ( 277.89 ms per token, 3.60 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 32084.58 ms / 339 runs ( 94.64 ms per token, 10.57 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 84452.24 ms / 379 tokens&lt;/p&gt; &lt;p&gt;rig 2&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 557.65 ms / 582 runs ( 0.96 ms per token, 1043.67 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 261281.02 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 569.29 ms / 41 tokens ( 13.89 ms per token, 72.02 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 142978.28 ms / 540 runs ( 264.77 ms per token, 3.78 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 222230.02 ms / 581 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (all GPUS)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 62.08 ms / 266 runs ( 0.23 ms per token, 4284.93 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 379939.37 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 9588.90 ms / 40 tokens ( 239.72 ms per token, 4.17 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 50867.07 ms / 243 runs ( 209.33 ms per token, 4.78 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 64102.87 ms / 283 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 1x3060) 2,2,2,2,1 (16gb/8gb)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 27.48 ms / 266 runs ( 0.10 ms per token, 9678.01 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 102374.35 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 13524.99 ms / 40 tokens ( 338.12 ms per token, 2.96 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 28475.16 ms / 243 runs ( 117.18 ms per token, 8.53 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 428941.78 ms / 283 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 1xP40), 2,2,2,2,1 (16gb/8gb)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 23.16 ms / 266 runs ( 0.09 ms per token, 11483.34 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 102172.20 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 20711.72 ms / 40 tokens ( 517.79 ms per token, 1.93 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 29402.94 ms / 243 runs ( 121.00 ms per token, 8.26 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 52413.97 ms / 283 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 1xP40), 2,2,2,2,2 (14gb)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 57.93 ms / 328 runs ( 0.18 ms per token, 5662.10 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 178429.43 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 11687.45 ms / 40 tokens ( 292.19 ms per token, 3.42 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 43853.93 ms / 305 runs ( 143.78 ms per token, 6.95 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 86921.61 ms / 345 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 2xP40) (12gb)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 54.29 ms / 266 runs ( 0.20 ms per token, 4899.25 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 273503.49 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 11791.10 ms / 40 tokens ( 294.78 ms per token, 3.39 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 42442.55 ms / 243 runs ( 174.66 ms per token, 5.73 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 59487.62 ms / 283 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 3xP40) (10gb)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 83.28 ms / 360 runs ( 0.23 ms per token, 4322.71 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 350843.76 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 37953.89 ms / 40 tokens ( 948.85 ms per token, 1.05 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 68081.76 ms / 337 runs ( 202.02 ms per token, 4.95 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 124884.83 ms / 377 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 3xP40) (10,10,10,10,1,1,1) to test RPC overhead (16.8gb on 3090s over 1.7gb on P40s)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 31.04 ms / 266 runs ( 0.12 ms per token, 8569.59 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 74052.34 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 20362.38 ms / 40 tokens ( 509.06 ms per token, 1.96 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 29414.99 ms / 243 runs ( 121.05 ms per token, 8.26 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 80676.37 ms / 283 tokens&lt;/p&gt; &lt;p&gt;rig 1/rig 2 (rig 1 all GPUS, rig 2 3xP40) (25,25,25,25,1,1,1) to test RPC overhead (17.3gb on 3090s over 0.89gb on P40s)&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 24.56 ms / 266 runs ( 0.09 ms per token, 10829.30 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 45330.16 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 39684.73 ms / 40 tokens ( 992.12 ms per token, 1.01 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 27593.63 ms / 243 runs ( 113.55 ms per token, 8.81 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 94779.46 ms / 283 tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dedc/llamacpp_rpc_is_great_the_network_is_not_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dedc/llamacpp_rpc_is_great_the_network_is_not_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dedc/llamacpp_rpc_is_great_the_network_is_not_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ep0m</id>
    <title>12V-2x6 Power Connector Cooks At Over 150°C With A "Water-Cooled" NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage</title>
    <updated>2025-03-09T18:56:09+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"&gt; &lt;img alt="12V-2x6 Power Connector Cooks At Over 150°C With A &amp;quot;Water-Cooled&amp;quot; NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage" src="https://external-preview.redd.it/Lr1gQ1IkpnQxgTpcgRzn8EDKNAzaJLo3Qv8Ts8UVing.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ff7729a068308654c949a0e21e53869bc166978" title="12V-2x6 Power Connector Cooks At Over 150°C With A &amp;quot;Water-Cooled&amp;quot; NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/12v-2x6-power-connector-cooks-at-over-150c-with-a-water-cooled-nvidia-geforce-rtx-5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7fviw</id>
    <title>is anyone else getting extremely nerfed results for qwq?</title>
    <updated>2025-03-09T19:46:40+00:00</updated>
    <author>
      <name>/u/Mr_Cuddlesz</name>
      <uri>https://old.reddit.com/user/Mr_Cuddlesz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im running qwq fp16 on my local machine but it seems to be performing much worse vs. qwq on qwen chat. is anyone else experiencing this? i am running this: &lt;a href="https://ollama.com/library/qwq:32b-fp16"&gt;https://ollama.com/library/qwq:32b-fp16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Cuddlesz"&gt; /u/Mr_Cuddlesz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T19:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j5kl</id>
    <title>Build a low cost (&lt;1300€) deep learning rig</title>
    <updated>2025-03-09T22:10:46+00:00</updated>
    <author>
      <name>/u/yachty66</name>
      <uri>https://old.reddit.com/user/yachty66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt; &lt;img alt="Build a low cost (&amp;lt;1300€) deep learning rig" src="https://external-preview.redd.it/kz76DidpUO1jZDbyqLId1mAwEm0aIMpcywQeyNnxncY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed352f3d0dd50f0d25d7b383b5540a87749fcfe1" title="Build a low cost (&amp;lt;1300€) deep learning rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all.&lt;/p&gt; &lt;p&gt;It's the first time for me building a computer - my goal was to make the build as cheap as possible while still having good performance, and the RTX 3090 FE seemed to be giving the best bang for the buck.&lt;/p&gt; &lt;p&gt;I used these parts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: RTX 3090 FE (used)&lt;/li&gt; &lt;li&gt;CPU: Intel i5 12400F&lt;/li&gt; &lt;li&gt;Motherboard: Asus PRIME B660M-K D4&lt;/li&gt; &lt;li&gt;RAM: Corsair Vengeance LPX 32GB (2x16GB)&lt;/li&gt; &lt;li&gt;Storage: WD Green SN3000 500GB NVMe&lt;/li&gt; &lt;li&gt;PSU: MSI MAG A750GL PCIE5 750W&lt;/li&gt; &lt;li&gt;CPU Cooler: ARCTIC Freezer 36&lt;/li&gt; &lt;li&gt;Case Fan: ARCTIC P12 PWM&lt;/li&gt; &lt;li&gt;Case: ASUS Prime AP201 MicroATX&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole build cost me less than 1,300€.&lt;/p&gt; &lt;p&gt;I have a more detailed explanation on how I did things and the links to the parts in my GitHub repo: &lt;a href="https://github.com/yachty66/aicomputer"&gt;https://github.com/yachty66/aicomputer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zimdxgommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9358dc0882ceaa3d89f49184369ee8bf7d949224"&gt;https://preview.redd.it/zimdxgommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9358dc0882ceaa3d89f49184369ee8bf7d949224&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hkorlhommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=afc00171321ec28ef30820102f42b5e1d3ad6fa2"&gt;https://preview.redd.it/hkorlhommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=afc00171321ec28ef30820102f42b5e1d3ad6fa2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/36jb4oommqne1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ceb0b30bac9df00d21636347a3368efbcb55539"&gt;https://preview.redd.it/36jb4oommqne1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ceb0b30bac9df00d21636347a3368efbcb55539&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yachty66"&gt; /u/yachty66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6w3qq</id>
    <title>PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters.</title>
    <updated>2025-03-09T01:11:29+00:00</updated>
    <author>
      <name>/u/computemachines</name>
      <uri>https://old.reddit.com/user/computemachines</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt; &lt;img alt="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." src="https://preview.redd.it/4k4rbdxjdkne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1088de160301ef1df0ee665bb6dbee41c324644" title="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computemachines"&gt; /u/computemachines &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k4rbdxjdkne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T01:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j76z48</id>
    <title>Which major open source model will be next? Llama, Mistral, Hermes, Nemotron, Qwen or Grok2?</title>
    <updated>2025-03-09T12:51:11+00:00</updated>
    <author>
      <name>/u/EmergencyLetter135</name>
      <uri>https://old.reddit.com/user/EmergencyLetter135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the Mistral 24B and the QwQ 32B, which larger model do you think will be launched next? What are your candidates? A 100B Llama, Mistral, Hermes, Nemotron, Qwen or Grok2? Who will be faster and release their larger model first? My money is on another Chinese model, as it seems to have a head start in this area despite the sanctions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmergencyLetter135"&gt; /u/EmergencyLetter135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j76z48/which_major_open_source_model_will_be_next_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j76z48/which_major_open_source_model_will_be_next_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j76z48/which_major_open_source_model_will_be_next_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T12:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7baw1</id>
    <title>What GPU do you use for 32B/70B models, and what speed do you get?</title>
    <updated>2025-03-09T16:28:29+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What GPU are you using for 32B or 70B models? How fast do they run in tokens per second?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T16:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72gw5</id>
    <title>AMD May Bring ROCm Support On Windows Operating System As AMD’s Vice President Nods For It</title>
    <updated>2025-03-09T07:30:47+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/"&gt;https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T07:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79oma</id>
    <title>Why ate we not seeing much desktop apps developed with local AI integration,by smaller developers?</title>
    <updated>2025-03-09T15:13:54+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean there is huge market out there and there are infinite categories of desktop apps that can benefit from inyegrating local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6yxdr</id>
    <title>Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf (and Thinking/Reasoning MOES...) ... 34+ new models (Lllamas, Qwen - MOES and not Moes..)</title>
    <updated>2025-03-09T03:46:25+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From David_AU ;&lt;/p&gt; &lt;p&gt;First two models based on Qwen's off the charts &amp;quot;QwQ 32B&amp;quot; model just released, with some extra power. Detailed instructions, and examples at each repo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Model, Free thinker, Extra Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular, Not so Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AND Qwen/Llama Thinking/Reasoning MOES - all sizes, shapes ...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;34&lt;/strong&gt; reasoning/thinking models (example generations, notes, instructions etc):&lt;/p&gt; &lt;p&gt;Includes Llama 3,3.1,3.2 and Qwens, DeepSeek/QwQ/DeepHermes in MOE and NON MOE config plus others:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is an interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf"&gt;https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For Qwens (12 models) only (Moes and/or Enhanced):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde"&gt;https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Separate source / full precision sections/collections at main repo here:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;656 Models, in 27 collections:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU"&gt;https://huggingface.co/DavidAU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Special service note for Lmstudio users:&lt;/p&gt; &lt;p&gt;The issue with QwQs (32B from Qwen and mine 35B) re: Templates/Jinja templates has been fixed. Make sure you update to build 0.3.12 ; otherwise manually select CHATML template to work with the new QwQ models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T03:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j75xpm</id>
    <title>Dumb question - I use Claude 3.5 A LOT, what setup would I need to create a comparable local solution?</title>
    <updated>2025-03-09T11:45:31+00:00</updated>
    <author>
      <name>/u/Friendly_Signature</name>
      <uri>https://old.reddit.com/user/Friendly_Signature</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a hobbyist coder that is now working on bigger personal builds. (I was Product guy and Scrum master for AGES, now I am trying putting the policies I saw around me enforced on my own personal build projects). &lt;/p&gt; &lt;p&gt;Loving that I am learning by DOING my own CI/CD, GitHub with apps and Actions, using Rust instead of python, sticking to DDD architecture, TD development, etc&lt;/p&gt; &lt;p&gt;I spend a lot on Claude, maybe enough that I could justify a decent hardware purchase. It seems the new Mac Studio M3 Ultra pre-config is aimed directly at this market?&lt;/p&gt; &lt;p&gt;Any feedback welcome :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly_Signature"&gt; /u/Friendly_Signature &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T11:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79ev6</id>
    <title>Open WebUi + Tailscale = Beauty</title>
    <updated>2025-03-09T15:01:05+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I might be late to this party but just wanted to advertise for anyone who needs a nudge, if you have a good solution for running local LLMs but find it difficult to take it everywhere with you, or find the noise of fans whirring up distracting to you or others around you, you should check this out.&lt;/p&gt; &lt;p&gt;I've been using Open Web UI for ages as my front end for Ollama and it is fantastic. When I was at home I could even use it on my phone via the same network.&lt;/p&gt; &lt;p&gt;At work a coworker recently suggested I look into Tailscale and wow I am blown away by this. In short, you can easily create your own VPN and never have to worry about setting up static IPs or VIPs or NAT traversal or port forwarding. Basically a simple installer on any device (including your phones).&lt;/p&gt; &lt;p&gt;With that done, you can then (for example) connect your phone directly to the Open WebUI you have running on your desktop at home from anywhere in the world, from any connection, and never have to think about the connectivity again. All e2e encrypted. Mesh network no so single point of failure. &lt;/p&gt; &lt;p&gt;Is anyone else using this? I searched and saw some side discussions but not a big dedicated thread recently.&lt;/p&gt; &lt;p&gt;10/10 experience and HIGHLY recommended to give it a try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7g30z</id>
    <title>When will Llama 4, Gemma 3, or Qwen 3 be released?</title>
    <updated>2025-03-09T19:55:53+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When do you guys think these SOTA models will be released? It's been like forever so do anything of you know if there is a specific date in which they will release the new models? Also, what kind of New advancements do you think these models will bring to the AI industry, how will they be different from our old models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j6cg</id>
    <title>&lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast</title>
    <updated>2025-03-09T22:11:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt; &lt;img alt="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" src="https://external-preview.redd.it/ZDFiNmN0NHptcW5lMdBuqabr-hQLmYC8Qi5X9EdtbTx_2YZ-hAZhcsR_hrB1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2014a6f010867f98da226a97f756cd7d035b3cb" title="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2wo0b8lqmqne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79obx</id>
    <title>Local Deep Research Update - I worked on your requested features and got also help from you</title>
    <updated>2025-03-09T15:13:32+00:00</updated>
    <author>
      <name>/u/ComplexIt</name>
      <uri>https://old.reddit.com/user/ComplexIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Runs 100% locally with Ollama or OpenAI-API Endpoint/vLLM - only search queries go to external services (Wikipedia, arXiv, DuckDuckGo, The Guardian) when needed. Works with the same models as before (Mistral, DeepSeek, etc.).&lt;/p&gt; &lt;p&gt;Quick install:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;&lt;code&gt;https://github.com/LearningCircuit/local-deep-research&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama pull mistral&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;python&lt;/code&gt; &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As many of you requested, I've added several new features to the Local Deep Research tool:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Auto Search Engine Selection&lt;/strong&gt;: The system intelligently selects the best search source based on your query (&lt;strong&gt;Wikipedia&lt;/strong&gt; for facts, &lt;strong&gt;arXiv&lt;/strong&gt; for academic content, your &lt;strong&gt;local documents&lt;/strong&gt; when relevant)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local RAG Support&lt;/strong&gt;: You can now create custom document collections for different topics and search through your own files along with online sources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In-line Citations&lt;/strong&gt;: Added better citation handling as requested&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Search Engines&lt;/strong&gt;: &lt;strong&gt;Now supports Wikipedia, arXiv, DuckDuckGo, The Guardian, and your local document collections&lt;/strong&gt; - it is easy for you to add your own search engines if needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;: A new web UI makes it easier to start research, track progress, and view results - it is created by a contributor(&lt;strong&gt;HashedViking&lt;/strong&gt;)!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you for all the contributions, feedback, suggestions, and stars - they've been essential in improving the tool!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;https://github.com/LearningCircuit/local-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexIt"&gt; /u/ComplexIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j77rkb</id>
    <title>I've made Deepseek R1 think in Spanish</title>
    <updated>2025-03-09T13:35:34+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"&gt; &lt;img alt="I've made Deepseek R1 think in Spanish" src="https://preview.redd.it/fx6kdf0w2one1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9ef3b20bb9a2eb6b38c32bf652878fd0dc519c3" title="I've made Deepseek R1 think in Spanish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally it only thinks in English (or in Chinese if you prompt in Chinese). So with this prompt I'll put in the comments its CoT is entirely in Spanish. I should note that I am not a native Spanish speaker. It was an experiment for me because normally it doesn't think in other languages even if you prompt so, but this prompt works. It should be applicable to other languages too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fx6kdf0w2one1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T13:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7dzao</id>
    <title>QWQ low score in Leaderboard, what happened?</title>
    <updated>2025-03-09T18:25:08+00:00</updated>
    <author>
      <name>/u/ipechman</name>
      <uri>https://old.reddit.com/user/ipechman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt; &lt;img alt="QWQ low score in Leaderboard, what happened?" src="https://preview.redd.it/77rco6vfipne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce1eb44fa048f06ef55f271c38be8e206c4ed0a5" title="QWQ low score in Leaderboard, what happened?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ipechman"&gt; /u/ipechman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77rco6vfipne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:25:08+00:00</published>
  </entry>
</feed>
