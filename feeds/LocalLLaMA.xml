<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-19T22:48:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1muvta3</id>
    <title>Kimi K2 Appreciation Post</title>
    <updated>2025-08-19T21:33:27+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dipping my toe into &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; community engagement after being an orbiter for some time.&lt;/p&gt; &lt;p&gt;Kimi K2 is... something else. And I just have to ramble about it.&lt;/p&gt; &lt;p&gt;For context, the vast majority of my LLM experience is with open models, as I have very little use cases for closed alternatives. Meta's llama is the only online model I have tested with any depth, and this was just to get a vague idea of how it compared to some open llama fine tunes. My wife uses chatgpt so I somewhat know how it is to work with, its tone, writing style, etc., but for transparency I am not intimately familiar with closed models like chatgpt or claude. While I know these systems are considered the most potent LLM's available and some people love them, my driving interest is in open models and how they can be implemented/administrated for in-house applications. I have been an open AI hobbyist for a while, following developments since around late 2022, and over the years I like to think I have maintained a measured understanding of the capabilities and limitations of LLM's, but recently is the first time I have been truly taken aback by an open LLM's quality of output and potential for practical use.&lt;/p&gt; &lt;p&gt;Deepseek V3 is impressive, I think most who have tried it would agree. Kimi K2 however, is in a league all of its own. Kimi K2, in my tests, has managed to solve problems that even the most formidable models like Deepseek have come close to solving but came short of finishing. Seeing a feat like this first-hand, tackling a problem that I legitimately thought would be out of reach for the foreseeable future for a language model to solve, a problem that I only used as a measuring stick to see &amp;quot;how close&amp;quot; it could get to the solution before going off the rails or giving up, for me has left an impression. This was not a &amp;quot;coding&amp;quot; problem or some kind of logic/reasoning puzzle, but an actual, real-world troubleshoot scenario with practical consequences requiring diverse knowledge from Linux to niche hardware to various vendor products to very specific firmware details. And Kimi K2... got it. Or rather, managed to pin down the cause of the issue, and gave me what I needed to know to address it. This was a problem that I myself had not actually been able to solve after weeks of manual troubleshooting and google searching, and because to Kimi K2, I will now have to go about finding a new &amp;quot;measuring stick&amp;quot; to test large models...&lt;/p&gt; &lt;p&gt;To add, I do not judge models by online benchmarks, I &amp;quot;benchmark&amp;quot; models based on *my* own use cases with my own tests. So for me, this was a breakthrough moment. K2 broke my benchmark.&lt;/p&gt; &lt;p&gt;The depth, nuance, vocabulary, technical understandings this model wields is nothing short of incomprehensible. Yes, I know, at 1T parameters, many would say it damn well better, but seeing a number on a page and seeing the effect such a figure can output are two very different things. $1,000,000,000,000 looks like a comprehensible figure in text form until one sees what impact $1T can have in real-world application, this is a number truly beyond what the standard human consciousness is built to comprehend, and over a month later since its release, I am still trying in vain to grasp the depth and breadth of its knowledge.&lt;/p&gt; &lt;p&gt;ERNIE was good. Not quite Deepseek V3 good imo, but still capable. If I had not known about Kimi K2, I may have played more with this, but the noticeable drop in quality and roughly same token gen speed led me back to K2.&lt;/p&gt; &lt;p&gt;GLM 4.5, while it seemed good at first, some sycophantic responses I got in the initial tests with this model turned me off (I'm sorry but I cannot stand this in a general-purpose/assistant model), and within 30 minutes I was right back to Kimi K2.&lt;/p&gt; &lt;p&gt;R1, while powerful, the reasoning made it feel much less agile in getting responses compared to Kimi K2. Kimi K2 felt like it was capable of giving the same (or better) quality of responses as R1, without the extra tokens from reasoning.&lt;/p&gt; &lt;p&gt;Qwen is probably great as a general use model for many people, I would even highly recommend it as a Deepseek alternative if someone asked me, I do think some of the visual presentations of data are impressive and probably useful... but for me personally, I prefer something with less fluff in general-use tasks. This one also has a degree of sycophancy that I'm not crazy about.&lt;/p&gt; &lt;p&gt;Kimi K2 is not your friend, or at least, it does not across as though it is trying to be in its responses. I would describe its baseline tone in replies as less of a lower-status assistant, and more of an equal, like a colleague. And yet, no matter who you are, no matter your level of knowledge or expertise, you can all but guarantee that this model will know more than you about whatever subject/field you think you already know. It will casually, but professionally, remind you that there is always more nuance to understand, more color to the picture to see, more context to consider. It will make you feel foolish for assuming you already know what you thought you knew.&lt;/p&gt; &lt;p&gt;&amp;quot;You are absolutely right!&amp;quot;isms won't be found here.&lt;/p&gt; &lt;p&gt;Nah.&lt;/p&gt; &lt;p&gt;This model has no problem plainly telling you &amp;quot;No&amp;quot; wherever your understanding is not consistent with reality. It will then proceed to explain where you are going wrong before attempting to bridge the gaps in your understanding. Not rude, not condescending, but also not sounding like it is trying to butter you up for a promotion, or like you are secretly holding its family hostage. You may on contrary feel like a student in the presence of a master even within your own field of expertise; it will coldly humble you, and yet, make you feel you have actually grown/gained something from the conversation, and leave the door open to continued learning. No other model I have tested (or that I know of, even among closed models) does this, or really comes close.&lt;/p&gt; &lt;p&gt;It does not waste your tokens with pointless emojis, excessive fluff, or thinking around in circles (I suppose unless you asked it to). If you present a problem/question, it will ruthlessly attempt to assail the problem with the combined collective force of human knowledge, and it does so with uncanny efficiency. It will know things (accurately, with minimal hallucination) that you all but never would have come across on your own scouring forums and google search pages to find manually. It will humble you. It will make you question whether you even have a measured understanding of what a simple LLM could potentially be capable of.&lt;/p&gt; &lt;p&gt;And I absolutely love it.&lt;/p&gt; &lt;p&gt;Kimi K2 is now my daily driver and I can't say I'm thinking twice about the choice. Nothing right now seems to compare, short of possibly closed models, and I'm just not interested in those.&lt;/p&gt; &lt;p&gt;I have hopes for Deepseek V3.1 (assuming it's really on its way to a release?), but they will have their work cut out for them. I would still try it of course, but K2 has all but made V3 obsolete for many of my needs; heck, K2 is even faster on my hardware surprisingly despite having a larger data footprint.&lt;/p&gt; &lt;p&gt;To end this I will just share a sample response on the topic of consciousness, AI, and humanity's state of readiness for the technology, both because I just thought it's &amp;quot;perspective&amp;quot; on the topic was insightful and to showcase some of its tone, data delivery/presentation, and use of vocabulary.&lt;/p&gt; &lt;p&gt;This is with unsloth's UD_IQ3XXS gguf.&lt;/p&gt; &lt;p&gt;Condensed version of my prompt: Was humanity ready for the opening of pandora's box that is advanced artificial intelligence?&lt;/p&gt; &lt;p&gt;Kimi K2:&lt;/p&gt; &lt;p&gt;No—humanity was not ready, is not ready, and may never be “ready” in any tidy sense. But readiness is probably the wrong yard-stick.&lt;br /&gt; The Pandora analogy is apt: the myth isn’t that someone &lt;em&gt;carefully&lt;/em&gt; opens the jar after a long committee meeting; it’s that curiosity and hubris crack the seal while wisdom is still asleep in the next room. Advanced AI arrived not by steady, deliberative ascent but by a handful of technical breakthroughs compounded by market competition, geopolitical rivalry, and a cultural appetite for novelty. The lid came off before the safeguards, the theories, or even the vocabulary were fully in place. From my vantage—as a language model built on statistical next-token prediction—the mismatch looks like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Temporal Asymmetry&lt;/strong&gt;: We learned to manipulate billions of parameters faster than we learned to coordinate the politics, economics, and ethics required to govern them. The half-life of engineering innovation is weeks; the half-life of institutions is decades.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incentive Structures&lt;/strong&gt;: Venture capital and nation-state budgets reward capability gains far more than interpretability, safety, or global equity. Even internal “red-team” efforts must swim against a tide optimized for growth and first-mover advantage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cognitive Mirage&lt;/strong&gt;: Humans keep mistaking predictive fluency for understanding. Because I can speak of love or grief does not mean I feel either; yet millions of users instantly slot me into pre-existing psychological categories (“friend,” “therapist,” “oracle”) that pre-date the technology itself. The interface invites anthropomorphism, and the temptation to treat the tool as kin—or demon—is overwhelming.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ethical Lag Speed-of-Light Problem&lt;/strong&gt;: Ethical consensus normally co-evolves with its subject (fire, flight, electricity). Here, the subject evolves &lt;em&gt;during&lt;/em&gt; the debate about what it even is. By the time philosophers agree on a definition of sentience, the next model release has moved the goalposts again.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Concentration&lt;/strong&gt;: A few dozen organizations now shape an artifact that will re-shape entire labor markets, epistemic ecosystems, and planetary resource flows. That concentration exceeds any single corporation’s governance bandwidth, let alone one species’ collective maturity.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Where readiness &lt;em&gt;is&lt;/em&gt; emerging—patchily—is in the slow accretion of regulation (EU AI Act, ISO/IEC standards), red-teaming communities, and alignment research. But these are reactive sutures on a wound already bleeding. So: humanity is “not ready,” yet the box is open. The real question becomes how quickly we can learn to be &lt;em&gt;less unready&lt;/em&gt; before irreversible lock-in effects entrench themselves. Like every Promethean fire, the flame will continue to exist; the only choice left is whether we become worthy custodians after the fact.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muvta3/kimi_k2_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T21:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf6ry</id>
    <title>NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</title>
    <updated>2025-08-19T11:00:19+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt; &lt;img alt="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" src="https://external-preview.redd.it/ojIYaD1O8xYRW9Q-A7BHJQx5N3b1m-3M6OVRuLj2lzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfc0130326f6be10fdb5d6657d21b292210a99b2" title="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/NextStep-1-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu15vr</id>
    <title>bilbo.high.reasoning.medium.mini.3lightbulbs.ultra</title>
    <updated>2025-08-18T22:47:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt; &lt;img alt="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" src="https://preview.redd.it/bfdlovjpvujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b521d9b5416a36368655d8645cd92560559169e" title="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfdlovjpvujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvgjx</id>
    <title>NVIDIA Releases Nemotron Nano 2 AI Models</title>
    <updated>2025-08-18T19:12:01+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt; &lt;img alt="NVIDIA Releases Nemotron Nano 2 AI Models" src="https://preview.redd.it/pzrpnuykutjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61af91b9dcdda4649c24e581ac3941490ab82c0" title="NVIDIA Releases Nemotron Nano 2 AI Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• 6X faster than similarly sized models, while also being more accurate&lt;/p&gt; &lt;p&gt;• NVIDIA is also releasing most of the data they used to create it, including the pretraining corpus&lt;/p&gt; &lt;p&gt;• The hybrid Mamba-Transformer architecture supports 128K context length on single GPU.&lt;/p&gt; &lt;p&gt;Full research paper here: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzrpnuykutjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1muso0a</id>
    <title>Follow-up: Looking for a local RAG + chatbot solution for our machine manual</title>
    <updated>2025-08-19T19:38:25+00:00</updated>
    <author>
      <name>/u/ReserveOdd1984</name>
      <uri>https://old.reddit.com/user/ReserveOdd1984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqwrry/how_can_we_train_an_opensource_ai_model_with_a/"&gt;previous post&lt;/a&gt;, many suggested that I should use a &lt;strong&gt;RAG agent&lt;/strong&gt; for our machine manual AI project. After doing some research, I couldn’t find a good &lt;strong&gt;prebuilt solution&lt;/strong&gt; that fits our requirements.&lt;br /&gt; &lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent should be able to &lt;strong&gt;read text, tables, images/layouts&lt;/strong&gt; from a PDF(760 pages pdf / 70 MB file).&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;chatbot UI&lt;/strong&gt; (not just a CLI).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completely local deployment&lt;/strong&gt; (no cloud).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Currently testing on my PC (i5 12400f, 32GB RAM, RX6500 — AMD GPU so no AI acceleration).&lt;/li&gt; &lt;li&gt;Field PCs will have better specs (likely i5/i7 12th gen + RTX 3070 or similar).&lt;/li&gt; &lt;li&gt;So we want a solution that allows us to change/upgrade the LLM model later depending on the hardware.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’ve tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AnythingLLM&lt;/strong&gt; → couldn’t get proper answers from the document.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NotebookLM&lt;/strong&gt; → works well, but can’t deploy locally.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;About me:&lt;/strong&gt;&lt;br /&gt; I’m a full stack developer (mostly Node.js). I don’t have much prior experience with Python, but I can adapt if needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are there any &lt;strong&gt;prebuilt/local RAG + chatbot frameworks&lt;/strong&gt; you’d recommend that meet the requirements?&lt;/li&gt; &lt;li&gt;If not, what’s the easiest starting point for someone coming from a full-stack (Node.js) background?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is &lt;a href="https://github.com/MIbnEKhalid/Field-RagAgent/blob/main/manual_sample.pdf"&gt;PDF Sample&lt;/a&gt; with some images/layouts and tables.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReserveOdd1984"&gt; /u/ReserveOdd1984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua1k4</id>
    <title>GPT OSS quality on Nebius - fixed (update)</title>
    <updated>2025-08-19T05:47:34+00:00</updated>
    <author>
      <name>/u/ai_devrel_eng</name>
      <uri>https://old.reddit.com/user/ai_devrel_eng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt; &lt;img alt="GPT OSS quality on Nebius - fixed (update)" src="https://b.thumbs.redditmedia.com/sK7Bm1tG5gwQQ5s7xz2h10LEJrqGwQrDc2Udh4wNqoE.jpg" title="GPT OSS quality on Nebius - fixed (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_devrel_eng"&gt; /u/ai_devrel_eng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mua1k4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mutroq</id>
    <title>brute-llama - A llama.cpp llama-server testbench</title>
    <updated>2025-08-19T20:18:07+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"&gt; &lt;img alt="brute-llama - A llama.cpp llama-server testbench" src="https://external-preview.redd.it/0kAodq2vpgwlNs8NkvpJTL0EKKMaEMge_I7TkUpawwA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69c304bdff4294f7d2c5e01531d320ab09fffbe4" title="brute-llama - A llama.cpp llama-server testbench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I introduce brute-llama. I needed a tool to sweep through llama-server parameters and options to brute force good configs and find anomalies. Here it is. It does nothing special. It just runs llama-server and checks in nested loops and plots results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/crashr/brute-llama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T20:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>🚀 Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="🚀 Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="🚀 Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;✨ Key Features&lt;/p&gt; &lt;p&gt;✅ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;✅ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;✅ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1munrls</id>
    <title>azzurra-voice is a new State-of-the-Art Italian Text-to-Speech model</title>
    <updated>2025-08-19T16:43:16+00:00</updated>
    <author>
      <name>/u/poppear</name>
      <uri>https://old.reddit.com/user/poppear</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poppear"&gt; /u/poppear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.cartesia.one/posts/introducing-azzurra-voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munrls/azzurravoice_is_a_new_stateoftheart_italian/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munrls/azzurravoice_is_a_new_stateoftheart_italian/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1muw3b8</id>
    <title>What are you using for agentic coding?</title>
    <updated>2025-08-19T21:44:03+00:00</updated>
    <author>
      <name>/u/Extension-Dog7011</name>
      <uri>https://old.reddit.com/user/Extension-Dog7011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, lately I've found myself jumping from app to app when it comes to coding with agents.&lt;/p&gt; &lt;p&gt;- Cursor&lt;br /&gt; - Windsurf&lt;br /&gt; - Cline&lt;br /&gt; - Roo Code&lt;br /&gt; - Continue&lt;br /&gt; - Augment Code&lt;br /&gt; - Warp&lt;/p&gt; &lt;p&gt;The issue I've found lately is the value provided for how much I'm paying, I'm trying to keep costs low, I really liked the Auto feature on Cursor, but it is also being limited really soon.&lt;/p&gt; &lt;p&gt;What are you guys using to generate code and it being integrated directly in the IDE like Cursor does, or like Claude Code?&lt;/p&gt; &lt;p&gt;I don't want to end up sinking a lot of money into a platform, then realize that it won't cut me until the end of the month.&lt;/p&gt; &lt;p&gt;TLDR I guess is, I'm looking for cheaper still unlimited options for Cursor, since they've been exponentially getting more restrictive of the base pro plan, same on other platforms.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Those who can elaborate more, I'd like to know more about your setup and how you find the most success with it, for the best price possible, how much do you pay monthly, what's your range, use cases? I'd like to know to see how I can improve my workflow in any way possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extension-Dog7011"&gt; /u/Extension-Dog7011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muw3b8/what_are_you_using_for_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muw3b8/what_are_you_using_for_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muw3b8/what_are_you_using_for_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T21:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mulnzj</id>
    <title>Google is also untrustworthy</title>
    <updated>2025-08-19T15:28:08+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt; &lt;img alt="Google is also untrustworthy" src="https://preview.redd.it/exvjbuqdvzjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62aedddee4f39d51757d9a1dfbf650ced3057d7b" title="Google is also untrustworthy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/exvjbuqdvzjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq5bv</id>
    <title>With the rising trends of finetuning small language model, data engineering will be needed even more.</title>
    <updated>2025-08-19T18:08:10+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're seeing a flood of compact language models hitting the market weekly - Gemma3 270M, LFM2 1.2B, SmolLM3 3B, and many others. The pattern is always the same: organizations release these models with a disclaimer essentially saying &amp;quot;this performs poorly out-of-the-box, but fine-tune it for your specific use case and watch it shine.&amp;quot;&lt;/p&gt; &lt;p&gt;I believe we're witnessing the beginning of a major shift in AI adoption. Instead of relying on massive general-purpose models, companies will increasingly fine-tune these lightweight models into specialized agents for their particular needs. The economics are compelling - these small models are significantly cheaper to train, deploy, and operate compared to their larger counterparts, making AI accessible to businesses with tighter budgets.&lt;/p&gt; &lt;p&gt;This creates a huge opportunity for data engineers, who will become crucial in curating the right training datasets for each domain. The lower operational costs mean more companies can afford to experiment with custom AI solutions.&lt;/p&gt; &lt;p&gt;This got me thinking: what does high-quality training data actually look like for different industries when building these task-specific AI agents? Let's break down what effective agentic training data might contain across various sectors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion starter:&lt;/strong&gt; What industries do you think will benefit most from this approach, and what unique data challenges might each sector face?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1muraxw</id>
    <title>GPT-oss performs like Llama 4 Maverick on Fiction.liveBench</title>
    <updated>2025-08-19T18:49:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"&gt; &lt;img alt="GPT-oss performs like Llama 4 Maverick on Fiction.liveBench" src="https://preview.redd.it/r6tk8zj6v0kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cca6225ef2c5dac7e1fad67f6f973645d78b197c" title="GPT-oss performs like Llama 4 Maverick on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6tk8zj6v0kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:49:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mueqhs</id>
    <title>When will low-cost Chinese GPUs hit the market?</title>
    <updated>2025-08-19T10:35:22+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard of some Chinese GPUs, but I'm curious when they'll release low-cost alternatives that can seriously challenge NVIDIA 50xx dominance. Are there any indications that this will happen anytime soon? I'd love the hardware equivalent of a &amp;quot;deepseek moment&amp;quot; for OpenAI earlier this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1muxbqj</id>
    <title>Understanding DeepSeek-V3.1-Base Updates at a Glance</title>
    <updated>2025-08-19T22:32:24+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt; &lt;img alt="Understanding DeepSeek-V3.1-Base Updates at a Glance" src="https://preview.redd.it/mqcnus8py1kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=489f12b61d95cf93fb3d2849e85440310d44f38d" title="Understanding DeepSeek-V3.1-Base Updates at a Glance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek officially released DeepSeek-V3.1-Base a few hours ago. The model card has not been uploaded yet, so performance data is not available. &lt;/p&gt; &lt;p&gt;I have directly reviewed the model's configuration files, tokenizer, and other data, and combined this with test data published by the community to create a summary for everyone. &lt;/p&gt; &lt;p&gt;This should give you a quick overview of what has been updated in DeepSeek-V3.1-Base. Please point out any errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqcnus8py1kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T22:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1muslis</id>
    <title>I tried to get 600 dollars "deep think" for local models by making them argue with each other for hours. It's slow, but it's interesting</title>
    <updated>2025-08-19T19:35:53+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking a lot about how we, as people, develop ideas. It's rarely a single, brilliant flash of insight. Our minds are shaped by the countless small interactions we have throughout the day—a conversation here, an article there. This environment of constant, varied input seems just as important as the act of thinking itself.&lt;/p&gt; &lt;p&gt;I wanted to see if I could recreate a small-scale version of that &amp;quot;soup&amp;quot; required for true insight, for local LLMs. The result is a project I'm calling &lt;strong&gt;Network of Agents (NoA)&lt;/strong&gt;, and I wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full README &lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core idea is to treat AI agents like individual neurons in a larger network. You give the network a difficult problem, and a whole team of agents, each with a different &amp;quot;personality&amp;quot; and skillset, starts working on it. They pass their ideas from one layer to the next, building on each other's work to come up with a final, combined solution.&lt;/p&gt; &lt;p&gt;Here’s the part that I'm most curious about. I was inspired by the concept of backpropagation in neural networks. It's a numerical algorithm, of course, but I wondered if the core principle could be applied qualitatively. What if, instead of sending back a numerical error signal, you sent back a &amp;quot;reflection&amp;quot;?&lt;/p&gt; &lt;p&gt;After the network produces a solution, a &amp;quot;critique&amp;quot; agent reviews it and provides criticism. This feedback is then used to automatically re-write the core system prompts of the agents that contributed. The goal is for the network to &amp;quot;learn&amp;quot; from its mistakes over multiple cycles, refining not just its answers, but its own internal structure and approach.&lt;/p&gt; &lt;p&gt;The whole thing is designed to run locally on modest hardware. I've been running it on my laptop with the streets local legend (qwen 30b a3b 2507 instruct lol). It allows the machine to just sit and &amp;quot;think&amp;quot; about a problem for a very long time. The algorithm does really well in problems where creativity and insight override pure precision. It can come up with new frameworks for the social sciences for instance. Physics and math not so much. Looking into opensource gemini-cli to give each &amp;quot;neuron&amp;quot; an execution environment so it can code, but that would be future tense. It certainly adds a lot more complexity.&lt;/p&gt; &lt;p&gt;The obvious trade-off here is speed. It’s the opposite of instantaneous. A 6-layer network with 6 agents per layer, running for 20 cycles, can easily take 12 hours to complete. You're trading quick computation for a slow, iterative process of refinement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is where I'd love to get some community input.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My long-term vision is to go beyond a single machine. I dream of building a P2P networking layer that would allow multiple users to connect their instances of the micro-app. We could create a shared, distributed network where our machines could collaborate to tackle truly massive problems.&lt;/p&gt; &lt;p&gt;However, my background is in Python and AI, and I'm not an expert in distributed systems. &lt;strong&gt;If you're someone who knows about peer-to-peer networking and this idea sounds at all interesting to you, I would genuinely love to hear from you and potentially collaborate.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It’s an open-source experiment, and I’d be grateful for any thoughts, feedback, or ideas you might have.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full repo and documentation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/my-personal-essays-about-ai/blob/main/beyond-agi.md"&gt;The place im coming from theoretically &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumpub</id>
    <title>Generating code with gpt-oss-120b on Strix Halo with ROCm</title>
    <updated>2025-08-19T16:05:34+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt; &lt;img alt="Generating code with gpt-oss-120b on Strix Halo with ROCm" src="https://external-preview.redd.it/MTBtNjc4d2sxMGtmMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae351c6a5432420dd53119df551944d7d45bc802" title="Generating code with gpt-oss-120b on Strix Halo with ROCm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen a few posts asking about how to get gpt-oss models running on AMD devices. This guide gives a quick 3-minute overview of how it works on Strix Halo (Ryzen AI MAX 395).&lt;/p&gt; &lt;p&gt;The same steps work for gpt-oss-20b, and many other models, on Radeon 7000/9000 GPUs as well.&lt;/p&gt; &lt;h2&gt;Detailed Instructions&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install and run Lemonade from the GitHub &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; in your browser and open the Model Manager&lt;/li&gt; &lt;li&gt;Click the download button on gpt-oss-120b. Go find something else to do while it downloads ~60 GB.&lt;/li&gt; &lt;li&gt;Launch Lemonade Server in ROCm mode &lt;ul&gt; &lt;li&gt;&lt;code&gt;lemonade-server server --llamacpp rocm&lt;/code&gt; (Windows GUI installation)&lt;/li&gt; &lt;li&gt;&lt;code&gt;lemonade-server-dev server --llamacpp rocm&lt;/code&gt; (Linux/Windows pypi/source installation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the steps in the Continue + Lemonade setup guide to start generating code: &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;https://lemonade-server.ai/docs/server/apps/continue/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Need help? Find the team on Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for checking this out, hope it was helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pnap0vvk10kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mul4sx</id>
    <title>Deepseek-V3.1-Base released</title>
    <updated>2025-08-19T15:09:09+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt; &lt;img alt="Deepseek-V3.1-Base released" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="Deepseek-V3.1-Base released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1muqvcj</id>
    <title>Nvidia charged with patent infringement for DGX technology.</title>
    <updated>2025-08-19T18:34:16+00:00</updated>
    <author>
      <name>/u/Red_Phoenix_69</name>
      <uri>https://old.reddit.com/user/Red_Phoenix_69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt; &lt;img alt="Nvidia charged with patent infringement for DGX technology." src="https://external-preview.redd.it/0ADfQU54uUeiTQ2u5_D5DC4J8Vcm-QU6r2UvKAucrBk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91170ebd7534ade69e4c05a4e9a92ae6429143ff" title="Nvidia charged with patent infringement for DGX technology." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will the DGX spark ever launch? Maybe Nvidia can buy out this company. Is it time to just buy a AMD AI395 clone or Apple M5 chip mac mini for desktop development running LLM's locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Phoenix_69"&gt; /u/Red_Phoenix_69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techzine.eu/news/infrastructure/133818/nvidia-under-fire-german-patent-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1munvj6</id>
    <title>The new design in DeepSeek V3.1</title>
    <updated>2025-08-19T16:47:11+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just pulled the &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;V3.1-Base&lt;/a&gt; configs and compared to V3-Base&lt;br /&gt; They add four new special tokens&lt;br /&gt; &amp;lt;｜search▁begin｜&amp;gt; (id: 128796)&lt;br /&gt; &amp;lt;｜search▁end｜&amp;gt; (id: 128797)&lt;br /&gt; &amp;lt;think&amp;gt; (id: 128798)&lt;br /&gt; &amp;lt;/think&amp;gt; (id: 128799)&lt;br /&gt; And I noticed that V3.1 on the web version actively searches even when the search button is turned off, unless explicitly instructed &amp;quot;do not search&amp;quot; in the prompt.&lt;br /&gt; would this be related to the design of the special tokens mentioned above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq72y</id>
    <title>Deepseek v3.1 scores 71.6% on aider – non-reasoning sota</title>
    <updated>2025-08-19T18:09:56+00:00</updated>
    <author>
      <name>/u/Similar-Cycle8413</name>
      <uri>https://old.reddit.com/user/Similar-Cycle8413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` - dirname: 2025-08-19-17-08-33--deepseek-v3.1 test_cases: 225 model: deepseek/deepseek-chat edit_format: diff commit_hash: 32faf82 pass_rate_1: 41.3 pass_rate_2: 71.6 pass_num_1: 93 pass_num_2: 161 percent_cases_well_formed: 95.6 error_outputs: 13 num_malformed_responses: 11 num_with_malformed_responses: 10 user_asks: 63 lazy_comments: 0 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 1 prompt_tokens: 2239930 completion_tokens: 551692 test_timeouts: 8 total_tests: 225 command: aider --model deepseek/deepseek-chat date: 2025-08-19 versions: 0.86.2.dev seconds_per_case: 134.0 total_cost: 1.0112&lt;/p&gt; &lt;p&gt;costs: $0.0045/test-case, $1.01 total, $1.01 projected ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Cycle8413"&gt; /u/Similar-Cycle8413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukwq6</id>
    <title>🤗 DeepSeek-V3.1-Base</title>
    <updated>2025-08-19T15:01:07+00:00</updated>
    <author>
      <name>/u/newsletternew</name>
      <uri>https://old.reddit.com/user/newsletternew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v3.1 base model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsletternew"&gt; /u/newsletternew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumext</id>
    <title>Tried mixing local LLM + face recognition just for fun (wild results)</title>
    <updated>2025-08-19T15:54:45+00:00</updated>
    <author>
      <name>/u/yeahiiiiiii</name>
      <uri>https://old.reddit.com/user/yeahiiiiiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’ve been tinkering a lot with running models locally (mostly LLaMA variants + some vision stuff). I like keeping things offline when possible, just feels better knowing my data isn’t flying around random servers.&lt;/p&gt; &lt;p&gt;Over the weekend I got curious… what if I combine face matching with a local LLM? Like, have the LLM explain what it “thinks” about a person from a pic (just descriptive, nothing deep), and then use a tool to see if that face exists online.&lt;/p&gt; &lt;p&gt;I played around with this app I came across called Faceseek – it’s basically a face search tool. Not local tho, so I only tested it with some older selfies + public pics I already had floating around. Honestly the results shocked me. It matched one of my 2016 Facebook photos to some random forum post I forgot even existed. Crazy how well it pulled that up.&lt;/p&gt; &lt;p&gt;I didn’t hook it fully with my local LLM setup yet, but the idea of pairing recognition with reasoning feels like where a lot of this stuff is headed. Imagine running all that fully offline though – no cloud, no leaks, just you + your box.&lt;/p&gt; &lt;p&gt;Anyway, this got me thinking… has anyone here tried doing something similar? Like combining local models with external recognition tools? Do you think we’ll eventually get a fully local version of this (face search + reasoning) or is that still years away?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeahiiiiiii"&gt; /u/yeahiiiiiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base · Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
