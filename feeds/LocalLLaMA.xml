<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-26T06:24:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ia2ws8</id>
    <title>Make any LLM to think deeper like OpenAI o1 and deepseek R1</title>
    <updated>2025-01-26T01:45:09+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"&gt; &lt;img alt="Make any LLM to think deeper like OpenAI o1 and deepseek R1" src="https://external-preview.redd.it/JT-N_EYTApyWq4Q3frr0DDXvZ0J3V57PGC5Im4LvWLo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02ce8dbc4a38dfaded62c9675a5d23a70ace31b4" title="Make any LLM to think deeper like OpenAI o1 and deepseek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey readers! Hope you are doing well! On October 2024 I reasearched and found a way to make sonnet to reason on par with OpenAI O1 and many people found that work useful and Now wrote an opensource library called LLM Reasoner which makes any LLM to think deeper like OpenAI o1 and deepseek R1 models which is built on top my previous work. from the example screenshot we can see that gpt4o count numbers of r's in strawberry&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2gl5g11rs8fe1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=598a028bee61c338a4d25b19b1413f33ca133aad"&gt;https://preview.redd.it/2gl5g11rs8fe1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=598a028bee61c338a4d25b19b1413f33ca133aad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM-Reasoner repo: &lt;a href="https://github.com/harishsg993010/LLM-Reasoner"&gt;https://github.com/harishsg993010/LLM-Reasoner&lt;/a&gt;&lt;br /&gt; PyPI: &lt;a href="https://pypi.org/project/llm-reasoner/"&gt;https://pypi.org/project/llm-reasoner/&lt;/a&gt;&lt;br /&gt; research work: &lt;a href="https://medium.com/@harishhacker3010/can-we-make-any-smaller-opensource-ai-models-smarter-than-human-1ea507e644a0"&gt;https://medium.com/@harishhacker3010/can-we-make-any-smaller-opensource-ai-models-smarter-than-human-1ea507e644a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if anyone of you people has any feeback or criticism about this project&lt;br /&gt; Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2ws8/make_any_llm_to_think_deeper_like_openai_o1_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T01:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9xcck</id>
    <title>Want to Build AI Agents? Tired of LangChain, CrewAI, AutoGen &amp; Other AI Frameworks? Read this! (Fully supports local open source models as well!)</title>
    <updated>2025-01-25T21:19:21+00:00</updated>
    <author>
      <name>/u/TheDeadlyPretzel</name>
      <uri>https://old.reddit.com/user/TheDeadlyPretzel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9xcck/want_to_build_ai_agents_tired_of_langchain_crewai/"&gt; &lt;img alt="Want to Build AI Agents? Tired of LangChain, CrewAI, AutoGen &amp;amp; Other AI Frameworks? Read this! (Fully supports local open source models as well!)" src="https://external-preview.redd.it/EvfUYzzBOVWnuwEkd3C7uuilibmfczubiiAkGmTLLZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=184958498fee51751afa6ca920d99c5853323116" title="Want to Build AI Agents? Tired of LangChain, CrewAI, AutoGen &amp;amp; Other AI Frameworks? Read this! (Fully supports local open source models as well!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheDeadlyPretzel"&gt; /u/TheDeadlyPretzel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/ai-advances/want-to-build-ai-agents-c83ab4535411?sk=b9429f7c57dbd3bda59f41154b65af35"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9xcck/want_to_build_ai_agents_tired_of_langchain_crewai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9xcck/want_to_build_ai_agents_tired_of_langchain_crewai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9znut</id>
    <title>Will there be a Whisper 4 model by OpenAI?</title>
    <updated>2025-01-25T23:04:43+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it, do you think there will be a release? If yes, what would you expect as features?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9znut/will_there_be_a_whisper_4_model_by_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9znut/will_there_be_a_whisper_4_model_by_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9znut/will_there_be_a_whisper_4_model_by_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T23:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia0j9o</id>
    <title>So what is now the best local AI for coding?</title>
    <updated>2025-01-25T23:46:34+00:00</updated>
    <author>
      <name>/u/Tenkinn</name>
      <uri>https://old.reddit.com/user/Tenkinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard that the distill versions of Deepseek r1 are not that good for coding compared to qwen 2.5 coder instruct and the full deepseek r1 version&lt;/p&gt; &lt;p&gt;also that the 32b qwen version is better than the 70b llama one&lt;/p&gt; &lt;p&gt;Is the full deepseek r1 the only model better than Claude sonnet 3.5 ? Is it worth it use use it through the api if we can run the 32b or 70b locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tenkinn"&gt; /u/Tenkinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T23:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia53oi</id>
    <title>A little scene I created using Qwen's new chat</title>
    <updated>2025-01-26T03:44:21+00:00</updated>
    <author>
      <name>/u/charmander_cha</name>
      <uri>https://old.reddit.com/user/charmander_cha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ia53oi/video/nu38fg31f9fe1/player"&gt;https://reddit.com/link/1ia53oi/video/nu38fg31f9fe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charmander_cha"&gt; /u/charmander_cha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia53oi/a_little_scene_i_created_using_qwens_new_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia53oi/a_little_scene_i_created_using_qwens_new_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia53oi/a_little_scene_i_created_using_qwens_new_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T03:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9dvrk</id>
    <title>Deepseek v3 will make MoE opensource models wayyy more common.</title>
    <updated>2025-01-25T03:22:08+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IDK why no one is talking about this but i just finished reading Deepseek v3's technical report, and how they’ve found innovative and novel solution for one of the biggest challenges with training MoE architectures which is irregular loss spiking.&lt;/p&gt; &lt;p&gt;this issue was probably the major reason why we haven’t seen widespread adoption of MoE models before. But now, with their novel solutions laid out in this open report, it’s likely that other companies will start implementing similar approaches.&lt;/p&gt; &lt;p&gt;I can already imagine a MoE powered Qwen or Llama becoming flagship models in future, just like deepseek&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9dvrk/deepseek_v3_will_make_moe_opensource_models_wayyy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T03:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9k18t</id>
    <title>I am simply blown away by this 32B model. It's a Sky-T1 + Fuse-O1 + DeepseekR1 + Qwen32B fusion. Please read the full post</title>
    <updated>2025-01-25T10:16:36+00:00</updated>
    <author>
      <name>/u/Educational_Gap5867</name>
      <uri>https://old.reddit.com/user/Educational_Gap5867</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model available here: &lt;a href="https://huggingface.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF"&gt;https://huggingface.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF&lt;/a&gt;&lt;br /&gt; Original reddit post by &lt;a href="/u/AaronFeng47"&gt;u/AaronFeng47&lt;/a&gt; : &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/&lt;/a&gt;&lt;br /&gt; Leetcode 1430. Prompt + Thought process here: &lt;a href="https://pastebin.com/hqWfbG7e"&gt;https://pastebin.com/hqWfbG7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am simply blown away. This AI's thought process is so smooth. Every time I thought it was overthinking it wasn't it was judging the cases.&lt;br /&gt; Only caveat. 1: I think if the question would have been unknown it would've fucked up the final output I think it got distracted because it thought all the way through in Python but then suddenly started writing final code in Java and I am not sure what that was about. I had no pre-prompted it or anything like that.&lt;/p&gt; &lt;p&gt;Commands I ran&lt;br /&gt; `ollama pull &lt;a href="http://hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF"&gt;hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF&lt;/a&gt; `&lt;br /&gt; `Ollama run &lt;a href="http://hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF%60"&gt;hf.co/sm54/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview-Q4_K_M-GGUF`&lt;/a&gt;&lt;br /&gt; `&amp;gt; PROMPT (check pastebin for Prompt) `&lt;/p&gt; &lt;p&gt;Final code here:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;```java public class Solution { public boolean doesPathExist(TreeNode root, int[] arr) { if (root == null || arr.length == 0) { return false; } Stack&amp;lt;Pair&amp;lt;TreeNode, Integer&amp;gt;&amp;gt; stack = new Stack&amp;lt;&amp;gt;(); stack.push(new Pair&amp;lt;&amp;gt;(root, 0)); while (!stack.isEmpty()) { Pair&amp;lt;TreeNode, Integer&amp;gt; currentPair = stack.pop(); TreeNode currentNode = currentPair.getKey(); int currentIndex = currentPair.getValue(); if (currentIndex &amp;gt;= arr.length) { continue; } if (currentNode.val != arr[currentIndex]) { continue; } // Check if this is the last element and a leaf if (currentIndex == arr.length - 1) { if (currentNode.left == null &amp;amp;&amp;amp; currentNode.right == null) { return true; } } else { // Push right first to process left later, maintaining order if (currentNode.right != null) { stack.push(new Pair&amp;lt;&amp;gt;(currentNode.right, currentIndex + 1)); } if (currentNode.left != null) { stack.push(new Pair&amp;lt;&amp;gt;(currentNode.left, currentIndex + 1)); } } } return false; } } // Helper class to store node and index class Pair&amp;lt;K, V&amp;gt; { private K key; private V value; public Pair(K key, V value) { this.key = key; this.value = value; } public K getKey() { return key; } public V getValue() { return value; } } ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a dry run the model did to confirm all the different test cases. Simply immaculate.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;```plaintext Let me test it against some examples. Test case 1: Tree: 0 / \ 1 2 arr = [0,1] Expected output: True (path root -&amp;gt; left child) Code execution: stack starts with [(root,0)] pop root, i=0. value matches arr[0] =0. i is not last index, so push right and left children with i+1=1. Now stack has (2,1), then (1,1). Next iteration: pop (1,1). node 1's value is 1 == arr[1]=1. Check if it's last index: yes. Check if leaf. Node 1 has no left or right? In this tree, node 1 is a leaf (assuming original tree as in test case). Yes: return True. Test case passes. Another test case: Tree: 0 / \ 1 2 arr = [0,2] Expected output: True. In code: stack has root with i=0. matches. push right (2,1), then left(1,1). pop (1,1): node.val is 1 != arr[1]=2? So skip. then pop (2,1). val is 2 == arr[1]. Check if last index: yes, and node 2 is a leaf. So return True. Test passes. Another test case: Tree: 0 / \ 1 2 / 3 arr = [0,1,3] Expected output: True. Code: stack starts with (root,0). val matches. i is not last index. Push left (1,1) and right (2,1). Next pop (2,1): value 2 != arr[1]=1? So skip. pop (1,1): val 1 == arr[1]. Not last index (len(arr)=3). push its children: left (3,2), no right. stack now has (3,2). Pop (3,2): val is 3 == arr[2]=3. Check if i==2 (last) and node is leaf. Node 3's left and right are None? So yes: return True. Test passes. Another test case: Tree: 0 / \ 1 2 / 3 arr = [0,1,4] In code: At (3,2), val is 3 !=4. so continue. No other possibilities. stack becomes empty: return False. Yes. Another test case where arr is longer than any path. Tree: 0 / \ 1 2 arr = [0,1,3] So the tree has no node beyond left child's left (which doesn't exist). So len(arr)=3, but all root-to-leaf paths have length 2. So expected output is False. In code: stack pops (root,0): val matches, push children with i=1. pop (2,1): not arr[1]=1? So no. pop(1,1): match. since len(arr)=3&amp;gt;1+1=2? Wait: node 1 has left child 3? Wait in previous example, but in this test case's tree: node 1 doesn't have any children? Because the tree is: root 0 with left 1 (no children) and right 2. So in code: At node 1, i=1. since len(arr)-1 =2, which is greater than 1. so node 1's val matches arr[1] (1), then push its children: none. Thus, stack after popping (1,1): nothing pushed. So stack becomes empty. No other elements. Return False. Which is correct. Another edge case: root is None and arr is [0]: return False. Yes. root is a single node with val 5 and arr [5]. Return True. In code: yes. So the code seems to handle all these cases. Thus, I think this should be the correct approach. ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wow.. just wow. I am really gonna use these think tags for my own leetcode prep.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Gap5867"&gt; /u/Educational_Gap5867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9k18t/i_am_simply_blown_away_by_this_32b_model_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T10:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia6re7</id>
    <title>I made a Free &amp; Open-Source FastAPI Template to build online services that uses LLMs!</title>
    <updated>2025-01-26T05:22:35+00:00</updated>
    <author>
      <name>/u/AleksCube</name>
      <uri>https://old.reddit.com/user/AleksCube</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"&gt; &lt;img alt="I made a Free &amp;amp; Open-Source FastAPI Template to build online services that uses LLMs!" src="https://external-preview.redd.it/ZXozaWh2dGl3OWZlMY4brPbXXnlynLbYgxRYsYbRz1arEGB1SqG_c2u4ImT_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee0d09f41a1db91ed052ee91e35a9037651d7579" title="I made a Free &amp;amp; Open-Source FastAPI Template to build online services that uses LLMs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AleksCube"&gt; /u/AleksCube &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iesj4wtiw9fe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T05:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia2vnu</id>
    <title>Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance</title>
    <updated>2025-01-26T01:43:29+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"&gt; &lt;img alt="Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance" src="https://external-preview.redd.it/1R6e2j35zcHWqfwZdGD3tdijh55XqSAoqHzUpCsZqFo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c0d3262bebfa9042aaf27888a03ccfe10eea737" title="Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Aider-AI/aider/pull/2998"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T01:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia3iwf</id>
    <title>Which one works better, llama 3.3 70b or deepseek r1 70b?</title>
    <updated>2025-01-26T02:17:57+00:00</updated>
    <author>
      <name>/u/SpecialistPear755</name>
      <uri>https://old.reddit.com/user/SpecialistPear755</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t see much comparison on this scale of parameters. Do you have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpecialistPear755"&gt; /u/SpecialistPear755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T02:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9txf3</id>
    <title>Deepseek is way better in Python code generation than ChatGPT (talking about the "free" versions of both)</title>
    <updated>2025-01-25T18:49:57+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't bought any subscriptions and im talking about the web based apps for both, and im just taking this opportunity to fanboy on deepseek because it produces super clean python code in one shot, whereas chat gpt generates a complex mess and i still had to specify some things again and again because it missed out on them in the initial prompt.&lt;br /&gt; I didn't generate a snippet out of scratch, i had an old function in python which i wanted to re-utilise for a similar use case, I wrote a detailed prompt to get what I need but ChatGPT still managed to screw up while deepseek nailed it in the first try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9x23l</id>
    <title>[Magnum/Rei] Mistral Nemo 12b</title>
    <updated>2025-01-25T21:06:33+00:00</updated>
    <author>
      <name>/u/lucyknada</name>
      <uri>https://old.reddit.com/user/lucyknada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi again!&lt;/p&gt; &lt;p&gt;We've got something exciting for you all - a small preview of what might become the first (or second?) stepping stone for Magnum v5.&lt;/p&gt; &lt;p&gt;One of our members (DeltaVector) has too run some experiments - on a more attainable range of 12b, this time with the help of Gryphe, DoctorShotgun and PocketDoc.&lt;/p&gt; &lt;p&gt;Our internal testing shows this experiment already beats v4 in almost every metric just like DoctorShotguns experiment did on L3.3 70b - and it also follows opus-style prefills very well!&lt;/p&gt; &lt;p&gt;This should serve as an amazing taste of whats to come once we work through the rest of the datasets and pipelines to fully start v5.&lt;/p&gt; &lt;p&gt;Weights and quants are here: &lt;a href="https://huggingface.co/collections/Delta-Vector/rei-12b-6795505005c4a94ebdfdeb39"&gt;https://huggingface.co/collections/Delta-Vector/rei-12b-6795505005c4a94ebdfdeb39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Have a great weekend! and thank you all for sticking with us for so long, we appreciate all of your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucyknada"&gt; /u/lucyknada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9t0x2</id>
    <title>How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI</title>
    <updated>2025-01-25T18:10:15+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt; &lt;img alt="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" src="https://external-preview.redd.it/GaYe6FpTRtNr23ADdM65dvNw3TVMjwFcEfKfrHC4ukE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa1a135c85bd082bf94671971fb8ea8e80f02eb2" title="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/deepseek-china-model-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia2rzn</id>
    <title>Flash Attention T5</title>
    <updated>2025-01-26T01:38:07+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"&gt; &lt;img alt="Flash Attention T5" src="https://external-preview.redd.it/Ep1yoPi5mHpASN_9oXAcIW-Bnp0muHVqmp_U98PZOrY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd014f4d0958f4a616b8cac67feeaf17a0abf78" title="Flash Attention T5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/CATIE-AQ/FAT5-report"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T01:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zdh3</id>
    <title>Best NSFW model for story telling?</title>
    <updated>2025-01-25T22:51:40+00:00</updated>
    <author>
      <name>/u/Might-Be-A-Ninja</name>
      <uri>https://old.reddit.com/user/Might-Be-A-Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if there are any models geared for it, but I want something that can write full stories, with me just giving it some direction&lt;/p&gt; &lt;p&gt;I am mainly into BDSM, if it matters&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Might-Be-A-Ninja"&gt; /u/Might-Be-A-Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T22:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9oqou</id>
    <title>Nvidia to wind down CUDA support for Maxwell and Pascal</title>
    <updated>2025-01-25T15:01:14+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Nvidia's release notes for CUDA 12.8 revealed that Maxwell, Pascal, and Volta GPUs will likely transition to the legacy driver branch. The document states that &amp;quot;architecture support for Maxwell, Pascal, and Volta is considered feature-complete and will be frozen in an upcoming release.&amp;quot; &lt;/p&gt; &lt;p&gt;I think most of us new this day was coming soon. I wouldn't fret too much about it though. This doesn't mean that the cards will stop working or any software built on CUDA will stop working anytime soon. Even if CUDA 12.8 is the last version to support Pascal, I think open source projects like Llama.cpp will continue supporting those cards for a few more years, given how widely used Pascal is in the community and the lack of any decently priced alternatives until now.&lt;/p&gt; &lt;p&gt;If anyone is considering buying a P40 for a new build, I don't think they should change their plans because of this announcement, especially if they find a good deal on the P40. &lt;/p&gt; &lt;p&gt;Personally, I have 10 P40s (just bought 5 last week at $180/card), 4 P100s, and 4 V100s and I'm not planning on retiring them anytime soon. They're great and work really well for my use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-starts-phasing-out-maxwell-pascal-and-volta-gpus-geforce-driver-support-status-unclear"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T15:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia4mx6</id>
    <title>Project Digits Memory Speed</title>
    <updated>2025-01-26T03:17:58+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently saw an accidentally leaked slide from Nvidia on Project Digits memory speed. It is 273 GB/s.&lt;/p&gt; &lt;p&gt;Also 128 GB is the base memory. Only storage will have “pay to upgrade” tiers.&lt;/p&gt; &lt;p&gt;Wanted to give credit to this user. Completely correct. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ"&gt;https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Hoping for a May launch I heard too.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T03:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia10ld</id>
    <title>Msty connecting to a Chinese server in Hong Kong</title>
    <updated>2025-01-26T00:09:44+00:00</updated>
    <author>
      <name>/u/urubuz</name>
      <uri>https://old.reddit.com/user/urubuz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt; &lt;img alt="Msty connecting to a Chinese server in Hong Kong" src="https://b.thumbs.redditmedia.com/tlmfTviR5XharLxlY5KMT6R5OwwGM__phYc7Jk5IKhY.jpg" title="Msty connecting to a Chinese server in Hong Kong" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://msty.app/privacy:"&gt;https://msty.app/privacy:&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; We do not gather any telemetry data except for app open ping. All data is stored locally on your device and is NEVER transmitted to our servers.&lt;/p&gt; &lt;p&gt;Here's what Little Snitch Mini is reporting when the app booted up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537"&gt;https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/urubuz"&gt; /u/urubuz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wnfs</id>
    <title>Why do openai and meta etc plan to spend so much on data centers? how do they make the money back?</title>
    <updated>2025-01-25T20:48:52+00:00</updated>
    <author>
      <name>/u/lblblllb</name>
      <uri>https://old.reddit.com/user/lblblllb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt already has over 180mm users, which is over half of US population. With exception of limitation on o1, the service uptime seems mostly fine so far? why spend up to 500bln to build data centers for exclusive use of openai that will depreciate very quickly(due to GPU depreciation)? Same for meta spending 60bln on AI. how do they plan to make the money back? seems like they really have to be able to use AI to replace most of the knowledge workers in order to make a return.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/"&gt;https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lblblllb"&gt; /u/lblblllb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia1d4t</id>
    <title>7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient</title>
    <updated>2025-01-26T00:26:33+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt; &lt;img alt="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" src="https://external-preview.redd.it/88h1mEnLx1t41jw5cSvvfzo0nRgYDTSe3ZMQUihAxm4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3ddfdce52f96a0c0d08d64102e978a19be46554" title="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hkust-nlp.notion.site/simplerl-reason"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia5mpb</id>
    <title>Compared DeepSeek-R1 to DeepSeek-R1-Zero: surprising results</title>
    <updated>2025-01-26T04:14:59+00:00</updated>
    <author>
      <name>/u/dubesor86</name>
      <uri>https://old.reddit.com/user/dubesor86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia5mpb/compared_deepseekr1_to_deepseekr1zero_surprising/"&gt; &lt;img alt="Compared DeepSeek-R1 to DeepSeek-R1-Zero: surprising results" src="https://preview.redd.it/o6fqrfqfk9fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525d889790251b2a9689302a0d045ea1b54a6050" title="Compared DeepSeek-R1 to DeepSeek-R1-Zero: surprising results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dubesor86"&gt; /u/dubesor86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o6fqrfqfk9fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia5mpb/compared_deepseekr1_to_deepseekr1zero_surprising/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia5mpb/compared_deepseekr1_to_deepseekr1zero_surprising/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T04:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wbya</id>
    <title>ByteDance announces Doubao-1.5-pro</title>
    <updated>2025-01-25T20:34:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt; &lt;img alt="ByteDance announces Doubao-1.5-pro" src="https://preview.redd.it/5pjykhaha7fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0df07e6b549319488a93d42063d7e338ff3b8b7" title="ByteDance announces Doubao-1.5-pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance announces Doubao-1.5-pro&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Includes a &amp;quot;Deep Thinking&amp;quot; mode, surpassing O1-preview and O1 models on the AIME benchmark.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms deepseek-v3, gpt4o, and llama3.1-405B on popular benchmarks. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built on a MoE architecture, with activated parameters far fewer than those in the above models. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Achieves a 7x MoE performance leverage—delivering dense model performance with just 1/7 of the activated parameters (e.g., 20B activated params = 140B dense performance). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Engineering-wise, features heterogeneous system design for prefill-decode and attn-fffn, maximizing throughput under low-latency requirements.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5pjykhaha7fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia40om</id>
    <title>Would give up a kidney for a local audio model that’s even half as good as Suno</title>
    <updated>2025-01-26T02:44:27+00:00</updated>
    <author>
      <name>/u/Effective_Garbage_34</name>
      <uri>https://old.reddit.com/user/Effective_Garbage_34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, I’ve tried pretty much every local audio model out there—MusicGen, AudioCraft, Coqui TTS, NSynth—whatever. And they all sound… bad. Like, really bad. Meanwhile, Suno is out here sounding like magic, and I’m just sitting here wondering: what the hell are they doing differently?&lt;/p&gt; &lt;p&gt;Is it their training data? Some proprietary wizardry? Did they make a deal with the devil? Whatever it is, local models are so far behind it’s almost depressing.&lt;/p&gt; &lt;p&gt;I’d love to get even a fraction of Suno’s quality in something I can run locally. Has anyone figured out a way forward? Is there hope for local models, or are we stuck dreaming from a distance?&lt;/p&gt; &lt;p&gt;Seriously, what’s the secret sauce? If anyone has insight, please share—I’m desperate over here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Garbage_34"&gt; /u/Effective_Garbage_34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T02:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9nqj9</id>
    <title>Full open source reproduction of R1 in progress ⏳</title>
    <updated>2025-01-25T14:11:35+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt; &lt;img alt="Full open source reproduction of R1 in progress ⏳" src="https://preview.redd.it/s5rmvdhtd5fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf96bf7e9979be87994f66f0537b9e70492b54b" title="Full open source reproduction of R1 in progress ⏳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5rmvdhtd5fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T14:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9y42v</id>
    <title>New OpenAI</title>
    <updated>2025-01-25T21:54:09+00:00</updated>
    <author>
      <name>/u/notomarsol</name>
      <uri>https://old.reddit.com/user/notomarsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt; &lt;img alt="New OpenAI" src="https://preview.redd.it/ppnejgtgo7fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e4cae2970050d080916629397ce588f1598ea49" title="New OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notomarsol"&gt; /u/notomarsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ppnejgtgo7fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:54:09+00:00</published>
  </entry>
</feed>
