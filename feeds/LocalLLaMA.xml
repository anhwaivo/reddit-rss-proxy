<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-04T20:34:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ihih1y</id>
    <title>Any good recommendation for an image model that isn't shite on 8GB VRAM?</title>
    <updated>2025-02-04T14:09:43+00:00</updated>
    <author>
      <name>/u/blueredscreen</name>
      <uri>https://old.reddit.com/user/blueredscreen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Getting issues with very plastic faces, terrible resolutions, but I'm wondering if the SOTA for non-millionaire PC owners has improved... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blueredscreen"&gt; /u/blueredscreen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihih1y/any_good_recommendation_for_an_image_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihih1y/any_good_recommendation_for_an_image_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihih1y/any_good_recommendation_for_an_image_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T14:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihl5k5</id>
    <title>We've made AI training assistant called Steev !</title>
    <updated>2025-02-04T16:08:01+00:00</updated>
    <author>
      <name>/u/Vivid-Entertainer752</name>
      <uri>https://old.reddit.com/user/Vivid-Entertainer752</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released Steev !&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ever feel drained from constantly monitoring your training curves?&lt;/strong&gt; Or frustrated when you realize‚Äîtoo late‚Äîyou made a mistake?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training AI models is resource-intensive&lt;/strong&gt;, so we're often glued to progress, hoping everything goes smoothly. But what if you didn‚Äôt have to be?&lt;/p&gt; &lt;p&gt;Introducing Steev‚Äîyour solution to eliminating the inefficiencies of AI model training.&lt;/p&gt; &lt;p&gt;Give it a try and let us know what you think! üëá&lt;/p&gt; &lt;p&gt;Steev&lt;br /&gt; : &lt;a href="https://www.steev.io/"&gt;https://www.steev.io/&lt;/a&gt;&lt;br /&gt; Tutorial: Fine-tuning Llama 8B distilled from DeepSeek-R1 using Unsloth&lt;br /&gt; : &lt;a href="https://tbd-labs-ai.github.io/steev-docs/tutorials/unsloth/"&gt;https://tbd-labs-ai.github.io/steev-docs/tutorials/unsloth/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Entertainer752"&gt; /u/Vivid-Entertainer752 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihl5k5/weve_made_ai_training_assistant_called_steev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihl5k5/weve_made_ai_training_assistant_called_steev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihl5k5/weve_made_ai_training_assistant_called_steev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqi3u</id>
    <title>RX 7900 XT budget build - worth it?</title>
    <updated>2025-02-04T19:45:05+00:00</updated>
    <author>
      <name>/u/djex81</name>
      <uri>https://old.reddit.com/user/djex81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking to put together an AI server that I can use for mainly running coding models. I am trying to keep the cost down so I will be using some spare parts I have on hand and also I don‚Äôt have 4k to drop on 2 x RTX 3090s (Canadian prices) right now.&lt;/p&gt; &lt;p&gt;I have an opportunity to buy &lt;strong&gt;2 x RX 7900 XTs&lt;/strong&gt; for around $1800. Would buying two RX 7900 XTs with the hardware below be worth it? My goal is for at least 15t/s with 30B models and if I can get that with larger models even better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i7-6850K&lt;br /&gt; &lt;strong&gt;Motherboard:&lt;/strong&gt; ASUS x99-Deluxe II&lt;br /&gt; &lt;strong&gt;RAM:&lt;/strong&gt; 32GB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; Thinking of a proxmox setup running LXC containers or I could do Ubuntu server. I won't be using Windows.&lt;br /&gt; &lt;strong&gt;Software:&lt;/strong&gt; Looking to use ollama with open-webui&lt;/p&gt; &lt;p&gt;I have read mixed reports on support for AMD GPUs and ROCm however many of the comments I read were over a year ago. It would be helpful if anyone running a RX 7900 XT or XTX would comment on their experience and performance they get. Also is multi AMD GPU supported?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djex81"&gt; /u/djex81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqi3u/rx_7900_xt_budget_build_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqi3u/rx_7900_xt_budget_build_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqi3u/rx_7900_xt_budget_build_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:45:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihdad2</id>
    <title>Best way to store LLM memory of the user</title>
    <updated>2025-02-04T08:39:09+00:00</updated>
    <author>
      <name>/u/jackiezhang95</name>
      <uri>https://old.reddit.com/user/jackiezhang95</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihdad2/best_way_to_store_llm_memory_of_the_user/"&gt; &lt;img alt="Best way to store LLM memory of the user" src="https://b.thumbs.redditmedia.com/-YELcpG0SXXqF0PXboEyXKj3c84ABmKxgKWuHC0B6kI.jpg" title="Best way to store LLM memory of the user" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working for a small startup as an AI engineer and want to seek advice from friends here. If you want LLM to remember all user conversations, this leads to excessive token consumption. I asked ChatGPT how to manage memory efficiently, and it suggested using structured notes‚Äîessentially summarizing key user details in natural language, like &amp;quot;User likes red&amp;quot; or &amp;quot;User is X years old.&amp;quot; I find this approach inconvenient. Ideally, I want the AI to recall specifics in conversations, like ‚ÄúJohn, how is your [task] progress that we talked about a few days ago?‚Äù. I‚Äôm hoping to design a system where AI organizes each conversation into a graph‚Äîkind of like a knowledge graph but more complex, with relationship networks and verb associations that AI understands but might not be human-readable. This way, we can highly compress textual information while allowing AI to retain user conversations efficiently. Has anyone tried this or has seen something similar or has better/alterantive methods, appreciate all the inputs :) THANK YOU!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0liywzhac3he1.jpg?width=355&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8e3d72138e6e2d275963ee36d7ebccee6ad55e27"&gt;This is a brainstorming of how to store the conversation as graph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackiezhang95"&gt; /u/jackiezhang95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihdad2/best_way_to_store_llm_memory_of_the_user/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihdad2/best_way_to_store_llm_memory_of_the_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihdad2/best_way_to_store_llm_memory_of_the_user/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T08:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihffra</id>
    <title>How do you currently access deepseek?</title>
    <updated>2025-02-04T11:19:31+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It just seems like the api + website is down all the time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihffra/how_do_you_currently_access_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihffra/how_do_you_currently_access_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihffra/how_do_you_currently_access_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T11:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpmg4</id>
    <title>Comparing different LLMs against different programming languages to see which are best for AI-driven coding</title>
    <updated>2025-02-04T19:09:11+00:00</updated>
    <author>
      <name>/u/terhechte</name>
      <uri>https://old.reddit.com/user/terhechte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"&gt; &lt;img alt="Comparing different LLMs against different programming languages to see which are best for AI-driven coding" src="https://external-preview.redd.it/f19UVEQKn2NUgogunr84spwvcNElFvuYeuIGFDIxA0k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e92484264a14897b17895cda1987bbd61718444e" title="Comparing different LLMs against different programming languages to see which are best for AI-driven coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terhechte"&gt; /u/terhechte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpmg4/comparing_different_llms_against_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihkl35</id>
    <title>Chain of Agents: Large language models collaborating on long-context tasks</title>
    <updated>2025-02-04T15:44:15+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihkl35/chain_of_agents_large_language_models/"&gt; &lt;img alt="Chain of Agents: Large language models collaborating on long-context tasks" src="https://external-preview.redd.it/FB87VRpzow9VURnv7lr9DIciMsAdtPslOqg25wmsjpM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecad125cfc220e2edee03bfb6fb8e92a060c1d04" title="Chain of Agents: Large language models collaborating on long-context tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihkl35/chain_of_agents_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihkl35/chain_of_agents_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T15:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih5b3q</id>
    <title>Finally Found a Use Case for a Local LLM That Couldn't Be Done Any Other Way</title>
    <updated>2025-02-04T00:53:46+00:00</updated>
    <author>
      <name>/u/Captain_Coffee_III</name>
      <uri>https://old.reddit.com/user/Captain_Coffee_III</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok, I now hate the title. But... &lt;/p&gt; &lt;p&gt;So this is a little bit of an edge case. I do old-school Industrial music as a hobby. Part of that is collecting sound samples from movies. That's part of the schtick from the '80s and '90s. Over the years, I've amassed a large amount of movies on DVD, which I've digitized. Thanks to the latest advancements that allow AI to strip out vocals, I can now capture just the spoken words from said movie.. which I then transcribed with OpenAI's Whisper. So I've been sitting here with a large database of sentences spoken in movies and not quite knowing what do do with it.&lt;/p&gt; &lt;p&gt;Enter one of the Llama 7B chat models. I thought that since the whole thing was based on the probability that tokens follow other tokens, I should be able to utilize that and find sentences that logically follow other sentences. When using the llama-cpp-python (cuda) module, you can tell it to track the probabilities of all the tokens so when I feed it two sentences, I can somewhat get an idea that they actually fit together. So phrases like &amp;quot;I ate the chicken.&amp;quot; and &amp;quot;That ain't my car.&amp;quot; have a lower probability matrix than if I ended it with &amp;quot;And it tasted good.&amp;quot; That was a no-go from the start though. I wanted to find sentences that logically fit together from random in 1500+ movies and each movie has about 1000 spoken lines. Nobody has time for that.&lt;/p&gt; &lt;p&gt;Round two. Prompt: &amp;quot;Given the theme '{Insert theme you want to classify by}', does the following phrase fit the theme? '{insert phrase here}', Answer yes or no. Answer:'&lt;/p&gt; &lt;p&gt;It's not super fast on my RTX2070, but I'm getting about one prompt every 0.8 seconds. But, it is totally digging through all the movies and finding individual lines that match up with a theme. The probability matrix actually works as well. I spent the morning throwing all kinds of crazy themes at it and it just nails them. I have over 15M lines of text to go through... and if I let it run continuously it would take 17 days to classify all lines to a single theme but having the Python script pick random movies then stopping when it finds the top 50 is totally good enough and can happen in hours.&lt;/p&gt; &lt;p&gt;There's no way I would pay for this volume of traffic on an paid API and even the 7B model can pull this off without a hitch. Precision isn't key here. And I can build a database of themes and have this churn away at night finding samples that match a theme. Absolutely loving this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain_Coffee_III"&gt; /u/Captain_Coffee_III &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih5b3q/finally_found_a_use_case_for_a_local_llm_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T00:53:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihmnbq</id>
    <title>Putting together all the LLM web search capable API available for developers</title>
    <updated>2025-02-04T17:08:45+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A developer list of currently available LLM APIs which are also capable of connecting to the internet: &lt;a href="https://github.com/vadimen/awesome_llm_api_with_web_search"&gt;https://github.com/vadimen/awesome_llm_api_with_web_search&lt;/a&gt;. Contains the available models and their prices. Because there are not many such providers, I thought it could be useful to have this list.&lt;/p&gt; &lt;p&gt;Everybody is welcome to contribute with a PR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihmnbq/putting_together_all_the_llm_web_search_capable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihmnbq/putting_together_all_the_llm_web_search_capable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihmnbq/putting_together_all_the_llm_web_search_capable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T17:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihgv43</id>
    <title>How will computer hardware change to cater to local LLMs?</title>
    <updated>2025-02-04T12:47:53+00:00</updated>
    <author>
      <name>/u/UnhingedSupernova</name>
      <uri>https://old.reddit.com/user/UnhingedSupernova</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think in the next 5 years, the demand for computer hardware is going to skyrocket, specifically hardware that's efficient enough to run something like DeepSeek 671B Parameter model with reasonable speed locally offline. (Or at least that's the goal of everyone here)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnhingedSupernova"&gt; /u/UnhingedSupernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihgv43/how_will_computer_hardware_change_to_cater_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihgv43/how_will_computer_hardware_change_to_cater_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihgv43/how_will_computer_hardware_change_to_cater_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T12:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihc6oi</id>
    <title>Someone made a solar system animation with mistral small 24b so I wanted to see what it would take for a smaller model to achieve the same or similar.</title>
    <updated>2025-02-04T07:15:09+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihc6oi/someone_made_a_solar_system_animation_with/"&gt; &lt;img alt="Someone made a solar system animation with mistral small 24b so I wanted to see what it would take for a smaller model to achieve the same or similar." src="https://external-preview.redd.it/MzZib3pxbHdvMmhlMWvr8G49Lc82c-F293AXTrEjb8OY97vj67Wghg0rS0ev.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=766de9e0609f73af78465493b52cc87c16695a4f" title="Someone made a solar system animation with mistral small 24b so I wanted to see what it would take for a smaller model to achieve the same or similar." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used the same original Prompt as him and needed an additional two prompts until it worked. Prompt 1: Create an interactive web page that animates the Sun and the planets in our Solar System. The animation should include the following features: Sun: A central, bright yellow circle representing the Sun. Planets: Eight planets (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune)&lt;/p&gt; &lt;p&gt;orbiting around the Sun with realistic relative sizes and distances. Orbits: Visible elliptical orbits for each planet to show their paths around the Sun. Animation: Smooth orbital motion for all planets, with varying speeds based on their actual orbital periods. Labels : Clickable labels for each planet that display additional information when hovered over or clicked (e.g., name, distance from the Sun, orbital period). Interactivity : Users should be able to pause and resume the animation using buttons.&lt;/p&gt; &lt;p&gt;Ensure the design is visually appealing with a dark background to enhance the visibility of the planets and their orbits. Use CSS for styling and JavaScript for the animation logic.&lt;/p&gt; &lt;p&gt;Prompt 2: Double check your code for errors&lt;/p&gt; &lt;p&gt;Prompt 3: &lt;/p&gt; &lt;p&gt;Problems in Your Code Planets are all stacked at (400px, 400px) Every planet is positioned at the same place (left: 400px; top: 400px;), so they overlap on the Sun. Use absolute positioning inside an orbit container and apply CSS animations for movement.&lt;/p&gt; &lt;p&gt;Only after pointing out its error did it finally get it right but for a 10 b model I think it did quite well even if it needed some poking in the right direction. I used Falcon3 10b in this and will try out later what the other small models will make with this prompt. Given them one chance to correct themself and pointing out errors to see if they will fix them. &lt;/p&gt; &lt;p&gt;As anything above 14b runs glacially slow on my machine what would you say are the best Coding llm 14b and under ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yrrxpppwo2he1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihc6oi/someone_made_a_solar_system_animation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihc6oi/someone_made_a_solar_system_animation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T07:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpvdm</id>
    <title>How does the use of 10 000 GPUs to train a model work?</title>
    <updated>2025-02-04T19:19:19+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the next state of the model depends on the previous state of the model by how gradient descent works, wouldn't you only need about 100 GPUs or so to train the largest models available? Like, you could only process the dataset serially, so how does hyperscaling of models work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpvdm/how_does_the_use_of_10_000_gpus_to_train_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihhlsl</id>
    <title>O3-mini-high LiveBench coding score seems fishy"</title>
    <updated>2025-02-04T13:26:58+00:00</updated>
    <author>
      <name>/u/Mother_Soraka</name>
      <uri>https://old.reddit.com/user/Mother_Soraka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihhlsl/o3minihigh_livebench_coding_score_seems_fishy/"&gt; &lt;img alt="O3-mini-high LiveBench coding score seems fishy&amp;quot;" src="https://b.thumbs.redditmedia.com/q45GHEO795HGXeRO03JiVh74dLpUXwWxfmKqhb8EstM.jpg" title="O3-mini-high LiveBench coding score seems fishy&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uagl0fz0f4he1.png?width=1339&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=24c8e255a96a4e140feb0476a51ffd4f21608f7f"&gt;???&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We observe diminishing returns across the board going from &amp;quot;O3-mini-Medium&amp;quot; to &amp;quot;O3-mini-High&amp;quot; compared to the gains from &amp;quot;Low&amp;quot; to &amp;quot;Medium&amp;quot;.&lt;/p&gt; &lt;p&gt;EXCEPT for the coding category, where the trend is completely opposite.&lt;/p&gt; &lt;p&gt;Even LiveCodeBench and Aider, which are purely coding benchmarks, show the same diminishing returns pattern.&lt;/p&gt; &lt;p&gt;So, is it possible that LiveBench made a mistake?&lt;/p&gt; &lt;p&gt;How do we explain this exceptional jump that goes against every other benchmark?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother_Soraka"&gt; /u/Mother_Soraka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihhlsl/o3minihigh_livebench_coding_score_seems_fishy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihhlsl/o3minihigh_livebench_coding_score_seems_fishy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihhlsl/o3minihigh_livebench_coding_score_seems_fishy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T13:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9h11</id>
    <title>Ok, you LLaMA-fobics, Claude does have a moat, and impressive one</title>
    <updated>2025-02-04T04:24:57+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you know me, you might know I eat local LLMs for breakfast, ever since the first Llama with its &amp;quot;I have a borked tokenizer, but I love you&amp;quot; vibes came about. So this isn't some uneducated guess.&lt;/p&gt; &lt;p&gt;A few days ago, I was doing some C++ coding and tried Claude, which was working shockingly well, until it wanted MoooOOOoooney. So I gave in, mid-code, just to see how far this would go.&lt;/p&gt; &lt;p&gt;Darn. Triple darn. Quadruple darn.&lt;/p&gt; &lt;p&gt;Here‚Äôs the skinny: No other model understands code with the shocking capabilities of Sonet 3.5. You can fight me on this, and I'll fight back.&lt;/p&gt; &lt;p&gt;This thing is insane. And I‚Äôm not just making some simple &amp;quot;snake game&amp;quot; stuff. I have 25 years of C++ under my belt, so when I need something, I need something I &lt;em&gt;actually&lt;/em&gt; struggle with.&lt;/p&gt; &lt;p&gt;There were so many instances where I felt this was Coding AI (and I‚Äôm &lt;em&gt;very&lt;/em&gt; cautious about calling token predictors AI), but it‚Äôs just &lt;em&gt;insane.&lt;/em&gt; In three days, I made a couple of classes that would have taken me months, and this thing chews through 10K-line classes like bubble gum.&lt;/p&gt; &lt;p&gt;Of course, I made it cry a few times when things didn‚Äôt work‚Ä¶ and didn‚Äôt work‚Ä¶ and &lt;em&gt;didn‚Äôt work.&lt;/em&gt; Then Claude wrote an entirely new set of code just to test the old code, and at the end we sorted it out.&lt;/p&gt; &lt;p&gt;A lot of my code was for visual components, so I‚Äôd describe what I saw on the screen. It was like programming over the phone, yet it still got things right!&lt;/p&gt; &lt;p&gt;Told it, &amp;quot;Add multithreading&amp;quot; boom. Done. Unique mutexes. Clean as a whistle.&lt;/p&gt; &lt;p&gt;Told it: &amp;quot;Add multiple undo and redo to this class: The simplest 5 minutes in my programming carrier - and I've been adding and struggling with undo/redo in my stuff many times. &lt;/p&gt; &lt;p&gt;The code it writes is &lt;em&gt;incredibly&lt;/em&gt; well-structured. I feel like a messy duck playing in the mud by comparison.&lt;/p&gt; &lt;p&gt;I realized a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It gives me the best solution when I &lt;em&gt;don‚Äôt&lt;/em&gt; over-explain (codexplain) how I &lt;em&gt;think&lt;/em&gt; the structure or flow should be. Instead, if I just let it do its thing and pretend I‚Äôm stupid, it works better.&lt;/li&gt; &lt;li&gt;Many times, it automatically adds things I &lt;em&gt;didn‚Äôt&lt;/em&gt; ask for, but would have ultimately needed, so it‚Äôs not just predicting tokens, it‚Äôs predicting my &lt;em&gt;next&lt;/em&gt; request.&lt;/li&gt; &lt;li&gt;More than once, it chose a future-proof, open-ended solution &lt;em&gt;as if&lt;/em&gt; it expected we‚Äôd be building on it further and I was pretty surprised later when I wanted to add something how ready the code was&lt;/li&gt; &lt;li&gt;It comprehends alien code like nothing else I‚Äôve seen. Just throw in my mess.&lt;/li&gt; &lt;li&gt;When I was wrong and it was right, it didn't took my wrong stance, but explained to me where I might got my idea wrong, even pointing on a part of the code I probably overlooked - which was the EXACT reason why I was wrong. When model can keep it's cool without trying to please me all the time, it is something!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My previous best model for coding was Google Gemini 2, but in comparison, it feels confused for serious code, creating complex confused structure that didn't work anyway. .&lt;/p&gt; &lt;p&gt;I got my money‚Äôs worth in the first &lt;em&gt;ten minutes.&lt;/em&gt; The next 30.98 days? Just a bonus.&lt;/p&gt; &lt;p&gt;I‚Äôm saying this because while I &lt;em&gt;love&lt;/em&gt; Llama and I‚Äôm deep into the local LLM phase, this actually feels like magic. &lt;em&gt;So someone does thing s right, IMHO.&lt;/em&gt;&lt;br /&gt; Also, it is still next token predictor, that's even more impressive than if it actually reads the code.....&lt;/p&gt; &lt;p&gt;My biggest nightmare now: What if they take it away.... or &amp;quot;improve&amp;quot; it....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih9h11/ok_you_llamafobics_claude_does_have_a_moat_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T04:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpzn2</id>
    <title>Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers</title>
    <updated>2025-02-04T19:24:03+00:00</updated>
    <author>
      <name>/u/thedudear</name>
      <uri>https://old.reddit.com/user/thedudear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt; &lt;img alt="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" src="https://b.thumbs.redditmedia.com/MEP1WTwX54qNwJ5gIxf_f4NzTdNmrqOfmjo_k4cVUrg.jpg" title="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I decided that three RTX 3090s janked together with brackets and risers just wasn‚Äôt enough; I wanted a cleaner setup and a fourth 3090. To make that happen, I needed a new platform.&lt;/p&gt; &lt;p&gt;My requirements were: at least four double-spaced PCIe x16 slots, ample high-speed storage interfaces, and ideally, high memory bandwidth to enable some level of CPU offloading without tanking inference speed. Intel‚Äôs new Xeon lineup didn‚Äôt appeal to me, the P/E core setup seems more geared towards datacenters, and the pricing was brutal. Initially, I considered Epyc Genoa, but with the launch of Turin and its Zen 5 cores plus higher DDR5 speeds, I decided to go straight for it.&lt;/p&gt; &lt;p&gt;Due to the size of the SP5 socket and its 12 memory channels, boards with full 12-channel support sacrifice PCIe slots. The only board that meets my PCIe requirements, the ASRock GENOAD8X-2T/TCM, has just 8 DIMM slots, meaning we have to say goodbye to four whole memory channels.&lt;/p&gt; &lt;p&gt;Getting it up and running was an adventure. At the time, ASRock hadn‚Äôt released any Turin-compatible BIOS ROMs, despite claiming that an update to 10.03 was required (which wasn‚Äôt even available for download). The beta ROM they supplied refused to flash, failing with no discernible reason. Eventually, I had to resort to a ROM programmer (CH341a) and got it running on version 10.05.&lt;/p&gt; &lt;p&gt;If anyone has questions about the board, BIOS, or setup, feel free to ask, I‚Äôve gotten way more familiar with this board than I ever intended to.&lt;/p&gt; &lt;p&gt;CPU: Epyc Turin 9355P - 32 Cores (8 CCD), 256 MB cache, 3.55 GHz Boosting 4.4 GHz - $3000 USD from cafe.electronics on Ebay (now ~$3300 USD). &lt;/p&gt; &lt;p&gt;RAM: 256 GB Corsair WS (CMA256GX5M8B5600C40) @ 5600 MHz - $1499 CAD (now ~$2400 - WTF!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.asrockrack.com/general/productdetail.asp?Model=GENOAD8X-2T/BCM#Specifications"&gt;Asrock GENOAD8X-2T/TCM Motherboard&lt;/a&gt; - ~$1500 CAD but going up in price&lt;/p&gt; &lt;p&gt;First off, a couple of benchmarks: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fag5favty5he1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a6b92917f908dedbe73201fc6fc48e820aa3a5"&gt;Passmark Memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p8e60vy946he1.png?width=879&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b08b8cc914a890e567b0e7aeb5f9e42251e855b9"&gt;Passmark CPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/slq3s3ub46he1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2f6711ae24b230edef6eeea872c229a293518be"&gt;CPU-Z Info Page - The chip seems to always be boosting to 4.4 GHz, which I don't mind. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekz7wf2d46he1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5112a56f91feb7ae1ea8bc946b5603e52a3ecb59"&gt;CPU-Z Bench - My i9 9820x would score ~7k @ 4.6 GHz. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;And finally some LMStudio (0 layers offloaded) tests: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/on0n624n66he1.png?width=340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d96479be841451a073caff569adb52d2e9387a00"&gt;Prompt: \&amp;quot;Write a 1000 word story about france's capital\&amp;quot; Llama-3.3-70B-Q8, 24 Threads. Model used 72 GB in RAM. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/je5ljie976he1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=809d046e8b19f1cdd903e09135bba50b734fae0f"&gt;Deepseek-R1-Distill-Llama-8B (Q8), 24 threads, 8.55 GB in memory. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm happy to run additional tests and benchmarks‚Äîjust wanted to put this out there so people have the info and can weigh in on what they'd like to see. CPU inference is very usable for smaller models (&amp;lt;20B), while larger ones are still best left to GPUs/cloud (not that we didn‚Äôt already know this).&lt;/p&gt; &lt;p&gt;That said, we‚Äôre on a promising trajectory. With a 12-DIMM board (e.g., Supermicro H13-SSL) or a dual-socket setup (pending improvements in multi-socket inference), we could, within a year or two, see CPU inference becoming cost-competitive with GPUs on a per-GB-of-memory basis. Genoa chips have dropped significantly in price over the past six months‚Äî9654 (96-core) now sells for $2,500‚Äì$3,000‚Äîmaking this even more feasible.&lt;/p&gt; &lt;p&gt;I'm optimistic about continued development in CPU inference frameworks, as they could help alleviate the current bottleneck: VRAM and Nvidia‚Äôs AI hardware monopoly. My main issue is that for pure inference, GPU compute power is vastly underutilized‚Äîmemory capacity and bandwidth are the real constraints. Yet consumers are forced to pay thousands for increasingly powerful GPUs when, for inference alone, that power is often unnecessary. Here‚Äôs hoping CPU inference keeps progressing!&lt;/p&gt; &lt;p&gt;Anyways, let me know your thoughts, and i'll do what I can to provide additional info. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedudear"&gt; /u/thedudear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihlx8q</id>
    <title>New "Kiwi" model on lmsys arena</title>
    <updated>2025-02-04T16:39:56+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like Grok-3 and Grok-3-mini to me...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihlx8q/new_kiwi_model_on_lmsys_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqwnd</id>
    <title>OpenAI deep research but it's open source</title>
    <updated>2025-02-04T20:01:31+00:00</updated>
    <author>
      <name>/u/Thomjazz</name>
      <uri>https://old.reddit.com/user/Thomjazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt; &lt;img alt="OpenAI deep research but it's open source" src="https://external-preview.redd.it/VhiwZJj7J5TUIfA6ujpiUeYD8CI4AKINeo7sLJZlD5Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e05cfb506bcac565f349480a4b0d9ba18f4768b1" title="OpenAI deep research but it's open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46"&gt;https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thomjazz"&gt; /u/Thomjazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih3nc6</id>
    <title>US Bill proposed to jail people who download Deepseek</title>
    <updated>2025-02-03T23:37:51+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt; &lt;img alt="US Bill proposed to jail people who download Deepseek" src="https://external-preview.redd.it/aUM4Zo5M60iArpLso3HHGaMmvhrgIEshjneVeh2Hvq4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4794828de090d8686e777cd2679cfb73fcb8c4f" title="US Bill proposed to jail people who download Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/senator-hawley-proposes-jail-time-for-people-who-download-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ih3nc6/us_bill_proposed_to_jail_people_who_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T23:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihm8pl</id>
    <title>Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!</title>
    <updated>2025-02-04T16:52:46+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt; &lt;img alt="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" src="https://external-preview.redd.it/Isnr897tQLSa3f7knpDj7eFO6fjkOWORPvRfD442vlo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24f28b0bdfd72abaef01b7d591bc2878895b848b" title="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Anubis-Pro-105B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf0gb</id>
    <title>DeepSeek-R1's correct answers are generally shorter</title>
    <updated>2025-02-04T10:49:48+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt; &lt;img alt="DeepSeek-R1's correct answers are generally shorter" src="https://preview.redd.it/duiwqfpzq3he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29a0abef7ab5410cdc5f56f319bd47c9d15c366b" title="DeepSeek-R1's correct answers are generally shorter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/duiwqfpzq3he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T10:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpdjh</id>
    <title>What to expect from Mistral's upcoming reasoning models?</title>
    <updated>2025-02-04T18:59:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt; &lt;img alt="What to expect from Mistral's upcoming reasoning models?" src="https://preview.redd.it/sa87uqtg66he1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4e05d3454d29e6d5755deb360afc707a7f84aa" title="What to expect from Mistral's upcoming reasoning models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sa87uqtg66he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T18:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihd0rr</id>
    <title>Deepseek researcher says it only took 2-3 weeks to train R1&amp;R1-Zero</title>
    <updated>2025-02-04T08:18:16+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt; &lt;img alt="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" src="https://b.thumbs.redditmedia.com/btJal_QKjmZuASeULU0hjpBHdbEWu1y_eBgzmjTEd1U.jpg" title="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ihd0rr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T08:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihph9f</id>
    <title>In case you thought your feedback was not being heard</title>
    <updated>2025-02-04T19:03:04+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt; &lt;img alt="In case you thought your feedback was not being heard" src="https://preview.redd.it/nvf2f1j876he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8871a5964dc8f2db918ba181e1157fc626b71" title="In case you thought your feedback was not being heard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nvf2f1j876he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihh15n</id>
    <title>Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination</title>
    <updated>2025-02-04T12:57:18+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt; &lt;img alt="Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination" src="https://b.thumbs.redditmedia.com/EnNfi2ijJYZ4t9ufiyUC2FCEhoi78PEnlcIcOHPzAtE.jpg" title="Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0"&gt;https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/"&gt;https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T12:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihjdh2</id>
    <title>China's OmniHuman-1 üåãüîÜ</title>
    <updated>2025-02-04T14:51:12+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihjdh2/chinas_omnihuman1/"&gt; &lt;img alt="China's OmniHuman-1 üåãüîÜ" src="https://external-preview.redd.it/Z3E2OWViMnZ4NGhlMTrmrmOiOifzLgIvJmxpKkr6cU0COigmuxJkoC_oGSXh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2436810af224c6342db453f80de7ac791a54865a" title="China's OmniHuman-1 üåãüîÜ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/44wrxa2vx4he1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihjdh2/chinas_omnihuman1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihjdh2/chinas_omnihuman1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T14:51:12+00:00</published>
  </entry>
</feed>
