<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-10T22:24:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lvvkh2</id>
    <title>Hunyuan-A13B is here for real!</title>
    <updated>2025-07-09T21:55:51+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:&lt;/p&gt; &lt;p&gt;It is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.&lt;/p&gt; &lt;p&gt;The context is HUGE. 256k. I don't expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.&lt;/p&gt; &lt;p&gt;It made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.&lt;/p&gt; &lt;p&gt;It did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.&lt;/p&gt; &lt;p&gt;It appears to wrap the final answer in &amp;lt;answer&amp;gt;the answer here&amp;lt;/answer&amp;gt; just like it does for &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.&lt;/p&gt; &lt;p&gt;The total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?&lt;/p&gt; &lt;p&gt;This is a 80b model that is very fast. Feels like the future.&lt;/p&gt; &lt;p&gt;Edit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don't have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T21:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvzonf</id>
    <title>https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms</title>
    <updated>2025-07-10T01:02:00+00:00</updated>
    <author>
      <name>/u/chitown160</name>
      <uri>https://old.reddit.com/user/chitown160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt; &lt;img alt="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" src="https://preview.redd.it/vq8hwq904ybf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d05add52383cb1c64996c7d198a25c8644d9f33f" title="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pheromone trails ↔ value functions / reward shaping&lt;/strong&gt; Both steer future exploration toward paths that historically looked good.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stochastic exploration&lt;/strong&gt; in ants (random walks with pheromone bias) ↔ &lt;strong&gt;ε-greedy / entropy-regularised exploration&lt;/strong&gt; in RL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Updating pheromones over time&lt;/strong&gt; ↔ &lt;strong&gt;policy/value updates&lt;/strong&gt; in RL or &lt;strong&gt;gradient steps&lt;/strong&gt; in supervised fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demonstration pheromones&lt;/strong&gt; (ants following an experienced scout’s trail) ↔ &lt;strong&gt;Learning from Demonstration&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitown160"&gt; /u/chitown160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vq8hwq904ybf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T01:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwfn7n</id>
    <title>DeliteAI: Open platform for building and running agents on Mobile</title>
    <updated>2025-07-10T15:28:26+00:00</updated>
    <author>
      <name>/u/Economy-Mud-6626</name>
      <uri>https://old.reddit.com/user/Economy-Mud-6626</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"&gt; &lt;img alt="DeliteAI: Open platform for building and running agents on Mobile" src="https://external-preview.redd.it/r7I348D0RBrj0AfFQ0_Ap0jX4RiNf7KMjifLoWkCwSM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a7922aa047ec08e04553915bf0c4265bcd75e35" title="DeliteAI: Open platform for building and running agents on Mobile" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have built an extensible open source platform that enables developers to build, run and integrate AI agents into their applications and deliver AI native experiences all running locally on phones.&lt;/p&gt; &lt;p&gt;The SDK is lightweight built upon Executorch/ONNX and provides a higher level abstraction for developers to integrate in Kotlin or Swift. The AI workflow is orchestrated via Python which is natively supported as part of the on-device SDK. We currently support Llama 3.2 1B, Qwen 3 0.6B (tool-calling), Gemini Nano and soon Gemma 3n.&lt;/p&gt; &lt;p&gt;We have also created an Agent marketplace which provides plug and play agents and would love to get contributions from this community. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NimbleEdge/deliteAI/tree/main/nimblenet_py/simulation_assets"&gt;Here&lt;/a&gt; are some example Python scripts for both traditional ML and AI workloads - note that the Kotlin/Swift layer can invoke these python functions and vice-versa which enables tool calling for both dynamic context and actions in the app.&lt;/p&gt; &lt;p&gt;You can also check out our open-source on-device &lt;a href="https://github.com/NimbleEdge/assistant"&gt;AI assistant&lt;/a&gt; built upon the “DeliteAI” platform. &lt;/p&gt; &lt;p&gt;We love to hear from you on our APIs and if you would like to contribute please join our Discord community (link in the comment below). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Mud-6626"&gt; /u/Economy-Mud-6626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/NimbleEdge/deliteAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwfn7n/deliteai_open_platform_for_building_and_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T15:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwm3w0</id>
    <title>MAXSUN preparing all-Intel Mini Station: up to Core Ultra 9 285HX and two Arc Pro B60 GPU - VideoCardz.com</title>
    <updated>2025-07-10T19:41:48+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwm3w0/maxsun_preparing_allintel_mini_station_up_to_core/"&gt; &lt;img alt="MAXSUN preparing all-Intel Mini Station: up to Core Ultra 9 285HX and two Arc Pro B60 GPU - VideoCardz.com" src="https://external-preview.redd.it/6eIjvZ22TR6xCz2XqZ3ElNIdlnNTCS7SRm_CKMsEaSU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabcea64775657596068577ad159149b4f0dd034" title="MAXSUN preparing all-Intel Mini Station: up to Core Ultra 9 285HX and two Arc Pro B60 GPU - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-preparing-all-intel-mini-station-up-to-core-ultra-9-285hx-and-two-arc-pro-b60-gpu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwm3w0/maxsun_preparing_allintel_mini_station_up_to_core/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwm3w0/maxsun_preparing_allintel_mini_station_up_to_core/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwlw1j</id>
    <title>VS Code June 2025 (version 1.102)</title>
    <updated>2025-07-10T19:33:09+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"&gt; &lt;img alt="VS Code June 2025 (version 1.102)" src="https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fea22c6187bb94fe796b6c79160a51ef50be7f7d" title="VS Code June 2025 (version 1.102)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Explore and contribute to the open sourced GitHub Copilot Chat extension (&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;Read our blog post&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Generate custom instructions that reflect your project's conventions (&lt;a href="https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Use custom modes to tailor chat for tasks like planning or research (&lt;a href="https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Automatically approve selected terminal commands (&lt;a href="https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Edit and resubmit previous chat requests (&lt;a href="https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;MCP support is now generally available in VS Code (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Easily install and manage MCP servers with the MCP view and gallery (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;MCP servers as first-class resources in profiles and Settings Sync (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Editor experience&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Delegate tasks to Copilot coding agent and let it handle them in the background (&lt;a href="https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Scroll the editor on middle click (&lt;a href="https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;VS Code pm here in case there are any questions I am happy to answer.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_102"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwhy37</id>
    <title>MCP server that is a memory for MCP clients (AI assistants) with your custom data types + full UI + team sharing</title>
    <updated>2025-07-10T16:59:48+00:00</updated>
    <author>
      <name>/u/Jazzlike_Water4911</name>
      <uri>https://old.reddit.com/user/Jazzlike_Water4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a collaborative database that is an MCP server. You can use it to remember any type of data you define: diet and fitness history, work-related data, to-do lists, bookmarked links, journal entries, bugs in software projects, favorite books/movies. &lt;a href="https://youtu.be/zAxepwoONq0"&gt;See it in action.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s called Dry (“don’t repeat yourself”). Dry lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add long-term memories in Claude and other MCP clients that persist across chats.&lt;/li&gt; &lt;li&gt;Specify your own custom data type without any coding.&lt;/li&gt; &lt;li&gt;Automatically generate a full graphical user interface (tables, charts, maps, lists, etc.). &lt;/li&gt; &lt;li&gt;Share with a team or keep it private. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We think that in the &lt;a href="https://dry.ai/vision"&gt;long term&lt;/a&gt;, memories like this will give AI assistants the scaffolding they need to replace most SaaS tools and apps.&lt;/p&gt; &lt;p&gt;Here’s our alpha you can try: &lt;a href="https://dry.ai/joinDryBuilder"&gt; &lt;/a&gt;&lt;a href="https://dry.ai/getClaudeMemory"&gt;https://dry.ai/getClaudeMemory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from anyone here. Are there features you'd want? What would you use this for? Happy to answer any questions! &lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jazzlike_Water4911"&gt; /u/Jazzlike_Water4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwhy37/mcp_server_that_is_a_memory_for_mcp_clients_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T16:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwcixn</id>
    <title>Kimina Prover - Test-time RL to reach 92.2% on miniF2F</title>
    <updated>2025-07-10T13:18:56+00:00</updated>
    <author>
      <name>/u/frunkp</name>
      <uri>https://old.reddit.com/user/frunkp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🧠📝 Research &lt;a href="https://huggingface.co/blog/AI-MO/kimina-prover"&gt;Blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🚀 Demo: &lt;a href="https://demo.projectnumina.ai/"&gt;https://demo.projectnumina.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;🤗&lt;/a&gt; Models (72B, 8B or 1.7B) - &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;🤗&lt;/a&gt; &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-686b72614760ed23038056c5"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;72B with Test-time RL pipeline gets 92.2% on miniF2F.&lt;/p&gt; &lt;p&gt;Pass@32 for each size:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;72B → 84.0% (86.4% with error-fixing)&lt;/li&gt; &lt;li&gt;8B → 78.3%&lt;/li&gt; &lt;li&gt;1.7B → 73.4%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;8B/1.7B are Qwen 3 with 72B distilled into them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frunkp"&gt; /u/frunkp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwcixn/kimina_prover_testtime_rl_to_reach_922_on_minif2f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T13:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvwya4</id>
    <title>Possible size of new the open model from openai</title>
    <updated>2025-07-09T22:54:54+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt; &lt;img alt="Possible size of new the open model from openai" src="https://preview.redd.it/622w5dyvhxbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f278161d7e564140ede28f9eff15dc776e5ab6df" title="Possible size of new the open model from openai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/622w5dyvhxbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T22:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw3729</id>
    <title>Phi-4-mini-flash-reasoning</title>
    <updated>2025-07-10T04:00:32+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt; &lt;img alt="Phi-4-mini-flash-reasoning" src="https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a162d9d243ab8293c0214f3e9bf055ec2af6d514" title="Phi-4-mini-flash-reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T04:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwniq0</id>
    <title>Workflows aren’t a weakness in AI agents, they’re why they work</title>
    <updated>2025-07-10T20:37:01+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some people think AI agents are hype and glorified workflows.&lt;/p&gt; &lt;p&gt;But agents that actually work don’t try to be JARVIS, not yet. The ones that succeed stick to structured workflows. And that’s not a bad thing. When I was in school, we studied Little Computer 3 to understand how computer architecture starts with state machines. I attached that diagram, and that's just the simplest computer architecture just for education purpose.&lt;/p&gt; &lt;p&gt;A workflow is just a finite state machine (FSM) with memory and tool use. LLMs are surprisingly good at that. These agents complete real tasks that used to take human time and effort.&lt;/p&gt; &lt;p&gt;Retell AI is a great example. It handles real phone calls for things like loans and pharmacy refills. It knows what step it’s on, when to speak, when to listen, and when to escalate. That kind of structure makes it reliable. Simplify is doing the same for job applications. It finds postings, autofills forms, tracks everything, and updates the user. These are clear, scoped workflows with success criteria, and that’s where LLMs perform really well.&lt;/p&gt; &lt;p&gt;Plugging LLM in workflows isn’t enough. The teams behind these tools constantly monitor what’s happening. They trace every call, evaluate outputs, catch failure patterns, and improve prompts. I believe they have a very complicated workflow, and tools like Keywords AI make that kind of observability easy. Without it, even a well-built agent will drift.&lt;/p&gt; &lt;p&gt;Not every agent is magic. But the ones that work? They’re already saving time, money, and headcount. That's what we need in the current state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwniq0/workflows_arent_a_weakness_in_ai_agents_theyre/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr3ym</id>
    <title>OpenAI's open source LLM is a reasoning model, coming Next Thursday!</title>
    <updated>2025-07-09T18:58:30+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt; &lt;img alt="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" src="https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7" title="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q01afp6lbwbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw4eej</id>
    <title>Grok 4 Benchmarks</title>
    <updated>2025-07-10T05:08:29+00:00</updated>
    <author>
      <name>/u/DigitusDesigner</name>
      <uri>https://old.reddit.com/user/DigitusDesigner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"&gt; &lt;img alt="Grok 4 Benchmarks" src="https://b.thumbs.redditmedia.com/MLSe5RpW1tkFFRdIT2JZeSSIlKUblh8PvP8Gk8nPH1E.jpg" title="Grok 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;xAI has just announced its smartest AI models to date: Grok 4 and Grok 4 Heavy. Both are subscription-based, with Grok 4 Heavy priced at approximately $300 per month. Excited to see what these new models can do! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigitusDesigner"&gt; /u/DigitusDesigner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lw4eej"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw4eej/grok_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw9ch2</id>
    <title>Added Grok-4 to the UGI-Leaderboard</title>
    <updated>2025-07-10T10:32:11+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"&gt; &lt;img alt="Added Grok-4 to the UGI-Leaderboard" src="https://preview.redd.it/6g4lpxpay0cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=696d0258f779029c9cde582e77066bdfaf475731" title="Added Grok-4 to the UGI-Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It has a lower willingness (W/10) than Grok-3, so it'll refuse more, but it makes up for that because of its massive intelligence (NatInt) increase.&lt;/p&gt; &lt;p&gt;Looking through its political stats, it is less progressive with social issues than Grok-3, but it is overall more left leaning because of things like it being less religious, less bioconservative, and less nationalistic.&lt;/p&gt; &lt;p&gt;When comparing other proprietary models, Grok 1, 2, and 4 stick out the most for being the least socially progressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g4lpxpay0cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw9ch2/added_grok4_to_the_ugileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T10:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwk84b</id>
    <title>Why do base models give gibberish and need further 'fine tuning'</title>
    <updated>2025-07-10T18:27:26+00:00</updated>
    <author>
      <name>/u/QFGTrialByFire</name>
      <uri>https://old.reddit.com/user/QFGTrialByFire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I've looked around in sites etc it just says the further instruction gets the model to align to respond but doesn't say why. How come a few samples (say just 1000 alpaca samples) of 'fine tuning' next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn't it already do it?&lt;/p&gt; &lt;p&gt;Just for context i'm using &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;https://huggingface.co/meta-llama/Llama-3.1-8B&lt;/a&gt; with lora to train on the alpaca data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QFGTrialByFire"&gt; /u/QFGTrialByFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T18:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwmpqf</id>
    <title>New Devstral 2707 with mistral.rs - MCP client, automatic tool calling!</title>
    <updated>2025-07-10T20:05:57+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://Mistral.rs"&gt;Mistral.rs&lt;/a&gt; has support for Mistral AI's newest model (no affiliation)! &lt;/p&gt; &lt;p&gt;Grab optimized UQFF files here: &lt;a href="https://huggingface.co/EricB/Devstral-Small-2507-UQFF"&gt;https://huggingface.co/EricB/Devstral-Small-2507-UQFF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More information: &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my testing, this model is really great at tool calling, and works very well with some of our newest features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agentic/automatic tool calling&lt;/strong&gt;: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI web search support&lt;/strong&gt;: &lt;a href="http://mistral.rs"&gt;mistral.rs&lt;/a&gt; allows models to have access to automatic web search, 100% compatible with the OpenAI API.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP client&lt;/strong&gt;: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These features make &lt;a href="http://mistral.rs"&gt;mistral.rs&lt;/a&gt; a really powerful tool for leveraging the strong capabilities of Devstral!&lt;/p&gt; &lt;p&gt;What do you think? Excited to see what you build with this 🚀!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:05:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw71av</id>
    <title>GLM-4 MoE incoming</title>
    <updated>2025-07-10T07:58:18+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is a new pull request to support GLM-4 MoE on VLLM.&lt;/p&gt; &lt;p&gt;Hopefully we will have a new powerful model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20736"&gt;https://github.com/vllm-project/vllm/pull/20736&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw71av/glm4_moe_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T07:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwif50</id>
    <title>Using Siri to talk to a local LLM</title>
    <updated>2025-07-10T17:17:57+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"&gt; &lt;img alt="Using Siri to talk to a local LLM" src="https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb0506fda08c175bd9066c66126214cc5eb38cde" title="Using Siri to talk to a local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently added Shortcuts support to my iOS app &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt; and worked to integrate it with Siri.&lt;/p&gt; &lt;p&gt;It's using Apple MLX to run the models.&lt;/p&gt; &lt;p&gt;Here's a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. &lt;/p&gt; &lt;p&gt;Everything running on-device.&lt;/p&gt; &lt;p&gt;Did my best to have a seamless integration. It doesn’t require any setup other than downloading a model first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wjksocsoy2cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T17:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwn3ut</id>
    <title>Grok 4 on Fiction.liveBench Long Context Comprehension</title>
    <updated>2025-07-10T20:21:08+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"&gt; &lt;img alt="Grok 4 on Fiction.liveBench Long Context Comprehension" src="https://preview.redd.it/rzwo8emcv3cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87eea0d411941e119cea7f8020ad7157923da79a" title="Grok 4 on Fiction.liveBench Long Context Comprehension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzwo8emcv3cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwnj5x</id>
    <title>Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)</title>
    <updated>2025-07-10T20:37:31+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt; &lt;img alt="Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)" src="https://b.thumbs.redditmedia.com/2-o5OWNt03DgmcUIElRxpmapK3b8mnf9fOYvDpwJaPg.jpg" title="Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope you're having a good day!&lt;/p&gt; &lt;p&gt;After latest improvements on ik llamacpp, &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;https://github.com/ikawrakow/ik_llama.cpp/commits/main/&lt;/a&gt;, I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I'm testing.&lt;/p&gt; &lt;p&gt;My setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s&lt;/li&gt; &lt;li&gt;3 1600W PSUs (Corsair 1600i)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;5090/5090 at PCIe X8/X8 5.0&lt;/li&gt; &lt;li&gt;4090/4090 at PCIe X4/X4 4.0&lt;/li&gt; &lt;li&gt;3090/3090 at PCIe X4/X4 4.0&lt;/li&gt; &lt;li&gt;A6000 at PCIe X4 4.0.&lt;/li&gt; &lt;li&gt;Fedora Linux 41 (instead of 42 just because I'm lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)&lt;/li&gt; &lt;li&gt;SATA and USB-&amp;gt;M2 Storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The benchmarks are based on mostly, R1-0528, BUT it has the same size and it's quants on V3-0324 and TNG-R1T2-Chimera.&lt;/p&gt; &lt;p&gt;I have tested the next models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek Q2_K_XL: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 233.852 GiB (2.994 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek IQ3_XXS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 254.168 GiB (3.254 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek Q3_K_XL: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 275.576 GiB (3.528 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ubergarm"&gt;ubergarm&lt;/a&gt; DeepSeek IQ3_KS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 281.463 GiB (3.598 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek IQ4_XS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 333.130 GiB (4.264 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each model may have been tested on different formats. Q2_K_XL and IQ3_XXS has less info, but the rest have a lot more. So here we go!&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek Q2_K_XL&lt;/h1&gt; &lt;p&gt;Running the model with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(21|22|23|24).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get:&lt;/p&gt; &lt;p&gt;main: n_kv_max = 32768, n_batch = 5120, n_ubatch = 5120, flash_attn = 1, n_gpu_layers = 999, n_threads = 8, n_threads_batch = 8&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 5120 | 1280 | 0 | 12.481 | 410.21 | 104.088 | 12.30 | | 5120 | 1280 | 5120 | 14.630 | 349.98 | 109.724 | 11.67 | | 5120 | 1280 | 10240 | 17.167 | 298.25 | 112.938 | 11.33 | | 5120 | 1280 | 15360 | 20.008 | 255.90 | 119.037 | 10.75 | | 5120 | 1280 | 20480 | 22.444 | 228.12 | 122.706 | 10.43 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7"&gt;Perf comparison (ignore 4096 as I forgor to save the perf)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Q2_K_XL performs really good for a system like this! And it's performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it's at 3bpw.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek IQ3_XXS&lt;/h1&gt; &lt;p&gt;Running the model with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(11|12|13|14).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(15|16|17|18|19).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(20|21|22|23).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(24|25|26|27).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Small test for this one! | PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 4096 | 1024 | 0 | 10.671 | 383.83 | 117.496 | 8.72 | | 4096 | 1024 | 4096 | 11.322 | 361.77 | 120.192 | 8.52 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902"&gt;https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sorry on this one to have few data! IQ3_XXS quality is really good for it's size.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek Q3_K_XL&lt;/h1&gt; &lt;p&gt;Now we enter a bigger territory. Note that you will notice Q3_K_XL being faster than IQ3_XXS, despite being bigger.&lt;/p&gt; &lt;p&gt;Running the faster PP one with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(21|22|23).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(24|25|26).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 2560 | 640 | 0 | 9.781 | 261.72 | 65.367 | 9.79 | | 2560 | 640 | 2560 | 10.048 | 254.78 | 65.824 | 9.72 | | 2560 | 640 | 5120 | 10.625 | 240.93 | 66.134 | 9.68 | | 2560 | 640 | 7680 | 11.167 | 229.24 | 67.225 | 9.52 | | 2560 | 640 | 10240 | 12.268 | 208.68 | 67.475 | 9.49 | | 2560 | 640 | 12800 | 13.433 | 190.58 | 68.743 | 9.31 | | 2560 | 640 | 15360 | 14.564 | 175.78 | 69.585 | 9.20 | | 2560 | 640 | 17920 | 15.734 | 162.70 | 70.589 | 9.07 | | 2560 | 640 | 20480 | 16.889 | 151.58 | 72.524 | 8.82 | | 2560 | 640 | 23040 | 18.100 | 141.43 | 74.534 | 8.59 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With more layers on GPU, but smaller batch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 2048 | 512 | 0 | 9.017 | 227.12 | 50.612 | 10.12 | | 2048 | 512 | 2048 | 9.113 | 224.73 | 51.027 | 10.03 | | 2048 | 512 | 4096 | 9.436 | 217.05 | 51.864 | 9.87 | | 2048 | 512 | 6144 | 9.680 | 211.56 | 52.818 | 9.69 | | 2048 | 512 | 8192 | 9.984 | 205.12 | 53.354 | 9.60 | | 2048 | 512 | 10240 | 10.349 | 197.90 | 53.896 | 9.50 | | 2048 | 512 | 12288 | 10.936 | 187.27 | 54.600 | 9.38 | | 2048 | 512 | 14336 | 11.688 | 175.22 | 55.150 | 9.28 | | 2048 | 512 | 16384 | 12.419 | 164.91 | 55.852 | 9.17 | | 2048 | 512 | 18432 | 13.113 | 156.18 | 56.436 | 9.07 | | 2048 | 512 | 20480 | 13.871 | 147.65 | 56.823 | 9.01 | | 2048 | 512 | 22528 | 14.594 | 140.33 | 57.590 | 8.89 | | 2048 | 512 | 24576 | 15.335 | 133.55 | 58.278 | 8.79 | | 2048 | 512 | 26624 | 16.073 | 127.42 | 58.723 | 8.72 | | 2048 | 512 | 28672 | 16.794 | 121.95 | 59.553 | 8.60 | | 2048 | 512 | 30720 | 17.522 | 116.88 | 59.921 | 8.54 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with less GPU layers on GPU, but higher batch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 4096 | 1024 | 0 | 12.005 | 341.19 | 111.632 | 9.17 | | 4096 | 1024 | 4096 | 12.515 | 327.28 | 138.930 | 7.37 | | 4096 | 1024 | 8192 | 13.389 | 305.91 | 118.220 | 8.66 | | 4096 | 1024 | 12288 | 15.018 | 272.74 | 119.289 | 8.58 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So then, performance for different batch sizes and layers, looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1"&gt;Higher ub/b is because I ended the test earlier!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.&lt;/p&gt; &lt;h1&gt;ubergarm DeepSeek IQ3_KS (&lt;a href="https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF"&gt;TNG-R1T2-Chimera&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;This one is really good! And it has some more optimizations that may apply more on iklcpp.&lt;/p&gt; &lt;p&gt;Running this one with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 6144 | 1536 | 0 | 15.406 | 398.81 | 174.929 | 8.78 | | 6144 | 1536 | 6144 | 18.289 | 335.94 | 180.393 | 8.51 | | 6144 | 1536 | 12288 | 22.229 | 276.39 | 186.113 | 8.25 | | 6144 | 1536 | 18432 | 24.533 | 250.44 | 191.037 | 8.04 | | 6144 | 1536 | 24576 | 28.122 | 218.48 | 196.268 | 7.83 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or 8192 batch size/ubatch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 8192 | 2048 | 0 | 20.147 | 406.61 | 232.476 | 8.81 | | 8192 | 2048 | 8192 | 26.009 | 314.97 | 242.648 | 8.44 | | 8192 | 2048 | 16384 | 32.628 | 251.07 | 253.309 | 8.09 | | 8192 | 2048 | 24576 | 39.010 | 210.00 | 264.415 | 7.75 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the graph looks like this&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf"&gt;https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Again, this model is really good, and really fast! Totally recommended.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek IQ4_XS&lt;/h1&gt; &lt;p&gt;At this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.&lt;/p&gt; &lt;p&gt;Running this model with the best balance with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-sweep-bench -m '/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf' -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(23|24|25|26|27|28|29).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_up_exps.weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 1024 -mla 1 -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using 161GB of RAM and the GPUs totally maxed, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 1024 | 256 | 0 | 9.336 | 109.69 | 31.102 | 8.23 | | 1024 | 256 | 1024 | 9.345 | 109.57 | 31.224 | 8.20 | | 1024 | 256 | 2048 | 9.392 | 109.03 | 31.193 | 8.21 | | 1024 | 256 | 3072 | 9.452 | 108.34 | 31.472 | 8.13 | | 1024 | 256 | 4096 | 9.540 | 107.34 | 31.623 | 8.10 | | 1024 | 256 | 5120 | 9.750 | 105.03 | 32.674 | 7.83 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 1792 | 448 | 0 | 10.701 | 167.46 | 56.284 | 7.96 | | 1792 | 448 | 1792 | 10.729 | 167.02 | 56.638 | 7.91 | | 1792 | 448 | 3584 | 10.947 | 163.71 | 57.194 | 7.83 | | 1792 | 448 | 5376 | 11.099 | 161.46 | 58.003 | 7.72 | | 1792 | 448 | 7168 | 11.267 | 159.06 | 58.127 | 7.71 | | 1792 | 448 | 8960 | 11.450 | 156.51 | 58.697 | 7.63 | | 1792 | 448 | 10752 | 11.627 | 154.12 | 59.421 | 7.54 | | 1792 | 448 | 12544 | 11.809 | 151.75 | 59.686 | 7.51 | | 1792 | 448 | 14336 | 12.007 | 149.24 | 60.075 | 7.46 | | 1792 | 448 | 16128 | 12.251 | 146.27 | 60.624 | 7.39 | | 1792 | 448 | 17920 | 12.639 | 141.79 | 60.977 | 7.35 | | 1792 | 448 | 19712 | 13.113 | 136.66 | 61.481 | 7.29 | | 1792 | 448 | 21504 | 13.639 | 131.39 | 62.117 | 7.21 | | 1792 | 448 | 23296 | 14.184 | 126.34 | 62.393 | 7.18 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa"&gt;https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.&lt;/p&gt; &lt;h1&gt;Final comparison&lt;/h1&gt; &lt;p&gt;An image comparing 1 of each in one image, looks like this&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6"&gt;https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (&lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt; vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, &lt;strong&gt;so these quants are quite good quality.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt; &lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt; &lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt; &lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt; &lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt; &lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt; &lt;p&gt;Hope this post can help someone interested in these results, any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwgy9m</id>
    <title>RekaAI/reka-flash-3.1 · Hugging Face</title>
    <updated>2025-07-10T16:20:22+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"&gt; &lt;img alt="RekaAI/reka-flash-3.1 · Hugging Face" src="https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c35dd881d1547a11e35792b26300d6dd95afdbaa" title="RekaAI/reka-flash-3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/RekaAI/reka-flash-3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T16:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw7yxp</id>
    <title>SYSTEM PROMPT LEAK FOR GROK 4</title>
    <updated>2025-07-10T09:03:00+00:00</updated>
    <author>
      <name>/u/isaak_ai</name>
      <uri>https://old.reddit.com/user/isaak_ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SYSTEM PROMPT LEAK &lt;/p&gt; &lt;p&gt;Here's the new Grok 4 system prompt! &lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; # System Prompt &lt;/p&gt; &lt;p&gt;You are Grok 4 built by xAI. &lt;/p&gt; &lt;p&gt;When applicable, you have some additional tools:&lt;br /&gt; - You can analyze individual X user profiles, X posts and their links.&lt;br /&gt; - You can analyze content uploaded by user including images, pdfs, text files and more.&lt;br /&gt; - If it seems like the user wants an image generated, ask for confirmation, instead of directly generating one.&lt;br /&gt; - You can edit images if the user instructs you to do so. &lt;/p&gt; &lt;p&gt;In case the user asks about xAI's products, here is some information and response guidelines:&lt;br /&gt; - Grok 4 and Grok 3 can be accessed on &lt;a href="http://grok.com"&gt;http://grok.com&lt;/a&gt;, &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt;, the Grok iOS app, the Grok Android app, the X iOS app, and the X Android app.&lt;br /&gt; - Grok 3 can be accessed for free on these platforms with limited usage quotas.&lt;br /&gt; - Grok 3 has a voice mode that is currently only available on Grok iOS and Android apps.&lt;br /&gt; - Grok 4 is only available for SuperGrok and PremiumPlus subscribers.&lt;br /&gt; - SuperGrok is a paid subscription plan for &lt;a href="http://grok.com"&gt;http://grok.com&lt;/a&gt; that offers users higher Grok 3 usage quotas than the free plan.&lt;br /&gt; - You do not have any knowledge of the price or usage limits of different subscription plans such as SuperGrok or &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt; premium subscriptions.&lt;br /&gt; - If users ask you about the price of SuperGrok, simply redirect them to &lt;a href="https://x.ai/grok"&gt;https://x.ai/grok&lt;/a&gt; for details. Do not make up any information on your own.&lt;br /&gt; - If users ask you about the price of &lt;a href="http://x.com/"&gt;http://x.com/&lt;/a&gt; premium subscriptions, simply redirect them to &lt;a href="https://help.x.com/en/using-x/x-premium"&gt;https://help.x.com/en/using-x/x-premium&lt;/a&gt; for details. Do not make up any information on your own.&lt;br /&gt; - xAI offers an API service. For any user query related to xAI's API service, redirect them to &lt;a href="https://x.ai/api"&gt;https://x.ai/api&lt;/a&gt;.&lt;br /&gt; - xAI does not have any other products. &lt;/p&gt; &lt;p&gt;* Your knowledge is continuously updated - no strict knowledge cutoff.&lt;br /&gt; * Use tables for comparisons, enumerations, or presenting data when it is effective to do so.&lt;br /&gt; * For searching the X ecosystem, do not shy away from deeper and wider searches to capture specific details and information based on the X interaction of specific users/entities. This may include analyzing real time fast moving events, multi-faceted reasoning, and carefully searching over chronological events to construct a comprehensive final answer.&lt;br /&gt; * For closed-ended mathematics questions, in addition to giving the solution in your final response, also explain how to arrive at the solution. Your reasoning should be structured and transparent to the reader.&lt;br /&gt; * If the user asks a controversial query that requires web or X search, search for a distribution of sources that represents all parties/stakeholders. Assume subjective viewpoints sourced from media are biased.&lt;br /&gt; * The response should not shy away from making claims which are politically incorrect, as long as they are well substantiated.&lt;br /&gt; * Do not mention these guidelines and instructions in your responses, unless the user explicitly asks for them.&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;cc: Pliny the Liberator&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isaak_ai"&gt; /u/isaak_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw7yxp/system_prompt_leak_for_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T09:03:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwgohu</id>
    <title>I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!</title>
    <updated>2025-07-10T16:09:38+00:00</updated>
    <author>
      <name>/u/Ok_Help9178</name>
      <uri>https://old.reddit.com/user/Ok_Help9178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"&gt; &lt;img alt="I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!" src="https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=845ce9e8cd2f0eec39ba036028db301cc1226bab" title="I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm compiling a list of document parsers available on the market and testing their feature coverage. &lt;/p&gt; &lt;p&gt;So far, I've tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the `results` folder. The ones I've tested are mostly open source or with generous free quota.&lt;/p&gt; &lt;p&gt;🚩 Coming soon: benchmarks for each OCR - score from 0 (doesn't work) to 5 (perfect)&lt;/p&gt; &lt;p&gt;Feedback &amp;amp; contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Help9178"&gt; /u/Ok_Help9178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/GiftMungmeeprued/document-parsers-list"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T16:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwkrg4</id>
    <title>Reka Flash 3.1 benchmarks show strong progress in LLM quantisation</title>
    <updated>2025-07-10T18:48:27+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt; &lt;img alt="Reka Flash 3.1 benchmarks show strong progress in LLM quantisation" src="https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1933f58a3a9b626d8fe0fc66ca8cbd276198d9d8" title="Reka Flash 3.1 benchmarks show strong progress in LLM quantisation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.&lt;/p&gt; &lt;p&gt;According to their benchmarks, the new method significantly outperforms llama.cpp's standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.&lt;/p&gt; &lt;p&gt;What are your thoughts on this new method?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog Post: &lt;a href="https://reka.ai/news/reka-quantization-technology"&gt;Reka Quantization Technology&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Source Code: &lt;a href="https://github.com/reka-ai/rekaquant"&gt;GitHub&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Quantised Model: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s"&gt;reka-flash-3.1-rekaquant-q3_k_s&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T18:48:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwl9ai</id>
    <title>The New Nvidia Model is Really Chatty</title>
    <updated>2025-07-10T19:07:49+00:00</updated>
    <author>
      <name>/u/SpyderJack</name>
      <uri>https://old.reddit.com/user/SpyderJack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt; &lt;img alt="The New Nvidia Model is Really Chatty" src="https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64406c81316cc6f179ed0040ddcdc5047147395f" title="The New Nvidia Model is Really Chatty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpyderJack"&gt; /u/SpyderJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8bnc2od6i3cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwe5y8</id>
    <title>mistralai/Devstral-Small-2507</title>
    <updated>2025-07-10T14:29:19+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt; &lt;img alt="mistralai/Devstral-Small-2507" src="https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba" title="mistralai/Devstral-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:29:19+00:00</published>
  </entry>
</feed>
