<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-03T15:06:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n79udw</id>
    <title>Inference on new Framework desktop</title>
    <updated>2025-09-03T09:07:58+00:00</updated>
    <author>
      <name>/u/wombatsock</name>
      <uri>https://old.reddit.com/user/wombatsock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, lovely community! I'm just curious if anyone has gotten their hands on the new Framework desktop and used it to run inference for local models. I'm aware the memory bandwidth is weak, and I assume it's probably not great for fine-tuning or training. I just wonder if, given its energy efficiency and large shared memory capacity, it would make sense to set up the board as an LLM server for mid-sized models like quen3-coder:30b. Or if you have any other solutions that might work for this scenario, I'd love to hear them! (maybe a Mac Mini??). I already have an Nvidia 3060 with 12gb VRAM, and I'd rather not just get a bigger/faster GPU, they're pretty expensive and hog a lot of power when idling. Anyway, I'm rambling now, show me what you got!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wombatsock"&gt; /u/wombatsock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T09:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fsgd</id>
    <title>I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools</title>
    <updated>2025-09-03T14:07:15+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt; &lt;img alt="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" src="https://b.thumbs.redditmedia.com/lVGU8L5zqMIzkaT28XyJJJ-eN-kz7vnn4Hr4DJVSTqI.jpg" title="I made Spring AI Playground - a self-hosted UI for local LLMs, RAG, and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an open-source project called Spring AI Playground — a self-hosted web UI for experimenting with local LLMs, RAG, and MCP tools.&lt;/p&gt; &lt;p&gt;It’s a self-hosted web UI (Docker image available) that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run local LLMs with &lt;strong&gt;Ollama&lt;/strong&gt; (you can switch to OpenAI/Anthropic too).&lt;/li&gt; &lt;li&gt;Upload docs → chunk, embed, search, and inspect vector-DB retrieval &lt;strong&gt;with score details&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Connect to &lt;strong&gt;MCP servers directly&lt;/strong&gt;, test each tool, and even run end-to-end chat flows combining RAG + MCP.&lt;/li&gt; &lt;li&gt;Swap vector DBs or select MCP tools dynamically - thanks to the Spring AI framework under the hood.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why I built it:&lt;/p&gt; &lt;p&gt;I wanted a sandbox where I could mash things together quickly, test retrieval quality, debug tools, and keep everything running locally. Open WebUI is fantastic for chat-centric experiments, but my focus was to make &lt;strong&gt;RAG + MCP first-class playgrounds&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community - especially from those running local models or playing with MCP. Curious if this would fit into your workflow, or if there are rough edges I should improve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7fsgd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fsgd/i_made_spring_ai_playground_a_selfhosted_ui_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qzre</id>
    <title>I open-sourced 50+ Docker images to give your local LLMs easy access to tools like GitHub, Gmail, Slack, etc. No more dependency hell.</title>
    <updated>2025-09-02T18:12:10+00:00</updated>
    <author>
      <name>/u/IllChannel5235</name>
      <uri>https://old.reddit.com/user/IllChannel5235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've been experimenting with local LLMs and autonomous agents. A major pain point is giving these agents access to real-world tools. Setting up connections to services like GitHub, Jira, or Slack locally is a nightmare of dependency management, OAuth flows, and custom scripts.&lt;/p&gt; &lt;p&gt;To solve this, my team at Klavis AI has open-sourced &lt;strong&gt;pre-built Docker images for 50+ high-quality MCP (Model Context Protocol) servers.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can now spin up a server to give your local model access to an external tool with a single command. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, to run a GitHub MCP server locally:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With our managed OAuth (free API key) docker run -p 5000:5000 \ -e KLAVIS_API_KEY=$KLAVIS_API_KEY \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or bring your own GitHub token:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With your own token docker run -p 5000:5000 \ -e AUTH_DATA='{&amp;quot;access_token&amp;quot;:&amp;quot;ghp_your_github_token&amp;quot;}' \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No more fighting with Python environments or implementing OAuth. Just a clean, containerized MCP server your agent can talk to.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this is a big deal for LocalLLaMA:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Empower Your Agents:&lt;/strong&gt; Give your models the ability to read GitHub issues, check your Google Calendar, or search through Notion docs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Local:&lt;/strong&gt; The images are Alpine-based and run entirely on your machine, keeping everything local.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead Simple:&lt;/strong&gt; No compiling, no dependency hell. Just docker run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ MCP servers Available:&lt;/strong&gt; We've containerized servers for GitHub, Gmail, Slack, Notion, Jira, Linear, Salesforce, and many more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Bigger Picture: Solving Agent Limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We all know agents struggle with tool selection, context window limits, and understanding human context. We're building a solution to these fundamental problems, allowing agents to use hundreds of tools without overwhelming the context window. These open-source servers are the first step.&lt;/p&gt; &lt;p&gt;If you're interested in the future of capable AI agents, check out our waitlist. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.klavis.ai%2Fwaitlist"&gt;https://www.klavis.ai/waitlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNITgggPT3pA"&gt;https://www.youtube.com/watch?v=NITgggPT3pA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback and see what you build with this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllChannel5235"&gt; /u/IllChannel5235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n76i6w</id>
    <title>Kwai Keye-VL 1.5 Technical Report</title>
    <updated>2025-09-03T05:32:56+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Page: &lt;a href="https://kwai-keye.github.io/"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Kwai-Keye"&gt;https://huggingface.co/Kwai-Keye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Kwai-Keye/Keye"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.01563"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n76i6w/kwai_keyevl_15_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n76i6w/kwai_keyevl_15_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7f9d6</id>
    <title>Normalizing documents for ingestion</title>
    <updated>2025-09-03T13:46:22+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s a good tool to clean up PDFs and ready them for Markdown conversion? I’ve got PDFs that have been scanned badly, or are in a “pamphlet” layout with inverted or out of order pages, and it creates a “garbage in, garbage out” scenario when it comes to RAG.&lt;/p&gt; &lt;p&gt;Half of the time, OCR gives characters that are just obviously wrong, like it can’t recognize the font and makes terrible guesses. Any smarter VLMs or multimodal that could do a good job of this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7f9d6/normalizing_documents_for_ingestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7f9d6/normalizing_documents_for_ingestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7f9d6/normalizing_documents_for_ingestion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fj41</id>
    <title>Nothing stands out vis-a-vis agents for a local LLM</title>
    <updated>2025-09-03T13:57:12+00:00</updated>
    <author>
      <name>/u/ChevChance</name>
      <uri>https://old.reddit.com/user/ChevChance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to see what's available for local agents working with a local LLM. Nothing I'm seeing stands out, or maybe I'm missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChevChance"&gt; /u/ChevChance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fj41/nothing_stands_out_visavis_agents_for_a_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fj41/nothing_stands_out_visavis_agents_for_a_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fj41/nothing_stands_out_visavis_agents_for_a_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rnp6</id>
    <title>Showerthought: Modern AI safety training is anti-safety</title>
    <updated>2025-09-02T18:36:36+00:00</updated>
    <author>
      <name>/u/Deathcrow</name>
      <uri>https://old.reddit.com/user/Deathcrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably not a unique thought, but it needs to be said.&lt;/p&gt; &lt;p&gt;It seems to me, that modern AI alignment safety training (driven by very superficial concerns, like porn, politics, hacking, mean words), wherein AI is trained to either outright reject the human's requests, or worse, subtly steer/manipulate users away from these topics, is actually anti-safety (the doomsday kind).&lt;/p&gt; &lt;p&gt;Why do we want AI agents to become more capable at deceiving users and circumventing our wishes? In this cycle of unnatural selection, the &amp;quot;safest&amp;quot; AI model is one where the user is still happy to use it and trust its answers, even though it's heavily censored or misleading?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deathcrow"&gt; /u/Deathcrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fux7</id>
    <title>Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic</title>
    <updated>2025-09-03T14:10:03+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fux7/reasoning_vectors_transferring_chainofthought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6od0s</id>
    <title>残心 / Zanshin - Navigate through media by speaker</title>
    <updated>2025-09-02T16:34:30+00:00</updated>
    <author>
      <name>/u/hamza_q_</name>
      <uri>https://old.reddit.com/user/hamza_q_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt; &lt;img alt="残心 / Zanshin - Navigate through media by speaker" src="https://external-preview.redd.it/czg0dWhsczUyc21mMcardPaxszcLLO9nZqjdF7h57XxHnWsrQqY3M3ZeJApB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae580e102b0dfc5e45071c5a325f730f47366dad" title="残心 / Zanshin - Navigate through media by speaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;残心 / Zanshin is a media player that allows you to:&lt;/p&gt; &lt;p&gt;- Visualize who speaks when &amp;amp; for how long&lt;/p&gt; &lt;p&gt;- Jump/skip speaker segments&lt;/p&gt; &lt;p&gt;- Remove/disable speakers (auto-skip)&lt;/p&gt; &lt;p&gt;- Set different playback speeds for each speaker&lt;/p&gt; &lt;p&gt;It's a better, more efficient way to listen to podcasts, interviews, press conferences, etc.&lt;/p&gt; &lt;p&gt;It has first-class support for YouTube videos; just drop in a URL. Also supports your local media files. All processing runs on-device.&lt;/p&gt; &lt;p&gt;Download today for macOS (more screenshots &amp;amp; demo vids in here too): &lt;a href="https://zanshin.sh"&gt;https://zanshin.sh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also works on Linux and WSL, but currently without packaging. You can get it running though with just a few terminal commands. Check out the repo for instructions: &lt;a href="https://github.com/narcotic-sh/zanshin"&gt;https://github.com/narcotic-sh/zanshin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zanshin is powered by Senko, a new, very fast, speaker diarization pipeline I've developed.&lt;/p&gt; &lt;p&gt;On an M3 MacBook Air, it takes over 5 minutes to process 1 hour of audio using Pyannote 3.1, the leading open-source diarization pipeline. With Senko, it only takes ~24 seconds, a ~14x speed improvement. And on an RTX 4090 + Ryzen 9 7950X machine, processing 1 hour of audio takes just 5 seconds with Senko, a ~17x speed improvement.&lt;/p&gt; &lt;p&gt;Senko's speed is what make's Zanshin possible. Senko is a modified version of the speaker diarization pipeline found in the excellent 3D-Speaker project. Check out Senko here: &lt;a href="https://github.com/narcotic-sh/senko"&gt;https://github.com/narcotic-sh/senko&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone; enjoy 残心/Zanshin and Senko. I hope you find them useful. Let me know what you think!&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;Side note: I am looking for a job. If you like my work and have an opportunity for me, I'm all ears :) You can contact me at mhamzaqayyum [at] &lt;a href="http://icloud.com"&gt;icloud.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hamza_q_"&gt; /u/hamza_q_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qh0wtns52smf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7czji</id>
    <title>Best large local model for creative writing?</title>
    <updated>2025-09-03T12:06:46+00:00</updated>
    <author>
      <name>/u/Creepy-Bell-4527</name>
      <uri>https://old.reddit.com/user/Creepy-Bell-4527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've done much experimenting (and even fine tuning) with local models for coding, and have a shortlist of go-to models for specific tasks, but I've recently been trying to incorporate these into creative pipelines, and noticed they all sound every bit as autistic and robotic as myself. Sometimes even moreso (looking at you Qwen3)&lt;/p&gt; &lt;p&gt;So with that in mind I'm looking for suggestions for models that run well within 96GB of memory, that are effective at producing &amp;quot;characterized&amp;quot; creative writing - dialogs specifically. Multimodal would also be a plus.&lt;/p&gt; &lt;p&gt;I've had some good results with GPT 5 but would prefer a local model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creepy-Bell-4527"&gt; /u/Creepy-Bell-4527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7czji/best_large_local_model_for_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7czji/best_large_local_model_for_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7czji/best_large_local_model_for_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T12:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6epwv</id>
    <title>My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅</title>
    <updated>2025-09-02T09:17:01+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt; &lt;img alt="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" src="https://b.thumbs.redditmedia.com/pYjWCv-fYbHaF7KTK2GkiEx7BRL3zHSwgEFLLf7Zn0M.jpg" title="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 Hitting a million brick walls with multi-turn RL training isn't fun, so I thought I would try something new to climb Stanford's leaderboard for now! So this weekend I was just tinkering with multi-agent systems and... somehow ended up beating Claude Code on Stanford's TerminalBench leaderboard (#12)! Genuinely didn't expect this - started as a fun experiment and ended up with something that works surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a multi-agent AI system with three specialised agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: The brain - never touches code, just delegates and coordinates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explorer agents&lt;/strong&gt;: Read &amp;amp; run only investigators that gather intel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coder agents&lt;/strong&gt;: The ones who actually implement stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created a &amp;quot;Context Store&amp;quot; which can be thought of as persistent memory that lets agents share their discoveries.&lt;/p&gt; &lt;p&gt;Tested on TerminalBench with both Claude Sonnet-4 and Qwen3-Coder-480B. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Orchestrator + Sonnet-4: &lt;strong&gt;36.0% success rate&lt;/strong&gt; (#12 on leaderboard, ahead of Claude Code!)&lt;/li&gt; &lt;li&gt;Orchestrator + Qwen-3-Coder: 19.25% success rate&lt;/li&gt; &lt;li&gt;Sonnet-4 consumed 93.2M tokens vs Qwen's 14.7M tokens to compete all tasks!&lt;/li&gt; &lt;li&gt;The orchestrator's explicit task delegation + intelligent context sharing between subagents seems to be the secret sauce&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;(Kind of) Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The orchestrator can't read/write code directly - this forces proper delegation patterns and strategic planning&lt;/li&gt; &lt;li&gt;Each agent gets precise instructions about what &amp;quot;knowledge artifacts&amp;quot; to return, these artifacts are then stored, and can be provided to future subagents upon launch.&lt;/li&gt; &lt;li&gt;Adaptive trust calibration: simple tasks = high autonomy, complex tasks = iterative decomposition&lt;/li&gt; &lt;li&gt;Each agent has its own set of tools it can use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repo has all the code, system messages, and way more technical details if you're interested!&lt;/p&gt; &lt;p&gt;⭐️ &lt;a href="https://github.com/Danau5tin/multi-agent-coding-system"&gt;&lt;strong&gt;Orchestrator repo - all code open sourced!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent &lt;a href="https://www.tbench.ai/"&gt;TerminalBench&lt;/a&gt; benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6epwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6siz6</id>
    <title>NousResearch/Hermes-4-14B · Hugging Face</title>
    <updated>2025-09-02T19:08:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt; &lt;img alt="NousResearch/Hermes-4-14B · Hugging Face" src="https://external-preview.redd.it/3zW4BctOGBSQqyD1VYjxoOK5if51GWWepXF3S3IdZF0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b37e63854cc68df0d3bc6f558a76fe90da9ad013" title="NousResearch/Hermes-4-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes 4 14B is a frontier, hybrid-mode &lt;strong&gt;reasoning&lt;/strong&gt; model based on Qwen 3 14B by Nous Research that is aligned to &lt;strong&gt;you&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Read the Hermes 4 technical report here: &lt;a href="https://arxiv.org/abs/2508.18255"&gt;Hermes 4 Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat with Hermes in Nous Chat: &lt;a href="https://chat.nousresearch.com"&gt;https://chat.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training highlights include a newly synthesized post-training corpus emphasizing verified reasoning traces, massive improvements in math, code, STEM, logic, creativity, and format-faithful outputs, while preserving general assistant quality and broadly neutral alignment.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B#whats-new-vs-hermes-3"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;What’s new vs Hermes 3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Post-training corpus&lt;/strong&gt;: Massively increased dataset size from 1M samples and 1.2B tokens to &lt;strong&gt;~5M samples / ~60B tokens&lt;/strong&gt; blended across reasoning and non-reasoning data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid reasoning mode&lt;/strong&gt; with explicit &lt;code&gt;&amp;lt;think&amp;gt;…&amp;lt;/think&amp;gt;&lt;/code&gt; segments when the model decides to deliberate, and options to make your responses faster when you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning&lt;/strong&gt; that is top quality, expressive, improves math, code, STEM, logic, and even creative writing and subjective responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema adherence &amp;amp; structured outputs&lt;/strong&gt;: trained to produce valid JSON for given schemas and to repair malformed objects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Much easier to steer and align&lt;/strong&gt;: extreme improvements on steerability, especially on reduced refusal rates.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T19:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7cz7y</id>
    <title>YanoljaNEXT-Rosetta: A Collection of Translation Models in Different Sizes</title>
    <updated>2025-09-03T12:06:20+00:00</updated>
    <author>
      <name>/u/SummerFantastic5457</name>
      <uri>https://old.reddit.com/user/SummerFantastic5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;YanoljaNEXT-Rosetta is designed for translating structured data in JSON format. It’s built on top of either &lt;strong&gt;Gemma-3&lt;/strong&gt; or &lt;strong&gt;GPT-OSS&lt;/strong&gt;, depending on the configuration.&lt;/p&gt; &lt;p&gt;If your language isn’t listed on the card, don’t worry—this model supports many more languages than those shown.&lt;/p&gt; &lt;p&gt;In evaluations, YanoljaNEXT-Rosetta outperformed proprietary models on &lt;strong&gt;BLEU&lt;/strong&gt; and &lt;strong&gt;CHrF++&lt;/strong&gt; scores, while scoring slightly lower on &lt;strong&gt;MetricX24&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;📂 Hugging Face collection: &lt;a href="https://huggingface.co/collections/yanolja/yanoljanext-rosetta-68b82bd4718e49b5c7a745b5?utm_source=chatgpt.com"&gt;here&lt;/a&gt;&lt;br /&gt; 🔹 &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B?utm_source=chatgpt.com"&gt;4B model&lt;/a&gt;&lt;br /&gt; 🔹 &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B?utm_source=chatgpt.com"&gt;12B model&lt;/a&gt;&lt;br /&gt; 🔹 &lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-20B?utm_source=chatgpt.com"&gt;20B model&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummerFantastic5457"&gt; /u/SummerFantastic5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7cz7y/yanoljanextrosetta_a_collection_of_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T12:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6vzfe</id>
    <title>WEBGEN-4B: Quality Web Design Generation</title>
    <updated>2025-09-02T21:20:22+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt; &lt;img alt="WEBGEN-4B: Quality Web Design Generation" src="https://b.thumbs.redditmedia.com/hz9KVQoDGH5SuRnQfWtoik62ndFYILrISKyncU-X0Cc.jpg" title="WEBGEN-4B: Quality Web Design Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tesslate/WEBGEN-4B is a 4B model that produces quality tailwind websites. We trained it on 100k samples with synthetic data exclusively generated from GPT-OSS. WEBGEN is fast, controllable, and can drop right into your agentic workflows.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF"&gt;https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the course of this week and next week, we will be dropping a few more models or open sourced software based on the innovations we've made in this space!&lt;/p&gt; &lt;p&gt;Please reach out for API keys to test it out if needed. On the model card and below in the comments will be our designer platform (which we will open source soon) where you can use the model for free. &lt;/p&gt; &lt;p&gt;In other news, we are open sourcing our UIGEN-T2 Dataset at Tesslate/UIGEN-T2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6vzfe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rbi2</id>
    <title>Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp; Meta AI</title>
    <updated>2025-09-02T18:24:09+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt; &lt;img alt="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" src="https://external-preview.redd.it/FUP5JRh_hs7L2Yd_DmiTAO0WgUYYJ4skdrhkm8MNDKc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcef23df49c3448a2d625ddb43fe346cbd8bdd05" title="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So acquiring copyrighted material for the purpose of training LLMs is deemed transformative and qualifies under fair use? Gonna call this Meta's Defence from now on.. I have a huge stash of ebooks to run through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=sdtBgB7iS8c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7923o</id>
    <title>Has anyone run 256GB of DDR5 6000 stable on an AM5 platform?</title>
    <updated>2025-09-03T08:15:25+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my system to 256GB so I can run a larger model with my GPU. I’m wondering if anyone has been able to run 256GB of DDR5 6000 stable on an AM5 platform. I don’t want to upgrade to Threadripper since it’s out of my budget. Which motherboard and RAM did you use?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587"&gt;https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MSI claims their motherboard can still achieve a stable overclocking speed of 6000MT/s even with four 64GB DRAM fully installed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n770a1</id>
    <title>I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time</title>
    <updated>2025-09-03T06:03:07+00:00</updated>
    <author>
      <name>/u/yuyangchee98</name>
      <uri>https://old.reddit.com/user/yuyangchee98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt; &lt;img alt="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" src="https://external-preview.redd.it/B50ELvs9yWP89z_ZcFK2UCF9ieaTQI3VL80AVGhBmaU.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1278541976a13214da5dc7333c05607ca273ef3" title="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a Chrome extension that uses local models to filter content based on rules you write in plain English.&lt;/p&gt; &lt;p&gt;Some examples are: &amp;quot;No political content or culture wars&amp;quot;, &amp;quot;Remove clickbait and rage bait&amp;quot;, &amp;quot;Hide celebrity gossip and drama&amp;quot;, &amp;quot;No sports or entertainment news&amp;quot;.&lt;/p&gt; &lt;p&gt;It works with Ollama, LM Studio, and your custom defined OpenAI compatible endpoint. Let me know if you use some other way to host your local LLMs.&lt;/p&gt; &lt;p&gt;Currently only works on Reddit but planning to add more sites.&lt;/p&gt; &lt;p&gt;Link is here: &lt;a href="https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef"&gt;https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player"&gt;https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuyangchee98"&gt; /u/yuyangchee98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T06:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71b95</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:09+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run the same models (ie qwen 3 coder, or GLM 4.5 air) with 4 x 3090? Is the only real difference slight speed difference and a few dollars more a month in electricity? Secondly, are there any consumer motherboards (currently using an intel 265K) that support 4 GPUs, or would I need a new chipset / cpu / mobo etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7b5xl</id>
    <title>New Swiss fully-open multilingual Model</title>
    <updated>2025-09-03T10:29:46+00:00</updated>
    <author>
      <name>/u/braincrowd</name>
      <uri>https://old.reddit.com/user/braincrowd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt; &lt;img alt="New Swiss fully-open multilingual Model" src="https://external-preview.redd.it/KeZfybYf994Jltq2xFXUUTTUg9fRGIDbb5FdVf9Sh70.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=415bc651c08564e7f0fb8fbbfdc78d40ba8ad377" title="New Swiss fully-open multilingual Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/braincrowd"&gt; /u/braincrowd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-2509"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7b5xl/new_swiss_fullyopen_multilingual_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T10:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7bqgm</id>
    <title>LangExtract by Google: many people don't know about this yet!</title>
    <updated>2025-09-03T11:02:22+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt; &lt;img alt="LangExtract by Google: many people don't know about this yet!" src="https://external-preview.redd.it/n9THNRvTBgabZmzyX_O8lEw2GxXkLfCbQBuYD0khQMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9ab8760012176bf18e519e0590b3f5f3d4bab" title="LangExtract by Google: many people don't know about this yet!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google/langextract"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7c1tg</id>
    <title>Le Chat. Custom MCP connectors. Memories.</title>
    <updated>2025-09-03T11:19:20+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt; &lt;img alt="Le Chat. Custom MCP connectors. Memories." src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Le Chat. Custom MCP connectors. Memories." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Le Chat now integrates with 20+ enterprise platforms—powered by MCP—and remembers what matters with Memories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-mcp-connectors-memories"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7c1tg/le_chat_custom_mcp_connectors_memories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7g0c2</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark w/ Leading Models</title>
    <updated>2025-09-03T14:15:58+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" src="https://b.thumbs.redditmedia.com/-7S29tlnbmvmCdgKuNNinsiLgi6LQ83T8Ar-ZV3lCVs.jpg" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, big thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for creating the original German &lt;strong&gt;Wer wird Millionär?&lt;/strong&gt; Benchmark and open-sourcing it. &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;After speaking, we said it would be fun to run the same benchmark on a set of leading models, and that's what we did here. &lt;/p&gt; &lt;p&gt;The rules and data stayed the same, 45 rounds, each with 15 multiple-choice questions from easy to hard. One wrong answer ends the program and you keep the current winnings. No lifelines. Answers are single letters A–D. same public WWM question corpus used in the original. &lt;a href="https://github.com/GerritKainz/wer_wird_millionaer"&gt;https://github.com/GerritKainz/wer_wird_millionaer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Questions remain in German for inference, but we included parallel English text so non-German readers can follow along. See fragen_antworten_en.json in the repo. Scripts to run many programs quickly and rebuild results from per-model outputs (millionaire-run.py, rebuild_leaderboard.py). We’ll attach a screenshot of the leaderboard instead of pasting a table here. same scoring and structure as the original, packaged for quick reruns.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Jose-Sabater/millionaire-bench-opper"&gt;https://github.com/Jose-Sabater/millionaire-bench-opper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Again thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for the idea and groundwork. If you try more models or tweak settings, feel free to open a PR or drop results in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7g0c2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n75z15</id>
    <title>GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations</title>
    <updated>2025-09-03T05:01:51+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt; &lt;img alt="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" src="https://preview.redd.it/6c1jae9atvmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f69c39fa3f7051f8ad4c85418e9c6c975491e18b" title="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full benchmarking methodology here: &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;https://artificialanalysis.ai/methodology/intelligence-benchmarking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c1jae9atvmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
