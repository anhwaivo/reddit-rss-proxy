<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-24T21:36:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lhbgcn</id>
    <title>A Great Breakdown of the "Disney vs Midjourney" Lawsuit Case</title>
    <updated>2025-06-22T00:51:14+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you all know by now, Disney has sued Midjourney on the basis that the latter trained its AI image generating models on copyrighted materials. &lt;/p&gt; &lt;p&gt;This is a serious case that we all should follow up closely. LegalEagle broke down the case in their new YouTube video linked below:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=zpcWv1lHU6I"&gt;https://www.youtube.com/watch?v=zpcWv1lHU6I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really hope Midjourney wins this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T00:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhi8p8</id>
    <title>How much performance am I losing using chipset vs CPU lanes on 3080ti?</title>
    <updated>2025-06-22T07:34:33+00:00</updated>
    <author>
      <name>/u/FactoryReboot</name>
      <uri>https://old.reddit.com/user/FactoryReboot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3080ti and an MSI Z790 gaming plus wifi. For some reason my pcie slot with the cpu lanes isnâ€™t working. The chipset one works fine. &lt;/p&gt; &lt;p&gt;How much performance should I expect to lose with local llama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FactoryReboot"&gt; /u/FactoryReboot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T07:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh2ffp</id>
    <title>CEO Bench: Can AI Replace the C-Suite?</title>
    <updated>2025-06-21T17:49:08+00:00</updated>
    <author>
      <name>/u/dave1010</name>
      <uri>https://old.reddit.com/user/dave1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put together a (slightly tongue in cheek) benchmark to test some LLMs. All open source and all the data is in the repo.&lt;/p&gt; &lt;p&gt;It makes use of the excellent &lt;code&gt;llm&lt;/code&gt; Python package from Simon Willison.&lt;/p&gt; &lt;p&gt;I've only benchmarked a couple of local models but want to see what the smallest LLM is that will score above the estimated &amp;quot;human CEO&amp;quot; performance. How long before a sub-1B parameter model performs better than a tech giant CEO?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dave1010"&gt; /u/dave1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ceo-bench.dave.engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh2ffp/ceo_bench_can_ai_replace_the_csuite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh2ffp/ceo_bench_can_ai_replace_the_csuite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T17:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhgvq4</id>
    <title>[OpenSource]Multi-LLM client - LLM Bridge</title>
    <updated>2025-06-22T06:05:04+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt; &lt;img alt="[OpenSource]Multi-LLM client - LLM Bridge" src="https://external-preview.redd.it/10mCBOjQL0RLrB--BfVKVkZcSDhwfEFJ4fJJfr9rSTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d26661e920b09c71c0e0d22c6b28b034db44cd7f" title="[OpenSource]Multi-LLM client - LLM Bridge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, I created a separate LLM client for Ollama for iOS and MacOS and released it as open source,&lt;/p&gt; &lt;p&gt;but I recreated it by integrating iOS and MacOS codes and adding APIs that support them based on Swift/SwiftUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/00dq12p66f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b97237c3558709596ef0396b5f5d197add9f794"&gt;https://preview.redd.it/00dq12p66f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b97237c3558709596ef0396b5f5d197add9f794&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Supports Ollama and LMStudio as local LLMs.&lt;/p&gt; &lt;p&gt;* If you open a port externally on the computer where LLM is installed on Ollama, you can use free LLM remotely.&lt;/p&gt; &lt;p&gt;* MLStudio is a local LLM management program with its own UI, and you can search and install models from HuggingFace, so you can experiment with various models.&lt;/p&gt; &lt;p&gt;* You can set the IP and port in LLM Bridge and receive responses to queries using the installed model.&lt;/p&gt; &lt;p&gt;* Supports OpenAI&lt;/p&gt; &lt;p&gt;* You can receive an API key, enter it in the app, and use ChatGtp through API calls.&lt;/p&gt; &lt;p&gt;* Using the API is cheaper than paying a monthly membership fee. * Claude support&lt;/p&gt; &lt;p&gt;* Use API Key&lt;/p&gt; &lt;p&gt;* Image transfer possible for image support models&lt;/p&gt; &lt;p&gt;* PDF, TXT file support&lt;/p&gt; &lt;p&gt;* Extract text using PDFKit and transfer it&lt;/p&gt; &lt;p&gt;* Text file support&lt;/p&gt; &lt;p&gt;* Open source&lt;/p&gt; &lt;p&gt;* Swift/SwiftUI&lt;/p&gt; &lt;p&gt;* Source link&lt;/p&gt; &lt;p&gt;* &lt;a href="https://github.com/bipark/swift_llm_bridge"&gt;https://github.com/bipark/swift_llm_bridge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T06:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgwsdr</id>
    <title>DeepSeek Guys Open-Source nano-vLLM</title>
    <updated>2025-06-21T13:38:49+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek guys just open-sourced &lt;a href="https://github.com/GeeeekExplorer/nano-vllm"&gt;nano-vLLM&lt;/a&gt;. Itâ€™s a lightweight vLLM implementation built from scratch.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;ðŸš€ &lt;strong&gt;Fast offline inference&lt;/strong&gt; - Comparable inference speeds to vLLM&lt;/li&gt; &lt;li&gt;ðŸ“– &lt;strong&gt;Readable codebase&lt;/strong&gt; - Clean implementation in ~ 1,200 lines of Python code&lt;/li&gt; &lt;li&gt;âš¡ &lt;strong&gt;Optimization Suite&lt;/strong&gt; - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhd1j0</id>
    <title>Some Observations using the RTX 6000 PRO Blackwell.</title>
    <updated>2025-06-22T02:17:39+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I would share some thoughts playing around with the RTX 6000 Pro 96GB Blackwell Workstation edition.&lt;/p&gt; &lt;p&gt;Using the card inside a Razer Core X GPU enclosure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I bought this bracket (&lt;a href="https://www.etsy.com/listing/1293010019/razer-core-x-bracket-for-corsair-power?ref=cart"&gt;link&lt;/a&gt;) and replaced the Razer Core X power supply with an SFX-L 1000W. Worked beautifully.&lt;/li&gt; &lt;li&gt;Razer Core X cannot handle a 600W card, the outside case gets very HOT with the RTX 6000 Blackwell 600 Watt workstation edition working.&lt;/li&gt; &lt;li&gt;I think this is a perfect use case for the 300W Max-Q edition.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Using the RTX 6000 96GB:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The RTX 6000 96GB Blackwell is bleeding edge. I had to build all libraries with the latest CUDA driver to get it to be usable. For Llama.cpp I had to build it and specifically set the flag to the CUDA architecture (the documents are misleading , need to set the min compute capability 90 not 120.)&lt;/li&gt; &lt;li&gt;When I built all the frame works the RTX 6000 allowed me to run bigger models but I noticed they ran kind of slow. At least with Llama I noticed it's not taking advantage of the architecture. I verified with Nvidia-smi that it was running on the card. The coding agent (llama-vscode, open-ai api) was dumber.&lt;/li&gt; &lt;li&gt;The dumber behavior was similar with freshly built VLLM and Open-Webui. Took so long to build PyTorch with the latest CUDA library to get it to work.&lt;/li&gt; &lt;li&gt;Switch back to the 3090 inside the Razer Core X and everything just works beautifully. The Qwen2.5 Coder 14B Instruct picked up on me converting c-style enums to C++ and it automatically suggested the next whole enum class vs Qwen 2.5 32B coder instruct FP16 and Q8.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I wasted way too much time (2 days?) rebuilding a bunch of libraries for Llama, VLM, etc.. to take advantage of RTX 6000 96GB. This includes time spent going the git issues with the RTX 6000. Don't get me started on some of these buggy/incorrect docker containers I tried to save build time. Props to LM studio for making using of the card though it felt dumber still.&lt;/p&gt; &lt;p&gt;Wish the A6000 and the 6000 ADA 48GB cards were cheaper though. I say if your time is a lot of money it's worth it for something that's stable, proven, and will work with all frameworks right out of the box.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lhd1j0/comment/mz3668v/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Proof&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: fixed typos. I suck at posting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T02:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhhs1r</id>
    <title>Best open agentic coding assistants that donâ€™t need an OpenAI key?</title>
    <updated>2025-06-22T07:03:21+00:00</updated>
    <author>
      <name>/u/Fabulous_Bluebird931</name>
      <uri>https://old.reddit.com/user/Fabulous_Bluebird931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for ai dev tools that actually let you use your own models, something agent-style that can analyse multiple files, track goals, and suggest edits/refactors, ideally all within vscode or terminal.&lt;/p&gt; &lt;p&gt;Iâ€™ve used Copilotâ€™s agent mode, but itâ€™s obviously tied to OpenAI. Iâ€™m more interested in&lt;/p&gt; &lt;p&gt;Tools that work with local models (via Ollama or similar)&lt;/p&gt; &lt;p&gt;API-pluggable setups (Gemini 1.5, deepseek, Qwen3, etc)&lt;/p&gt; &lt;p&gt;Agents that can track tasks, not just generate single responses&lt;/p&gt; &lt;p&gt;Iâ€™ve been trying Blackboxâ€™s vscode integration, which has some agentic behaviour now. Also tried cline and roo, which are promising for CLI work.&lt;/p&gt; &lt;p&gt;But most tools either&lt;/p&gt; &lt;p&gt;Require a paid key to do anything useful Arenâ€™t flexible with models&lt;/p&gt; &lt;p&gt;Or donâ€™t handle full-project context&lt;/p&gt; &lt;p&gt;anyone found a combo that works well with open models and integrates tightly with your coding environment? Not looking for prompt uis, looking for workflow tools please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Bluebird931"&gt; /u/Fabulous_Bluebird931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T07:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhdu5q</id>
    <title>The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers</title>
    <updated>2025-06-22T03:01:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt; &lt;img alt="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" src="https://external-preview.redd.it/luq7KUXprwBLhy7hPjQyYQsNRRg4Up-CaR0a3rU5fm0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71e8ed354e2ca20af1eb468a8e9d764bccbcb15a" title="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE - Someone has tested these models at FP16 on 3 attempts per problem versus my Q4_K_S on 1 attempt per problem. See the results here: &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B/discussions/2"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B/discussions/2&lt;/a&gt; Huge thanks to &lt;a href="https://huggingface.co/none-user"&gt;none-user&lt;/a&gt; for doing this! Both SLERP merges performed better than their parents, with the Qwen tokenizer based merge (Q3T) being the best of the bunch. I'm very surprised by how good these merges turned out. It seems to me the excellent results is a combination of these factors; both models not being just finetunes, but different fully trained models from the ground up using the same base model, and still sharing the same architecture, plus both tokenizers having nearly 100% vocab overlap. The qwen tokenizer being particularly more impressive makes the merge using this tokenizer the best of the bunch. This scored as well as qwen3 30b-a3b at q8_0 in the same test while using the same amount of tokens (see here for s qwen3 30b-a3b and gemma 3 27b &lt;a href="https://github.com/Belluxx/LocalAIME/blob/main/media/accuracy%5C_comparison.png"&gt;https://github.com/Belluxx/LocalAIME/blob/main/media/accuracy\_comparison.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I was interested in merging &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;DeepSeek-R1-0528-Qwen3-8B&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen3-8B&lt;/a&gt; as they were both my two favorite under 10b~ models, and finding the Deepseek distill especially impressive. Noted in their model card was the following:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which made me realize, they were both good merge candidates for each other, both being not finetunes, but fully trained models off the Qwen3-8B-Base, and even sharing the same favored sampler settings. The only real difference were the tokenizers. This took me to a crossroads, which tokenizer should my merge inherit? Asking around, I was told there shouldn't be much difference, but I ended up finding out very differently once I did some actual testing. The TL;DR is, the Qwen tokenizer seems to perform better &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; use far less tokens for it's thinking. It is a larger tokenizer I noted, and was told that means the tokenizer is more optimized, but I was skeptical about this and decided to test it.&lt;/p&gt; &lt;p&gt;This turned out not to be a not so easy endeavor, since the benchmark I decided on (LocalAIME by &lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; which I thank for making and sharing this tool), takes rather long to complete when you use a thinking model, since they require quite a few tokens to get to their answer with any amount of accuracy. I first tested with 4k context, then 8k, then briefly even 16k before realizing the LLM responses were still getting cut off, resulting in poor accuracy. GLM 9B did not have this issue, and used very few tokens in comparison even with context set to 30k. Testing took very long, but with the help of others from the KoboldAI server (shout out to everyone there willing to help, a lot of people volunteered their help, who I will accredit below), we were able to eventually get it done.&lt;/p&gt; &lt;p&gt;This is the most useful graph that came of this, you can see below models using the Qwen tokenizer used less tokens than any of the models using the Deepseek tokenizer, and had higher accuracy. Both merges also performed better than their same tokenizer parent model counterparts. I was actually surprised since I quite preferred the R1 Distill to the Qwen3 instruct model, and had thought it was better before this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lbpldqh57e8f1.png?width=2969&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41dd5f79caaa5a59c3e89cf26accf2b4fc062693"&gt;Model Performance VS Tokens Generated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would have liked to have tested at a higher precision, like Q8_0, and on more problem attempts (like 3-5) for better quality data but didn't have the means to. If anyone with the means to do so is interested in giving it a try, please feel free to reach out to me for help, or if anyone wants to loan me their hardware I would be more than happy to run the tests again under better settings.&lt;/p&gt; &lt;p&gt;For anyone interested, more information is available in the model cards of the merges I made, which I will link below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;w/ Qwen3 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;w/ Deepseek R1 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently only my own static GGUF quants are available (in Q4_K_S and Q8_0) but hopefully others will provide more soon enough.&lt;/p&gt; &lt;p&gt;I've stored all my raw data, and test results in a repository here: &lt;a href="https://github.com/lemon07r/LocalAIME_results"&gt;https://github.com/lemon07r/LocalAIME_results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Special Thanks to The Following People&lt;/strong&gt; (for making this possible)&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eisenstein for their modified fork of LocalAIME to work better with KoboldCPP and modified sampler settings for Qwen/Deepseek models, and doing half of my testing for me on his machine. Also helping me with a lot of my troubleshooting.&lt;/li&gt; &lt;li&gt;Twistedshadows for loaning me some of their runpod hours to do my testing.&lt;/li&gt; &lt;li&gt;Henky as well, for also loaning me some of their runpod hours, and helping me troubleshoot some issues with getting KCPP to work with LocalAIME&lt;/li&gt; &lt;li&gt;Everyone else on the KoboldAI discord server, there were more than a few willing to help me out in the way of advice, troubleshooting, or offering me their machines or runpod hours to help with testing if the above didn't get to it first.&lt;/li&gt; &lt;li&gt;&lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; for making and sharing his LocalAIME tool&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For full transparency, I do want to disclaim that this method isn't really an amazing way to test tokenizers against each other, since the deepseek part of the two merges are still trained using the deepseek tokenizer, and the qwen part with it's own tokenizer* (see below, turns out, this doesn't really apply here). You would have to train two different versions from the ground up using the different tokenizers on the same exact data to get a completely fair assessment. I still think this testing and further testing is worth doing to see how these merges perform in comparison to their parents, and under which tokenizer they perform better.&lt;/p&gt; &lt;p&gt;*EDIT - Under further investigation I've found the Deepseek tokenizer and qwen tokenizer have virtually a 100% vocab overlap, making them pretty much interchangeable, and using models trained using either the perfect candidates for testing both tokenizers against each other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhed49</id>
    <title>50 days building a tiny language model from scratch, what Iâ€™ve learned so far</title>
    <updated>2025-06-22T03:31:14+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Iâ€™m starting a new weekday series on June 23 at 9:00 AM PDT where Iâ€™ll spend 50 days coding a two LLM (15â€“30M parameters) from the ground up: no massive GPU cluster, just a regular laptop or modest GPU.&lt;/p&gt; &lt;p&gt;Each post will cover one topic:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data collection and subword tokenization&lt;/li&gt; &lt;li&gt;Embeddings and positional encodings&lt;/li&gt; &lt;li&gt;Attention heads and feed-forward layers&lt;/li&gt; &lt;li&gt;Training loops, loss functions, optimizers&lt;/li&gt; &lt;li&gt;Evaluation metrics and sample generation&lt;/li&gt; &lt;li&gt;Bonus deep dives: MoE, multi-token prediction,etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why bother with tiny models?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They run on the CPU.&lt;/li&gt; &lt;li&gt;You get daily feedback loops.&lt;/li&gt; &lt;li&gt;Building every component yourself cements your understanding.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Iâ€™ve already tried:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A 30 M-parameter GPT variant for childrenâ€™s stories&lt;/li&gt; &lt;li&gt;A 15 M-parameter DeepSeek model with Mixture-of-Experts&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Iâ€™ll drop links to the code in the first comment.&lt;/p&gt; &lt;p&gt;Looking forward to the discussion and to learning together. See you on Day 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmdzg</id>
    <title>We built a tool that helps you plan features before using AI to code (public beta launch)</title>
    <updated>2025-06-24T20:41:25+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"&gt; &lt;img alt="We built a tool that helps you plan features before using AI to code (public beta launch)" src="https://external-preview.redd.it/OTEzaXJneGJzeDhmMZvu8dVTETA4R0lbzOCMpCYSy2EGu4LkODdfToxoMFWa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1115bcb79b3be529b91bb0243365e1b51c25a6b1" title="We built a tool that helps you plan features before using AI to code (public beta launch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8td2dkxbsx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmzvi</id>
    <title>Falcon H1 Models</title>
    <updated>2025-06-24T21:05:04+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is this model family slept on ? From what i understood its a new hybrid architecture and it has alreally good results. Am i missing something? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljncfs</id>
    <title>What are your go-to models for daily use? Please also comment about your quantization of choice</title>
    <updated>2025-06-24T21:18:53+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ljncfs"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnoj7</id>
    <title>AMD Instinct MI60 (32gb VRAM) "llama bench" results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected</title>
    <updated>2025-06-24T21:32:35+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt; &lt;img alt="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" src="https://b.thumbs.redditmedia.com/IWP60MgSnWzXR7H6EGZicI90kN9NpZLeyDsSansVnlA.jpg" title="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just completed a new build and (finally) have everything running as I wanted it to when I spec'd out the build. I'll be making a separate post about that as I'm now my own sovereign nation state for media, home automation (including voice activated commands), security cameras and local AI which I'm thrilled about...but, like I said, that's for a separate post.&lt;/p&gt; &lt;p&gt;This one is with regard to the MI60 GPU which I'm very happy with given my use case. I bought two of them on eBay, got one for right around $300 and the other for just shy of $500. Turns out I only need one as I can fit both of the models I'm using (one for HomeAssistant and the other for Frigate security camera feed processing) onto the same GPU with more than acceptable results. I might keep the second one for other models, but for the time being it's not installed. &lt;strong&gt;EDIT:&lt;/strong&gt; Forgot to mention I'm running Ubuntu 24.04 on the server.&lt;/p&gt; &lt;p&gt;For HomeAssistant I get results back in less than two seconds for voice activated commands like &amp;quot;it's a little dark in the living room and the cats are meowing at me because they're hungry&amp;quot; (it brightens the lights and feeds the cats, obviously). For Frigate it takes about 10 seconds after a camera has noticed an object of interest to return back what was observed (here is a copy/paste of an example of data returned from one of my camera feeds: &amp;quot;&lt;em&gt;Person detected. The person is a man wearing a black sleeveless top and red shorts. He is standing on the deck holding a drink. Given their casual demeanor this does not appear to be suspicious.&lt;/em&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Notes about the setup for the GPU, for some reason I'm unable to get the powercap set to anything higher than 225w (I've got a 1000w PSU, I've tried the physical switch on the card, I've looked for different vbios versions for the card and can't locate any...it's frustrating, but is what it is...it's supposed to be a 300tdp card). I was able to slightly increase it because while it won't allow me to change the powercap to anything higher, I was able to set the &amp;quot;overdrive&amp;quot; to allow for a 20% increase. With the cooling shroud for the GPU (photo at bottom of post) even at full bore, the GPU has never gone over 64 degrees Celsius&lt;/p&gt; &lt;p&gt;Here are some &amp;quot;llama-bench&amp;quot; results of various models that I was testing before settling on the two I'm using (noted below):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | pp512 | 581.33 Â± 0.16 | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | tg128 | 64.82 Â± 0.04 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | pp512 | 587.76 Â± 1.04 | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | tg128 | 43.50 Â± 0.18 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Hermes-3-Llama-3.1-8B.Q8_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Hermes-3-Llama-3.1-8B.Q8_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | pp512 | 582.56 Â± 0.62 | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | tg128 | 52.94 Â± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Meta-Llama-3-8B-Instruct.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Meta-Llama-3-8B-Instruct.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | pp512 | 1214.07 Â± 1.93 | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | tg128 | 70.56 Â± 0.12 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | pp512 | 420.61 Â± 0.18 | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | tg128 | 31.03 Â± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | pp512 | 188.13 Â± 0.03 | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | tg128 | 27.37 Â± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | pp512 | 257.37 Â± 0.04 | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | tg128 | 17.65 Â± 0.02 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;nexusraven-v2-13b.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/nexusraven-v2-13b.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | pp512 | 704.18 Â± 0.29 | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | tg128 | 52.75 Â± 0.07 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-30B-A3B-Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | pp512 | 1165.52 Â± 4.04 | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | tg128 | 68.26 Â± 0.13 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_1.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-32B-Q4_1.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | pp512 | 270.18 Â± 0.14 | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | tg128 | 21.59 Â± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a photo of the build for anyone interested (total of 11 drives, a mix of NVME, HDD and SSD):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0"&gt;https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmlcn</id>
    <title>Vision model for detecting welds?</title>
    <updated>2025-06-24T20:49:23+00:00</updated>
    <author>
      <name>/u/-Fake_GTD</name>
      <uri>https://old.reddit.com/user/-Fake_GTD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I searched for &amp;quot;best vision models&amp;quot; up to date, but are there any difference between industry applications and &amp;quot;document scanning&amp;quot; models? Should we proceed to fine-tine them with photos to identify correct welds vs incorrect welds?&lt;/p&gt; &lt;p&gt;Can anyone guide us regarding vision model in industry applications (mainly construction industry)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Fake_GTD"&gt; /u/-Fake_GTD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljhg1i</id>
    <title>I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other.</title>
    <updated>2025-06-24T17:33:15+00:00</updated>
    <author>
      <name>/u/CharlesStross</name>
      <uri>https://old.reddit.com/user/CharlesStross</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt; &lt;img alt="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." src="https://external-preview.redd.it/rna5zREg5_FzFmMGv-Mzfn4pHDOOgy6GUqSdq0vIQVE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61a501a09debdd7a58e2f8b7a92cf6aad5a73838" title="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharlesStross"&gt; /u/CharlesStross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jkingsman/resonant-chat-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T17:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlyrs</id>
    <title>Is it normal to have significantly more performance from Qwen 235B compared to Qwen 32B when doing partial offloading?</title>
    <updated>2025-06-24T20:24:47+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded than for the entire 32B model.here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded to the cpu than for the entire 32B model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; &amp;quot;Qwen3:32B&amp;quot;: proxy: http://127.0.0.1:9995 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9995 --no-webui --no-warmup --model ~/raid/models/Qwen3-32B-Q4_K_M.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 34 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 &amp;quot;Qwen3:235B&amp;quot;: proxy: http://127.0.0.1:9993 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9993 --no-webui --no-warmup --model ~/raid/models/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 95 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 --override-tensor exps=CPU &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn09w</id>
    <title>I made a free iOS app for people who run LLMs locally. Itâ€™s a chatbot that you can use away from home to interact with an LLM that runs locally on your desktop Mac.</title>
    <updated>2025-06-24T21:05:30+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is easy enough that anyone can use it. No tunnel or port forwarding needed.&lt;/p&gt; &lt;p&gt;The app is called LLM Pigeon and has a companion app called LLM Pigeon Server for Mac.&lt;br /&gt; It works like a carrier pigeon :). It uses iCloud to append each prompt and response to a file on iCloud.&lt;br /&gt; Itâ€™s not totally local because iCloud is involved, but I trust iCloud with all my files anyway (most people do) and I donâ€™t trust AI companies. &lt;/p&gt; &lt;p&gt;The iOS app is a simple Chatbot app. The MacOS app is a simple bridge to LMStudio or Ollama. Just insert the model name you are running on LMStudio or Ollama and itâ€™s ready to go.&lt;br /&gt; I also added 5 in-built models so even people who are not familiar with Ollama or LMStudio can use this.&lt;/p&gt; &lt;p&gt;I find it super cool that I can chat anywhere with Qwen3-30B running on my Mac at home. &lt;/p&gt; &lt;p&gt;The apps are open source and these are the repos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They are both on the App Store. Here are the links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS. I hope this isn't viewed as self promotion because the app is free, collects no data and is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnmj9</id>
    <title>Google researcher requesting feedback on the next Gemma.</title>
    <updated>2025-06-24T21:30:18+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt; &lt;img alt="Google researcher requesting feedback on the next Gemma." src="https://a.thumbs.redditmedia.com/YXztzxUAkpa8OQtPRt3lxinca8NVcah5DIxz1ZPOgn4.jpg" title="Google researcher requesting feedback on the next Gemma." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kr52i2mn0y8f1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f654b4d8fc807a8722055201e8c097168452937f"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/osanseviero/status/1937453755261243600"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm gpu poor. 8-12B models are perfect for me. What are yout thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm13j</id>
    <title>0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?</title>
    <updated>2025-06-24T20:27:16+00:00</updated>
    <author>
      <name>/u/BasicCoconut9187</name>
      <uri>https://old.reddit.com/user/BasicCoconut9187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt; &lt;img alt="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" src="https://b.thumbs.redditmedia.com/YwYzRHAaFEgB8IqsgHGymFQqUiU6aiL80kikS_RSFhM.jpg" title="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/bgzyj1lypx8f1.gif"&gt;Now I've got your attention, I hope!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi there everyone!&lt;/p&gt; &lt;p&gt;I've just recently assembled an entire home server system, however, for some reason, the performance I'm getting is atrocious with 1TB of DDR4 2400MHz RAM on EPYC 7C13 running on Gigabyte MZ32-AR1. I'm getting 1-3 tok/s on prompt eval (depending on context), and 0.3-0.6 tok/s generation.&lt;/p&gt; &lt;p&gt;Now, the model I'm running is Ubergarm's R1 0528 IQ4_KS_R4, on ik_llama, so that's a bit different than what a lot of people here are running. However, on the more 'standard' R1 GGUFs from Unsloth, the performance is even worse, and that's true across everything I've tried, Kobold.cpp, LMstudio, Ollama, etc. True of other LLMs as well such as Qwen, people report way better tok/s with the same/almost the same CPU and system.&lt;/p&gt; &lt;p&gt;So, here's my request, if anyone is in the know, can you please share the BIOS options that I should use to optimize this CPU for LLM interference? I'm ready to sacrifice pretty much any setting/feature if that means I will be able to get this running in line with what other people online are getting.&lt;/p&gt; &lt;p&gt;Also, I know what you think, the model is entirely mlock'ed and is using 128 threads, my OS is Ubuntu 25.04, and other than Ubuntu's tendency to set locked memory to just 128 or so gigs every time I reboot which can be simply fixed with sudo su and then ulimit -Hl and -l, I don't seem to have any issues on the OS side, so that's where my entire guess of this being the BIOS settings fault comes from.&lt;/p&gt; &lt;p&gt;Thank you so much for reading all of this, and have a great day!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BasicCoconut9187"&gt; /u/BasicCoconut9187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn4h8</id>
    <title>Why is my llama so dumb?</title>
    <updated>2025-06-24T21:10:06+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: DeepSeek R1 Distill Llama 70B&lt;/p&gt; &lt;p&gt;GPU+Hardware: Vulkan on AMD AI Max+ 395 128GB VRAM &lt;/p&gt; &lt;p&gt;Program+Options:&lt;br /&gt; - GPU Offload Max&lt;br /&gt; - CPU Thread Pool Size 16&lt;br /&gt; - Offload KV Cache: Yes&lt;br /&gt; - Keep Model in Memory: Yes&lt;br /&gt; - Try mmap(): Yes&lt;br /&gt; - K Cache Quantization Type: Q4_0 &lt;/p&gt; &lt;p&gt;So the question is, when asking basic questions, it consistently gets the answer wrong. And does a whole lot of that &amp;quot;thinking&amp;quot;: &lt;/p&gt; &lt;p&gt;&amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Okay so i'm trying to understand&amp;quot;&lt;br /&gt; etc&lt;br /&gt; etc. &lt;/p&gt; &lt;p&gt;I'm not complaining about speed. More that the accuracy for something as basic as &amp;quot;explain this common linux command&amp;quot; and it is super wordy and then ultimately comes to the wrong conclusion. &lt;/p&gt; &lt;p&gt;I'm using LM Studio btw. &lt;/p&gt; &lt;p&gt;Is there a good primer for setting these LLMs up for success? What do you recommend? Have I done something stupid myself?&lt;br /&gt; Thanks in advance for any help/suggestions! &lt;/p&gt; &lt;p&gt;p.s. I do plan on running and testing ROCm, but i've only got so much time in a day and i'm a newbie to the LLM space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm32s</id>
    <title>Agent Arena â€“ crowdsourced testbed for evaluating AI agents in the wild</title>
    <updated>2025-06-24T20:29:27+00:00</updated>
    <author>
      <name>/u/tejpal-obl</name>
      <uri>https://old.reddit.com/user/tejpal-obl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched Agent Arena -- a crowdsourced testbed for evaluating AI agents in the wild. Think Chatbot Arena, but for agents.&lt;/p&gt; &lt;p&gt;Itâ€™s completely free to run matches. We cover the inference.&lt;/p&gt; &lt;p&gt;I always find myself debating whether to use 4o or o3, but now I just try both on Agent Arena!&lt;/p&gt; &lt;p&gt;Try it out: &lt;a href="https://obl.dev/"&gt;https://obl.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tejpal-obl"&gt; /u/tejpal-obl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm2n2</id>
    <title>Polaris: A Post-training recipe for scaling RL on Advanced ReasonIng models</title>
    <updated>2025-06-24T20:28:55+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ChenxinAn-fdu/POLARIS"&gt;Here is the link.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have no idea what it is but it was released a few days ago and has an intriguing concept so I decided to post here to see if anyone knows about this. It seems pretty new but its some sort of post-training RL with a unique approach that claims a Qwen3-4b performance boost that surpasses Claude-4-Opus, Grok-3-Beta, and o3-mini-high.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt. I am not in any way affiliated with this project. Someone simply recommended it to me so I posted it here to gather your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
</feed>
