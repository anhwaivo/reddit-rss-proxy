<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-23T17:22:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jhfz4n</id>
    <title>What's the status of using a local LLM for software development?</title>
    <updated>2025-03-22T19:05:30+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please help an old programmer navigate the maze that is the current LLM-enabled SW stacks.&lt;/p&gt; &lt;p&gt;I'm sure that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;I won't use Claude or any online LLM&lt;/strong&gt;. Just a local model that is small enough to leave enough room for context (eg Qwen2.5 Coder 14B).&lt;/li&gt; &lt;li&gt;I need a tool that can feed an entire project to an LLM as context.&lt;/li&gt; &lt;li&gt;I know how to code but want to use an LLM to do the boilerplate stuff, not to take full control of a project.&lt;/li&gt; &lt;li&gt;Preferably FOSS.&lt;/li&gt; &lt;li&gt;Preferably integrated into a solid IDE, rather then being standalone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji2xh3</id>
    <title>What would you consider great small models for information summarization that could fit in 8GB of VRAM?</title>
    <updated>2025-03-23T16:17:20+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious what would be considered some of the strongest smaller models that could fit in 8GB of VRAM these days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2xh3/what_would_you_consider_great_small_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2xh3/what_would_you_consider_great_small_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2xh3/what_would_you_consider_great_small_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T16:17:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh7c6e</id>
    <title>My 4x3090 eGPU collection</title>
    <updated>2025-03-22T12:28:25+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt; &lt;img alt="My 4x3090 eGPU collection" src="https://b.thumbs.redditmedia.com/tuwbOdIfpLg-K_qo2ArzC4oMvVIIdECI4tmNxUjTuKA.jpg" title="My 4x3090 eGPU collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 more 3090s ready to hook up to the 2nd Thunderbolt port in the back when I get the UT4g docks in. &lt;/p&gt; &lt;p&gt;Will need to find an area with more room though ðŸ˜…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh7c6e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T12:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji3xr3</id>
    <title>Best LM Studio model for 12GB VRAM and Python?</title>
    <updated>2025-03-23T17:00:43+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basicaly title - best LM Studio model for 12GB VRAM and Python with large context and output ? I'm having trouble generating ChatGPT and Deepseek over 25kB size of python scripts (over this I'm getting broken scripts). Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji3xr3/best_lm_studio_model_for_12gb_vram_and_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji3xr3/best_lm_studio_model_for_12gb_vram_and_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji3xr3/best_lm_studio_model_for_12gb_vram_and_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T17:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhxiei</id>
    <title>Ways the batch generate embeddings (python). is vLLM the only way?</title>
    <updated>2025-03-23T11:51:05+00:00</updated>
    <author>
      <name>/u/Moreh</name>
      <uri>https://old.reddit.com/user/Moreh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;as per title. I am trying to use vLLM but it doesnt play nice with those that are GPU poor!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreh"&gt; /u/Moreh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpgum</id>
    <title>Llama 3.3 70B vs Nemotron Super 49B (Based on Lllama 3.3)</title>
    <updated>2025-03-23T02:43:49+00:00</updated>
    <author>
      <name>/u/Prestigious-Use5483</name>
      <uri>https://old.reddit.com/user/Prestigious-Use5483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys like using better? I haven't tested Nemotron Super 49B much, but I absolute loved llama 3.3 70B. Please share the reason you prefer one over the other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Use5483"&gt; /u/Prestigious-Use5483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T02:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhrg80</id>
    <title>I updated Deep Research at Home to collect user input and output way better reports. Here's a PDF of a search in action</title>
    <updated>2025-03-23T04:39:55+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sapphire-maryrose-59.tiiny.site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrg80/i_updated_deep_research_at_home_to_collect_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrg80/i_updated_deep_research_at_home_to_collect_user/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T04:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhdpjk</id>
    <title>Fallen Gemma3 4B 12B 27B - An unholy trinity with no positivity! For users, mergers and cooks!</title>
    <updated>2025-03-22T17:27:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a complete decensor tune, but it should be absent of positivity.&lt;/p&gt; &lt;p&gt;Vision works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhf6x3</id>
    <title>Has anyone switched from remote models (claude, etc.) models to local? Meaning did your investment pay off?</title>
    <updated>2025-03-22T18:31:34+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously a 70b or 32b model won't be as good as Claude API, on the other hand, many are spending $10 to $30+ per day on the API, so it could be a lot cheaper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6lsx</id>
    <title>OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision.</title>
    <updated>2025-03-22T11:44:18+00:00</updated>
    <author>
      <name>/u/lessis_amess</name>
      <uri>https://old.reddit.com/user/lessis_amess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt; &lt;img alt="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." src="https://preview.redd.it/x942twbra8qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79683f47809a02571ff90500acb5d28a046d6940" title="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;O1 Pro costs 33 times more than Claude 3.7 Sonnet, yet in many cases delivers less capability. GPT-4.5 costs 25 times more and itâ€™s an old model with a cut-off date from November.&lt;/p&gt; &lt;p&gt;Why release old, overpriced models to developers who care most about cost efficiency?&lt;/p&gt; &lt;p&gt;This isn't an accident.&lt;/p&gt; &lt;p&gt;It's anchoring.&lt;/p&gt; &lt;p&gt;Anchoring works by establishing an initial reference point. Once that reference exists, subsequent judgments revolve around it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Show something expensive.&lt;/li&gt; &lt;li&gt;Show something less expensive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The second thing seems like a bargain.&lt;/p&gt; &lt;p&gt;The expensive API models reset our expectations. For years, AI got cheaper while getting smarter. OpenAI wants to break that pattern. They're saying high intelligence costs money. Big models cost money. They're claiming they don't even profit from these prices.&lt;/p&gt; &lt;p&gt;When they release their next frontier model at a &amp;quot;lower&amp;quot; price, you'll think it's reasonable. But it will still cost more than what we paid before this reset. The new &amp;quot;cheap&amp;quot; will be expensive by last year's standards.&lt;/p&gt; &lt;p&gt;OpenAI claims these models lose money. Maybe. But they're conditioning the market to accept higher prices for whatever comes next. The API release is just the first move in a longer game.&lt;/p&gt; &lt;p&gt;This was not a confused move. Itâ€™s smart business. (i'm VERY happy we have open-source)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro"&gt;https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lessis_amess"&gt; /u/lessis_amess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x942twbra8qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhx20u</id>
    <title>14B @ 8Bit or 27B @ 4Bit -- T/s, quality of response, max context size in VRAM limits</title>
    <updated>2025-03-23T11:21:17+00:00</updated>
    <author>
      <name>/u/Professional_Row_967</name>
      <uri>https://old.reddit.com/user/Professional_Row_967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL'DR: 14B Model @ 8bit or 27B Model @ 4bit is likely to be better&lt;/p&gt; &lt;p&gt;Short of running extensive benchmarks, just casual observation using limited test scenarios might not reveal the right picture, so wondering if there any well-established consensus already in the community around this, i.e. which of the 2 models is going to perform better, 14B model (say gemma3) with 8bit quantization or 27B model with 4bit quantization under following constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM limited to max 20GB (basically 20GB out of 24GB URAM of Mac M4 mini)&lt;/li&gt; &lt;li&gt;Need large context window (min 32K but in some cases perhaps 64K or even 128K, VRAM permitting, but also with acceptable output token/sec)&lt;/li&gt; &lt;li&gt;Quality of response (hallucination, relevance, repetition, bias, contextual understanding issues etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Can the answers be safely considered to be pretty much true for other models (say phi4, or llama-3.3) as well ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Row_967"&gt; /u/Professional_Row_967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhhsgv</id>
    <title>Qwen2.5-Omni Incoming? Huggingface Transformers PR 36752</title>
    <updated>2025-03-22T20:25:00+00:00</updated>
    <author>
      <name>/u/Inevitable_Sea8804</name>
      <uri>https://old.reddit.com/user/Inevitable_Sea8804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Haven't seen anyone bring this up, so making a post here...&lt;/p&gt; &lt;p&gt;Using DeepSeek-R1 to summarize the features of this model based on PR commits:&lt;/p&gt; &lt;hr /&gt; &lt;h1&gt;&lt;strong&gt;Qwen2.5-Omni Technical Summary&lt;/strong&gt;&lt;/h1&gt; &lt;h2&gt;&lt;strong&gt;1. Basic Information&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Scale&lt;/strong&gt;: 7B parameter version (&amp;quot;Qwen/Qwen2.5-Omni-7B&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Fully open-sourced under Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;2. Input/Output Modalities&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language instructions&lt;/li&gt; &lt;li&gt;&lt;em&gt;Images&lt;/em&gt;: Common formats (JPEG/PNG)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Audio&lt;/em&gt;: WAV/MP3 (requires FFmpeg)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Video&lt;/em&gt;: MP4 with audio track extraction&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Capabilities&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language responses&lt;/li&gt; &lt;li&gt;&lt;em&gt;Speech&lt;/em&gt;: 24kHz natural speech (streaming supported)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;3. Architectural Design&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Encoder&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Block-wise Processing&lt;/em&gt;: Decouples long-sequence handling between encoder (perception) and LLM (sequence modeling)&lt;/li&gt; &lt;li&gt;&lt;em&gt;TMRoPE&lt;/em&gt;: Time-aligned Multimodal Rotary Positional Encoding for audio-video synchronization&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual-path Generation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Thinker&lt;/em&gt;: Text-generating LLM backbone&lt;/li&gt; &lt;li&gt;&lt;em&gt;Talker&lt;/em&gt;: Dual-track AR model for audio token generation using Thinker's hidden states&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming Optimization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Sliding-window Diffusion Transformer (DiT) reduces audio latency&lt;/li&gt; &lt;li&gt;Simultaneous text/speech streaming output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;4. Technical Highlights&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Processing&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;End-to-end joint training without intermediate representations&lt;/li&gt; &lt;li&gt;Supports arbitrary modality combinations (single/mixed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Attention&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Native FlashAttention 2 support&lt;/li&gt; &lt;li&gt;Compatible with PyTorch SDPA&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Customization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Prebuilt voices: &lt;code&gt;Cherry&lt;/code&gt; (female) &amp;amp; &lt;code&gt;Ethan&lt;/code&gt; (male)&lt;/li&gt; &lt;li&gt;Dynamic voice switching via &lt;code&gt;spk&lt;/code&gt; parameter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment Flexibility&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Disable speech output to save VRAM (~2GB)&lt;/li&gt; &lt;li&gt;Text-only mode (&lt;code&gt;return_audio=False&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;5. Performance&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Benchmarks&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;SOTA on Omni-Bench&lt;/li&gt; &lt;li&gt;Outperforms same-scale Qwen2-VL/Qwen2-Audio in vision/audio tasks&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Understanding&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;First open-source model with text-level E2E speech instruction following&lt;/li&gt; &lt;li&gt;Matches text-input performance on MMLU/GSM8K with speech inputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;6. Implementation Details&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Auto device mapping (&lt;code&gt;device_map=&amp;quot;auto&amp;quot;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Mixed precision (&lt;code&gt;bfloat16/float16&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processing Pipeline&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Unified &lt;code&gt;Qwen2_5OmniProcessor&lt;/code&gt; handles multimodal inputs&lt;/li&gt; &lt;li&gt;Batch processing of mixed media combinations&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;7. Requirements&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;System Prompt&lt;/strong&gt;: Mandatory for full functionality:&lt;br /&gt; &lt;code&gt; &amp;quot;You are Qwen... capable of generating text and speech.&amp;quot; &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;FlashAttention 2 (optional acceleration)&lt;/li&gt; &lt;li&gt;FFmpeg (video/non-WAV audio processing)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;This architecture achieves deep multimodal fusion through innovative designs while maintaining strong text capabilities, significantly advancing audiovisual understanding/generation for multimodal agent development.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Also from the PR:&lt;/p&gt; &lt;p&gt;&lt;em&gt;We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omniâ€™s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can the community help confirm whether this PR is legit?&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(Original PR: &lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Sea8804"&gt; /u/Inevitable_Sea8804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T20:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6y0</id>
    <title>Are any of the big API providers (OpenAI, Anthropic, etc) actually making money, or are all of them operating at a loss and burning through investment cash?</title>
    <updated>2025-03-22T23:03:34+00:00</updated>
    <author>
      <name>/u/AnticitizenPrime</name>
      <uri>https://old.reddit.com/user/AnticitizenPrime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a consensus right now that local LLMs are not cheaper to run than the myriad of APIs out there at this time, when you consider the initial investment in hardware, the cost of energy, etc. The reasons for going local are for privacy, independence, hobbyism, tinkering/training your own stuff, working offline, or just the wow factor of being able to hold a conversation with your GPU.&lt;/p&gt; &lt;p&gt;But is that necessarily the case? Is it possible that these low API costs are unsustainable in the long term?&lt;/p&gt; &lt;p&gt;Genuinely curious. As far as I know, no LLM provider has turned a profit thus far, but I'd welcome a correction if I'm wrong.&lt;/p&gt; &lt;p&gt;I'm just wondering if the conception that 'local isn't as cheap as APIs' might not hold true anymore after all the investment money dries up and these companies need to actually price their API usage in a way that keeps the lights on and the GPUs going brrr.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnticitizenPrime"&gt; /u/AnticitizenPrime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhuow6</id>
    <title>Looks like RWKV v7 support is in llama now?</title>
    <updated>2025-03-23T08:30:43+00:00</updated>
    <author>
      <name>/u/TJSnider1984</name>
      <uri>https://old.reddit.com/user/TJSnider1984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12412"&gt;https://github.com/ggml-org/llama.cpp/pull/12412&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll have to build it and see..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TJSnider1984"&gt; /u/TJSnider1984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T08:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhyg3l</id>
    <title>Nvidia Jetson Thor AGX specs</title>
    <updated>2025-03-23T12:45:16+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;@SureshotM6 who attend to GTC &amp;quot;An Introduction to Building Humanoid Robots&amp;quot; reported Jetson Thor AGX specs:&lt;/p&gt; &lt;p&gt;â€¢ Available in June 2025&lt;/p&gt; &lt;p&gt;â€¢ 2560 CUDA cores, 96 Tensor cores (+25% from Orin AGX)&lt;/p&gt; &lt;p&gt;â€¢ 7.8 FP32 TFLOPS (47% faster than Jetson Orin AGX at 5.32 FP32 TFLOPS)&lt;/p&gt; &lt;p&gt;â€¢ 2000 FP4 TOPS&lt;/p&gt; &lt;p&gt;â€¢ 1000 FP8 TOPS (Orin AGX is 275 INT8 TOPS; Blackwell has same INT8/FP8 performance)&lt;/p&gt; &lt;p&gt;â€¢ 14 ARMv9 cores at 2.6x performance of Orin cores (Orin has 12 cores)&lt;/p&gt; &lt;p&gt;â€¢ 128GB of RAM (Orin AGX is 64GB)&lt;/p&gt; &lt;p&gt;â€¢ 273GB/s RAM bandwidth (33% faster than Orin AGX at 204.8GB/s)&lt;/p&gt; &lt;p&gt;â€¢ 120W max power (double Orin AGX at 60W)&lt;/p&gt; &lt;p&gt;â€¢ 4x 25GbE&lt;/p&gt; &lt;p&gt;â€¢ 1x 5GbE (at least present on devkit)&lt;/p&gt; &lt;p&gt;â€¢ 12 lanes PCle Gen5 (32GT/s per lane).&lt;/p&gt; &lt;p&gt;â€¢ 100mm x 87mm (same as existing AGX)&lt;/p&gt; &lt;p&gt;â€¢ All 1/O interfaces for devkit &amp;quot;on one side of board&amp;quot;&lt;/p&gt; &lt;p&gt;â€¢ Integrated 1TB NVMe storage on devkit&lt;/p&gt; &lt;p&gt;As I told in my post on DGX Sparks, it is really similar to Jetson, while one is designed for on premise, jetson are made for embedded&lt;/p&gt; &lt;p&gt;The number of Cuda core and tensor core could give us some hints on the DGX Sparks number that's still not release&lt;/p&gt; &lt;p&gt;The OS is not specified but it will be probably Jetpack (Jetson Linux/Ubuntu based with librairies for AI)&lt;/p&gt; &lt;p&gt;Note: With enhancement on Nvidia arm based hardware we should see more aarch64 and wheel software&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhyg3l/nvidia_jetson_thor_agx_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T12:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhrqov</id>
    <title>Here's another AMD Strix Halo Mini PC announcement with video of it running a 70B Q8 model.</title>
    <updated>2025-03-23T04:58:43+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the Sixunited 395+ Mini PC. It's also supposed to come out in May. It's all in Chinese. I do see what appears to be 3 token scroll across the screen. Which I assume means it's 3tk/s. Considering it's a 70GB model, that makes sense considering the memory bandwidth of Strix Halo.&lt;/p&gt; &lt;p&gt;The LLM stuff starts at about the 4 min mark.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1xhKsenE4T"&gt;https://www.bilibili.com/video/BV1xhKsenE4T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T04:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhgpew</id>
    <title>nsfw orpheus tts?</title>
    <updated>2025-03-22T19:37:43+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im currently in the data curation / filtering / cleaning phase&lt;/p&gt; &lt;p&gt;but i would like to see how many local guys would be interested in a tts for there anime waifus that can make &amp;quot;interesting&amp;quot; emotional noises&lt;/p&gt; &lt;p&gt;Total audio events found: 181218&lt;/p&gt; &lt;p&gt;(sighs): 8594&lt;/p&gt; &lt;p&gt;(laughs): 68590&lt;/p&gt; &lt;p&gt;(gasps): 14113&lt;/p&gt; &lt;p&gt;(moans): 20576&lt;/p&gt; &lt;p&gt;(whimpers): 418&lt;/p&gt; &lt;p&gt;(breathing): 114&lt;/p&gt; &lt;p&gt;(pants): 776&lt;/p&gt; &lt;p&gt;and many more ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhsqlr</id>
    <title>How does Groq.com do it? (Groq not Elon's grok)</title>
    <updated>2025-03-23T06:05:49+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does groq run llms so fast? Is it just very high power or they use some technique?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T06:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6jp</id>
    <title>Gemma3 is outperforming a ton of models on fine-tuning / world knowledge</title>
    <updated>2025-03-22T23:03:02+00:00</updated>
    <author>
      <name>/u/fluxwave</name>
      <uri>https://old.reddit.com/user/fluxwave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt; &lt;img alt="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" src="https://external-preview.redd.it/XifaOkXuUsJa3iGAsHuitV7h5kD9H1PhRfgSOnPUnbc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1663259cbca9e5b87b6c2d99b3d23f12f5a2118" title="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea"&gt;https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At fine-tuning they seem to be smashing evals -- see this tweet above from OpenPipe. &lt;/p&gt; &lt;p&gt;Then in world-knowledge (or at least this smaller task of identifying the gender of scholars across history) a 12B model beat OpenAI's gpt-4o-mini. This is using no fine-tuning. &lt;a href="https://thedataquarry.com/blog/using-llms-to-enrich-datasets/"&gt;https://thedataquarry.com/blog/using-llms-to-enrich-datasets/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p11ujen8nbqe1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897f8506ee01cffcbad459d11da436a2e1521501"&gt;Written by Prashanth Rao&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(disclaimer: Prashanth is a member of the BAML community -- our prompting DSL / toolchain &lt;a href="https://github.com/BoundaryML/baml"&gt;https://github.com/BoundaryML/baml&lt;/a&gt; , but he works at KuzuDB).&lt;/p&gt; &lt;p&gt;Has anyone else seen amazing results with Gemma3? Curious to see if people have tried it more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fluxwave"&gt; /u/fluxwave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji2grb</id>
    <title>A770 vs 9070XT benchmarks</title>
    <updated>2025-03-23T15:57:27+00:00</updated>
    <author>
      <name>/u/DurianyDo</name>
      <uri>https://old.reddit.com/user/DurianyDo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;9900X, X870, 96GB 5200MHz CL40, Sparkle Titan OC edition, Gigabyte Gaming OC.&lt;/p&gt; &lt;p&gt;Ubuntu 24.10 default drivers for AMD and Intel&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks with Flash Attention:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;./llama-bench -ngl 100 -fa 1 -t 24 -m &amp;quot;~/Mistral-Small-24B-Instruct-2501-Q4_K_L.gguf&amp;quot; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;type&lt;/th&gt; &lt;th align="left"&gt;A770&lt;/th&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;30.83&lt;/td&gt; &lt;td align="left"&gt;248.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.48&lt;/td&gt; &lt;td align="left"&gt;19.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;./llama-bench -ngl 100 -fa 1 -t 24 -m &amp;quot;~/Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf&amp;quot;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;type&lt;/th&gt; &lt;th align="left"&gt;A770&lt;/th&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;93.08&lt;/td&gt; &lt;td align="left"&gt;412.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;16.59&lt;/td&gt; &lt;td align="left"&gt;30.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;...and then during benchmarking I found that there's more performance without FA :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;9070XT Without Flash Attention:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;./llama-bench -m &amp;quot;Mistral-Small-24B-Instruct-2501-Q4_K_L.gguf&amp;quot; and ./llama-bench -m &amp;quot;Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf&amp;quot; &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;9070XT&lt;/th&gt; &lt;th align="left"&gt;Mistral-Small-24B-I-Q4KL&lt;/th&gt; &lt;th align="left"&gt;Llama-3.1-8B-I-Q5KS&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;No FA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;451.34&lt;/td&gt; &lt;td align="left"&gt;1268.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;33.55&lt;/td&gt; &lt;td align="left"&gt;84.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;With FA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;248.07&lt;/td&gt; &lt;td align="left"&gt;412.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.28&lt;/td&gt; &lt;td align="left"&gt;30.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DurianyDo"&gt; /u/DurianyDo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji2grb/a770_vs_9070xt_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T15:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhzd0x</id>
    <title>Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)</title>
    <updated>2025-03-23T13:33:25+00:00</updated>
    <author>
      <name>/u/SamchonFramework</name>
      <uri>https://old.reddit.com/user/SamchonFramework</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"&gt; &lt;img alt="Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)" src="https://external-preview.redd.it/z2FVrkSNjmQYb0AhBKdMT7w4IJXCJkDclbDm6XF3Sl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ff6f86ee0f4f38f43271a3071e071d123fc574" title="Accomplished Agentic AI by DDD (Document Driven Development) and CDD (Compiler Driven Development)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SamchonFramework"&gt; /u/SamchonFramework &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wrtnlabs.io/agentica/docs/concepts/document-driven-development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhzd0x/accomplished_agentic_ai_by_ddd_document_driven/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T13:33:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji32vh</id>
    <title>Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO</title>
    <updated>2025-03-23T16:23:49+00:00</updated>
    <author>
      <name>/u/KTibow</name>
      <uri>https://old.reddit.com/user/KTibow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"&gt; &lt;img alt="Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO" src="https://external-preview.redd.it/TAZjkkE0Ie6I8uHJeC_LAchU08G_51bEcBfU7K5RFMM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d788038bc811df56653c5f2bd40a0746e3f2efe" title="Understanding R1-Zero-Like Training - Deepseek v3 and Qwen can reason without RL, GRPO has a bug, and introducing Dr. GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KTibow"&gt; /u/KTibow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sail-sg/understand-r1-zero"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji32vh/understanding_r1zerolike_training_deepseek_v3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T16:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv57y</id>
    <title>Finally some good news for older hardware pricing</title>
    <updated>2025-03-23T09:04:21+00:00</updated>
    <author>
      <name>/u/xlrz28xd</name>
      <uri>https://old.reddit.com/user/xlrz28xd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3"&gt;https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I said before that when Blackwell starts shipping in volume, you couldn't give Hoppers away,&amp;quot; he said at Nvidia's big AI conference Tuesday.&lt;/p&gt; &lt;p&gt;&amp;quot;There are circumstances where Hopper is fine,&amp;quot; he added. &amp;quot;Not many.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And then:&lt;/p&gt; &lt;p&gt;CFO Brian Olsavsky said on Amazon's earnings call last month that the company &amp;quot;observed an increased pace of technology development, particularly in the area of artificial intelligence and machine learning.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;As a result, we're decreasing the useful life for a subset of our servers and networking equipment from 6 years to 5 years, beginning in January 2025,&amp;quot; Olsavsky said, adding that this will cut operating income this year by about $700 million.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Then, more bad news: Amazon &amp;quot;early-retired&amp;quot; some of its servers and network equipment, Olsavsky said, adding that this &amp;quot;accelerated depreciation&amp;quot; cost about $920 million and that the company expects it will decrease operating income in 2025 by about $600 million.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xlrz28xd"&gt; /u/xlrz28xd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T09:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji0fwh</id>
    <title>Qwq gets bad reviews because it's used wrong</title>
    <updated>2025-03-23T14:25:50+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all, Loaded up with these parameters in ollama:&lt;/p&gt; &lt;p&gt;temperature 0.6&lt;br /&gt; top_p 0.95&lt;br /&gt; top_k 40&lt;br /&gt; repeat_penalty 1&lt;br /&gt; num_ctx 16,384 &lt;/p&gt; &lt;p&gt;Using a logic that does &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; feed the thinking proces into the context,&lt;br /&gt; Its the best local modal available right now, &lt;strong&gt;&lt;em&gt;I think I will die on this hill.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;But you can proof me wrong, tell me about a task or prompt another model can do better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ji0fwh/qwq_gets_bad_reviews_because_its_used_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T14:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhwr2p</id>
    <title>Next Gemma versions wishlist</title>
    <updated>2025-03-23T11:00:25+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Omar from the Gemma team. Few months ago, we &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hchoyy/open_models_wishlist/"&gt;asked for user feedback &lt;/a&gt;and incorporated it into Gemma 3: longer context, a smaller model, vision input, multilinguality, and so on, while doing a nice lmsys jump! We also made sure to collaborate with OS maintainers to have decent support at day-0 in your favorite tools, including vision in llama.cpp!&lt;/p&gt; &lt;p&gt;Now, it's time to look into the future. What would you like to see for future Gemma versions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:00:25+00:00</published>
  </entry>
</feed>
