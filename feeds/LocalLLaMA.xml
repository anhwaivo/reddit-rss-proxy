<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-25T16:25:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k6xczy</id>
    <title>Deepcogito Cogito v1 preview 14B Quantized Benchmark</title>
    <updated>2025-04-24T17:00:04+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm GPU poor (3060TI with 8GB VRAM) and started using the 14B Deepcogito model based on Qwen 2.5 after seeing their post.&lt;/p&gt; &lt;p&gt;Best Quantization I can use with a decent speed is Q5K_S with a a generation speed varying from 5-10tk/s depending on the context.&lt;/p&gt; &lt;p&gt;From daily usage it seems great: great at instruction following, good text understanding, very good in multi language, not SOTA at coding but it is not my primary use case.&lt;/p&gt; &lt;p&gt;So I wanted to assess how the quant affected the performance and run a subset (9 hour of test) of MMLU-PRO (20%) to have an idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO (no reasoning)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;overall&lt;/th&gt; &lt;th align="left"&gt;biology&lt;/th&gt; &lt;th align="left"&gt;business&lt;/th&gt; &lt;th align="left"&gt;chemistry&lt;/th&gt; &lt;th align="left"&gt;computer science&lt;/th&gt; &lt;th align="left"&gt;economics&lt;/th&gt; &lt;th align="left"&gt;engineering&lt;/th&gt; &lt;th align="left"&gt;health&lt;/th&gt; &lt;th align="left"&gt;history&lt;/th&gt; &lt;th align="left"&gt;law&lt;/th&gt; &lt;th align="left"&gt;math&lt;/th&gt; &lt;th align="left"&gt;philosophy&lt;/th&gt; &lt;th align="left"&gt;physics&lt;/th&gt; &lt;th align="left"&gt;psychology&lt;/th&gt; &lt;th align="left"&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;69.32&lt;/td&gt; &lt;td align="left"&gt;81.12&lt;/td&gt; &lt;td align="left"&gt;71.97&lt;/td&gt; &lt;td align="left"&gt;68.14&lt;/td&gt; &lt;td align="left"&gt;74.39&lt;/td&gt; &lt;td align="left"&gt;82.14&lt;/td&gt; &lt;td align="left"&gt;56.48&lt;/td&gt; &lt;td align="left"&gt;71.17&lt;/td&gt; &lt;td align="left"&gt;67.11&lt;/td&gt; &lt;td align="left"&gt;54.09&lt;/td&gt; &lt;td align="left"&gt;78.89&lt;/td&gt; &lt;td align="left"&gt;69.70&lt;/td&gt; &lt;td align="left"&gt;62.16&lt;/td&gt; &lt;td align="left"&gt;79.87&lt;/td&gt; &lt;td align="left"&gt;63.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;An overall of 69.32 is in line with the 70.91 claimed in Deepcogito blog post.&lt;/p&gt; &lt;p&gt;Then I wanted to check the difference between Reasoning and No Reasoning and I choose GPQA diamond for this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPQA no reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.41919191919191917 Refusal fraction: 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;GPQA reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.54 Refusal fraction: 0,020202020202 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The refusal fraction where due to thinking process entering in a loop generating the same sentence over and over again.&lt;/p&gt; &lt;p&gt;This are incredible results considering that according to &lt;a href="https://epoch.ai/data/ai-benchmarking-dashboard"&gt;https://epoch.ai/data/ai-benchmarking-dashboard&lt;/a&gt; and to &lt;a href="https://qwenlm.github.io/blog/qwen2.5-llm/"&gt;https://qwenlm.github.io/blog/qwen2.5-llm/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-14B ==&amp;gt; 0.447 Qwen 2.5 14B ==&amp;gt; 0.328 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both at full precision.&lt;/p&gt; &lt;p&gt;These are numbers in par with a couple of higher class LLMs and also the Reasoning mode is quite usable and usually not generating a lot of tokens for thinking.&lt;/p&gt; &lt;p&gt;I definitely recommend this model in favour of Gemma3 or Mistral Small for us GPU poors and I would really love to see how the 32B version perform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7oqc2</id>
    <title>Multiple eGPUs — what downsides are there?</title>
    <updated>2025-04-25T16:08:28+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an ITX computer, and it has one 4090 FE. I want more GPU power (don’t we all?), but I’m reluctant to rebuild an entire new computer to fit in more GPUs.&lt;/p&gt; &lt;p&gt;What downsides are there to buying multiple eGPU enclosures for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7oqc2/multiple_egpus_what_downsides_are_there/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7oqc2/multiple_egpus_what_downsides_are_there/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7oqc2/multiple_egpus_what_downsides_are_there/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T16:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7h8i5</id>
    <title>Playing around with local AI using Svelte, Ollama, and Tauri</title>
    <updated>2025-04-25T10:03:25+00:00</updated>
    <author>
      <name>/u/HugoDzz</name>
      <uri>https://old.reddit.com/user/HugoDzz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7h8i5/playing_around_with_local_ai_using_svelte_ollama/"&gt; &lt;img alt="Playing around with local AI using Svelte, Ollama, and Tauri" src="https://external-preview.redd.it/eXo0dThrc3RleXdlMYJ7TjwC-MuNs3Q8b2AUuM3aKkqtmo6GeWDrUfPUdO1F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8940bc57a6e76e16e57561d1673fe78cb998b543" title="Playing around with local AI using Svelte, Ollama, and Tauri" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HugoDzz"&gt; /u/HugoDzz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1gxttlsteywe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7h8i5/playing_around_with_local_ai_using_svelte_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7h8i5/playing_around_with_local_ai_using_svelte_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T10:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xiy1</id>
    <title>RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x</title>
    <updated>2025-04-24T17:06:34+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt; &lt;img alt="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" src="https://external-preview.redd.it/eiCf8y8ncWiZV_kRkOqMb9U44-ptjGTnaw-ROU8qPKM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b1fc08d775d6fe0c3886c63639965284e34fe2" title="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Our testing revealed that despite having less VRAM than both the A100 (80GB) and RTX 6000 Ada (48GB), the RTX 5090 with its 32GB of memory consistently delivered superior performance across all token lengths and batch sizes.&lt;/p&gt; &lt;p&gt;To put the pricing in perspective, the 5090 costs $0.89/hr in Secure Cloud, compared to the $0.77/hr for the RTX 6000 Ada, and $1.64/hr for the A100. But aside from the standpoint of VRAM (the 5090 has the least, at 32GB) it handily outperforms both of them. If you are serving a model on an A100 though you could simply rent a 2x 5090 pod for about the same price and likely get double the token throughput - so for LLMs, at least, it appears there is a new sheriff in town.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.runpod.io/rtx-5090-llm-benchmarks-for-ai-is-it-the-best-gpu-for-ml/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7opd3</id>
    <title>Interactive Visualization of Grammar-Based Sampling</title>
    <updated>2025-04-25T16:07:20+00:00</updated>
    <author>
      <name>/u/Appropriate-Yak5959</name>
      <uri>https://old.reddit.com/user/Appropriate-Yak5959</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://michaelgiba.com/grammar-based/index.html"&gt;http://michaelgiba.com/grammar-based/index.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To help me understand how structured outputs are generated through local llama I created this interactive page. Check it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Yak5959"&gt; /u/Appropriate-Yak5959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7opd3/interactive_visualization_of_grammarbased_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7opd3/interactive_visualization_of_grammarbased_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7opd3/interactive_visualization_of_grammarbased_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T16:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7cvjr</id>
    <title>llama4 Scout 31tok/sec on dual 3090 + P40</title>
    <updated>2025-04-25T05:00:28+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7cvjr/llama4_scout_31toksec_on_dual_3090_p40/"&gt; &lt;img alt="llama4 Scout 31tok/sec on dual 3090 + P40" src="https://external-preview.redd.it/cjkzZmhqaG52d3dlMVXjerzhcH-b7Q4Q2hx5viTIB_FNu9CjHdNtLi7ZknFq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92b9673d4bea43f750adbc2ee7a14208907422e5" title="llama4 Scout 31tok/sec on dual 3090 + P40" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing out Unsloth's latest dynamic quants (Q4_K_XL) on 2x3090 and a P40. The P40 is a third the speed of the 3090s but still manages to get 31 tokens/second. &lt;/p&gt; &lt;p&gt;I normally run llama3.3 70B Q4_K_M with llama3.2 3B as a draft model. The same test is about 20tok/sec. So a 10tok/sec increase. &lt;/p&gt; &lt;p&gt;Power usage is about the same too, 420W, as the P40s limit the 3090s a bit. &lt;/p&gt; &lt;p&gt;I'll have to give llama4 a spin to see how it feels over llama3.3 for my use case. &lt;/p&gt; &lt;p&gt;Here's my llama-swap configs for the models: &lt;/p&gt; &lt;p&gt;```yaml &amp;quot;llama-70B-dry-draft&amp;quot;: proxy: &amp;quot;&lt;a href="http://127.0.0.1:9602"&gt;http://127.0.0.1:9602&lt;/a&gt;&amp;quot; cmd: &amp;gt; /mnt/nvme/llama-server/llama-server-latest --host 127.0.0.1 --port 9602 --flash-attn --metrics --ctx-size 32000 --ctx-size-draft 32000 --cache-type-k q8_0 --cache-type-v q8_0 -ngl 99 -ngld 99 --draft-max 8 --draft-min 1 --draft-p-min 0.9 --device-draft CUDA2 --tensor-split 1,1,0,0 --model /mnt/nvme/models/Llama-3.3-70B-Instruct-Q4_K_M.gguf --model-draft /mnt/nvme/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf --dry-multiplier 0.8&lt;/p&gt; &lt;p&gt;&amp;quot;llama4-scout&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-6f0,GPU-f10&amp;quot; proxy: &amp;quot;&lt;a href="http://127.0.0.1:9602"&gt;http://127.0.0.1:9602&lt;/a&gt;&amp;quot; cmd: &amp;gt; /mnt/nvme/llama-server/llama-server-latest --host 127.0.0.1 --port 9602 --flash-attn --metrics --ctx-size 32000 --ctx-size-draft 32000 --cache-type-k q8_0 --cache-type-v q8_0 -ngl 99 --model /mnt/nvme/models/unsloth/llama-4/UD-Q4_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL-00001-of-00002.gguf --samplers &amp;quot;top_k;top_p;min_p;dry;temperature;typ_p;xtc&amp;quot; --dry-multiplier 0.8 --temp 0.6 --min-p 0.01 --top-p 0.9 ```&lt;/p&gt; &lt;p&gt;Thanks to the unsloth team for awesome quants and guides! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y9jothhnvwwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7cvjr/llama4_scout_31toksec_on_dual_3090_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7cvjr/llama4_scout_31toksec_on_dual_3090_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T05:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k75lh0</id>
    <title>Mac Studio m3 Ultra getting surprising speeds on Llama 4 Maverick</title>
    <updated>2025-04-24T22:41:41+00:00</updated>
    <author>
      <name>/u/200206487</name>
      <uri>https://old.reddit.com/user/200206487</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k75lh0/mac_studio_m3_ultra_getting_surprising_speeds_on/"&gt; &lt;img alt="Mac Studio m3 Ultra getting surprising speeds on Llama 4 Maverick" src="https://preview.redd.it/7naiq1a92vwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65725c4a58537005c021b3025b57745699dea3bc" title="Mac Studio m3 Ultra getting surprising speeds on Llama 4 Maverick" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mac Studio M3 Ultra 256GB running seemingly high token generation on Llama 4 Maverick Q4 MLX.&lt;/p&gt; &lt;p&gt;It is surprising to me because I’m new to everything terminal, ai, and python. Coming from and continuing to use LM Studio for models such as Mistral Large 2411 GGUF, and it is pretty slow for what I felt was a big ass purchase. Found out about MLX versions of models a few months ago as well as MoE models, and it seems to be better (from my experience and anecdotes I’ve read).&lt;/p&gt; &lt;p&gt;I made a bet with myself that MoE models would become more available and would shine with Mac based on my research. So I got the 256GB of ram version with a 2TB TB5 drive storing my models (thanks Mac Sound Solutions!). Now I have to figure out how to increase token output and pretty much write the code that LM Studio would have as either default or easily used by a GUI. Still though, I had to share with you all just how cool it is to see this Mac generating seemingly good speeds since I’ve learned so much here. I’ll try longer context and whatnot as I figure it out, but what a dream!&lt;/p&gt; &lt;p&gt;I could also just be delusional and once this hits like, idk, 10k context then it all goes down to zip. Still, cool!&lt;/p&gt; &lt;p&gt;TLDR; I made a bet that Mac Studio M3 Ultra 256GB is all I need for now to run awesome MoE models at great speeds (it works!). Loaded Maverick Q4 MLX and it just flies, faster than even models half its size, literally. Had to share because this is really cool, wanted to share some data regarding this specific Mac variant, and I’ve learned a ton thanks to the community here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/200206487"&gt; /u/200206487 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7naiq1a92vwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k75lh0/mac_studio_m3_ultra_getting_surprising_speeds_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k75lh0/mac_studio_m3_ultra_getting_surprising_speeds_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T22:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7m902</id>
    <title>Further explorations of 3090 idle power.</title>
    <updated>2025-04-25T14:25:11+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following on from my post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I started to investigate further:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On an VM that was upgraded, I wasn't able to get idle power down, there were maybe too many things that was preventing GPU from going idle, so I started from a clean slate which worked&lt;/li&gt; &lt;li&gt;There were many strange interactions. I noticed that when starting an program on one GPU, it kicked another unrelated GPU out of its low idle power state.&lt;/li&gt; &lt;li&gt;using nvidia-smi to reset the GPU restores low idle power after whatever breaks the low idle power&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I now replaced my P102-100 idling at 7W (which I used purely for low idle power) with my 3090 as now I can get that to idle at 9W.&lt;/p&gt; &lt;p&gt;I will do some longer term testing to see if it maintains this.&lt;/p&gt; &lt;p&gt;I also found that my newly compiled version of llama.cpp breaks idle power. &lt;/p&gt; &lt;p&gt;The older one I built at commit 6152129d05870cb38162c422c6ba80434e021e9f with CUDA 12.3 maintains idle power.&lt;/p&gt; &lt;p&gt;Building current version with CUDA 12.8 has poor idle power characteristics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m902/further_explorations_of_3090_idle_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m902/further_explorations_of_3090_idle_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m902/further_explorations_of_3090_idle_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7b16b</id>
    <title>Tina: Tiny Reasoning Models via LoRA</title>
    <updated>2025-04-25T03:15:19+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b16b/tina_tiny_reasoning_models_via_lora/"&gt; &lt;img alt="Tina: Tiny Reasoning Models via LoRA" src="https://external-preview.redd.it/2l5XveVVtO0RR14BTRaA53RvbMsjyO0Bvv0miba3ivc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed193f7053082f7c7ee06bae579be5b381f83a16" title="Tina: Tiny Reasoning Models via LoRA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tina-Yi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b16b/tina_tiny_reasoning_models_via_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b16b/tina_tiny_reasoning_models_via_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T03:15:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7faed</id>
    <title>Concerned about economical feasibility of LLMs: Are we about to see enshittification of them? (Price hikes, smaller models for paying users)</title>
    <updated>2025-04-25T07:43:33+00:00</updated>
    <author>
      <name>/u/Endonium</name>
      <uri>https://old.reddit.com/user/Endonium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM inference is highly expensive, which is why OpenAI loses money giving users on the Pro plan unlimited access to its models, despite the $200/month price tag.&lt;/p&gt; &lt;p&gt;I enjoy using ChatGPT, Gemini, and Claude as a programmer, but am becoming increasingly concerned at the inability to extract profits from them. I don't worry about their executives and their wealth, of course, but being unprofitable means price hikes could be heading our way.&lt;/p&gt; &lt;p&gt;I'm worried because investments (OpenAI) or loss leading (Google) are unsustainable long-term, and so we might see massive increases in inference costs (both API and UI monthly subscription) in the coming years, and/or less access to high-parameter count models like o3 and Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;I can't see how this won't happen, except for a breakthrough in GPU/TPU architectures increasing FLOPS by a few orders of magnitude, and/or a move from the Transformer architecture to something else that'll be more efficient.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Endonium"&gt; /u/Endonium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7faed/concerned_about_economical_feasibility_of_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7faed/concerned_about_economical_feasibility_of_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7faed/concerned_about_economical_feasibility_of_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T07:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7m1km</id>
    <title>What tools are you using to manage a shared enterprise prompt library?</title>
    <updated>2025-04-25T14:16:46+00:00</updated>
    <author>
      <name>/u/jetsetter</name>
      <uri>https://old.reddit.com/user/jetsetter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for ways to manage a shared prompt library across multiple business groups within an enterprise. &lt;/p&gt; &lt;p&gt;Ideally, teams should be able to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Author and organize prompts (with tagging or folder structures)&lt;/li&gt; &lt;li&gt;Share prompts across departments (og yahoo-style categorization)&lt;/li&gt; &lt;li&gt;Leave comments or suggest edits&lt;/li&gt; &lt;li&gt;View version history and changes&lt;/li&gt; &lt;li&gt;Use prompts in web chat or assistant-style UI interfaces&lt;/li&gt; &lt;li&gt;(Optionally) link prompts to systems like &lt;strong&gt;Jira&lt;/strong&gt; or &lt;strong&gt;Confluence&lt;/strong&gt; :P&lt;/li&gt; &lt;li&gt;(Optionally) prompt performance benchmarking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The end users are mostly internal employees using prompts to interact with LLMs for things like task triage, summarization, and report generation. End users work in sales, marketing or engineering.&lt;/p&gt; &lt;p&gt;I may be describing a ~platform here but am interested in whatever tooling (internal or external) folks here are using—whether it’s a full platform, lightweight markdown in gists or snippets, or something else entirely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jetsetter"&gt; /u/jetsetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m1km/what_tools_are_you_using_to_manage_a_shared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m1km/what_tools_are_you_using_to_manage_a_shared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7m1km/what_tools_are_you_using_to_manage_a_shared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T14:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7dar7</id>
    <title>EasyWhisperUI Now on macOS – Native Metal GPU Acceleration | Open Source Whisper Desktop App (Windows &amp; Mac)</title>
    <updated>2025-04-25T05:26:54+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm happy to say my application EasyWhisperUI now has full &lt;strong&gt;macOS&lt;/strong&gt; support thanks to an amazing contribution from &lt;a href="https://github.com/celerycoloured"&gt;&lt;strong&gt;u/celerycoloured&lt;/strong&gt;&lt;/a&gt;, who ported it. Mac users, if you're looking for a free transcription application, I'd love to see your results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Major Update: macOS Support&lt;/h1&gt; &lt;p&gt;Thanks to &lt;a href="https://github.com/celerycoloured"&gt;&lt;strong&gt;celerycoloured&lt;/strong&gt;&lt;/a&gt; on GitHub, &lt;strong&gt;EasyWhisper UI now runs natively on macOS&lt;/strong&gt; — with full &lt;strong&gt;Metal API&lt;/strong&gt; GPU acceleration.&lt;br /&gt; You can now transcribe using the power of your Mac’s GPU (Apple Silicon supported).&lt;/p&gt; &lt;p&gt;Huge credit to celerycoloured for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Porting the UI to macOS&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;QDesktopServices&lt;/code&gt; for file opening&lt;/li&gt; &lt;li&gt;Adding a macOS app bundle builder with Whisper compiled inside&lt;/li&gt; &lt;li&gt;Handling paths cleanly across platforms &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/pull/6"&gt;Pull Request #6&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;macOS support (M1, M2, M3 — all Apple Silicon)&lt;/li&gt; &lt;li&gt;Windows 10/11 support&lt;/li&gt; &lt;li&gt;GPU acceleration via Vulkan (Windows) and Metal (macOS)&lt;/li&gt; &lt;li&gt;Batch processing — drag in multiple files or use &amp;quot;Open With&amp;quot; on many at once&lt;/li&gt; &lt;li&gt;Fully C++&lt;/li&gt; &lt;li&gt;Auto-converts to &lt;code&gt;.mp3&lt;/code&gt; if needed using FFmpeg&lt;/li&gt; &lt;li&gt;Dropdowns to pick model and language&lt;/li&gt; &lt;li&gt;Additional arguments textbox for Whisper advanced settings&lt;/li&gt; &lt;li&gt;Automatically downloads missing models&lt;/li&gt; &lt;li&gt;Real-time console output&lt;/li&gt; &lt;li&gt;Choose &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.srt&lt;/code&gt; output (with timestamps)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Windows 10/11 with VulkanSDK support (almost all modern systems)&lt;/li&gt; &lt;li&gt;macOS (Apple Silicon: M1, M2, M3)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s completely free to use.&lt;/p&gt; &lt;h1&gt;Credits&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt; by Georgi Gerganov&lt;/li&gt; &lt;li&gt;FFmpeg builds by &lt;a href="https://www.gyan.dev/ffmpeg/"&gt;Gyan.dev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Built with Qt&lt;/li&gt; &lt;li&gt;Installer built with Inno Setup&lt;/li&gt; &lt;li&gt;macOS port by &lt;a href="https://github.com/celerycoloured"&gt;celerycoloured&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want a simple, native, fast Whisper app for both Windows and macOS without needing to deal with Python or scripts, give EasyWhisperUI a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7dar7/easywhisperui_now_on_macos_native_metal_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7dar7/easywhisperui_now_on_macos_native_metal_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7dar7/easywhisperui_now_on_macos_native_metal_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T05:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k71a8u</id>
    <title>Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning</title>
    <updated>2025-04-24T19:37:46+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"&gt; &lt;img alt="Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning" src="https://preview.redd.it/bjl1n0kv4uwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2219d7ccfb6673fb96bd1244f81a0ef209ca4828" title="Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a new model called &lt;strong&gt;Veritas-12B&lt;/strong&gt;. Specifically finetuned for tasks involving &lt;strong&gt;philosophy, logical reasoning, and critical thinking&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it's good at:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Deep philosophical discussions:&lt;/strong&gt; Exploring complex ideas, ethics, and different schools of thought.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logical consistency:&lt;/strong&gt; Sticking to logic, spotting inconsistencies in arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyzing arguments:&lt;/strong&gt; Breaking down complex points, evaluating reasons and conclusions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explaining complex concepts:&lt;/strong&gt; Articulating abstract ideas clearly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who might find it interesting?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Anyone interested in using an LLM for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exploring philosophical questions&lt;/li&gt; &lt;li&gt;Analyzing texts or arguments&lt;/li&gt; &lt;li&gt;Debate preparation&lt;/li&gt; &lt;li&gt;Structured dialogue requiring logical flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Things to keep in mind:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's built for analysis and reasoning, so it might not be the best fit for super casual chat or purely creative writing. Responses can sometimes be more formal or dense.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Veritas-12B is an UNCENSORED model.&lt;/strong&gt; This means it can generate responses that could be offensive, harmful, unethical, or inappropriate. Please be aware of this and use it responsibly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Where to find it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can find the model details on Hugging Face: &lt;a href="https://huggingface.co/soob3123/Veritas-12B"&gt;soob3123/Veritas-12B · Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF version (Q4_0):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2Fsoob3123%2FVeritas-12B-Q4_0-GGUF"&gt;https://huggingface.co/soob3123/Veritas-12B-Q4_0-GGUF&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model card has an example comparing its output to the base model when describing an image, showing its more analytical/philosophical approach.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bjl1n0kv4uwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T19:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o884</id>
    <title>Android AI agent based on object detection and LLMs</title>
    <updated>2025-04-25T15:47:26+00:00</updated>
    <author>
      <name>/u/saccharineboi</name>
      <uri>https://old.reddit.com/user/saccharineboi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"&gt; &lt;img alt="Android AI agent based on object detection and LLMs" src="https://external-preview.redd.it/cDg0M2JqZnE0MHhlMXhgiTVNaxVoQRD1C2MXVA7X3PPwZtUbpbDgJ5BuncDZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3752d41933290108d1701e5c4033d05f0c2b4001" title="Android AI agent based on object detection and LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend has open-sourced deki, an AI agent for Android OS.&lt;/p&gt; &lt;p&gt;It is an Android AI agent powered by ML model, which is fully open-sourced.&lt;/p&gt; &lt;p&gt;It understands what’s on your screen and can perform tasks based on your voice or text commands.&lt;/p&gt; &lt;p&gt;Some examples:&lt;br /&gt; * &amp;quot;Write my friend &amp;quot;some_name&amp;quot; in WhatsApp that I'll be 15 minutes late&amp;quot;&lt;br /&gt; * &amp;quot;Open Twitter in the browser and write a post about something&amp;quot;&lt;br /&gt; * &amp;quot;Read my latest notifications&amp;quot;&lt;br /&gt; * &amp;quot;Write a linkedin post about something&amp;quot;&lt;/p&gt; &lt;p&gt;Currently, it works only on Android — but support for other OS is planned.&lt;/p&gt; &lt;p&gt;The ML and backend codes were also fully open-sourced.&lt;/p&gt; &lt;p&gt;Video prompt example:&lt;/p&gt; &lt;p&gt;&amp;quot;Open linkedin, tap post and write: hi, it is deki, and now I am open sourced. But don't send, just return&amp;quot;&lt;/p&gt; &lt;p&gt;You can find other AI agent demos and usage examples, like, code generation or object detection on github.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/RasulOs/deki"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: GPLv3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saccharineboi"&gt; /u/saccharineboi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/isn6vhfq40xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o884/android_ai_agent_based_on_object_detection_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7b5j6</id>
    <title>Developed a website for modelling LLM throughput</title>
    <updated>2025-04-25T03:21:57+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b5j6/developed_a_website_for_modelling_llm_throughput/"&gt; &lt;img alt="Developed a website for modelling LLM throughput" src="https://external-preview.redd.it/8P3kKONxqp49lLZzilGLkIwcXiwOoVAslJpLelCDfEw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36c88c2d31e0cd3fa47c34722489596c9f735a9d" title="Developed a website for modelling LLM throughput" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;You can simply copy and paste the model config from Hugging Face, and it will automatically extract the necessary information for calculations. It also supports Gated FFN and GQA to improve calculation accuracy.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Todo:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MoE&lt;/li&gt; &lt;li&gt;Encoder-Decoder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this because the old Desmos version had several serious flaws, and many people complained it was hard to use. So I spent some time developing this website, hope it helps!&lt;/p&gt; &lt;p&gt;&lt;a href="https://slack-agent.github.io/LLM-Performance-Visualizer/"&gt;https://slack-agent.github.io/LLM-Performance-Visualizer/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k7b5j6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b5j6/developed_a_website_for_modelling_llm_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7b5j6/developed_a_website_for_modelling_llm_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T03:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k71mab</id>
    <title>Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence</title>
    <updated>2025-04-24T19:51:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt; &lt;img alt="Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence" src="https://external-preview.redd.it/fxYCW6fqdbJ5RWjh_x1fsIyj0ZtZFx8MOAvXVxIw2PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e74df95b54af72feafa558281ef5e11bc4e8a7c" title="Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I'm super excited to announce our new revamped 2.0 version of our Dynamic quants which outperform leading quantization methods on 5-shot MMLU and KL Divergence!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For accurate benchmarking, we built an evaluation framework to match the reported 5-shot MMLU scores of Llama 4 and Gemma 3. This allowed apples-to-apples comparisons between full-precision vs. Dynamic v2.0, &lt;strong&gt;QAT&lt;/strong&gt; and &lt;strong&gt;standard imatrix GGUF&lt;/strong&gt; quants. See benchmark details below or check our Docs for full analysis: &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-v2.0-ggufs"&gt;https://docs.unsloth.ai/basics/unsloth-dynamic-v2.0-ggufs&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For dynamic 2.0 GGUFs, we report &lt;strong&gt;KL Divergence&lt;/strong&gt; and Disk Space change. Our Gemma 3 Q3_K_XL quant for example reduces the KL Divergence by 7.5% whilst increasing in only 2% of disk space!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d2upyhrp5uwe1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7972946d6a21bd516022779337d6b3b70a13a77d"&gt;https://preview.redd.it/d2upyhrp5uwe1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7972946d6a21bd516022779337d6b3b70a13a77d&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;According to the paper &amp;quot;Accuracy is Not All You Need&amp;quot; &lt;a href="https://arxiv.org/abs/2407.09141"&gt;https://arxiv.org/abs/2407.09141&lt;/a&gt;, the authors showcase how &lt;strong&gt;perplexity is a bad metric since it's a geometric mean, and so output tokens can cancel out&lt;/strong&gt;. It's best to directly report &amp;quot;Flips&amp;quot;, which is how answers change from being incorrect to correct and vice versa.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x1dcukp76uwe1.png?width=1991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c6a92749133cf53ad5b88824ca023347c40036"&gt;https://preview.redd.it/x1dcukp76uwe1.png?width=1991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c6a92749133cf53ad5b88824ca023347c40036&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In fact I was having some issues with Gemma 3 - layer pruning methods and old methods did not seem to work at all with Gemma 3 (my guess is it's due to the 4 layernorms). The paper shows if you prune layers, the &amp;quot;flips&amp;quot; increase dramatically. &lt;strong&gt;They also show KL Divergence to be around 98% correlated with &amp;quot;flips&amp;quot;&lt;/strong&gt;, so my goal is to reduce it!&lt;/li&gt; &lt;li&gt;Also I found current standard imatrix quants overfit on Wikitext - the perplexity is always lower when using these datasets, and I decided to instead use &lt;strong&gt;conversational style datasets sourced from high quality outputs from LLMs with 100% manual inspection (took me many days!!)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Going forward, all GGUF uploads will leverage Dynamic 2.0 along with our hand curated &lt;strong&gt;300K–1.5M token calibration dataset&lt;/strong&gt; to improve conversational chat performance. Safetensors 4-bit BnB uploads might also be updated later.&lt;/li&gt; &lt;li&gt;Gemma 3 27B details on KLD below:&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant type&lt;/th&gt; &lt;th align="left"&gt;KLD old&lt;/th&gt; &lt;th align="left"&gt;Old GB&lt;/th&gt; &lt;th align="left"&gt;KLD New&lt;/th&gt; &lt;th align="left"&gt;New GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;1.035688&lt;/td&gt; &lt;td align="left"&gt;5.83&lt;/td&gt; &lt;td align="left"&gt;0.972932&lt;/td&gt; &lt;td align="left"&gt;6.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;0.832252&lt;/td&gt; &lt;td align="left"&gt;6.33&lt;/td&gt; &lt;td align="left"&gt;0.800049&lt;/td&gt; &lt;td align="left"&gt;6.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;0.535764&lt;/td&gt; &lt;td align="left"&gt;7.16&lt;/td&gt; &lt;td align="left"&gt;0.521039&lt;/td&gt; &lt;td align="left"&gt;7.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_M&lt;/td&gt; &lt;td align="left"&gt;0.26554&lt;/td&gt; &lt;td align="left"&gt;8.84&lt;/td&gt; &lt;td align="left"&gt;0.258192&lt;/td&gt; &lt;td align="left"&gt;8.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.229671&lt;/td&gt; &lt;td align="left"&gt;9.78&lt;/td&gt; &lt;td align="left"&gt;0.220937&lt;/td&gt; &lt;td align="left"&gt;9.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.087845&lt;/td&gt; &lt;td align="left"&gt;12.51&lt;/td&gt; &lt;td align="left"&gt;0.080617&lt;/td&gt; &lt;td align="left"&gt;12.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.024916&lt;/td&gt; &lt;td align="left"&gt;15.41&lt;/td&gt; &lt;td align="left"&gt;0.023701&lt;/td&gt; &lt;td align="left"&gt;15.64&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;We also helped and fixed a few Llama 4 bugs:&lt;/h1&gt; &lt;p&gt;Llama 4 Scout changed the RoPE Scaling configuration in their official repo. We helped resolve issues in llama.cpp to enable this &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12889"&gt;change here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g8et5pp67uwe1.png?width=2091&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a30f52ee76504d889f44f2c3950a4e8027686d6"&gt;https://preview.redd.it/g8et5pp67uwe1.png?width=2091&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a30f52ee76504d889f44f2c3950a4e8027686d6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 4's QK Norm's epsilon for both Scout and Maverick should be from the config file - this means using 1e-05 and not 1e-06. We helped resolve these in &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12889"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/huggingface/transformers/pull/37418"&gt;transformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Llama 4 team and vLLM also independently fixed an issue with QK Norm being shared across all heads (should not be so) &lt;a href="https://github.com/vllm-project/vllm/pull/16311"&gt;here&lt;/a&gt;. MMLU Pro increased from 68.58% to 71.53% accuracy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/WolframRvnwlf/status/1909735579564331016"&gt;Wolfram Ravenwolf&lt;/a&gt; showcased how our GGUFs via llama.cpp attain much higher accuracy than third party inference providers - this was most likely a combination of improper implementation and issues explained above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dynamic v2.0 GGUFs&lt;/strong&gt; (you can also view &lt;a href="https://huggingface.co/collections/unsloth/unsloth-dynamic-v20-quants-68060d147e9b9231112823e6"&gt;all GGUFs here&lt;/a&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF-UD"&gt;R1&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD"&gt;V3-0324&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Llama:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;4 (Scout)&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-GGUF"&gt;3.1 (8B)&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 3:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-GGUF"&gt;4B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-GGUF"&gt;12B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF"&gt;27B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF"&gt;Small-3.1-2503&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;MMLU 5 shot Benchmarks for Gemma 3 27B betweeen QAT and normal:&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;TLDR - Our dynamic 4bit quant gets +1% in MMLU vs QAT whilst being 2GB smaller!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;More details here: &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-v2.0-ggufs"&gt;https://docs.unsloth.ai/basics/unsloth-dynamic-v2.0-ggufs&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Unsloth&lt;/th&gt; &lt;th&gt;Unsloth + QAT&lt;/th&gt; &lt;th&gt;Disk Size&lt;/th&gt; &lt;th&gt;Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IQ1_S&lt;/td&gt; &lt;td&gt;41.87&lt;/td&gt; &lt;td&gt;43.37&lt;/td&gt; &lt;td&gt;6.06&lt;/td&gt; &lt;td&gt;3.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ1_M&lt;/td&gt; &lt;td&gt;48.10&lt;/td&gt; &lt;td&gt;47.23&lt;/td&gt; &lt;td&gt;6.51&lt;/td&gt; &lt;td&gt;3.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q2_K_XL&lt;/td&gt; &lt;td&gt;68.70&lt;/td&gt; &lt;td&gt;67.77&lt;/td&gt; &lt;td&gt;9.95&lt;/td&gt; &lt;td&gt;4.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q3_K_XL&lt;/td&gt; &lt;td&gt;70.87&lt;/td&gt; &lt;td&gt;69.50&lt;/td&gt; &lt;td&gt;12.76&lt;/td&gt; &lt;td&gt;3.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Q4_K_XL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;71.47&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;71.07&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;15.64&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2.94&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q5_K_M&lt;/td&gt; &lt;td&gt;71.77&lt;/td&gt; &lt;td&gt;71.23&lt;/td&gt; &lt;td&gt;17.95&lt;/td&gt; &lt;td&gt;2.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q6_K&lt;/td&gt; &lt;td&gt;71.87&lt;/td&gt; &lt;td&gt;71.60&lt;/td&gt; &lt;td&gt;20.64&lt;/td&gt; &lt;td&gt;2.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;71.60&lt;/td&gt; &lt;td&gt;71.53&lt;/td&gt; &lt;td&gt;26.74&lt;/td&gt; &lt;td&gt;1.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google QAT&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;70.64&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;17.2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2.65&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T19:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6zn5h</id>
    <title>New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?</title>
    <updated>2025-04-24T18:31:34+00:00</updated>
    <author>
      <name>/u/Additional-Hour6038</name>
      <uri>https://old.reddit.com/user/Additional-Hour6038</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt; &lt;img alt="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" src="https://preview.redd.it/a6awqhrhmtwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a0c258afc7e096b062e3e8afff59d5e57504b75" title="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No benchmaxxing on this one! &lt;a href="http://alphaxiv.org/abs/2504.16074"&gt;http://alphaxiv.org/abs/2504.16074&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional-Hour6038"&gt; /u/Additional-Hour6038 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6awqhrhmtwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7ictf</id>
    <title>olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview</title>
    <updated>2025-04-25T11:14:56+00:00</updated>
    <author>
      <name>/u/hdmcndog</name>
      <uri>https://old.reddit.com/user/hdmcndog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"&gt; &lt;img alt="olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview" src="https://external-preview.redd.it/4RZYg5749xg-1AffmqgsNSXXr7Iuj60lXffvU2kpcPo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c7c740e1d09161f2481a17c5befe38bca7b30de" title="olmOCR-7B-faithful by TNG, a fine-tuned version of olmOCR-7B-0225-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A fine-tuned version of olmOCR-7B-0225-preview that aims to extract &lt;em&gt;all&lt;/em&gt; information from documents, including header and footer information.&lt;/p&gt; &lt;p&gt;Release article: &lt;a href="https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine"&gt;https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hdmcndog"&gt; /u/hdmcndog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/olmOCR-7B-faithful"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ictf/olmocr7bfaithful_by_tng_a_finetuned_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T11:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k76ztc</id>
    <title>I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!</title>
    <updated>2025-04-24T23:48:13+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt; &lt;img alt="I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!" src="https://external-preview.redd.it/ZWg3ODc5bHFjdndlMdAf36ezY_hex0Hwu237_4wVe3-ifn3RUf3HJXpttA9U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=510833b73d6e9075fc9b3ec4be11e243ba72c9e2" title="I built a free, local open-source alternative to lovable/v0/bolt... now supporting local models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi localLlama&lt;/p&gt; &lt;p&gt;I’m excited to share an early release of &lt;a href="http://dyad.sh/"&gt;&lt;strong&gt;Dyad&lt;/strong&gt;&lt;/a&gt; — a free, local, open-source AI app builder. It's designed as an alternative to v0, Lovable, and Bolt, but without the lock-in or limitations.&lt;/p&gt; &lt;p&gt;Here’s what makes Dyad different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; - Dyad runs entirely on your computer, making it fast and frictionless. Because your code lives locally, you can easily switch back and forth between Dyad and your IDE like Cursor, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run local models&lt;/strong&gt; - I've just added &lt;a href="https://www.dyad.sh/docs/guides/ai-models/local-models"&gt;Ollama integration&lt;/a&gt;, letting you build with your favorite local LLMs!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; - Dyad is free and bring-your-own API key. This means you can use your free Gemini API key and get 25 free messages/day with Gemini Pro 2.5!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download it &lt;a href="http://dyad.sh/"&gt;here&lt;/a&gt;. It’s totally free and works on Mac &amp;amp; Windows.&lt;/p&gt; &lt;p&gt;I’d love your feedback. Feel free to comment here or join &lt;a href="https://www.reddit.com/r/dyadbuilders/"&gt;r/dyadbuilders&lt;/a&gt; — I’m building based on community input!&lt;/p&gt; &lt;p&gt;P.S. I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt;shared&lt;/a&gt; an earlier version a few weeks back - appreciate everyone's feedback, based on that I rewrote Dyad and made it much simpler to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/krhz58lqcvwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T23:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7j2h5</id>
    <title>Modular have come a long way in just 3 years</title>
    <updated>2025-04-25T11:55:06+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In their latest presentation, they talk about how they now have support for CPU (x86 &amp;amp; ARM since 2023) and NVIDIA &amp;amp; AMD GPU's (I believe that it is currently optimized for A100, H100 &amp;amp; MI300X. There might be more, but those are the models that I have seen mentioned).&lt;/p&gt; &lt;p&gt;They have already open sourced some of their code and will soon release ~250k lines of GPU kernel code, and we will soon get to know how the Python operability is getting along to.&lt;/p&gt; &lt;p&gt;They have a new simpler license for Mojo and MAX.&lt;/p&gt; &lt;p&gt;Presentation (unfortunately bad audio): &lt;a href="https://www.youtube.com/live/uul6hZ5NXC8"&gt;https://www.youtube.com/live/uul6hZ5NXC8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Article from EE Times: &lt;a href="https://www.eetimes.com/after-three-years-modulars-cuda-alternative-is-ready/"&gt;https://www.eetimes.com/after-three-years-modulars-cuda-alternative-is-ready/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7j2h5/modular_have_come_a_long_way_in_just_3_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T11:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7kv9a</id>
    <title>Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations</title>
    <updated>2025-04-25T13:25:15+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt; &lt;img alt="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" src="https://external-preview.redd.it/yTiUURrBkqcGYJGBhqzC01YOstzVvXfVd3FxAo3YWYU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73fdf48b98f8c4bbf03db30badce672add745943" title="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Intel-PyTorch-Extension-2.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7e542</id>
    <title>7B Reasoning Rust Coding Model with Open Dataset</title>
    <updated>2025-04-25T06:22:07+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"&gt; &lt;img alt="7B Reasoning Rust Coding Model with Open Dataset" src="https://external-preview.redd.it/p7L3vw8UA3QYYsIQPN70mTI04OM5s45JyiPaERXOxBg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=314df762ea670ace7afe4fd1f6277bc8c4f4c048" title="7B Reasoning Rust Coding Model with Open Dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tesslate/Tessa-Rust-T1-7B-Q8_0-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T06:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7k1ck</id>
    <title>No thinking, is the right way to think?</title>
    <updated>2025-04-25T12:45:32+00:00</updated>
    <author>
      <name>/u/Eralyon</name>
      <uri>https://old.reddit.com/user/Eralyon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.09858"&gt;https://arxiv.org/abs/2504.09858&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR:&lt;br /&gt; Bypassing the thinking process, forcing the beginning of the answer by &amp;quot;Thinking: Okay, I think I have finished thinking&amp;quot; (lol), they get similar/better inference results !!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eralyon"&gt; /u/Eralyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o89n</id>
    <title>We compress any BF16 model to ~70% size during inference, while keeping the output LOSSLESS so that you can fit in more ERP context or run larger models.</title>
    <updated>2025-04-25T15:47:29+00:00</updated>
    <author>
      <name>/u/choHZ</name>
      <uri>https://old.reddit.com/user/choHZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glad to share another interesting piece of work from us: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;&lt;strong&gt;70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DF11)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tl;dr of this work is super simple. We — and several prior works — noticed that while &lt;strong&gt;BF16&lt;/strong&gt; is often promoted as a “more range, less precision” alternative to FP16 (especially to avoid value overflow/underflow during training), &lt;strong&gt;its range part (exponent bits) ends up being pretty redundant once the model is trained.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, although BF16 as a data format can represent a wide range of numbers, most trained models' exponents are plenty sparse. In practice, the exponent bits carry around 2.6 bits of actual information on average — far from the full 8 bits they're assigned.&lt;/p&gt; &lt;p&gt;This opens the door for classic Huffman coding — where shorter bit sequences are assigned to more frequent values — to &lt;strong&gt;compress the model weights&lt;/strong&gt; into a new data format we call &lt;strong&gt;DFloat11/DF11&lt;/strong&gt;, resulting in a &lt;strong&gt;LOSSLESS compression down to ~11 bits&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;But isn’t this just Zip?&lt;/h1&gt; &lt;p&gt;Not exactly. It is true that tools like Zip also leverage Huffman coding, but the tricky part here is &lt;strong&gt;making it memory efficient during inference&lt;/strong&gt;, as end users are probably not gonna be too trilled if it just makes model checkpoint downloads a bit faster (in all fairness, smaller chekpoints means a lot when training at scale, but that's not a problem for everyday users).&lt;/p&gt; &lt;p&gt;What does matter to everyday users is &lt;strong&gt;making the memory footprint smaller during GPU inference, which requires nontrivial efforts.&lt;/strong&gt; But we have figured it out, and we’ve open-sourced the code.&lt;/p&gt; &lt;p&gt;So now you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run models that previously didn’t fit into your GPU memory.&lt;/li&gt; &lt;li&gt;Or run the same model with &lt;strong&gt;larger batch sizes and/or longer sequences&lt;/strong&gt; (very handy for those lengthy EPRs, or so I have heard).&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPU Type&lt;/th&gt; &lt;th align="left"&gt;Method&lt;/th&gt; &lt;th align="left"&gt;Successfully Run?&lt;/th&gt; &lt;th align="left"&gt;Required Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-405B-Instruct&lt;/td&gt; &lt;td align="left"&gt;8×H100-80G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;811.71 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;551.22 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×H200-141G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;141.11 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;96.14 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×A6000-48G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;65.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;45.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-Distill-Llama-8B&lt;/td&gt; &lt;td align="left"&gt;1×RTX 5080-16G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;16.06 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;11.23 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some research promo posts try to surgercoat their weakness or tradeoff, thats not us. So here's are some honest FAQs:&lt;/p&gt; &lt;h1&gt;What’s the catch?&lt;/h1&gt; &lt;p&gt;Like all compression work, there’s a cost to decompressing. And here are some efficiency reports.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On an A100 with batch size 128, DF11 is &lt;strong&gt;basically just as fast&lt;/strong&gt; as BF16 (1.02x difference, assuming both version fits in the GPUs with the same batch size). See Figure 9.&lt;/li&gt; &lt;li&gt;It is up to &lt;strong&gt;38.8x faster&lt;/strong&gt; than CPU offloading, so if you have a model that can't be run on your GPU in BF16, but can in DF11, there are plenty sweet performance gains over CPU offloading — one of the other popular way to run larger-than-capacity models. See Figure 3.&lt;/li&gt; &lt;li&gt;With the model weight being compressed, you can use the saved real estate for larger batch size or longer context length. This is expecially significant if the model is already tightly fitted in GPU. See Figure 4.&lt;/li&gt; &lt;li&gt;What about batch size 1 latency when both versions (DF11 &amp;amp; BF16) can fit in a single GPU? This is where DF11 is the weakest — we observe ~40% slower (2k/100 tokens for in/out). So there is not much motivation in using DF11 if you are not trying to run larger model/bigger batch size/longer sequence length.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why not just (lossy) quantize to 8-bit?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The short answer is you should totally do that if you are satisfied with the output lossy 8-bit quantization with respect to your task. But how do you really know it is always good?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many benchmark literature suggest that compressing a model (weight-only or otherwise) to 8-bit-ish is typically a safe operation, even though it's technically lossy. What we found, however, is that while this claim is often made in quantization papers, their benchmarks tend to focus on general tasks like MMLU and Commonsense Reasoning; which do not present a comprehensive picture of model capability.&lt;/p&gt; &lt;p&gt;More challenging benchmarks — such as those involving complex reasoning — and real-world user preferences often reveal noticeable differences. One good example is Chatbot Arena indicates the 8-bit and 16-bit Llama 3.1 405b tend to behave quite differently on some categories of tasks (e.g., Math and Coding).&lt;/p&gt; &lt;p&gt;Although the broader question: &lt;em&gt;“Which specific task, on which model, using which quantization technique, under what conditions, will lead to a noticeable drop compared to FP16/BF16?”&lt;/em&gt; is likely to remain open-ended simply due to the sheer amount of potential combinations and definition of “noticable.” &lt;strong&gt;It is fair to say that lossy quantization introduces complexities that some end-users would prefer to avoid, since it creates uncontrolled variables that must be empirically stress-tested for each deployment scenario.&lt;/strong&gt; DF11 offeres an alternative that avoids this concern 100%.&lt;/p&gt; &lt;h1&gt;What about finetuning?&lt;/h1&gt; &lt;p&gt;Our method could potentially pair well with PEFT methods like LoRA, where the base weights are frozen. But since we compress block-wise, we can’t just apply it naively without breaking gradients. We're actively exploring this direction. If it works, if would potentially become a QLoRA alternative where you can lossly LoRA finetune a model with reduced memory footprint.&lt;/p&gt; &lt;p&gt;(As always, happy to answer questions or chat until my advisor notices I’m doomscrolling socials during work hours :&amp;gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;https://arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/LeanModels/DFloat11"&gt;https://github.com/LeanModels/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choHZ"&gt; /u/choHZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7krlm</id>
    <title>Gemma 3 fakes (and ignores) the system prompt</title>
    <updated>2025-04-25T13:20:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt; &lt;img alt="Gemma 3 fakes (and ignores) the system prompt" src="https://preview.redd.it/xuycbwnk4zwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fba119d92fca9059223ac136a22602c0f3b43b8" title="Gemma 3 fakes (and ignores) the system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screenshot shows what Gemma 3 said when I pointed out that it wasn't following its system prompt properly. &amp;quot;Who reads the fine print? 😉&amp;quot; - really, seriously, WTF?&lt;/p&gt; &lt;p&gt;At first I thought it may be an issue with the format/quant, an inference engine bug or just my settings or prompt. But digging deeper, I realized I had been fooled: While the [Gemma 3 chat template](&lt;a href="https://huggingface.co/google/gemma-3-27b-it/blob/main/chat%5C_template.json"&gt;https://huggingface.co/google/gemma-3-27b-it/blob/main/chat\_template.json&lt;/a&gt;) *does* support a system role, all it *really* does is dump the system prompt into the first user message. That's both ugly *and* unreliable - doesn't even use any special tokens, so there's no way for the model to differentiate between what the system (platform/dev) specified as general instructions and what the (possibly untrusted) user said. 🙈&lt;/p&gt; &lt;p&gt;Sure, the model still follows instructions like any other user input - but it never learned to treat them as higher-level system rules, so they're basically &amp;quot;optional&amp;quot;, which is why it ignored mine like &amp;quot;fine print&amp;quot;. That makes Gemma 3 utterly unreliable - so I'm switching to Mistral Small 3.1 24B Instruct 2503 which has proper system prompt support.&lt;/p&gt; &lt;p&gt;Hopefully Google will provide *real* system prompt support in Gemma 4 - or the community will deliver a better finetune in the meantime. For now, I'm hoping Mistral's vision capability gets wider support, since that's one feature I'll miss from Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xuycbwnk4zwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:20:27+00:00</published>
  </entry>
</feed>
