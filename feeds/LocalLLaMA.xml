<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-19T19:48:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lfd7m6</id>
    <title>Is there a way that I can have a llm or some kind of vision model identify different types of animals on a low power device like a pi?</title>
    <updated>2025-06-19T15:09:29+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At my job there's an issue of one kind of animal eating all the food meant for another kind of animal. For instance, there will be a deer feeder but the goats will find it and live by the feeder. I want the feeder to identify the type of animal before activating. I can do this with a PC, but some of these feeders are in remote areas without hundreds of watts of power. If I can do it with a pi, even if it takes a minute to process, it would save a bunch of money from being wasted on making goats fat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7m6/is_there_a_way_that_i_can_have_a_llm_or_some_kind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7m6/is_there_a_way_that_i_can_have_a_llm_or_some_kind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7m6/is_there_a_way_that_i_can_have_a_llm_or_some_kind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:09:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfddq1</id>
    <title>Best offline image processor model?</title>
    <updated>2025-06-19T15:16:30+00:00</updated>
    <author>
      <name>/u/chiknugcontinuum</name>
      <uri>https://old.reddit.com/user/chiknugcontinuum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to set up an image processor that can distinguish what car is what.. make and model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chiknugcontinuum"&gt; /u/chiknugcontinuum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfddq1/best_offline_image_processor_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfddq1/best_offline_image_processor_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfddq1/best_offline_image_processor_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:16:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lersrw</id>
    <title>Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training</title>
    <updated>2025-06-18T20:33:11+00:00</updated>
    <author>
      <name>/u/Heralax_Tekran</name>
      <uri>https://old.reddit.com/user/Heralax_Tekran</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt; &lt;img alt="Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training" src="https://external-preview.redd.it/JPdazJ6jtyR317Uj2SGJFQQZYzaRBapP-lbz0ow2wM8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8583a011c64efbc563707ac8996f32baf680fa6e" title="Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Over the past year and a half&lt;/strong&gt; I've been working on the problem of &lt;strong&gt;factual finetuning&lt;/strong&gt; -- &lt;strong&gt;training an open-source LLM on new facts&lt;/strong&gt; so that it learns those facts, essentially extending its knowledge cutoff. Now that I've made significant progress on the problem, I just released &lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;&lt;strong&gt;Augmentoolkit 3.0&lt;/strong&gt; &lt;/a&gt;— an easy-to-use dataset generation and model training tool. Add documents, click a button, and Augmentoolkit will do everything for you: it'll generate a domain-specific dataset, combine it with a balanced amount of generic data, automatically train a model on it, download it, quantize it, and run it for inference (accessible with a built-in chat interface). The project (and its demo models) are fully open-source. I even trained a model to run inside Augmentoolkit itself, allowing for faster &lt;strong&gt;local dataset generation&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This update took more than six months and thousands of dollars to put together, and represents &lt;strong&gt;a complete rewrite and overhaul of the original project.&lt;/strong&gt; It includes 16 prebuilt dataset generation pipelines and the extensively-documented code and conventions to build more. Beyond just factual finetuning, it even &lt;strong&gt;includes an experimental&lt;/strong&gt; &lt;a href="https://github.com/e-p-armstrong/augmentoolkit/blob/master/docs/grpo.md"&gt;&lt;strong&gt;GRPO pipeline&lt;/strong&gt;&lt;/a&gt; that lets you &lt;strong&gt;train a model to do any conceivable task&lt;/strong&gt; by just &lt;strong&gt;writing a prompt to grade that task.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jnr3luv5zq7f1.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2aed0b5ba86c0945dc41ea084445744784d42e6"&gt;https://preview.redd.it/jnr3luv5zq7f1.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2aed0b5ba86c0945dc41ea084445744784d42e6&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Project&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=E9TyyZzIMyY&amp;amp;ab_channel=Augmentoolkit"&gt;Train your first model in 13 minutes quickstart tutorial video&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo model (what the quickstart produces) &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Heralax/llama-Augmentoolkit-Quickstart-Factual-Demo-Example"&gt;Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset and training configs are fully open source. The config is literally the quickstart config; the dataset is&lt;/li&gt; &lt;li&gt;The demo model is an LLM trained on a subset of the US Army Field Manuals -- the best free and open modern source of comprehensive documentation on a well-known field that I have found. This is also because I trained a model on these in the past and so training on them now serves as a good comparison between the power of the current tool compared to its previous version.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Experimental GRPO models &lt;ul&gt; &lt;li&gt;Now that Augmentoolkit includes the ability to grade models for their performance on a task, I naturally wanted to try this out, and on a task that people are familiar with.&lt;/li&gt; &lt;li&gt;I produced two RP models (base: Mistral 7b v0.2) with the intent of maximizing writing style quality and emotion, while minimizing GPT-isms.&lt;/li&gt; &lt;li&gt;One model has thought processes, the other does not. The non-thought-process model came out better for reasons described in the model card.&lt;/li&gt; &lt;li&gt;Non-reasoner &lt;a href="https://huggingface.co/Heralax/llama-gRPo-emotions-nothoughts"&gt;https://huggingface.co/Heralax/llama-gRPo-emotions-nothoughts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Reasoner &lt;a href="https://huggingface.co/Heralax/llama-gRPo-thoughtprocess"&gt;https://huggingface.co/Heralax/llama-gRPo-thoughtprocess&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Process to Reproduce&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Clone &lt;ul&gt; &lt;li&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/e-p-armstrong/augmentoolkit.git"&gt;&lt;code&gt;https://github.com/e-p-armstrong/augmentoolkit.git&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Run Start Script &lt;ul&gt; &lt;li&gt;Local or Online&lt;/li&gt; &lt;li&gt;Mac &lt;ul&gt; &lt;li&gt;&lt;code&gt;bash&lt;/code&gt; &lt;a href="http://macos.sh/"&gt;&lt;code&gt;macos.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;bash local_macos.sh&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Linux &lt;ul&gt; &lt;li&gt;&lt;code&gt;bash&lt;/code&gt; &lt;a href="http://linux.sh/"&gt;&lt;code&gt;linux.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;bash local_linux.sh&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Windows + warning &lt;ul&gt; &lt;li&gt;Use WSL. If you don't want to, you will have to use the CLI instead. Instructions are in the readme in the quickstart page.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Add API keys or use the local model &lt;ul&gt; &lt;li&gt;I trained a 7b model that is purpose-built to run Augmentoolkit pipelines (Apache license). This means that you can probably generate data at a decent speed on your own computer. It will definitely be slower than with an API, but it will be &lt;em&gt;much&lt;/em&gt; better than trying to generate tens of millions of tokens with a local 70b.&lt;/li&gt; &lt;li&gt;There are separate start scripts for local datagen.&lt;/li&gt; &lt;li&gt;You'll probably only be able to get good dataset generation speed on a linux machine even though it does technically run on Mac, since Llama.cpp is MUCH slower than vLLM (which is Linux-only).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click the &amp;quot;run&amp;quot; Button&lt;/li&gt; &lt;li&gt;Get Your Model &lt;ul&gt; &lt;li&gt;The integrated chat interface will automatically let you chat with it when the training and quanting is finished&lt;/li&gt; &lt;li&gt;The model will also automatically be pushed to Hugging Face (make sure you have enough space!)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Uses&lt;/h1&gt; &lt;p&gt;Besides faster generation times and lower costs, an expert AI that is trained on a domain gains a &amp;quot;big-picture&amp;quot; understanding of the subject that a generalist just won't have. It's the difference between giving a new student a class's full textbook and asking them to write an exam, versus asking a graduate student in that subject to write the exam. The new student probably won't even know where in that book they should look for the information they need, and even if they see the correct context, there's no guarantee that they understands what it means or how it fits into the bigger picture.&lt;/p&gt; &lt;p&gt;Also, trying to build AI apps based on closed-source LLMs released by big labs sucks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The lack of stable checkpoints under the control of the person running the model, makes the tech unstable and unpredictable to build on.&lt;/li&gt; &lt;li&gt;Capabilities change without warning and models are frequently made worse.&lt;/li&gt; &lt;li&gt;People building with AI have to work around the LLMs they are using (a moving target), rather than make the LLMs they are using fit into their system&lt;/li&gt; &lt;li&gt;Refusals force people deploying models to dance around the stuck-up morality of these models while developing.&lt;/li&gt; &lt;li&gt;Closed-source labs charge obscene prices, doing monopolistic rent collecting and impacting the margins of their customers.&lt;/li&gt; &lt;li&gt;Using closed-source labs is a privacy nightmare, especially now that API providers may be required by law to save and log formerly-private API requests.&lt;/li&gt; &lt;li&gt;Different companies have to all work with the same set of models, which have the same knowledge, the same capabilities, the same opinions, and they all sound more or less the same.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But current open-source models often either suffer from a severe lack of capability, or are massive enough that they might as well be closed-source for most of the people trying to run them. The proposed solution? Small, efficient, powerful models that achieve superior performance on the things they are being used for (and sacrifice performance in the areas they &lt;em&gt;aren't&lt;/em&gt; being used for) which are trained for their task and are controlled by the companies that use them.&lt;/p&gt; &lt;p&gt;With &lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;&lt;strong&gt;Augmentoolkit&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You train your models, decide when those models update, and have full transparency over what went into them.&lt;/li&gt; &lt;li&gt;Capabilities change only when the company wants, and no one is forcing them to make their models worse.&lt;/li&gt; &lt;li&gt;People working with AI can customize the model they are using to function as part of the system they are designing, rather than having to twist their system to match a model.&lt;/li&gt; &lt;li&gt;Since you control the data it is built on, the model is only as restricted as you want it to be.&lt;/li&gt; &lt;li&gt;7 billion parameter models (the standard size Augmentoolkit trains) are so cheap to run it is absurd. They can run on a laptop, even.&lt;/li&gt; &lt;li&gt;Because you control your model, you control your inference, and you control your customers' data.&lt;/li&gt; &lt;li&gt;With your model's capabilities being fully customizable, your AI sounds like &lt;em&gt;your&lt;/em&gt; AI, and has the opinions and capabilities that you want it to have.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Furthermore, the open-source indie finetuning scene has been on life support, largely due to a lack of ability to make data, and the difficulty of getting started with (and getting results with) training, compared to methods like merging. Now that data is far easier to make, and training for specific objectives is much easier to do, and there is a good baseline with training wheels included that makes getting started easy, the hope is that people can iterate on finetunes and the scene can have new life.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Augmentoolkit&lt;/a&gt; is taking a bet on an open-source future powered by small, efficient, Specialist Language Models.&lt;/p&gt; &lt;h1&gt;Cool things of note&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Factually-finetuned models can actually cite what files they are remembering information from, and with a good degree of accuracy at that. This is not exclusive to the domain of RAG anymore.&lt;/li&gt; &lt;li&gt;Augmentoolkit models by default use a custom prompt template because it turns out that making SFT data look more like pretraining data in its structure helps models use their pretraining skills during chat settings. This includes factual recall.&lt;/li&gt; &lt;li&gt;Augmentoolkit was used to create the dataset generation model that runs Augmentoolkit's pipelines. You can find the config used to make the dataset (2.5 gigabytes) in the &lt;code&gt;generation/core_composition/meta_datagen&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;There's a pipeline for turning normal SFT data into reasoning SFT data that can give a good cold start to models that you want to give thought processes to. A number of datasets converted using this pipeline &lt;a href="https://huggingface.co/Augmentoolkit"&gt;are available on Hugging Face&lt;/a&gt;, fully open-source.&lt;/li&gt; &lt;li&gt;Augmentoolkit does not just automatically train models on the domain-specific data you generate: to ensure that there is enough data made for the model to 1) generalize and 2) learn the actual capability of conversation, Augmentoolkit will balance your domain-specific data with generic conversational data, ensuring that the LLM becomes smarter while retaining all of the question-answering capabilities imparted by the facts it is being trained on.&lt;/li&gt; &lt;li&gt;If you just want to make data and don't want to automatically train models, there's a config file option for that of course.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why do all this + Vision&lt;/h1&gt; &lt;p&gt;I believe AI alignment is solved when individuals and orgs can make their AI act as they want it to, rather than having to settle for a one-size-fits-all solution. The moment people can use AI specialized to their domains, is also the moment when AI stops being slightly wrong at everything, and starts being incredibly useful across different fields. Furthermore, we must do everything we can to avoid a specific type of AI-powered future: the AI-powered future where what AI believes and is capable of doing is entirely controlled by a select few. Open source has to survive and thrive for this technology to be used right. As many people as possible must be able to control AI.&lt;/p&gt; &lt;p&gt;I want to stop a slop-pocalypse. I want to stop a future of extortionate rent-collecting by the established labs. I want open-source finetuning, even by individuals, to thrive. I want people to be able to be artists, with data their paintbrush and AI weights their canvas.&lt;/p&gt; &lt;p&gt;Teaching models facts was the first step, and I believe this first step has now been taken. It was probably one of the hardest; best to get it out of the way sooner. After this, I'm going to be making coding expert models for specific languages, and I will also improve the &lt;a href="https://github.com/e-p-armstrong/augmentoolkit/blob/master/docs/grpo.md"&gt;GRPO pipeline&lt;/a&gt;, which allows for models to be trained to do &lt;em&gt;literally anything&lt;/em&gt; better. I encourage you to fork the project so that you can make your own data, so that you can create your own pipelines, and so that you can keep the spirit of open-source finetuning and experimentation alive. I also encourage you to star the project, because I like it when &amp;quot;number go up&amp;quot;.&lt;/p&gt; &lt;p&gt;Huge thanks to Austin Cook and all of Alignment Lab AI for helping me with ideas and with getting this out there. Look out for some cool stuff from them soon, by the way :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Happy hacking!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heralax_Tekran"&gt; /u/Heralax_Tekran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T20:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1leyi70</id>
    <title>Self-hosting LLaMA: What are your biggest pain points?</title>
    <updated>2025-06-19T01:34:58+00:00</updated>
    <author>
      <name>/u/Sriyakee</name>
      <uri>https://old.reddit.com/user/Sriyakee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow llama enthusiasts!&lt;/p&gt; &lt;p&gt;Setting aside compute, what has been the biggest issues that you guys have faced when trying to self host models? e.g:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running out of GPU memory or dealing with slow inference times&lt;/li&gt; &lt;li&gt;Struggling to optimize model performance for specific use cases&lt;/li&gt; &lt;li&gt;Privacy?&lt;/li&gt; &lt;li&gt;Scaling models to handle high traffic or large datasets&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sriyakee"&gt; /u/Sriyakee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T01:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfh1s0</id>
    <title>cheapest computer to install an rtx 3090 for inference ?</title>
    <updated>2025-06-19T17:42:51+00:00</updated>
    <author>
      <name>/u/vdiallonort</name>
      <uri>https://old.reddit.com/user/vdiallonort</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I need a second rig to run Magistral Q6 with an RTX3090 (I already have the 3090). I am actually running Magistral on an AMD 7950X, 128GB RAM, ProArt X870E , RTX 3090, and I get 30 tokens/s. Now I need a second rig for a second person with the same performance. I know the CPU should not impact a lot because the model is fully GPU. I am looking to buy something used (I have a spare 850W PSU). How low do you think I can go ?&lt;/p&gt; &lt;p&gt;Regards&lt;/p&gt; &lt;p&gt;Vincent &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vdiallonort"&gt; /u/vdiallonort &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh1s0/cheapest_computer_to_install_an_rtx_3090_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh1s0/cheapest_computer_to_install_an_rtx_3090_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh1s0/cheapest_computer_to_install_an_rtx_3090_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfd7e2</id>
    <title>Has anyone tried the new ICONN-1 (an Apache licensed model)</title>
    <updated>2025-06-19T15:09:13+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"&gt; &lt;img alt="Has anyone tried the new ICONN-1 (an Apache licensed model)" src="https://external-preview.redd.it/SPYrTwyJE3TQKvjnrmxAQjGjLKoUyWEDHwmv3_PzeoA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba21b522cf50bc1f2cabea8c087c1617d7ee467b" title="Has anyone tried the new ICONN-1 (an Apache licensed model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A post was made by the creators on the Huggingface subreddit. I haven’t had a chance to use it yet. Has anyone else?&lt;/p&gt; &lt;p&gt;It isn’t clear at a quick glance if this is a dense model or MoE. The description mentions MoE so I assume it is, but no discussion on the expert size.&lt;/p&gt; &lt;p&gt;Supposedly this is a new base model, but I wonder if it’s a ‘MoE’ made of existing Mistral models. The creator mentioned spending 50k on training it in the huggingface subreddit post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ICONNAI/ICONN-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lff4ni</id>
    <title>5090 benchmarks - where are they?</title>
    <updated>2025-06-19T16:27:11+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As much as I love my hybrid 28GB setup, I would love a few more tokens.&lt;/p&gt; &lt;p&gt;Qwen3 32b Q4KL gives me around 16 tps initially @ 32k context. What are you 5090 owners getting?&lt;/p&gt; &lt;p&gt;Does anyone even have a 5090? 3090 all the way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lff4ni/5090_benchmarks_where_are_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lff4ni/5090_benchmarks_where_are_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lff4ni/5090_benchmarks_where_are_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T16:27:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfh3lc</id>
    <title>Is DDR4 and PCIe 3.0 holding back my inference speed?</title>
    <updated>2025-06-19T17:44:51+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running Llama-CPP on two Rx 6800's (~512GB/s memory bandwidth) - each one getting 8 pcie lanes. I have a Ryzen 9 3950x paired with this and 64GB of 2900mhz DDR4 in dual-channel.&lt;/p&gt; &lt;p&gt;I'm extremely pleased with inference speeds for models that fit on one GPU, but I have a weird cap of ~40 tokens/second when using models that require both GPUs that I can't seem to surpass (example: on smaller quants of Qwen3-30-a3b). In addition to this, startup time (whether on CPU, one GPU, or two GPU's) is quite slow.&lt;/p&gt; &lt;p&gt;My system seems healthy and benching the bandwidth of the individual cards seems fine and I've tried any/all combinations of settings and ROCm versions to no avail. The last thing I could think of is that my platform is relatively old.&lt;/p&gt; &lt;p&gt;Do you think upgrading to a DDR5 platform with PCIe 4/5 lanes would provide a noticeable benefit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh3lc/is_ddr4_and_pcie_30_holding_back_my_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh3lc/is_ddr4_and_pcie_30_holding_back_my_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfh3lc/is_ddr4_and_pcie_30_holding_back_my_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfhdnb</id>
    <title>[Setup discussion] AMD RX 7900 XTX workstation for local LLMs — Linux or Windows as host OS?</title>
    <updated>2025-06-19T17:55:43+00:00</updated>
    <author>
      <name>/u/ElkanRoelen</name>
      <uri>https://old.reddit.com/user/ElkanRoelen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m a software developer and currently building a workstation to run local LLMs. I want to experiment with agents, text-to-speech, image generation, multi-user interfaces, etc. The goal is broad: from hobby projects to a shared AI assistant for my family.&lt;/p&gt; &lt;p&gt;Specs: • GPU: RX 7900 XTX 24GB • CPU: i7-14700K • RAM: 96 GB DDR5 6000 • Use case: Always-on (24/7), multi-user, remotely accessible&lt;/p&gt; &lt;p&gt;What the machine will be used for: • Running LLMs locally (accessed via web UI by multiple users) • Experiments with agents / memory / TTS / image generation • Docker containers for local network services • GitHub self-hosted runner (needs to stay active) • VPN server for remote access • Remote .NET development (Visual Studio on Windows) • Remote gaming (Steam + Parsec/Moonlight)&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;The challenge:&lt;/p&gt; &lt;p&gt;Linux is clearly the better platform for LLM workloads (ROCm support, better tooling, Docker compatibility). But for gaming and .NET development, Windows is more practical.&lt;/p&gt; &lt;p&gt;Dual-boot is highly undesirable, and possibly even unworkable: This machine needs to stay online 24/7 (for remote access, GitHub runner, VPN, etc.), so rebooting into a second OS isn’t a good option.&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;My questions: 1. Is Windows with ROCm support a viable base for running LLMs on the RX 7900 XTX? Or are there still major limitations and instability? 2. Can AMD GPUs be accessed properly in Docker on Windows (either native or via WSL2)? Or is full GPU access only reliable under a Linux host? 3. Would it be smarter to run Linux as the host and Windows in a VM (for dev/gaming)? Has anyone gotten that working with AMD GPU passthrough? 4. What’s a good starting point for running LLMs on AMD hardware? I’m new to tools like LM Studio and Open WebUI — which do you recommend? 5. Are there any benchmarks or comparisons specifically for AMD GPUs and LLM inference? 6. What’s a solid multi-user frontend for local LLMs? Ideally something that supports different users with their own chat history/context.&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Any insights, tips, links, or examples of working setups are very welcome 🙏 Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElkanRoelen"&gt; /u/ElkanRoelen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhdnb/setup_discussion_amd_rx_7900_xtx_workstation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhdnb/setup_discussion_amd_rx_7900_xtx_workstation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhdnb/setup_discussion_amd_rx_7900_xtx_workstation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:55:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfhm4m</id>
    <title>I have an dual xeon e5-2680v2 with 64gb of ram, what is the best local llm I can run ?</title>
    <updated>2025-06-19T18:04:49+00:00</updated>
    <author>
      <name>/u/eightbitgamefan</name>
      <uri>https://old.reddit.com/user/eightbitgamefan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what the title says, I have an dual xeon e5-2680v2 with 64gb of ram, what is the best local llm I can run ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eightbitgamefan"&gt; /u/eightbitgamefan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhm4m/i_have_an_dual_xeon_e52680v2_with_64gb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhm4m/i_have_an_dual_xeon_e52680v2_with_64gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhm4m/i_have_an_dual_xeon_e52680v2_with_64gb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfeein</id>
    <title>[Project] DeepSeek-Based 15M-Parameter Model for Children’s Stories (Open Source)</title>
    <updated>2025-06-19T15:58:06+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt; &lt;img alt="[Project] DeepSeek-Based 15M-Parameter Model for Children’s Stories (Open Source)" src="https://external-preview.redd.it/USEPksTbnhSpjNDP3AWTvRB_hIM8jFv6ba_v6qu8L9U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78593e09dc3ea14e60985fc4600b4c2f05f69690" title="[Project] DeepSeek-Based 15M-Parameter Model for Children’s Stories (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/49xxdwf6pw7f1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41f5e4e22b0b053f35293d4bf29db60bbbda8f7f"&gt;https://preview.redd.it/49xxdwf6pw7f1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41f5e4e22b0b053f35293d4bf29db60bbbda8f7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been exploring how far tiny language models can go when optimized for specific tasks.&lt;/p&gt; &lt;p&gt;Recently, I built a 15M-parameter model using DeepSeek’s architecture (MLA + MoE + Multi-token prediction), trained on a dataset of high-quality children’s stories.&lt;/p&gt; &lt;p&gt;Instead of fine-tuning GPT-2, this one was built from scratch using PyTorch 2.0. The goal: a resource-efficient storytelling model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multihead Latent Attention&lt;/li&gt; &lt;li&gt;Mixture of Experts (4 experts, top-2 routing)&lt;/li&gt; &lt;li&gt;Multi-token prediction&lt;/li&gt; &lt;li&gt;RoPE embeddings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code &amp;amp; Model:&lt;/strong&gt;&lt;br /&gt; &lt;a href="http://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear thoughts from others working on small models or DeepSeek-based setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1leyzxp</id>
    <title>Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp; Jetson!</title>
    <updated>2025-06-19T01:59:17+00:00</updated>
    <author>
      <name>/u/FutureProofHomes</name>
      <uri>https://old.reddit.com/user/FutureProofHomes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"&gt; &lt;img alt="Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp;amp; Jetson!" src="https://external-preview.redd.it/1cKeuQGVkwjz1OX7NjtAv9GHzhusji4vD5LLPS4kBVk.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f09968e3b0f627cdcdb3a1244eedbba09400f1" title="Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp;amp; Jetson!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; We built a &lt;strong&gt;100% private, AI-powered voice assistant&lt;/strong&gt; for your smart home — runs locally on &lt;strong&gt;Jetson&lt;/strong&gt;, uses &lt;strong&gt;Llama models&lt;/strong&gt;, connects to our &lt;strong&gt;open-source Sonos-like speaker&lt;/strong&gt;, and integrates with &lt;strong&gt;Home Assistant&lt;/strong&gt; to control basically &lt;em&gt;everything&lt;/em&gt;. No cloud. Just fast, private, real-time control.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Wassup Llama friends!&lt;/p&gt; &lt;p&gt;I started a YouTube channel showing how to build a private/local voice assistant (think Alexa, but off-grid). It kinda/sorta blew up… and that led to a full-blown hardware startup.&lt;/p&gt; &lt;p&gt;We built a &lt;strong&gt;local LLM server and conversational voice pipeline&lt;/strong&gt; on Jetson hardware, then connected it wirelessly to our &lt;strong&gt;open-source smart speaker&lt;/strong&gt; (like a DIY Sonos One). Then we layered in robust &lt;strong&gt;tool-calling support to integrate with Home Assistant&lt;/strong&gt;, unlocking full control over your smart home — lights, sensors, thermostats, you name it.&lt;/p&gt; &lt;p&gt;End result? A &lt;strong&gt;100% private, local voice assistant&lt;/strong&gt; for the smart home. No cloud. No spying. Just you, your home, and a talking box that &lt;em&gt;actually respects your privacy&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;We’re call ourselves &lt;strong&gt;FutureProofHomes&lt;/strong&gt;, and we’d love a little LocalLLaMA love to help spread the word.&lt;/p&gt; &lt;p&gt;Check us out @ &lt;a href="https://FutureProofHomes.ai"&gt;FutureProofHomes.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FutureProofHomes"&gt; /u/FutureProofHomes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WrreIi8LCiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T01:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfhjja</id>
    <title>New Finnish models (Poro 2) based on Llama 3.1 8B and 70B</title>
    <updated>2025-06-19T18:02:04+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Poro 2 models are based on Llama 3.1 for both 8B and 70B versions. They've been continually pre-trained on 165B tokens using a carefully balanced mix of Finnish, English, code, and math data.&lt;/p&gt; &lt;p&gt;In my opinion they perform better than Gemma 3 at least when it comes to Finnish. Gemma 3 is probably still smarter but won't work as well for Finnish. It's also much better at Finnish when comparing to Llama 3.1. Especially the 8B model is a huge difference. Other new models generally suck at Finnish besides DeepSeekV3/R1, so this is a pretty good release for GPU poor people.&lt;/p&gt; &lt;p&gt;Poro 2 Collection:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/LumiOpen/poro-2-6835bec8186e98712b061f02"&gt;https://huggingface.co/collections/LumiOpen/poro-2-6835bec8186e98712b061f02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs (only for Instruct):&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/Llama-Poro-2-70B-Instruct-GGUF"&gt;https://huggingface.co/mradermacher/Llama-Poro-2-70B-Instruct-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/Llama-Poro-2-8B-Instruct-GGUF"&gt;https://huggingface.co/mradermacher/Llama-Poro-2-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfbqgw</id>
    <title>Local AI setup 1x5090, 5x3090</title>
    <updated>2025-06-19T14:08:38+00:00</updated>
    <author>
      <name>/u/Emergency_Fuel_2988</name>
      <uri>https://old.reddit.com/user/Emergency_Fuel_2988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt; &lt;img alt="Local AI setup 1x5090, 5x3090" src="https://a.thumbs.redditmedia.com/4j9dI7_6M7_J4HmChLe-I_nyPiLiYdWe2VXne3OTO88.jpg" title="Local AI setup 1x5090, 5x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/orhogvwgow7f1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6388beca42f8ede320b52252ebfc59fed1bcb171"&gt;https://preview.redd.it/orhogvwgow7f1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6388beca42f8ede320b52252ebfc59fed1bcb171&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/orbkwkvkow7f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1e41d0223bad473c21ff4618a8c15113d06bbf5"&gt;https://preview.redd.it/orbkwkvkow7f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1e41d0223bad473c21ff4618a8c15113d06bbf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x25gl8mrqw7f1.png?width=1150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35bf80c636b87db84ea3bf203be424c9c0350c81"&gt;https://preview.redd.it/x25gl8mrqw7f1.png?width=1150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35bf80c636b87db84ea3bf203be424c9c0350c81&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rd8cy8mrqw7f1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8a641c5fe1582f4499ea1193d7267e805da6284"&gt;https://preview.redd.it/rd8cy8mrqw7f1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8a641c5fe1582f4499ea1193d7267e805da6284&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxun35mrqw7f1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4c8b5ec6b6685230f3aeda1c21fbe3ca7d5c7b6"&gt;https://preview.redd.it/gxun35mrqw7f1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4c8b5ec6b6685230f3aeda1c21fbe3ca7d5c7b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I’ve been building lately: a local multi-model AI stack that’s getting kind of wild (in a good way)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Been heads-down working on a local AI stack that’s all about fast iteration and strong reasoning, fully running on consumer GPUs. It’s still evolving, but here’s what the current setup looks like:&lt;/p&gt; &lt;h1&gt;🧑‍💻 Coding Assistant&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Devstral Q6 on LMStudio&lt;br /&gt; &lt;strong&gt;Specs:&lt;/strong&gt; Q4 KV cache, 128K context, running on a 5090&lt;br /&gt; Getting ~72 tokens/sec and still have 4GB VRAM free. Might try upping the quant if quality holds, or keep it as-is to push for a 40K token context experiment later.&lt;/p&gt; &lt;h1&gt;🧠 Reasoning Engine&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Magistral Q4 on LMStudio&lt;br /&gt; &lt;strong&gt;Specs:&lt;/strong&gt; Q8 KV cache, 128K context, running on a single 3090&lt;br /&gt; Tuned more for heavy-duty reasoning tasks. Performs effectively up to 40K context.&lt;/p&gt; &lt;h1&gt;🧪 Eval + Experimentation&lt;/h1&gt; &lt;p&gt;Using local Arize Phoenix for evals, tracing, and tweaking. Super useful to visualize what’s actually happening under the hood.&lt;/p&gt; &lt;h1&gt;📁 Codebase Indexing&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Using:&lt;/strong&gt; Roo Code&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3 8B embedding model, FP16, 40K context, 4096D embeddings&lt;/li&gt; &lt;li&gt;Running on a dedicated 3090&lt;/li&gt; &lt;li&gt;Talking to Qdrant (GPU mode), though having a minor issue where embedding vectors aren’t passing through cleanly—might just need to dig into what’s getting sent/received.&lt;/li&gt; &lt;li&gt;Would love a way to dedicate &lt;em&gt;part&lt;/em&gt; of a GPU just to embedding workloads. Anyone done that? ✅ Indexing status: green&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔜 What’s next&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Testing &lt;strong&gt;Kimi-Dev 72B&lt;/strong&gt; (EXL3 quant @ 5bpw, layer split) across 3x3090s—two for layers, one for the context window—via TextGenWebUI or vLLM on WSL2&lt;/li&gt; &lt;li&gt;Also experimenting with an &lt;strong&gt;8B reranker model&lt;/strong&gt; on a single 3090 to improve retrieval quality, still playing around with where it best fits in the workflow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This stack is definitely becoming a bit of a GPU jungle, but the speed and flexibility it gives are worth it.&lt;/p&gt; &lt;p&gt;If you're working on similar local inference workflows—or know a good way to do smart GPU assignment in multi-model setups—I’m super interested in this one challenge:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When a smaller model fails (say, after 3 tries), auto-escalate to a larger model with the same context, and save the larger model’s response as a reference for the smaller one in the future.&lt;/strong&gt; Would be awesome to see something like that integrated into Roo Code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency_Fuel_2988"&gt; /u/Emergency_Fuel_2988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T14:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lei5mb</id>
    <title>Oops</title>
    <updated>2025-06-18T14:12:39+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt; &lt;img alt="Oops" src="https://preview.redd.it/iv35yrek1p7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a1be0e37ffab5a4926e5a5a7a869b2ee3a9c853" title="Oops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iv35yrek1p7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T14:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfetix</id>
    <title>Computer-Use on Windows Sandbox</title>
    <updated>2025-06-19T16:14:44+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"&gt; &lt;img alt="Computer-Use on Windows Sandbox" src="https://external-preview.redd.it/MHY2YzU5dThzdzdmMUUIhfD3WmHuxYkgbFXnt7PvLDhATd-8_6cYVR-PGp7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7144e3f113d350586da9cabf1761e65562ea4e" title="Computer-Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2xrdz059sw7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T16:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lewhla</id>
    <title>We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!</title>
    <updated>2025-06-18T23:55:55+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt; &lt;img alt="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" src="https://preview.redd.it/775o8e8hxr7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c12230c686bdb16949fed6cf8cf00afff6399ea3" title="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.&lt;/p&gt; &lt;p&gt;In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/LMCache/LMCache"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/775o8e8hxr7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T23:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgp3i</id>
    <title>Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon</title>
    <updated>2025-06-19T17:28:59+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"&gt; &lt;img alt="Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon" src="https://external-preview.redd.it/hH0pP3ONlv9RFU_tt26eUVTIN9Qz11vaCtIPHTz4lhc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aabe4d7154b0327e0fc0818f68c0eae9ed8d9581" title="Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=KQDpE2SLzbA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfjjxh</id>
    <title>that's 500 IQ move</title>
    <updated>2025-06-19T19:21:47+00:00</updated>
    <author>
      <name>/u/BoringAd6806</name>
      <uri>https://old.reddit.com/user/BoringAd6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjjxh/thats_500_iq_move/"&gt; &lt;img alt="that's 500 IQ move" src="https://preview.redd.it/duqrjaumpx7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=320383a08e8da36e1748b9c18f4152434799bc85" title="that's 500 IQ move" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoringAd6806"&gt; /u/BoringAd6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/duqrjaumpx7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjjxh/thats_500_iq_move/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjjxh/thats_500_iq_move/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T19:21:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfe33m</id>
    <title>Skywork-SWE-32B</title>
    <updated>2025-06-19T15:45:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-SWE-32B"&gt;https://huggingface.co/Skywork/Skywork-SWE-32B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Skywork-SWE-32B&lt;/em&gt;&lt;/strong&gt; is a code agent model developed by &lt;a href="https://skywork.ai/home"&gt;Skywork AI&lt;/a&gt;, specifically designed for software engineering (SWE) tasks. It demonstrates strong performance across several key metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skywork-SWE-32B attains 38.0% pass@1 accuracy on the &lt;a href="https://www.swebench.com"&gt;SWE-bench Verified&lt;/a&gt; benchmark, outperforming previous open-source SoTA &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B"&gt;Qwen2.5-Coder-32B-based&lt;/a&gt; LLMs built on the &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; agent framework.&lt;/li&gt; &lt;li&gt;When incorporated with test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SoTA results for sub-32B parameter models.&lt;/li&gt; &lt;li&gt;We clearly demonstrate the data scaling law phenomenon for software engineering capabilities in LLMs, with no signs of saturation at 8209 collected training trajectories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF is progress &lt;a href="https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF"&gt;https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgfu5</id>
    <title>AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!</title>
    <updated>2025-06-19T17:18:57+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt; &lt;img alt="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" src="https://external-preview.redd.it/snRoVONGmevA0S70HKIe-_OVILdqukspGvuQ8vgG6Fg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=854af80e0d39d84623d14e840f4c075c5351100c" title="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;, it’s been a bit since my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;post&lt;/a&gt; introducing &lt;a href="https://lemonade-server.ai"&gt;Lemonade Server&lt;/a&gt;, AMD’s open-source local LLM server that prioritizes NPU and GPU acceleration.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to sincerely thank the community here for all the feedback on that post! It’s time for an update, and I hope you’ll agree we took the feedback to heart and did our best to deliver.&lt;/p&gt; &lt;p&gt;The biggest changes since the last post are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;🦙Added llama.cpp, GGUF, and Vulkan support as an additional backend alongside ONNX. This adds support for: A) GPU acceleration on Ryzen™ AI 7000/8000/300, Radeon™ 7000/9000, and many other device families. B) Tons of new models, including VLMs.&lt;/li&gt; &lt;li&gt;🐧Ubuntu is now a fully supported operating system for llama.cpp+GGUF+Vulkan (GPU)+CPU, as well as ONNX+CPU.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;ONNX+NPU support in Linux, as well as NPU support in llama.cpp, are a work in progress.&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;💻Added a web app for model management (list/install/delete models) and basic LLM chat. Open it by pointing your browser at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; while the server is running.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;🤖Added support for streaming tool calling (all backends) and demonstrated it in our &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/local-tiny-agents--mcp-agents-on-ryzen-ai-with-lemonade-server.html"&gt;MCP + tiny-agents blog post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;✨Polished overall look and feel: new getting started website at &lt;a href="https://lemonade-server.ai"&gt;https://lemonade-server.ai&lt;/a&gt;, install in under 2 minutes, and server launches in under 2 seconds.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With the added support for Ubuntu and llama.cpp, Lemonade Server should give great performance on many more PCs than it did 2 months ago. The team here at AMD would be very grateful if y'all could try it out with your favorite apps (I like Open WebUI) and give us another round of feedback. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfgfu5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf9wof</id>
    <title>Explain AI and MCP to a 5 year old in the 90s</title>
    <updated>2025-06-19T12:45:32+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt; &lt;img alt="Explain AI and MCP to a 5 year old in the 90s" src="https://external-preview.redd.it/64oqjh3Mi1lX6NMRJ57nKz9L5oT26BTAsIGTdNtrvn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf2b1569a37ddebb64bb52fbe85643cb9a0b4ec" title="Explain AI and MCP to a 5 year old in the 90s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf9wof"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lficpj</id>
    <title>Kyutai's STT with semantic VAD now opensource</title>
    <updated>2025-06-19T18:33:58+00:00</updated>
    <author>
      <name>/u/phhusson</name>
      <uri>https://old.reddit.com/user/phhusson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai published their latest tech demo few weeks ago, unmute.sh. It is an impressive voice-to-voice assistant using a 3rd-party text-to-text LLM (gemma), while retaining the conversation low latency of Moshi.&lt;/p&gt; &lt;p&gt;They are currently opensourcing the various components for that.&lt;/p&gt; &lt;p&gt;The first component they opensourced is their STT, available at &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The best feature of that STT is Semantic VAD. In a local assistant, the VAD is a component that determines when to stop listening to a request. Most local VAD are sadly not very sophisticated, and won't allow you to pause or think in the middle of your sentence.&lt;/p&gt; &lt;p&gt;The Semantic VAD in Kyutai's STT will allow local assistant to be much more comfortable to use.&lt;/p&gt; &lt;p&gt;Hopefully we'll also get the streaming LLM integration and TTS from them soon, to be able to have our own low-latency local voice-to-voice assistant 🤞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phhusson"&gt; /u/phhusson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgqkd</id>
    <title>Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts</title>
    <updated>2025-06-19T17:30:37+00:00</updated>
    <author>
      <name>/u/choose_a_guest</name>
      <uri>https://old.reddit.com/user/choose_a_guest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt; &lt;img alt="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" src="https://preview.redd.it/niqpo23p5x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22e9ad07139fcbaf4f1d83ce46f5c89ca3c94565" title="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Meta Platforms tried to poach OpenAI employees by offering signing bonuses as high as $100 million, with even larger annual compensation packages, OpenAI chief executive Sam Altman said.&amp;quot;&lt;br /&gt; &lt;a href="https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html"&gt;https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choose_a_guest"&gt; /u/choose_a_guest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niqpo23p5x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf5yog</id>
    <title>Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp; more stable now</title>
    <updated>2025-06-19T08:52:09+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt; &lt;img alt="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" src="https://external-preview.redd.it/9cdGcDSZyRjc_UX2wAVlzVVLsf-enKZlEOwOWD7xiXc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25d6c033d9aec8e5c2afd0f97b93b27a4a07b430" title="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan v0.6.0 is out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully redesigned UI&lt;/li&gt; &lt;li&gt;Switched from Electron to Tauri for lighter and more efficient performance&lt;/li&gt; &lt;li&gt;You can create your own assistants with instructions &amp;amp; custom model settings&lt;/li&gt; &lt;li&gt;New themes &amp;amp; customization settings (e.g. font size, code block highlighting style)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Including improvements to thread handling and UI behavior to tweaking extension settings, cleanup, log improvements, and more.&lt;/p&gt; &lt;p&gt;Update your Jan or download the latest here: &lt;a href="https://jan.ai"&gt;https://jan.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes here: &lt;a href="https://github.com/menloresearch/jan/releases/tag/v0.6.0"&gt;https://github.com/menloresearch/jan/releases/tag/v0.6.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If you'd like to play with the new Jan but has not download a model via Jan, please import your GGUF models via Settings -&amp;gt; Model Providers -&amp;gt; llama.cpp -&amp;gt; Import. See the latest image in the post to do that.&lt;/li&gt; &lt;li&gt;Jan is going to get bigger update soon on MCP usage, we're testing MCP usage with our MCP-specific model, &lt;a href="https://huggingface.co/collections/Menlo/jan-nano-684f6ebfe9ed640fddc55be7"&gt;Jan Nano&lt;/a&gt;, that surpass DeepSeek V3 671B on agentic use cases. If you'd like to test it as well, feel free to join our Discord to see the build links.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf5yog"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T08:52:09+00:00</published>
  </entry>
</feed>
