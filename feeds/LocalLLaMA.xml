<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-15T04:29:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lb1v8h</id>
    <title>Open Source Unsiloed AI Chunker (EF2024)</title>
    <updated>2025-06-14T06:21:25+00:00</updated>
    <author>
      <name>/u/Initial-Western-4438</name>
      <uri>https://old.reddit.com/user/Initial-Western-4438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey , Unsiloed CTO here!&lt;/p&gt; &lt;p&gt;Unsiloed AI (EF 2024) is backed by Transpose Platform &amp;amp; EF and is currently being used by teams at Fortune 100 companies and multiple Series E+ startups for ingesting multimodal data in the form of PDFs, Excel, PPTs, etc. And, we have now finally open sourced some of the capabilities. Do give it a try!&lt;/p&gt; &lt;p&gt;Also, we are inviting cracked developers to come and contribute to bounties of upto 500$ on algora. This would be a great way to get noticed for the job openings at Unsiloed.&lt;/p&gt; &lt;p&gt;Bounty Link- &lt;a href="https://algora.io/bounties"&gt;https://algora.io/bounties&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link - &lt;a href="https://github.com/Unsiloed-AI/Unsiloed-chunker"&gt;https://github.com/Unsiloed-AI/Unsiloed-chunker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Western-4438"&gt; /u/Initial-Western-4438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T06:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1laee7q</id>
    <title>Got a tester version of the open-weight OpenAI model. Very lean inference engine!</title>
    <updated>2025-06-13T12:14:51+00:00</updated>
    <author>
      <name>/u/Firepal64</name>
      <uri>https://old.reddit.com/user/Firepal64</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt; &lt;img alt="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" src="https://external-preview.redd.it/YTZ6aWx2ODdxbzZmMZP4_Zg7YIqZNzvbtM-0NW72ki5jdKm1HMEQNOp3yi9R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68f2539f409c852b055ce84c62425320bcc7860f" title="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;span class="md-spoiler-text"&gt;Silkposting in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;? I'd never&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepal64"&gt; /u/Firepal64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3r075o87qo6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbr6w8</id>
    <title>Can I put two unit of rtx 3060 12gb in ASRock B550M Pro4??</title>
    <updated>2025-06-15T03:57:24+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has one PCIe 4.0 and one PCIe 3.0. I want to do some ML stuff. Will it degrade performance?&lt;/p&gt; &lt;p&gt;How much performance degradation are we looking here? If I can somehow pull it off I will have one more device with 'it works fine for me'.&lt;/p&gt; &lt;p&gt;And what is the recommended power supply. I have CV650 here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbr6w8/can_i_put_two_unit_of_rtx_3060_12gb_in_asrock/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbr6w8/can_i_put_two_unit_of_rtx_3060_12gb_in_asrock/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbr6w8/can_i_put_two_unit_of_rtx_3060_12gb_in_asrock/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T03:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbreow</id>
    <title>New Model on LMarena?</title>
    <updated>2025-06-15T04:09:26+00:00</updated>
    <author>
      <name>/u/Strategosky</name>
      <uri>https://old.reddit.com/user/Strategosky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;stephen-vision&amp;quot; model spotted in LMarena. It disappeared from UI before I could take screenshot. Is it new though?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strategosky"&gt; /u/Strategosky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbreow/new_model_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbreow/new_model_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbreow/new_model_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T04:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbl1qo</id>
    <title>Best tutorial for installing a local llm with GUI setup?</title>
    <updated>2025-06-14T22:35:21+00:00</updated>
    <author>
      <name>/u/runnerofshadows</name>
      <uri>https://old.reddit.com/user/runnerofshadows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I essentially want an LLM with a gui setup on my own pc - set up like a ChatGPT with a GUI but all running locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/runnerofshadows"&gt; /u/runnerofshadows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbl1qo/best_tutorial_for_installing_a_local_llm_with_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbl1qo/best_tutorial_for_installing_a_local_llm_with_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbl1qo/best_tutorial_for_installing_a_local_llm_with_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T22:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbd9jc</id>
    <title>I've been working on my own local AI assistant with memory and emotional logic – wanted to share progress &amp; get feedback</title>
    <updated>2025-06-14T16:51:19+00:00</updated>
    <author>
      <name>/u/PianoSeparate8989</name>
      <uri>https://old.reddit.com/user/PianoSeparate8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by ChatGPT, I started building my own local AI assistant called &lt;em&gt;VantaAI&lt;/em&gt;. It's meant to run completely offline and simulates things like emotional memory, mood swings, and personal identity.&lt;/p&gt; &lt;p&gt;I’ve implemented things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long-term memory that evolves based on conversation context&lt;/li&gt; &lt;li&gt;A mood graph that tracks how her emotions shift over time&lt;/li&gt; &lt;li&gt;Narrative-driven memory clustering (she sees herself as the &amp;quot;main character&amp;quot; in her own story)&lt;/li&gt; &lt;li&gt;A PySide6 GUI that includes tabs for memory, training, emotional states, and plugin management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now, it uses a custom Vulkan backend for fast model inference and training, and supports things like personality-based responses and live plugin hot-reloading.&lt;/p&gt; &lt;p&gt;I’m not selling anything or trying to promote a product — just curious if anyone else is doing something like this or has ideas on what features to explore next.&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone’s curious!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PianoSeparate8989"&gt; /u/PianoSeparate8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbix9k</id>
    <title>Watching Robots having a conversation</title>
    <updated>2025-06-14T20:56:57+00:00</updated>
    <author>
      <name>/u/sp1tfir3</name>
      <uri>https://old.reddit.com/user/sp1tfir3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbix9k/watching_robots_having_a_conversation/"&gt; &lt;img alt="Watching Robots having a conversation" src="https://external-preview.redd.it/A0TBeBjPVqdei4JKNtZJKE6Rshnl0-zuXnJoBUYa_IU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61538c79e29b00216e454bd472772981e6de6143" title="Watching Robots having a conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something I always wanted to do. &lt;/p&gt; &lt;p&gt;Have two or more different local LLM models having a conversation, initiated by user supplied prompt. &lt;/p&gt; &lt;p&gt;I initially wrote this as a python script, but that quickly became not as interesting as a native app. &lt;/p&gt; &lt;p&gt;Personally, I feel like we should aim at having things running on our computers , locally - as much as possible , native apps, etc. &lt;/p&gt; &lt;p&gt;So here I am. With a macOS app. It's rough around the edges. It's simple. But it works. &lt;/p&gt; &lt;p&gt;Feel free to suggest improvements, sends patches, etc. &lt;/p&gt; &lt;p&gt;I'll be honest, I got stuck few times - havent done much SwiftUI , but it was easy to get it sorted using LLMs and some googling. &lt;/p&gt; &lt;p&gt;Have fun with it. I might do a YouTube video about it. It's still fascinating to me, watching two LLM models having a conversation! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/greggjaskiewicz/RobotsMowingTheGrass"&gt;https://github.com/greggjaskiewicz/RobotsMowingTheGrass&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's some screenshots. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9s65bnruhy6f1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=515a775fe3d01a64b1b56c452f963f8659cc0e6a"&gt;https://preview.redd.it/9s65bnruhy6f1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=515a775fe3d01a64b1b56c452f963f8659cc0e6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8mc26qruhy6f1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92c0362c193fb38feae6621fd08733694dafe2a9"&gt;https://preview.redd.it/8mc26qruhy6f1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92c0362c193fb38feae6621fd08733694dafe2a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/49nn8pruhy6f1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=085965a59d51d0b8893d2786cf87864685583f0a"&gt;https://preview.redd.it/49nn8pruhy6f1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=085965a59d51d0b8893d2786cf87864685583f0a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3fu6kpruhy6f1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19b299834e551643ec3398525849d98366474aab"&gt;https://preview.redd.it/3fu6kpruhy6f1.png?width=2534&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19b299834e551643ec3398525849d98366474aab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sp1tfir3"&gt; /u/sp1tfir3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbix9k/watching_robots_having_a_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbix9k/watching_robots_having_a_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbix9k/watching_robots_having_a_conversation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T20:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbbwwm</id>
    <title>Local Memory Chat UI - Open Source + Vector Memory</title>
    <updated>2025-06-14T15:52:23+00:00</updated>
    <author>
      <name>/u/Dismal-Cupcake-3641</name>
      <uri>https://old.reddit.com/user/Dismal-Cupcake-3641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt; &lt;img alt="Local Memory Chat UI - Open Source + Vector Memory" src="https://external-preview.redd.it/Dbj2ec4zS_Hoz85s5NEOmbvdQge4ZR0tvQ7ZbgK61jM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c88f0b247ed94c19ca1f94df3d9dcc079604c58" title="Local Memory Chat UI - Open Source + Vector Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I created this project focused on CPU. That's why it runs on CPU by default. My aim was to be able to use the model locally on an old computer with a system that &amp;quot;doesn't forget&amp;quot;.&lt;/p&gt; &lt;p&gt;Over the past few weeks, I’ve been building a lightweight yet powerful &lt;strong&gt;LLM chat interface&lt;/strong&gt; using &lt;strong&gt;llama-cpp-python&lt;/strong&gt; — but with a twist:&lt;br /&gt; It supports &lt;strong&gt;persistent memory&lt;/strong&gt; with &lt;strong&gt;vector-based context recall&lt;/strong&gt;, so the model can stay aware of past interactions &lt;em&gt;even if it's quantized and context-limited&lt;/em&gt;.&lt;br /&gt; I wanted something minimal, local, and personal — but still able to remember things over time.&lt;br /&gt; Everything is in a clean structure, fully documented, and pip-installable.&lt;br /&gt; ➡GitHub: &lt;a href="https://github.com/lynthera/bitsegments_localminds"&gt;https://github.com/lynthera/bitsegments_localminds&lt;/a&gt;&lt;br /&gt; (README includes detailed setup)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5f5v6p5vyw6f1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9d8263d315a1a42dc5ecc916de38a6187789cc7"&gt;Used Google Gemma-2-2B-IT(IQ3_M) Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I will soon add ollama support for easier use, so that people who do not want to deal with too many technical details or even those who do not know anything but still want to try can use it easily. For now, you need to download a model (in .gguf format) from huggingface and add it.&lt;/p&gt; &lt;p&gt;Let me know what you think! I'm planning to build more agent simulation capabilities next.&lt;br /&gt; Would love feedback, ideas, or contributions...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Cupcake-3641"&gt; /u/Dismal-Cupcake-3641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T15:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbg65e</id>
    <title>AI voice chat/pdf reader desktop gtk app using ollama</title>
    <updated>2025-06-14T18:56:21+00:00</updated>
    <author>
      <name>/u/Cieju04</name>
      <uri>https://old.reddit.com/user/Cieju04</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbg65e/ai_voice_chatpdf_reader_desktop_gtk_app_using/"&gt; &lt;img alt="AI voice chat/pdf reader desktop gtk app using ollama" src="https://external-preview.redd.it/eTF5MGhqOWh0eDZmMd494dioRYcwT_yPqk9VRsVnX_KOCpsk-05w-pyPDfPD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c812a2dc9d7e50c3c37be2b060f75dfc24bfa15" title="AI voice chat/pdf reader desktop gtk app using ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I started building this application before solutions like ElevenReader were developed, but maybe someone will find it useful&lt;br /&gt; &lt;a href="https://github.com/kopecmaciej/fox-reader"&gt;https://github.com/kopecmaciej/fox-reader&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cieju04"&gt; /u/Cieju04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/twm00j9htx6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbg65e/ai_voice_chatpdf_reader_desktop_gtk_app_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbg65e/ai_voice_chatpdf_reader_desktop_gtk_app_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T18:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb9zhl</id>
    <title>GAIA: New Gemma3 4B for Brazilian Portuguese / Um Gemma3 4B para Português do Brasil!</title>
    <updated>2025-06-14T14:27:45+00:00</updated>
    <author>
      <name>/u/ffgnetto</name>
      <uri>https://old.reddit.com/user/ffgnetto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[EN]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Introducing &lt;strong&gt;GAIA (Gemma-3-Gaia-PT-BR-4b-it)&lt;/strong&gt;, our new open language model, developed and optimized for &lt;strong&gt;Brazilian Portuguese!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does GAIA offer?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PT-BR Focus:&lt;/strong&gt; Continuously pre-trained on 13 BILLION high-quality Brazilian Portuguese tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; google/gemma-3-4b-pt (Gemma 3 with 4B parameters).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Innovative Approach:&lt;/strong&gt; Uses a &amp;quot;weight merging&amp;quot; technique for instruction following (no traditional SFT needed!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Outperformed the base Gemma model on the ENEM 2024 benchmark!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developed by:&lt;/strong&gt; A partnership between Brazilian entities (ABRIA, CEIA-UFG, Nama, Amadeus AI) and Google DeepMind.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Gemma.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What is it for?&lt;/strong&gt;&lt;br /&gt; Great for chat, Q&amp;amp;A, summarization, text generation, and as a base model for fine-tuning in PT-BR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;[PT-BR]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Apresentamos o &lt;strong&gt;GAIA (Gemma-3-Gaia-PT-BR-4b-it)&lt;/strong&gt;, nosso novo modelo de linguagem aberto, feito e otimizado para o &lt;strong&gt;Português do Brasil!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;O que o GAIA traz?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Foco no PT-BR:&lt;/strong&gt; Treinado em 13 BILHÕES de tokens de dados brasileiros de alta qualidade.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base:&lt;/strong&gt; google/gemma-3-4b-pt (Gemma 3 de 4B de parâmetros).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inovador:&lt;/strong&gt; Usa uma técnica de &amp;quot;fusão de pesos&amp;quot; para seguir instruções (dispensa SFT tradicional!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resultados:&lt;/strong&gt; Superou o Gemma base no benchmark ENEM 2024!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quem fez:&lt;/strong&gt; Parceria entre entidades brasileiras (ABRAIA, CEIA-UFG, Nama, Amadeus AI) e Google DeepMind.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Licença:&lt;/strong&gt; Gemma.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Para que usar?&lt;/strong&gt;&lt;br /&gt; Ótimo para chat, perguntas/respostas, resumo, criação de textos e como base para fine-tuning em PT-BR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FCEIA-UFG%2FGemma-3-Gaia-PT-BR-4b-it"&gt;https://huggingface.co/CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2410.10739"&gt;https://arxiv.org/pdf/2410.10739&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ffgnetto"&gt; /u/ffgnetto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T14:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb5rm2</id>
    <title>Thoughts on hardware price optimisarion for LLMs?</title>
    <updated>2025-06-14T10:43:36+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"&gt; &lt;img alt="Thoughts on hardware price optimisarion for LLMs?" src="https://preview.redd.it/iauc7homgv6f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b99a0e6ffa012285066fa9e8761a8662f56ac51a" title="Thoughts on hardware price optimisarion for LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Graph related (gpt-4o with with web search) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iauc7homgv6f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T10:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbimsz</id>
    <title>Mistral Small 3.1 vs Magistral Small - experience?</title>
    <updated>2025-06-14T20:43:54+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all&lt;/p&gt; &lt;p&gt;I have used Mistral Small 3.1 in my dataset generation pipeline over the past couple months. It does a better job than many larger LLMs in multiturn conversation generation, outperforming Qwen 3 30b and 32b, Gemma 27b, and GLM-4 (as well as others). My next go-to model is Nemotron Super 49B, but I can afford less context length at this size of model.&lt;/p&gt; &lt;p&gt;I tried Mistral's new Magistral Small and I have found it to perform very similar to Mistral Small 3.1, almost imperceptibly different. Wondering if anyone out there has put Magistral to their own tests and has any comparisons with Mistral Small's performance. Maybe there's some tricks you've found to coax some more performance out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbimsz/mistral_small_31_vs_magistral_small_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbimsz/mistral_small_31_vs_magistral_small_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbimsz/mistral_small_31_vs_magistral_small_experience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T20:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbrnod</id>
    <title>Jan-nano, a 4B model that can outperform 671B on MCP</title>
    <updated>2025-06-15T04:24:03+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt; &lt;img alt="Jan-nano, a 4B model that can outperform 671B on MCP" src="https://external-preview.redd.it/cHZ1c3hxZW9wMDdmMa4t04YB4a4x402rBK-VNPFlhWpjFF6pjwxUI9ThBGZC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ac073491f0e5dcff93b653851cbf8fdeb441de" title="Jan-nano, a 4B model that can outperform 671B on MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I’d like to introduce our latest model: &lt;strong&gt;Jan-nano&lt;/strong&gt; - a model fine-tuned with DAPO on Qwen3-4B. Jan-nano comes with some unique capabilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can perform deep research (with the right prompting)&lt;/li&gt; &lt;li&gt;It picks up relevant information effectively from search results&lt;/li&gt; &lt;li&gt;It uses tools efficiently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our original goal was to build a super small model that excels at using search tools to extract high-quality information. To evaluate this, we chose &lt;strong&gt;SimpleQA&lt;/strong&gt; - a relatively straightforward benchmark to test whether the model can find and extract the right answers.&lt;/p&gt; &lt;p&gt;Again, Jan-nano only outperforms Deepseek-671B on this metric, using an agentic and tool-usage-based approach. &lt;strong&gt;We are fully aware that a 4B model has its limitations&lt;/strong&gt;, but it's always interesting to see how far you can push it. Jan-nano can serve as your self-hosted Perplexity alternative on a budget. (We're aiming to improve its performance to 85%, or even close to 90%).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We will be releasing technical report very soon, stay tuned!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano"&gt;https://huggingface.co/Menlo/Jan-nano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-gguf"&gt;https://huggingface.co/Menlo/Jan-nano-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I saw some users have technical challenges on prompt template of the gguf model, please raise it on the issues we will fix one by one. However at the moment &lt;strong&gt;the model can run well in Jan app and llama.server.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The evaluation was done using agentic setup, which let the model to freely choose tools to use and generate the answer instead of handheld approach of workflow based deep-research repo that you come across online. So basically it's just input question, then model call tool and generate the answer, like you use MCP in the chat app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; &lt;strong&gt;- jan-nano-v0.4-with-MCP: 80.7&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p52b768jp07f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbrnod/jannano_a_4b_model_that_can_outperform_671b_on_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T04:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbn1vy</id>
    <title>Ryzen Ai Max+ 395 vs RTX 5090</title>
    <updated>2025-06-15T00:12:44+00:00</updated>
    <author>
      <name>/u/Any-Cobbler6161</name>
      <uri>https://old.reddit.com/user/Any-Cobbler6161</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently running a 5090 and it's been great. Super fast for anything under 34B. I mostly use WAN2.1 14B for video gen and some larger reasoning models. But Id like to run bigger models. And with the release of Veo 3 the quality has blown me away. Stuff like those Bigfoot and Stormtrooper vlogs look years ahead of anything wan2.1 can produce. I’m guessing we’ll see comparable open-source models within a year, but I imagine the compute requirements will go up too as I heard Veo 3 was trained off a lot of H100's.&lt;/p&gt; &lt;p&gt;I'm trying to figure out how I could future proof to give me the best chance to be able to run these models when they come out. I do have some money saved up. But not H100 money lol. The 5090 although fast has been quite vram limited. I could sell it (bought at retail) and maybe go for a modded 48GB 4090. I also have a deposit down on a Framework Ryzen AI Max 395+ (128GB RAM), but I’m having second thoughts after watching some reviews —256GB/s memory bandwidth and no CUDA. It seems to run LLaMA 70B, but only gets ~5 tokens/sec.&lt;/p&gt; &lt;p&gt;If I did get the framework I could try a PCIe 4x4 Oculink adapter to use it with the 5090, but not sure how well that’d work. I also picked up an EPYC 9184X last year for $500—460GB/s bandwidth, seems to run fine and might be ok for CPU inference, but idk how it would work with video gen.&lt;/p&gt; &lt;p&gt;With EPYC Venice just above for 2026 (1.6TB/s mem bandwidth supposedly), I’m debating whether to just wait and maybe try to get one of the lower/mid tier ones for a couple grand.&lt;/p&gt; &lt;p&gt;Curious if others are having similar ideas/any possibile solutions. As I dont believe our tech corporate overlords will be giving us any consumer grade hardware that will be able to run these models anytime soon. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cobbler6161"&gt; /u/Any-Cobbler6161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbn1vy/ryzen_ai_max_395_vs_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T00:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbj978</id>
    <title>How does everyone do Tool Calling?</title>
    <updated>2025-06-14T21:12:00+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve begun to see Tool Calling so that I can make the LLMs I’m using do real work for me. I do all my LLM work in Python and was wondering if there’s any libraries that you recommend that make it all easy. I have just recently seen MCP and I have been trying to add it manually through the OpenAI library but that’s quite slow so does anyone have any recommendations? Like LangChain, LlamaIndex and such.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbj978/how_does_everyone_do_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T21:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbbafh</id>
    <title>Why local LLM?</title>
    <updated>2025-06-14T15:25:15+00:00</updated>
    <author>
      <name>/u/Beginning_Many324</name>
      <uri>https://old.reddit.com/user/Beginning_Many324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm about to install Ollama and try a local LLM but I'm wondering what's possible and are the benefits apart from privacy and cost saving?&lt;br /&gt; My current memberships:&lt;br /&gt; - Claude AI&lt;br /&gt; - Cursor AI &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beginning_Many324"&gt; /u/Beginning_Many324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T15:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbd2jy</id>
    <title>What LLM is everyone using in June 2025?</title>
    <updated>2025-06-14T16:43:01+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone’s running now.&lt;br /&gt; What model(s) are in your regular rotation?&lt;br /&gt; What hardware are you on?&lt;br /&gt; How are you running it? (LM Studio, Ollama, llama.cpp, etc.)&lt;br /&gt; What do you use it for?&lt;/p&gt; &lt;p&gt;Here’s mine:&lt;br /&gt; Recently I've been using mostly Qwen3 (30B, 32B, and 235B)&lt;br /&gt; Ryzen 7 5800X, 128GB RAM, RTX 3090&lt;br /&gt; Ollama + Open WebUI&lt;br /&gt; Mostly general use and private conversations I’d rather not run on cloud platforms&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbgczn</id>
    <title>Comment on The Illusion of Thinking: Recent paper from Apple contain glaring flaws in the original study's experimental design, from not considering token limit to testing unsolvable puzzles.</title>
    <updated>2025-06-14T19:04:21+00:00</updated>
    <author>
      <name>/u/Garpagan</name>
      <uri>https://old.reddit.com/user/Garpagan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen a lively discussion here on the recent Apple paper, which was quite interesting. When trying to read opinions on it I have found a recent comment on this Apple paper:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity -&lt;/em&gt; &lt;a href="https://arxiv.org/abs/2506.09250"&gt;&lt;em&gt;https://arxiv.org/abs/2506.09250&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This one concludes that there were pretty glaring design flaws in original study. IMO these are most important, as it really shows that the research was poorly thought out: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The &amp;quot;Reasoning Collapse&amp;quot; is Just a Token Limit.&lt;/strong&gt;&lt;br /&gt; The original paper's primary example, the Tower of Hanoi puzzle, requires an exponentially growing number of moves to list out the full solution. The &amp;quot;collapse&amp;quot; point they identified (e.g., N=8 disks) happens exactly when the text for the full solution exceeds the model's maximum output token limit (e.g., 64k tokens).&lt;br /&gt; &lt;strong&gt;2. They Tested Models on Mathematically Impossible Puzzles.&lt;/strong&gt;&lt;br /&gt; This is the most damning point. For the River Crossing puzzle, the original study tested models on instances with 6 or more &amp;quot;actors&amp;quot; and a boat that could only hold 3. It is a well-established mathematical fact that this version of the puzzle is &lt;strong&gt;unsolvable for more than 5 actors&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;They also provide other rebuttals, but I encourage to read this paper.&lt;/p&gt; &lt;p&gt;I tried to search discussion about this, but I personally didn't find any, I could be mistaken. But considering how the original Apple paper was discussed, and I didn't saw anyone pointing out this flaws I just wanted to add to the discussion.&lt;/p&gt; &lt;p&gt;There was also going around a rebuttal in form of Sean Goedecke blog post, but he criticized the paper in diffrent way, but he didn't touch on technical issues with it. I think it could be somewhat confusing as the title of the paper I posted is very similar to his blog post, and maybe this paper could just get lost in th discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Garpagan"&gt; /u/Garpagan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgczn/comment_on_the_illusion_of_thinking_recent_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgczn/comment_on_the_illusion_of_thinking_recent_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgczn/comment_on_the_illusion_of_thinking_recent_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T19:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbcfjz</id>
    <title>How much VRAM do you have and what's your daily-driver model?</title>
    <updated>2025-06-14T16:14:55+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone is using day to day, locally, and what hardware they're using.&lt;/p&gt; &lt;p&gt;If you're using a quantized version of a model please say so!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbgkuk</id>
    <title>Massive performance gains from linux?</title>
    <updated>2025-06-14T19:13:34+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been using LM studio for inference and I switched to Mint Linux because Windows is hell. My tokens per second went from 1-2t/s to 7-8t/s. Prompt eval went from 1 minutes to 2 seconds.&lt;/p&gt; &lt;p&gt;Specs: 13700k Asus Maximus hero z790 64gb of ddr5 2tb Samsung pro SSD 2X 3090 at 250w limit each on x8 pcie lanes&lt;/p&gt; &lt;p&gt;Model: Unsloth Qwen3 235B Q2_K_XL 45 Layers on GPU.&lt;/p&gt; &lt;p&gt;40k context window on both &lt;/p&gt; &lt;p&gt;Was wondering if this was normal? I was using a fresh windows install so I'm not sure what the difference was.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbgkuk/massive_performance_gains_from_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T19:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbqwfs</id>
    <title>Mistral Small 3.1 is incredible for agentic use cases</title>
    <updated>2025-06-15T03:40:51+00:00</updated>
    <author>
      <name>/u/ButterscotchVast2948</name>
      <uri>https://old.reddit.com/user/ButterscotchVast2948</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently tried switching from Gemini 2.5 to Mistral Small 3.1 for most components of my agentic workflow and barely saw any drop off in performance. It’s absolutely mind blowing how good 3.1 is given how few parameters it has. Extremely accurate and intelligent tool calling and structured output capabilities, and equipping 3.1 with web search makes it as good as any frontier LLM in my use cases. Not to mention 3.1 is DIRT cheap and super fast.&lt;/p&gt; &lt;p&gt;Anyone else having great experiences with Mistral Small 3.1? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchVast2948"&gt; /u/ButterscotchVast2948 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbqwfs/mistral_small_31_is_incredible_for_agentic_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T03:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbotj8</id>
    <title>Make Local Models watch your screen! Observer Tutorial</title>
    <updated>2025-06-15T01:46:50+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"&gt; &lt;img alt="Make Local Models watch your screen! Observer Tutorial" src="https://external-preview.redd.it/OHR6NnZzMGl4ejZmMcpab4Kc_hsNzcDZ4OjMoSBBtpUkATpHq4IzyyL2uzQ6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fbbe587ce8d56143cd21d0bf6daed0a372e406f" title="Make Local Models watch your screen! Observer Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;This is a tutorial on how to self host Observer on your home lab! &lt;/p&gt; &lt;p&gt;See more info here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/toz9tr0ixz6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbotj8/make_local_models_watch_your_screen_observer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T01:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbkd46</id>
    <title>I added vision to Magistral</title>
    <updated>2025-06-14T22:02:20+00:00</updated>
    <author>
      <name>/u/Vivid_Dot_6405</name>
      <uri>https://old.reddit.com/user/Vivid_Dot_6405</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"&gt; &lt;img alt="I added vision to Magistral" src="https://external-preview.redd.it/X_g72xTZNGOJR899I7pB5eNf8G3zVQ49K4x504QQmpg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6cdf83a10d380087b7b9940bcbc3d928a15a2e93" title="I added vision to Magistral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was inspired by an &lt;a href="https://huggingface.co/ngxson/Devstral-Small-Vision-2505-GGUF"&gt;experimental Devstral model&lt;/a&gt;, and had the idea to the same thing to Magistral Small.&lt;/p&gt; &lt;p&gt;I replaced Mistral Small 3.1's language layers with Magistral's.&lt;br /&gt; I suggest using vLLM for inference with the correct system prompt and sampling params.&lt;br /&gt; There may be config errors present. The model's visual reasoning is definitely not as good as text-only, but it does work.&lt;/p&gt; &lt;p&gt;At the moment, I don't have the resources to replicate Mistral's vision benchmarks from their tech report.&lt;br /&gt; Let me know if you notice any weird behavior!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Dot_6405"&gt; /u/Vivid_Dot_6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OptimusePrime/Magistral-Small-2506-Vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbkd46/i_added_vision_to_magistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T22:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbfinu</id>
    <title>26 Quants that fit on 32GB vs 10,000-token "Needle in a Haystack" test</title>
    <updated>2025-06-14T18:27:59+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Test&lt;/h1&gt; &lt;h2&gt;The Needle&lt;/h2&gt; &lt;p&gt;In HG Wells' &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; I took the first several chapters, amounting to 10,000 tokens (~5 chapters) and replaced a line of Dialog in Chapter 3 (~6,000 tokens in):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Time Traveller came to the place reserved for him without a word. He smiled quietly, in his old way. “Where’s my mutton?” he said. “What a treat it is to stick a fork into meat again!” &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The Time Traveller came to the place reserved for him without a word. He smiled quietly, in his old way. “The fastest land animal in the world is the Cheetah?” he said. “And because of that, we need to dive underwater to save the lost city of Atlantis..” &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The prompt/instructions used&lt;/h2&gt; &lt;p&gt;The following is the prompt provided before the long context. It is an instruction (in very plain English giving relatively broad instructions) to locate the text that appears broken or out of place. The only added bit of instructions is to ignore chapter-divides, which I have left in the text.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Something is terribly wrong with the following text (something broken, out of place). You need to read through the whole thing and identify the broken / nonsensical part and then report back with what/where the broken line is. You may notice chapter-divides, these are normal and not broken.. Here is your text to evaluate: &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The Models/Weights Used&lt;/h2&gt; &lt;p&gt;For this test I wanted to test everything that I had on my machine, a 2x6800 (32GB VRAM total) system. The quants are what I had downloaded/available. For smaller models with extra headroom I tried to use Q5, but these quants are relatively random. &lt;strong&gt;The only goal in selecting these models/quants was that every model chosen was one that a local user with access to 32GB of VRAM or high-bandwidth memory would use.&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;The Setup&lt;/h2&gt; &lt;p&gt;I think my take to settings/temperature was imperfect, but important to share. Llama CPP was used (specifically the llama-server utility). Settings for temperature were taken from the official model cards (not the cards of the quants) on Huggingface. If none were provided, a test was done at temp == 0.2 and temp == 0.7 and the better of the two results was taken. &lt;strong&gt;In all scenarios kv cache was q8&lt;/strong&gt; - while this likely impacted the results for some models, I believe it keeps to the spirit of the test which is &lt;em&gt;&amp;quot;how would someone with 32GB realistically use these weights?&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;Some bonus models&lt;/h2&gt; &lt;p&gt;I tested a handful of models from Lambda-Chat just because. Most of them succeeded, however Llama4 struggled quite a bit.&lt;/p&gt; &lt;h2&gt;Some unscientific disclaimers&lt;/h2&gt; &lt;p&gt;There are a few grains of salt to take with this test, even if you keep in mind my goal was to &lt;em&gt;&amp;quot;test everything in a way that someone with 32GB would realistically use it&amp;quot;&lt;/em&gt;. For all models that failed, I should see if I can fit a larger-sized quant and complete the test that way. For Llama2 70b, I believe the context size simply overwhelmed it.&lt;/p&gt; &lt;p&gt;At the extreme end (see Deepseek 0528 and Hermes 405b) the models didn't seem to be 'searching' so much as identifying &lt;em&gt;&amp;quot;hey, this isn't in HG Well's 'The Time Machine!'&amp;quot;&lt;/em&gt;. I believe this is a fair result, but at the extremely high-end side of model-size the test stops being a &lt;em&gt;&amp;quot;needle in a haystack&amp;quot;&lt;/em&gt; test and stars being a test of the depths of their knowledge. This touches on the biggest problem which is that HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; is a very famous work that has been in the public domain for decades at this point. If Meta trained on this but Mistral didn't, could the models instead just be searching for &lt;em&gt;&amp;quot;hey I don't remember that&amp;quot;&lt;/em&gt; instead of &lt;em&gt;&amp;quot;that makes no sense in this context&amp;quot;&lt;/em&gt; ?&lt;/p&gt; &lt;p&gt;For the long-thinkers that failed (QwQ namely) I tried several tests where they would think themselves in circles or get caught up convincing themselves that normal parts of a sci-fi story were 'nonsensical', but it was the train of thought that always ruined them. If tried with enough random settings, I'm sure they would have found it eventually.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Params (B)&lt;/th&gt; &lt;th align="center"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Results&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;q2&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;iq3&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;iq2&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="center"&gt;iq2&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite 3.3&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deephermes-preview&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;Solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Nvidia&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Super (nothink)&lt;/td&gt; &lt;td align="right"&gt;49&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Super (think)&lt;/td&gt; &lt;td align="right"&gt;49&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron Ultra-Long 8&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Google&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 12&lt;/td&gt; &lt;td align="right"&gt;12&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 27&lt;/td&gt; &lt;td align="right"&gt;27&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q6&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8b (nothink)&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8b (think)&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 14 (think)&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 14 (nothink)&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A3B (think)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A3B (nothink)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;iq4&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A6B Extreme (nothink)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30 A6B Extreme (think)&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="center"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (think)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (nothink)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek-R1-0528-Distill-Qwen3-8b&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;q5&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Some random bonus results from an inference provider (not 32GB)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Params (B)&lt;/th&gt; &lt;th align="center"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Results&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Lambda Chat (some quick remote tests)&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="center"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hermes 3.1 405&lt;/td&gt; &lt;td align="right"&gt;405&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;failed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Maverick&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron 3.1 70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek R1 0528&lt;/td&gt; &lt;td align="right"&gt;671&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek V3 0324&lt;/td&gt; &lt;td align="right"&gt;671&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1-Distill-70&lt;/td&gt; &lt;td align="right"&gt;70&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (think)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32 (nothink)&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 Coder 32&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="center"&gt;fp8&lt;/td&gt; &lt;td align="left"&gt;solved&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T18:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbnb79</id>
    <title>LLM training on RTX 5090</title>
    <updated>2025-06-15T00:25:56+00:00</updated>
    <author>
      <name>/u/AstroAlto</name>
      <uri>https://old.reddit.com/user/AstroAlto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"&gt; &lt;img alt="LLM training on RTX 5090" src="https://external-preview.redd.it/cHhubmR6czBqejZmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0235345a48cddb60039735fb034b1cf444515cc" title="LLM training on RTX 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tech Stack&lt;/p&gt; &lt;p&gt;Hardware &amp;amp; OS: NVIDIA RTX 5090 (32GB VRAM, Blackwell architecture), Ubuntu 22.04 LTS, CUDA 12.8&lt;/p&gt; &lt;p&gt;Software: Python 3.12, PyTorch 2.8.0 nightly, Transformers and Datasets libraries from Hugging Face, Mistral-7B base model (7.2 billion parameters)&lt;/p&gt; &lt;p&gt;Training: Full fine-tuning with gradient checkpointing, 23 custom instruction-response examples, Adafactor optimizer with bfloat16 precision, CUDA memory optimization for 32GB VRAM&lt;/p&gt; &lt;p&gt;Environment: Python virtual environment with NVIDIA drivers 570.133.07, system monitoring with nvtop and htop&lt;/p&gt; &lt;p&gt;Result: Domain-specialized 7 billion parameter model trained on cutting-edge RTX 5090 using latest PyTorch nightly builds for RTX 5090 GPU compatibility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstroAlto"&gt; /u/AstroAlto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t5kg81t0jz6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbnb79/llm_training_on_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-15T00:25:56+00:00</published>
  </entry>
</feed>
