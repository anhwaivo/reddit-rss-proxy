<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-21T17:23:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k4hmjx</id>
    <title>Budget Dual 3090 Build Advice</title>
    <updated>2025-04-21T16:26:54+00:00</updated>
    <author>
      <name>/u/JustTooKrul</name>
      <uri>https://old.reddit.com/user/JustTooKrul</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, I have been all through the posts on here about 3090 builds and a lot of the detailed advice is from 10+ months ago and it seems prices have shifted a lot. I have two 3090's from prior computer builds that I am looking to consolidate into a rig for running a local AI stack and get far better performance than my existing single-3090 rig. I should say that I have no experience with server- or workstation-class hardware (e.g. Xeon or Epyc machines).&lt;/p&gt; &lt;p&gt;I'd like the ability to expand in the future if I can pickup additional cards at relatively cheap prices. I'm also looking for a build that's as compact as possible--if that means expanding in the future will be complicated, then so be it. I'd rather have a compact dual-3090 machine and have to use retimers and an external mounting solution than a massive build with dual-3090's today and additional room for two more 3090's that might never actually get utilized. &lt;/p&gt; &lt;p&gt;From everything I have seen, it seems that I can limit the PSU needed by capping the power usage of the 3090's with little / no performance hit and ensuring I have enough RAM to match or exceed the VRAM is preferred. With that in mind, I would usually go to a website like &lt;a href="http://pcpartpicker.com"&gt;pcpartpicker.com&lt;/a&gt; and just start adding things that worked together and then order it all, but this is a more specialized situation and any advice or best practices from folks with experience with similar builds would be appreciated.&lt;/p&gt; &lt;p&gt;And, as I mentioned, I'm trying to keep costs low as I have already procured the highest cost items with the two 3090's.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help and advice here! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustTooKrul"&gt; /u/JustTooKrul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4hmjx/budget_dual_3090_build_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4hmjx/budget_dual_3090_build_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4hmjx/budget_dual_3090_build_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T16:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4i4sn</id>
    <title>What LLM woudl you recommend for OCR?</title>
    <updated>2025-04-21T16:46:56+00:00</updated>
    <author>
      <name>/u/sbs1799</name>
      <uri>https://old.reddit.com/user/sbs1799</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to extract text from PDFs that are not really well scanned. As such, tesseract output had issues. I am wondering if any local llms provide more reliable OCR. What model(s) would you recommend I try on my Mac?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbs1799"&gt; /u/sbs1799 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4i4sn/what_llm_woudl_you_recommend_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4i4sn/what_llm_woudl_you_recommend_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4i4sn/what_llm_woudl_you_recommend_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T16:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3pw8n</id>
    <title>Intel releases AI Playground software for generative AI as open source</title>
    <updated>2025-04-20T16:05:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt; &lt;img alt="Intel releases AI Playground software for generative AI as open source" src="https://external-preview.redd.it/UgxZ6n5LChFWd0HyYltaWavgJZl1YXoXz0YJ03N3rv0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb487f61c95f7de558ca1ddf5ec1b62010b36b5e" title="Intel releases AI Playground software for generative AI as open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Announcement video: &lt;a href="https://www.youtube.com/watch?v=dlNvZu-vzxU"&gt;https://www.youtube.com/watch?v=dlNvZu-vzxU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt; AI Playground open source project and AI PC starter app for doing AI image creation, image stylizing, and chatbot on a PC powered by an Intel® Arc™ GPU. AI Playground leverages libraries from GitHub and Huggingface which may not be available in all countries world-wide. AI Playground supports many Gen AI libraries and models including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image Diffusion: Stable Diffusion 1.5, SDXL, Flux.1-Schnell, LTX-Video&lt;/li&gt; &lt;li&gt;LLM: Safetensor PyTorch LLMs - DeepSeek R1 models, Phi3, Qwen2, Mistral, GGUF LLMs - Llama 3.1, Llama 3.2: OpenVINO - TinyLlama, Mistral 7B, Phi3 mini, Phi3.5 mini&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/intel/AI-Playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T16:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4fm5q</id>
    <title>Noob request: Coding model for specific framework</title>
    <updated>2025-04-21T14:58:46+00:00</updated>
    <author>
      <name>/u/rodlib</name>
      <uri>https://old.reddit.com/user/rodlib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a pre-trained model to help me coding, either with fresh knowledge or that can be able to be updated. &lt;/p&gt; &lt;p&gt;I'm aware of Gemini of Claude are the best AI services for coding, but I get frustrated anytime I ask them to write for the latest framework version I'm working on. I tried adding the latest official documentation, but I'm my case, it's been worthless (probabbly my fault for not understand how it works).&lt;/p&gt; &lt;p&gt;I know the basics for RAG, but before going deeper in that, I want to check if there is any alternative.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodlib"&gt; /u/rodlib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fm5q/noob_request_coding_model_for_specific_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fm5q/noob_request_coding_model_for_specific_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fm5q/noob_request_coding_model_for_specific_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T14:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k45plp</id>
    <title>A collection of benchmarks for LLM inference engines: SGLang vs vLLM</title>
    <updated>2025-04-21T05:08:31+00:00</updated>
    <author>
      <name>/u/Michaelvll</name>
      <uri>https://old.reddit.com/user/Michaelvll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt; &lt;img alt="A collection of benchmarks for LLM inference engines: SGLang vs vLLM" src="https://external-preview.redd.it/GB15PFHcTZYqNqz5LwH26yT5wqSIL5PcEgrLsjCjETM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb815c9e568ec4387033a7258b7b6f6ec5f87982" title="A collection of benchmarks for LLM inference engines: SGLang vs vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Competition in open source could advance the technology rapidly. &lt;/p&gt; &lt;p&gt;Both vLLM and SGLang teams are amazing, speeding up the LLM inference, but the recent arguments for the different benchmark numbers confused me quite a bit. &lt;/p&gt; &lt;p&gt;I deeply respect both teams and trust their results, so I created a collection of benchmarks from both systems to learn more: &lt;a href="https://github.com/Michaelvll/llm-ie-benchmarks"&gt;https://github.com/Michaelvll/llm-ie-benchmarks&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I created a few &lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt; YAMLs for those benchmarks, so they can be easily run with a single command, ensuring consistent and reproducible infrastructure deployment across benchmarks. &lt;/p&gt; &lt;p&gt;Thanks to the high availability of H200 on Nebius cloud, I ran those benchmarks on 8 H200 GPUs. &lt;/p&gt; &lt;p&gt;Some findings are quite surprising:&lt;br /&gt; 1. Even though the two benchmark scripts are similar: derived from the same source, they generate contradictory results. That makes me wonder if the benchmarks reflect the performance, or whether the implementation of the benchmarks matters more.&lt;br /&gt; 2. The benchmarks are fragile: simply changing the number of prompts can flip the conclusion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/whh6rmwtv2we1.png?width=1263&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=449371de63393db56bcd6688a71185ea6fef8768"&gt;Reproducing benchmark by vLLM team&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tytcqsvyv2we1.png?width=1178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=959da8b6727bbdbdd0fcee55c91c51efb64efc6e"&gt;Reproducing benchmark by SGLang team&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Later, SGLang maintainer submitted a &lt;a href="https://github.com/Michaelvll/llm-ie-benchmarks/pull/1"&gt;PR&lt;/a&gt; to our GitHub repo to update the optimal flags to be used for the benchmark: using &lt;code&gt;0.4.5.post2&lt;/code&gt; release, removing the &lt;code&gt;--enable-dp-attention&lt;/code&gt;, and adding three retries for warmup:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwyoxmrud4we1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7727ea1dcf5da0dcd78e013c70b68f95bc0904d2"&gt;Benchmark from SGLang team with optimal flags&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, if we change the number of prompts to 200 (vs 50 from the official benchmark), the performance conclusion flips. &lt;/p&gt; &lt;p&gt;That said, these benchmarks may be quite fragile, not reflecting the serving performance in a real application -- the input/output lengths could vary.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrx0u8s7e4we1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=092a61ecce69ee5f0dd2a33055388c1743af0cbd"&gt;Benchmark from SGLang team with optimal flags and 200 prompts in total&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Michaelvll"&gt; /u/Michaelvll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T05:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k423l6</id>
    <title>Which drawing do you think is better? What does your LLM output?</title>
    <updated>2025-04-21T01:43:36+00:00</updated>
    <author>
      <name>/u/BlaiseLabs</name>
      <uri>https://old.reddit.com/user/BlaiseLabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"&gt; &lt;img alt="Which drawing do you think is better? What does your LLM output?" src="https://preview.redd.it/vmsd8uf2f3we1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a81ff88fa2df5f09153edaca6969a11f10e9caa1" title="Which drawing do you think is better? What does your LLM output?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What output do you get when asking an LLM to draw a face with matplotlib? Any tips or techniques you’d recommend for better results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlaiseLabs"&gt; /u/BlaiseLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vmsd8uf2f3we1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T01:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4caaw</id>
    <title>Local RAG tool that doesn't use embedding</title>
    <updated>2025-04-21T12:28:21+00:00</updated>
    <author>
      <name>/u/lily_34</name>
      <uri>https://old.reddit.com/user/lily_34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAG - retrieval augmented generation - involves searching for relevant information, and adding it to the context, before starting the generation.&lt;/p&gt; &lt;p&gt;It seems most RAG tools use embedding and similaroty search to find relevant information. Are there any RAG tools that use other kind of search/information retirieval?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lily_34"&gt; /u/lily_34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4caaw/local_rag_tool_that_doesnt_use_embedding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4caaw/local_rag_tool_that_doesnt_use_embedding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4caaw/local_rag_tool_that_doesnt_use_embedding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T12:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4favx</id>
    <title>RAG retrieval slows down as knowledge base grows - Anyone solve this at scale?</title>
    <updated>2025-04-21T14:45:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here’s my dilemma. My RAG is dialed in and performing great in the relevance department, but it seems like as we add more documents to our knowledge base, the overall time from prompt to result gets slower and slower. My users are patient, but I think asking them to wait any longer than 45 seconds per prompt is too long in my opinion. I need to find something to improve RAG retrieval times.&lt;/p&gt; &lt;p&gt;Here’s my setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open WebUI (latest version) running in its own Azure VM (Dockerized)&lt;/li&gt; &lt;li&gt;Ollama running in its own GPU-enabled VM in Azure (with dual H100s)&lt;/li&gt; &lt;li&gt;QwQ 32b FP16 as the main LLM&lt;/li&gt; &lt;li&gt;Qwen 2.5 1.5b FP16 as the task model (chat title generation, Retrieval Query gen, web query gen, etc)&lt;/li&gt; &lt;li&gt;Nomic-embed-text for embedding model (running on Ollama Server)&lt;/li&gt; &lt;li&gt;all-MiniLM-L12-v2 for reranking model for hybrid search (running on the OWUI server because you can’t run a reranking model on Ollama using OWUI for some unknown reason) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;RAG Embedding / Retrieval settings: - Vector DB = ChromaDB using default Open WebUI settings (running inside the OWUI Docker container) - Chunk size = 2000 - Chunk overlap = 500 (25% of chunk size as is the accepted standard) - Top K = 10 - Too K Reranker = 10 - Relevance Threshold = 0 - RAG template = OWUI 0.6.5 default RAG prompt template - Full Context Mode = OFF - Content Extraction Engine = Apache Tika&lt;/p&gt; &lt;p&gt;Knowledgebase details: - 7 separate document collections containing approximately 400 total PDFS and TXT files between 100k to 3mb each. Most average around 1mb. &lt;/p&gt; &lt;p&gt;Again, other than speed, my RAG is doing very well, but our knowledge bases are going to have a lot more documents in them soon and I can’t have this process getting much slower or I’m going to start getting user complaints. &lt;/p&gt; &lt;p&gt;One caveat: I’m only allowed to run Windows-based servers, no pure Linux VMs are allowed in my organization. I can run WSL though, just not standalone Linux. So vLLM is not currently an option. &lt;/p&gt; &lt;p&gt;For those running RAG at “production” scale, how do you make it fast without going to 3rd party services? I need to keep all my RAG knowledge bases “local” (within my own private tenant). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4favx/rag_retrieval_slows_down_as_knowledge_base_grows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4favx/rag_retrieval_slows_down_as_knowledge_base_grows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4favx/rag_retrieval_slows_down_as_knowledge_base_grows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T14:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4gl0k</id>
    <title>Trying to add emotion conditioning to Gemma-3</title>
    <updated>2025-04-21T15:37:25+00:00</updated>
    <author>
      <name>/u/FOerlikon</name>
      <uri>https://old.reddit.com/user/FOerlikon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"&gt; &lt;img alt="Trying to add emotion conditioning to Gemma-3" src="https://b.thumbs.redditmedia.com/rHRvAjz9hBfUOUAXuneChyyaVvWZbDMq2GT3_bBVjlk.jpg" title="Trying to add emotion conditioning to Gemma-3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was curious to make LLM influenced by something more than just the text, so I made a small attempt to add emotional input to smallest Gemma-3-1B, which is honestly pretty inconsistent, and it was only trained on short sequences of synthetic dataset with emotion markers.&lt;/p&gt; &lt;p&gt;The idea: alongside text there is an emotion vector, and it trainable projection then added to the token embeddings before they go into the transformer layers, and trainable LoRA is added on top. &lt;/p&gt; &lt;p&gt;Here are some (cherry picked) results, generated per same input/seed/temp but with different joy/sadness. I found them kind of intriguing to share (even though the dataset looks similar)&lt;/p&gt; &lt;p&gt;My question is has anyone else has played around with similar conditioning? Does this kind approach even make much sense to explore further? I mostly see RP-finetunes when searching for existing emotion models.&lt;/p&gt; &lt;p&gt;Curious to hear any thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FOerlikon"&gt; /u/FOerlikon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4gl0k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49p6y</id>
    <title>Still no contestant to NeMo in the 12B range for RP?</title>
    <updated>2025-04-21T09:51:20+00:00</updated>
    <author>
      <name>/u/Xhatz</name>
      <uri>https://old.reddit.com/user/Xhatz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering what are y'all using for roleplay or ERP in that range. I've tested more than a hundred models and also fine-tunes of NeMo but not a single one has beaten Mag-Mell, a 1 yo fine-tune, for me, in storytelling, instruction following...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhatz"&gt; /u/Xhatz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4b5xl</id>
    <title>I built a Local AI Voice Assistant with Ollama + gTTS with interruption</title>
    <updated>2025-04-21T11:26:21+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It’s fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am working on integrating Kokoro STT at the moment, and perhaps Sesame in the coming days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k48u73</id>
    <title>Is Google’s Titans architecture doomed by its short context size?</title>
    <updated>2025-04-21T08:50:53+00:00</updated>
    <author>
      <name>/u/eesahe</name>
      <uri>https://old.reddit.com/user/eesahe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.00663"&gt;Paper link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Titans is hyped for its &amp;quot;learn‑at‑inference&amp;quot; long‑term memory, but the tradeoff is that it only has a tiny context window - in the paper they train their experiment models with a 4 K context size.&lt;/p&gt; &lt;p&gt;That context size cannot be easily scaled up because keeping the long-term memory updated becomes unfeasibly expensive with a longer context window, as I understand it.&lt;/p&gt; &lt;p&gt;Titans performs very well in some benchmarks with &amp;gt; 2 M‑token sequences, but I wonder if splitting the input into tiny windows and then compressing that into long-term memory vectors could end in some big tradeoffs outside of the test cases shown, due to losing direct access to the original sequence?&lt;/p&gt; &lt;p&gt;I wonder could that be part of why we haven't seen any models trained with this architecture yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eesahe"&gt; /u/eesahe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T08:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k47wlc</id>
    <title>🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!</title>
    <updated>2025-04-21T07:44:24+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt; &lt;img alt="🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!" src="https://external-preview.redd.it/NXEwbDIyYmU3NXdlMdvd-QuaL2Iymjf8AR2toHyHT4xxu-3H8nMusFAc2zhu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baa192645e7ad1e9a4b256373e06dac1ec7f2235" title="🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hgg9u2be75we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T07:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4f3a2</id>
    <title>Local LLM performance results on Raspberry Pi devices</title>
    <updated>2025-04-21T14:36:47+00:00</updated>
    <author>
      <name>/u/fatihustun</name>
      <uri>https://old.reddit.com/user/fatihustun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt; &lt;img alt="Local LLM performance results on Raspberry Pi devices" src="https://b.thumbs.redditmedia.com/chzBzCsnADn6F4qhCPwQHx6IBkrGvkQhmVAcHbvNnDM.jpg" title="Local LLM performance results on Raspberry Pi devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Method (very basic):&lt;/strong&gt;&lt;br /&gt; I simply installed Ollama and downloaded some small models (listed in the table) to my Raspberry Pi devices, which have a clean Raspbian OS (lite) 64-bit OS, nothing else installed/used. I run models with the &amp;quot;--verbose&amp;quot; parameter to get the performance value after each question. I asked 5 same questions to each model and took the average.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are the results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/igp229o077we1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa65f28686c212c76f04c344ea767b20cdbe2196"&gt;https://preview.redd.it/igp229o077we1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa65f28686c212c76f04c344ea767b20cdbe2196&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If you have run a local model on a Raspberry Pi device, please share the model and the device variant with its performance result.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fatihustun"&gt; /u/fatihustun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T14:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3wuud</id>
    <title>nsfw orpheus early v1</title>
    <updated>2025-04-20T21:21:27+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview"&gt;https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;update: &amp;quot;v2-later checkpoint still early&amp;quot; -&amp;gt; &lt;a href="https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview-v1-8600"&gt;https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview-v1-8600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;can do the common sounds / generalises pretty well - preview has only 1 voice but good enough to get an idea of where we are heading&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T21:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43htm</id>
    <title>Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image</title>
    <updated>2025-04-21T02:58:47+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt; &lt;img alt="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" src="https://b.thumbs.redditmedia.com/CBmqcJTwYbnQt5krSlEFj-sxPkmBsS_zYzMxkxvM3-Y.jpg" title="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InstantCharacter is an innovative, tuning-free method designed to achieve character-preserving generation from a single image&lt;/p&gt; &lt;p&gt;One image + text → custom poses, styles &amp;amp; scenes 1️⃣ First framework to balance character consistency, image quality, &amp;amp; open-domain flexibility/generalization 2️⃣ Compatible with Flux, delivering high-fidelity, text-controllable results 3️⃣ Comparable to industry leaders like GPT-4o in precision &amp;amp; adaptability&lt;/p&gt; &lt;p&gt;Try it yourself on： 🔗Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/InstantX/InstantCharacter"&gt;https://huggingface.co/spaces/InstantX/InstantCharacter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dive Deep into InstantCharacter: 🔗Project Page: &lt;a href="https://instantcharacter.github.io/"&gt;https://instantcharacter.github.io/&lt;/a&gt; 🔗Code: &lt;a href="https://github.com/Tencent/InstantCharacter"&gt;https://github.com/Tencent/InstantCharacter&lt;/a&gt; 🔗Paper：&lt;a href="https://arxiv.org/abs/2504.12395"&gt;https://arxiv.org/abs/2504.12395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k43htm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4fwck</id>
    <title>The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension</title>
    <updated>2025-04-21T15:09:57+00:00</updated>
    <author>
      <name>/u/zanatas</name>
      <uri>https://old.reddit.com/user/zanatas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"&gt; &lt;img alt="The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension" src="https://preview.redd.it/v6dt6jrre7we1.gif?width=640&amp;amp;crop=smart&amp;amp;s=083ba989dbd3b12db4098ac42b096e6b6d88fee0" title="The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: someone at work made a joke about creating a really unhelpful Clippy-like assistant that exclusively gives you weird suggestions, one thing led to another and I ended up making a whole Chrome extension.&lt;/p&gt; &lt;p&gt;It was part me having the habit of transforming throwaway jokes into very convoluted projects, part a ✨ViBeCoDiNg✨ exercise, part growing up in the early days of the internet, where stuff was just dumb/fun for no reason (I blame Johnny Castaway and those damn Macaronis dancing Macarena).&lt;/p&gt; &lt;p&gt;You'll need either Ollama (lets you pick any model, send in page context) or a Gemini API key (likely better/more creative performance, but only reads the URL of the tab). &lt;/p&gt; &lt;p&gt;Full source here: &lt;a href="https://github.com/yankooliveira/toads"&gt;https://github.com/yankooliveira/toads&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zanatas"&gt; /u/zanatas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6dt6jrre7we1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43x1h</id>
    <title>Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)</title>
    <updated>2025-04-21T03:21:32+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt; &lt;img alt="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" src="https://external-preview.redd.it/ZHd1MzQzZGp3M3dlMYSg_wSm3961EYqonF0X5c18rpErhfTomdHPrQd5DrBK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c49081fab5d934bbcd3802ccff3f83e4f2270f76" title="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8hsjp4q1w3we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T03:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43g7a</id>
    <title>Why are so many companies putting so much investment into free open source AI?</title>
    <updated>2025-04-21T02:56:18+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand alot of the big pictures for these companies, but considering how many open source options we have and how they will continue to get better. How will these companies like OpenAI or Google ever make back their investment?&lt;/p&gt; &lt;p&gt;Personally i have never had to stay subscribed to a company because there's so many free alternatives. Not to mention all these companies have really good free options of the best models.&lt;/p&gt; &lt;p&gt;Unless one starts screaming ahead of the rest in terms of performance what is their end goal?&lt;/p&gt; &lt;p&gt;Not that I'm complaining, just want to know.&lt;/p&gt; &lt;p&gt;EDIT: I should probably say i know OpenAI isn't open source yet from what i know but they also offer a very high quality free plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4blth</id>
    <title>🚀 Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)</title>
    <updated>2025-04-21T11:51:54+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"&gt; &lt;img alt="🚀 Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)" src="https://b.thumbs.redditmedia.com/2-0WhWHvw-I2OXhO_fkhxKNabIJ7N9A1mmJoTYLUdfM.jpg" title="🚀 Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuing my journey documenting self-hosted AI tools - today I’m dropping a new tutorial on how to run the amazing LightRAG project on your own bare metal server with a GPU… in just minutes 🤯&lt;/p&gt; &lt;p&gt;Thanks to full automation (Ansible + Docker Compose + Sbnb Linux), you can go from an empty machine with no OS to a fully running RAG pipeline.&lt;/p&gt; &lt;p&gt;TL;DR: Start with a blank PC with a GPU. End with an advanced RAG system, ready to answer your questions.&lt;/p&gt; &lt;p&gt;Tutorial link: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy experimenting! Let me know if you try it or run into anything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4blth"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4gqje</id>
    <title>[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli</title>
    <updated>2025-04-21T15:46:41+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt; &lt;img alt="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" src="https://external-preview.redd.it/chyB3Fwy2UcKLBJMyzabSe7PfMaM2G1ZJw5k660LQOY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97a934134489f4b1d7e573c0218731d1a8a5d5b" title="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/84a9bf2fc2875205f0806fbbfbb66dc67204094c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4avlq</id>
    <title>What's the best models available today to run on systems with 8 GB / 16 GB / 24 GB / 48 GB / 72 GB / 96 GB of VRAM today?</title>
    <updated>2025-04-21T11:08:51+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, since many aren't that experienced with running local LLMs and the choice of models, what are the best models available today for the different ranges of VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h0n</id>
    <title>24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?</title>
    <updated>2025-04-21T09:35:34+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt; &lt;img alt="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" src="https://external-preview.redd.it/WdUGpP3unGKFZihZrELR3GH6ZUOa768rHIdn2YSXrsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=461a9ba85d2877f5c00bb8c11f93f1ceac11d893" title="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4god7</id>
    <title>GLM-4 32B is mind blowing</title>
    <updated>2025-04-21T15:41:35+00:00</updated>
    <author>
      <name>/u/Timely_Second_6414</name>
      <uri>https://old.reddit.com/user/Timely_Second_6414</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt; &lt;img alt="GLM-4 32B is mind blowing" src="https://external-preview.redd.it/KGA1Keg1D7oCkdV6UW_ifq_mQe-5jNP1DvhwwJ2Stbs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128cb2de26af5c23c04d8bec8b39d61ce2d36274" title="GLM-4 32B is mind blowing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/815w430kg7we1/player"&gt;GLM-4 32B pygame earth simulation, I tried this with gemini 2.5 flash which gave an error as output.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Title says it all. I tested out GLM-4 32B Q8 locally using PiDack's llama.cpp pr (&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12957/"&gt;https://github.com/ggml-org/llama.cpp/pull/12957/&lt;/a&gt;) as ggufs are currently broken.&lt;/p&gt; &lt;p&gt;I am absolutely amazed by this model. It outperforms every single other ~32B local model and even outperforms 72B models. It's literally Gemini 2.5 flash (non reasoning) at home, but better. It's also fantastic with tool calling and works well with cline/aider.&lt;/p&gt; &lt;p&gt;But the thing I like the most is that this model is not afraid to output a lot of code. It does not truncate anything or leave out implementation details. Below I will provide an example where it 0-shot produced 630 lines of code (I had to ask it to continue because the response got cut off at line 550). I have no idea how they trained this, but I am really hoping qwen 3 does something similar. &lt;/p&gt; &lt;p&gt;Below are some examples of 0 shot requests comparing GLM 4 versus gemini 2.5 flash (non-reasoning). GLM is run locally with temp 0.6 and top_p 0.95 at Q8. Output speed is 22t/s for me on 3x 3090.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solar system&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: Create a realistic rendition of our solar system using html, css and js. Make it stunning! reply with one file.&lt;/p&gt; &lt;p&gt;Gemini response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/vhn6r9kmi7we1/player"&gt;Gemini 2.5 flash: nothing is interactible, planets dont move at all&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/ylcl9s4ri7we1/player"&gt;GLM-4-32B response. Sun label and orbit rings are off, but it looks way better and theres way more detail.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Neural network visualization&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: code me a beautiful animation/visualization in html, css, js of how neural networks learn. Make it stunningly beautiful, yet intuitive to understand. Respond with all the code in 1 file. You can use threejs&lt;/p&gt; &lt;p&gt;Gemini:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/nkgj1wc1j7we1/player"&gt;Gemini response: network looks good, but again nothing moves, no interactions.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/equidag5j7we1/player"&gt;GLM 4 response (one shot 630 lines of code): It tried to plot data that will be fit on the axes. Although you dont see the fitting process you can see the neurons firing and changing in size based on their weight. Theres also sliders to adjust lr and hidden size. Not perfect, but still better.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also did a few other prompts and GLM generally outperformed gemini on most tests. Note that this is only Q8, I imaging full precision might be even a little better. &lt;/p&gt; &lt;p&gt;Please share your experiences or examples if you have tried the model. I havent tested the reasoning variant yet, but I imagine its also very good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Second_6414"&gt; /u/Timely_Second_6414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4e9pd</id>
    <title>The AI team at Google have reached the surprising conclusion that quantizing weights from 16-bits to 4-bits leads to a 4x reduction of VRAM usage!</title>
    <updated>2025-04-21T14:01:45+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4e9pd/the_ai_team_at_google_have_reached_the_surprising/"&gt; &lt;img alt="The AI team at Google have reached the surprising conclusion that quantizing weights from 16-bits to 4-bits leads to a 4x reduction of VRAM usage!" src="https://preview.redd.it/flecddmd27we1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beae2c8b953dad50af5fad36e7f92570c2853197" title="The AI team at Google have reached the surprising conclusion that quantizing weights from 16-bits to 4-bits leads to a 4x reduction of VRAM usage!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/flecddmd27we1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4e9pd/the_ai_team_at_google_have_reached_the_surprising/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4e9pd/the_ai_team_at_google_have_reached_the_surprising/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T14:01:45+00:00</published>
  </entry>
</feed>
