<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-22T15:37:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kspe8c</id>
    <title>Best local model OCR solution for PDF document PII redaction app with bounding boxes</title>
    <updated>2025-05-22T12:26:16+00:00</updated>
    <author>
      <name>/u/Sonnyjimmy</name>
      <uri>https://old.reddit.com/user/Sonnyjimmy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I'm a long term lurker in LocalLLaMA. I've created an open source Python/Gradio-based app for redacting personally-identifiable (PII) information from PDF documents, images and tabular data files - you can try it out &lt;a href="https://huggingface.co/spaces/seanpedrickcase/document_redaction"&gt;here&lt;/a&gt; on Hugging Face spaces. The source code on GitHub &lt;a href="https://github.com/seanpedrick-case/doc_redaction"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The app allows users to extract text from documents, using PikePDF/Tesseract OCR locally, or AWS Textract if on cloud, and then identify PII using either Spacy locally or AWS Comprehend if on cloud. The app also has a redaction review GUI, where users can go page by page to modify suggested redactions and add/delete as required before creating a final redacted document (user guide &lt;a href="https://seanpedrick-case.github.io/doc_redaction/#user-guide"&gt;here&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Currently, users mostly use the AWS text extraction service (Textract) as it gives the best results from the existing model choice. but I would like to add in a high quality local OCR option to be able to provide an alternative that does not incur API charges for each use. The existing local OCR option, Tesseract, only works on very simple PDFs, which have typed text and not too much going else going on on the page. But it is fast, and can identify word-level bounding boxes accurately (a requirement for redaction), which a lot of the other OCR options do not as far as I know.&lt;/p&gt; &lt;p&gt;I'm considering a 'mixed' approach. This is to let Tesseract do a first pass to identify 'easy' text (due to its speed), then keep aside the boxes where it has low confidence in its results, and cut out images from the coordinates of the low-confidence 'difficult' boxes to pass onto a vision LLM (e.g. Qwen2.5-VL), or another alternative lower-resource hungry option like PaddleOCR, Surya, or EasyOCR. Ideally, I would like to be able to deploy the app on an instance without a GPU, and still get a page processed within max 5 seconds if at all possible (probably dreaming, hah).&lt;/p&gt; &lt;p&gt;Do you think the above approach could work? What do you think would be the best local model choice for OCR in this case?&lt;/p&gt; &lt;p&gt;Thanks everyone for your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sonnyjimmy"&gt; /u/Sonnyjimmy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kspe8c/best_local_model_ocr_solution_for_pdf_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kspe8c/best_local_model_ocr_solution_for_pdf_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kspe8c/best_local_model_ocr_solution_for_pdf_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T12:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5sh4</id>
    <title>Broke down and bought a Mac Mini - my processes run 5x faster</title>
    <updated>2025-05-21T18:50:39+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran my process on my $850 Beelink Ryzen 9 32gb machine and it took 4 hours to run - the process calls my 8g llm 42 times during the run. It took 4 hours and 18 minutes. The Mac Mini with an M4 Pro chip and 24gb memory took 47 minutes. &lt;/p&gt; &lt;p&gt;It’s a keeper - I’m returning my Beelink. That unified memory in the Mac used half the memory and used the GPU. &lt;/p&gt; &lt;p&gt;I know I could have bought a used gamer rig cheaper but for a lot of reasons - this is perfect for me. I would much prefer not using the MacOS - Windows is a PITA but I’m used to it. It took about 2 hours of cursing to install my stack and port my code. &lt;/p&gt; &lt;p&gt;I have 2 weeks to return it and I’m going to push this thing to the limits. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T18:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksekcn</id>
    <title>Announcing: TiānshūBench 0.0!</title>
    <updated>2025-05-22T01:18:06+00:00</updated>
    <author>
      <name>/u/JeepyTea</name>
      <uri>https://old.reddit.com/user/JeepyTea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"&gt; &lt;img alt="Announcing: TiānshūBench 0.0!" src="https://preview.redd.it/5ykvwmvqh82f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8c728159de15d99f83c21a026feae2e4d1542f" title="Announcing: TiānshūBench 0.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama-sté, local llama-wranglers!&lt;/p&gt; &lt;p&gt;I'm happy to announce that I’ve started work on TiānshūBench (天书Bench), a novel benchmark for evaluating Large Language Models' ability to understand and generate code.&lt;/p&gt; &lt;p&gt;Its distinctive feature is a series of tests which challenge the LLM to solve programming problems in an obscure programming language. Importantly, the &lt;strong&gt;&lt;em&gt;language features are randomized on every test question&lt;/em&gt;&lt;/strong&gt;, helping to ensure that the test questions and answers do not enter the training set. Like the mystical &amp;quot;heavenly script&amp;quot; that inspired its name, the syntax appears foreign at first glance, but the underlying logic remains consistent.&lt;/p&gt; &lt;p&gt;The goal of TiānshūBench is to determine if an AI system truly understands concepts and instructions, or merely reproduces familiar patterns. I believe this approach has a higher ceiling than ARC2, which relies upon ambiguous visual symbols, instead of the well-defined and agreed upon use of language in TiānshūBench.&lt;/p&gt; &lt;p&gt;Here are the results of version 0.0 of TiānshūBench:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;=== Statistics by LLM ===&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/deepseek-r1:14b: 18/50 passed (36.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/phi4:14b-q4_K_M: 10/50 passed (20.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama/qwen3:14b: 23/50 passed (46.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The models I tested are limited by my puny 12 GB 3060 card. If you’d like to see other models tested in the future, let me know.&lt;/p&gt; &lt;p&gt;Also, I believe there are some tweaks needed to ollama to make it perform better, so I’ll be working on those.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;=== Statistics by Problem ID ===&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 0: 3/30 passed (10.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 1: 8/30 passed (26.67%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 2: 7/30 passed (23.33%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 3: 18/30 passed (60.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Test Case 4: 15/30 passed (50.0%)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Initial test cases included a &amp;quot;Hello World&amp;quot; type program, a task requiring input and output, and a filtering task. There is no limit to how sophisticated the tests could be. My next test cases will probably include some beginner programming exercises like counting and sorting. I can see a future when more sophisticated tasks are given, like parsers, databases, and even programming languages!&lt;/p&gt; &lt;p&gt;Future work here will also include multi-shot tests, as that's gives more models a chance to show their true abilities. I also want to be able to make the language even more random, swapping around even more features. Finally, I want to nail down the language description that's fed in as part of the test prompt so there’s no ambiguity when it comes to the meaning of the control structures and other features.&lt;/p&gt; &lt;p&gt;Hit me up if you have any questions or comments, or want to help out. I need more test cases, coding help, access to more powerful hardware, and LLM usage credits!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeepyTea"&gt; /u/JeepyTea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ykvwmvqh82f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksekcn/announcing_tiānshūbench_00/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T01:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksn0y4</id>
    <title>How to check the relative quality of quantized models?</title>
    <updated>2025-05-22T10:08:41+00:00</updated>
    <author>
      <name>/u/sbs1799</name>
      <uri>https://old.reddit.com/user/sbs1799</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am novice in the technical space of LLM. So please bear with me if this is a stupid question. &lt;/p&gt; &lt;p&gt;I understand that in most cases if one were interested in running a open llm on their mac laptops or desktops with NVIDIA gpus, one would be making use of quantized models. For my study purposes, I wanted to pick three best models that fit in m3 128 gb or NVIDIA 48 gb RAM. How do I go about identifying the quality of various quantized - q4, q8, qat, moe etc.* - models? &lt;/p&gt; &lt;p&gt;Is there a place where I can see how q4 quantized Qwen 3 32B compares to say Gemma 3 27B Instruct Q8 model? I am wondering if various quantized versions of different models are themselves subjected to some bechmark tests and relatively ranked by someone?&lt;/p&gt; &lt;p&gt;(* I also admit I don't understand what these different versions mean, except that Q4 is smaller and somewhat less accurate than Q8 and Q16)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbs1799"&gt; /u/sbs1799 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksn0y4/how_to_check_the_relative_quality_of_quantized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksn0y4/how_to_check_the_relative_quality_of_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksn0y4/how_to_check_the_relative_quality_of_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T10:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1krs40j</id>
    <title>Why nobody mentioned "Gemini Diffusion" here? It's a BIG deal</title>
    <updated>2025-05-21T07:42:08+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt; &lt;img alt="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" src="https://external-preview.redd.it/dFWSMq_9jHPdMVGchDlKvt7rzCFhQEFmxZm8XKq654M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b69152f4cc7971773a476232dcff0de3690e29e" title="Why nobody mentioned &amp;quot;Gemini Diffusion&amp;quot; here? It's a BIG deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has the capacity and capability to change the standard for LLMs from autoregressive generation to diffusion generation.&lt;/p&gt; &lt;p&gt;Google showed their Language diffusion model (Gemini Diffusion, visit the linked page for more info and benchmarks) yesterday/today (depends on your timezone), and it was extremely fast and (according to them) only half the size of similar performing models. They showed benchmark scores of the diffusion model compared to Gemini 2.0 Flash-lite, which is a tiny model already.&lt;/p&gt; &lt;p&gt;I know, it's LocalLLaMA, but if Google can prove that diffusion models work at scale, they are a far more viable option for local inference, given the speed gains.&lt;/p&gt; &lt;p&gt;And let's not forget that, since diffusion LLMs process the whole text at once iteratively, it doesn't need KV-Caching. Therefore, it could be more memory efficient. It also has &amp;quot;test time scaling&amp;quot; by nature, since the more passes it is given to iterate, the better the resulting answer, without needing CoT (It can do it in latent space, even, which is much better than discrete tokenspace CoT). &lt;/p&gt; &lt;p&gt;What do you guys think? Is it a good thing for the Local-AI community in the long run that Google is R&amp;amp;D-ing a fresh approach? They’ve got massive resources. They can prove if diffusion models work at scale (bigger models) in future.&lt;/p&gt; &lt;p&gt;(PS: I used a (of course, ethically sourced, local) LLM to correct grammar and structure the text, otherwise it'd be a wall of text) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemini-diffusion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krs40j/why_nobody_mentioned_gemini_diffusion_here_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryxdg</id>
    <title>Meet Mistral Devstral, SOTA open model designed specifically for coding agents</title>
    <updated>2025-05-21T14:15:57+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open Weights : &lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;https://huggingface.co/mistralai/Devstral-Small-2505&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF : &lt;a href="https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF"&gt;https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryxdg/meet_mistral_devstral_sota_open_model_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:15:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks18uf</id>
    <title>Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM</title>
    <updated>2025-05-21T15:50:12+00:00</updated>
    <author>
      <name>/u/erdaltoprak</name>
      <uri>https://old.reddit.com/user/erdaltoprak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt; &lt;img alt="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" src="https://preview.redd.it/ddhhql5ap52f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac522114e2ed7386b3d3e60852472eaf2f4b906" title="Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full model announcement post on the Mistral blog &lt;a href="https://mistral.ai/news/devstral"&gt;https://mistral.ai/news/devstral&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erdaltoprak"&gt; /u/erdaltoprak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ddhhql5ap52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks18uf/mistrals_new_devstral_coding_model_running_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T15:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks1ncf</id>
    <title>Anyone else feel like LLMs aren't actually getting that much better?</title>
    <updated>2025-05-21T16:06:15+00:00</updated>
    <author>
      <name>/u/Swimming_Beginning24</name>
      <uri>https://old.reddit.com/user/Swimming_Beginning24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the game since GPT-3.5 (and even before then with Github Copilot). Over the last 2-3 years I've tried most of the top LLMs: all of the GPT iterations, all of the Claude's, Mistral's, LLama's, Deepseek's, Qwen's, and now Gemini 2.5 Pro Preview 05-06.&lt;/p&gt; &lt;p&gt;Based on benchmarks and LMSYS Arena, one would expect something like the newest Gemini 2.5 Pro to be leaps and bounds ahead of what GPT-3.5 or GPT-4 was. I feel like it's not. My use case is generally technical: longer form coding and system design sorts of questions. I occasionally also have models draft out longer English texts like reports or briefs.&lt;/p&gt; &lt;p&gt;Overall I feel like models still have the same problems that they did when ChatGPT first came out: hallucination, generic LLM babble, hard-to-find bugs in code, system designs that might check out on first pass but aren't fully thought out.&lt;/p&gt; &lt;p&gt;Don't get me wrong, LLMs are still incredible time savers, but they have been since the beginning. I don't know if my prompting techniques are to blame? I don't really engineer prompts at all besides explaining the problem and context as thoroughly as I can.&lt;/p&gt; &lt;p&gt;Does anyone else feel the same way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Beginning24"&gt; /u/Swimming_Beginning24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T16:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kryybf</id>
    <title>mistralai/Devstral-Small-2505 · Hugging Face</title>
    <updated>2025-05-21T14:17:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt; &lt;img alt="mistralai/Devstral-Small-2505 · Hugging Face" src="https://external-preview.redd.it/5v7V2smikryAtPAPRovLgRwqCqqgG7mLENcd1_6EmM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8ec3b7f12dc09c129535d0279c6db5801db61aa" title="mistralai/Devstral-Small-2505 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2505"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kryybf/mistralaidevstralsmall2505_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1krzpyp</id>
    <title>medgemma-4b the Pharmacist 🤣</title>
    <updated>2025-05-21T14:49:13+00:00</updated>
    <author>
      <name>/u/AlternativePlum5151</name>
      <uri>https://old.reddit.com/user/AlternativePlum5151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google’s new OS medical model gave in to the dark side far too easily. I had to laugh. I expected it to put up a little more of a fight, but there you go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlternativePlum5151"&gt; /u/AlternativePlum5151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f25nhvxqd52f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krzpyp/medgemma4b_the_pharmacist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T14:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksdox8</id>
    <title>Harnessing the Universal Geometry of Embeddings</title>
    <updated>2025-05-22T00:32:59+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.12540"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdox8/harnessing_the_universal_geometry_of_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdox8/harnessing_the_universal_geometry_of_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T00:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksh780</id>
    <title>In video intel talks a bit about battlematrix 192GB VRAM</title>
    <updated>2025-05-22T03:37:40+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Intel Sr. Director of Discrete Graphics Qi Lin to learn more about a new breed of inference workstations codenamed Project Battlematrix and the Intel Arc Pro B60 GPUs that help them accelerate local AI workloads. The B60 brings 24GB of VRAM to accommodate larger AI models and supports multi-GPU inferencing with up to eight cards. Project Battlematrix workstations combine these cards with a containerized Linux software stack that’s optimized for LLMs and designed to simplify deployment, and partners have the flexibility to offer different designs based on customer needs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tzOXwxXkjFA"&gt;https://www.youtube.com/watch?v=tzOXwxXkjFA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksh780/in_video_intel_talks_a_bit_about_battlematrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T03:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksrxm7</id>
    <title>Intuitive explanation on diffusion language models (dLLMs) and why they may be far superior to autoregressive for most uses (append &amp; amend VS mutate &amp; defragment)</title>
    <updated>2025-05-22T14:20:49+00:00</updated>
    <author>
      <name>/u/psychonucks</name>
      <uri>https://old.reddit.com/user/psychonucks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been preaching diffusion LLMs for a month now and I believe I can explain clearly why it could be superior to autoregressive, or perhaps they are two complementary hemispheres in a more complete being. Before getting into the theory, let's look at one application first, how I think coding agents are gonna go down with diffusion:&lt;/p&gt; &lt;p&gt;Diffusion LLMs with reinforcement learning for agentic coding are going to be utterly nuts. Imagine memory-mapping a region of the context to some text documents and giving the model commands to scroll the view or follow references and jump around files. DLLMs can edit files directly without an intermediate apply model or outputting diffs. Any mutation made by the model to the tokens in the context would directly be saved to disk in the corresponding file. These models don't accumulate deltas, they remain at ground truth. This means that the running representation of the code it's editing is always in its least complex representation. It isn't some functional operation chain of &lt;code&gt;original + delta + ...&lt;/code&gt; it's mutating the original directly. (inherently less mode-collapsing) Furthermore the memory-mapped file region can be anywhere in the context. The next generation of coding agents is probably like a chunk of context that is allocated to contain some memory-mapped file editing &amp;amp; reading regions, and some prompts or reasoning area. LLMs could have their own &amp;quot;vim&amp;quot; equivalent for code navigation, and maybe they could even fit multiple regions in one context to navigate them separately in parallel and cross-reference data. The model could teach itself to choose dynamically between one large view buffer over one file, or many tiny views over many files, dividing up the context window to have multiple parallel probe points, which could be more useful for tracing an exception. Imagine the policies that can be discovered automatically by RL.&lt;/p&gt; &lt;p&gt;One creative inference system I am eager to try is to set-up a 1D cellular automaton which generates floats over the text in an anisotropic landscape fashion (think perlin noise, how it is irregular and cannot be predicted) and calculating the perplexity and varentropy on each token, and then injecting the tokens with noise that is masked by the varentropy &amp;amp; automaton's activation, or injecting space or tokens. This essentially creates a guided search at high variance pressure points in the text and causes the text to &amp;quot;unroll&amp;quot; wherever ambiguity lies. Each unrolling point may result in another unrelated part of the text shooting up in varentropy because it suddenly changes the meaning, so this could be a potent test-time scaling loop that goes on for a very long time unrolling a small seed to document to a massive well-thought out essay or thesis or whatever creative work you are asking the system. This is a strategy in the near future I believe could do things we might call super-intelligence.&lt;/p&gt; &lt;p&gt;An autoregressive model cannot do this because it can only append and amend. It can call tools like sed to mutate text, but it's not differentiable and doesn't learn mechanics of mutation. Diffusion models are more resistant to degeneration and can recover better. If an output degenerates in an autoregressive model, it has to amend the crap (&amp;quot;I apologize, I have made a mistake&amp;quot;) and cannot actually erase from its context window. It can't defragment text or optimize it like diffusers, certainly not as a native operation. Diffusion LLMs will result in models that &amp;quot;just do things&amp;quot;. The model doesn't have to say &amp;quot;wait, I see the problem&amp;quot; because the code is labeled as a problem-state by nature of its encoding and there are natural gradients that the model can climb or navigate that bridge problem-state to correctness-state.&lt;/p&gt; &lt;p&gt;Diffusion language models cut out an unnecessary operation, which albeit does raise question as to safety. We will not understand anymore why the ideas or code that appears on the screen is as it is unless we decisively RL a scratchpad, training the model to reserve some context buffer for a reasoning scratch pad. BTW as we said earlier with diffusion LLMs we can do in-painting just like image models, by masking which tokens should be frozen or allowed to change. That means you can hard-code a sequential unmasking schedule over certain views, and possibly get sequential-style reasoning in parallel with the memory-mapped code editing regions. And this is why I took such a long roundabout way to this explanation. Now finally we can see why diffusion language models are simply superior: they can be trained to support reasoning in parallel as they edit code. &lt;strong&gt;Diffusion LLMs generalize the autoregressive model through sequential unmasking schedules, and allow the model to be progressively taken out of distribution into the full-space of non-sequential idea formation that is private to the human brain and not found in any dataset.&lt;/strong&gt; By bootstrapping this spectrum, now humans can manually program it and bias the models closer to the way it works for us, or hand-design something even more powerful or obtuse than human imagination. Like all models, it does not &amp;quot;learn&amp;quot; but rather guesses / discovers a weight structure that can explain the dataset. The base output of a diffusion LLM is not that newsworthy. Sure it's faster and it looks really cool, but at a glance it's not clear why this would be better than what the same dataset could train in auto-regressive. No, it's the fact that we have a new pool of representations and operations that we can rearrange to construct something closer to the way that humans use their brains, or directly crystallizing it by random search guided by RL objectives. &lt;/p&gt; &lt;p&gt;We should think of diffusion LLMs as an &lt;strong&gt;evolution operator or physics engine for a context window&lt;/strong&gt;. It's a super-massive ruleset which defines how a given context (text document) is allowed to mutate, iterate, or be stepped forward in time. It's a scaled up cellular automaton. What everybody should keep in mind here is that diffusion LLMs can mutate infinitely. There is no 'maximum context window' in a dLLM because the append / amend history is unnecessary. The model can work on a document for 13 hours, optimizing tokens. Text is transformative, compounds on itselfs, and rewrites itself. Text is self-aware and cognizant of its own state of being. In an image diffusion model, the rules are programmed by a prompt that is separate from the output. But language diffusion models are different, because the prompt and the output are the same. Diffusion LLMs are more resistant to out of distribution areas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psychonucks"&gt; /u/psychonucks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksrxm7/intuitive_explanation_on_diffusion_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksrxm7/intuitive_explanation_on_diffusion_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksrxm7/intuitive_explanation_on_diffusion_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T14:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kstdhn</id>
    <title>Notes on AlphaEvolve: Are we closing in on Singularity?</title>
    <updated>2025-05-22T15:19:33+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepMind released the AlphaEvolve paper last week, which, considering what they have achieved, is arguably one of the most important papers of the year. But I found the discourse around it was very thin, not many who actively cover the AI space have talked much about it.&lt;/p&gt; &lt;p&gt;So, I made some notes on the important aspects of AlphaEvolve.&lt;/p&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;DeepMind calls it an &amp;quot;agent&amp;quot;, but it was not your run-of-the-mill agent, but a meta-cognitive system. The agent architecture has the following components&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Problem: An entire codebase or a part of it marked with # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END. Only this part of it will be evolved.&lt;/li&gt; &lt;li&gt;LLM ensemble: They used Gemini 2.0 Pro for complex reasoning and 2.5 flash for faster operations.&lt;/li&gt; &lt;li&gt;Evolutionary database: The most important part, the database uses map-elite and Island architecture to store solutions and inspirations. &lt;/li&gt; &lt;li&gt;Prompt Sampling: A combination of previous best results, inspirations, and human contexts for improving the existing solution.&lt;/li&gt; &lt;li&gt;Evaluation Framework: A Python function for evaluating the answers, and it returns array of scalars.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Working in brief&lt;/h1&gt; &lt;p&gt;The database maintains &amp;quot;parent&amp;quot; programs marked for improvement and &amp;quot;inspirations&amp;quot; for adding diversity to the solution. (The name &amp;quot;AlphaEvolve&amp;quot; itself actually comes from it being an &amp;quot;Alpha&amp;quot; series agent that &amp;quot;Evolves&amp;quot; solutions, rather than just this parent/inspiration idea).&lt;/p&gt; &lt;p&gt;Here’s how it generally flows: the AlphaEvolve system gets the initial codebase. Then, for each step, the &lt;strong&gt;prompt sampler&lt;/strong&gt; cleverly picks out parent program(s) to work on and some inspiration programs. It bundles these up with &lt;strong&gt;feedback from past attempts (like scores or even what an LLM thought about previous versions)&lt;/strong&gt;, plus any handy human context. This whole package goes to the LLMs.&lt;/p&gt; &lt;p&gt;The new solution they come up with (the &amp;quot;child&amp;quot;) gets graded by the &lt;strong&gt;evaluation function&lt;/strong&gt;. Finally, these child solutions, with their new grades, are stored back in the database.&lt;/p&gt; &lt;h1&gt;The Outcome&lt;/h1&gt; &lt;p&gt;The most interesting part even with older models like Gemini 2.0 Pro and Flash, when AlphaEvolve took on over 50 open math problems, it managed to match the best solutions out there for 75% of them, actually found better answers for another 20%, and only came up short on a tiny 5%!&lt;/p&gt; &lt;p&gt;Out of all, DeepMind is most proud of AlphaEvolve surpassing Strassen's 56-year-old algorithm for 4x4 complex matrix multiplication by finding a method with 48 scalar multiplications.&lt;/p&gt; &lt;p&gt;And also the agent improved Google's infra by speeding up Gemini LLM training by ~1%, improving data centre job scheduling to recover ~0.7% of fleet-wide compute resources, optimising TPU circuit designs, and accelerating compiler-generated code for AI kernels by up to 32%.&lt;/p&gt; &lt;p&gt;This is the best agent scaffolding to date. The fact that they pulled this off with an outdated Gemini, imagine what they can do with the current SOTA. This makes it one thing clear: what we're lacking for efficient agent swarms doing tasks is the right abstractions. Though the cost of operation is not disclosed.&lt;/p&gt; &lt;p&gt;For a detailed blog post, check this out: &lt;a href="https://composio.dev/blog/alphaevolve-evolutionary-agent-from-deepmind/"&gt;AlphaEvolve: the self-evolving agent from DeepMind&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It'd be interesting to see if they ever release it in the wild or if any other lab picks it up. This is certainly the best frontier for building agents.&lt;/p&gt; &lt;p&gt;Would love to know your thoughts on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T15:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksdeup</id>
    <title>4-bit quantized Moondream: 42% less memory with 99.4% accuracy</title>
    <updated>2025-05-22T00:19:04+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/smaller-faster-moondream-with-qat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdeup/4bit_quantized_moondream_42_less_memory_with_994/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksdeup/4bit_quantized_moondream_42_less_memory_with_994/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T00:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksfqc4</id>
    <title>Open-Sourced Multimodal Large Diffusion Language Models</title>
    <updated>2025-05-22T02:18:45+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"&gt; &lt;img alt="Open-Sourced Multimodal Large Diffusion Language Models" src="https://external-preview.redd.it/iZId4FACbwvJcU6NEqYQYxxICVbn6LyYgehUX8eXjRY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e1a6cba69a1a9f17b0fbd00276cd418ded8eda" title="Open-Sourced Multimodal Large Diffusion Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MMaDA is a new family of &lt;strong&gt;multimodal diffusion foundation models&lt;/strong&gt; designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. MMaDA is distinguished by three key innovations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MMaDA adopts a &lt;strong&gt;unified diffusion architecture&lt;/strong&gt; with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components.&lt;/li&gt; &lt;li&gt;MMaDA introduces a &lt;strong&gt;mixed long chain-of-thought (CoT) fine-tuning&lt;/strong&gt; strategy that curates a unified CoT format across modalities.&lt;/li&gt; &lt;li&gt;MMaDA adopts a unified policy-gradient-based RL algorithm, which we call &lt;strong&gt;UniGRPO&lt;/strong&gt;, tailored for diffusion foundation models. Utilizing diversified reward modeling, &lt;strong&gt;UniGRPO&lt;/strong&gt; unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Gen-Verse/MMaDA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfqc4/opensourced_multimodal_large_diffusion_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T02:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmvab</id>
    <title>RpR-v4 now with less repetition and impersonation!</title>
    <updated>2025-05-22T09:58:43+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmvab/rprv4_now_with_less_repetition_and_impersonation/"&gt; &lt;img alt="RpR-v4 now with less repetition and impersonation!" src="https://external-preview.redd.it/PUya7H9A7A-uaz_ICZ2xgjCKFF5-tr6wZqYRRVu-rws.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f372281d0007f52ba9c74a9d741c9d4fa813fe88" title="RpR-v4 now with less repetition and impersonation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/QwQ-32B-ArliAI-RpR-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmvab/rprv4_now_with_less_repetition_and_impersonation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmvab/rprv4_now_with_less_repetition_and_impersonation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T09:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmhe9</id>
    <title>MMaDA: Multimodal Large Diffusion Language Models</title>
    <updated>2025-05-22T09:31:41+00:00</updated>
    <author>
      <name>/u/First_Ground_9849</name>
      <uri>https://old.reddit.com/user/First_Ground_9849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Gen-Verse/MMaDA"&gt;https://github.com/Gen-Verse/MMaDA&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Gen-Verse/MMaDA-8B-Base"&gt;https://huggingface.co/Gen-Verse/MMaDA-8B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Ground_9849"&gt; /u/First_Ground_9849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmhe9/mmada_multimodal_large_diffusion_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmhe9/mmada_multimodal_large_diffusion_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmhe9/mmada_multimodal_large_diffusion_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T09:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksjee6</id>
    <title>Falcon-H1: hybrid Transformer–SSM model series from 0.5B to 34B</title>
    <updated>2025-05-22T05:52:10+00:00</updated>
    <author>
      <name>/u/JingweiZUO</name>
      <uri>https://old.reddit.com/user/JingweiZUO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🔬 Hybrid architecture: Attention + Mamba2 heads in parallel&lt;/p&gt; &lt;p&gt;🧠 From 0.5B, 1.5B, 1.5B-Deep,3B, 7B to 34B&lt;/p&gt; &lt;p&gt;📏 up to 256K context&lt;/p&gt; &lt;p&gt;🔥 Outperforming and rivaling top Transformer models like Qwen3-32B, Qwen2.5-72B, Llama4-Scout-17B/109B, and Gemma3-27B — consistently outperforming models up to 2× their size. &lt;/p&gt; &lt;p&gt;💥 Falcon-H1-0.5B ≈ typical 7B models from 2024, Falcon-H1-1.5B-Deep ≈ current leading 7B–10B models &lt;/p&gt; &lt;p&gt;🌍 Multilingual: Native support for 18 languages (scalable to 100+)&lt;/p&gt; &lt;p&gt;⚙️ Customized μP recipe + optimized data strategy&lt;/p&gt; &lt;p&gt;🤖 Integrated to vLLM, Hugging Face Transformers, and llama.cpp — with more coming soon&lt;/p&gt; &lt;p&gt;All the comments and feedback from the community are greatly welcome.&lt;/p&gt; &lt;p&gt;Blogpost: &lt;a href="https://falcon-lm.github.io/blog/falcon-h1/"&gt;https://falcon-lm.github.io/blog/falcon-h1/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/tiiuae/falcon-h1"&gt;https://github.com/tiiuae/falcon-h1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JingweiZUO"&gt; /u/JingweiZUO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjee6/falconh1_hybrid_transformerssm_model_series_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T05:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksfos8</id>
    <title>Why has no one been talking about Open Hands so far?</title>
    <updated>2025-05-22T02:16:28+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I just stumbled across Open Hands while checking out Mistral’s new Devstral model—and honestly, I was really impressed. The agent itself seems super capable, yet I feel like barely anyone is talking about it?&lt;/p&gt; &lt;p&gt;What’s weird is that OpenHands has 54k+ stars on GitHub. For comparison: Roo Code sits at ~14k, and Cline is around 44k. So it’s clearly on the radar of devs. But when you go look it up on YouTube or Reddit—nothing. Practically no real discussion, no deep dives, barely any content.&lt;/p&gt; &lt;p&gt;And I’m just sitting here wondering… why?&lt;/p&gt; &lt;p&gt;From what I’ve seen so far, it seems just as capable as the other top open-source agents. So are you guys using OpenHands? Is there some kind of limitation I’ve missed? Or is it just a case of bad marketing/no community hype?&lt;/p&gt; &lt;p&gt;Curious to hear your thoughts.&lt;/p&gt; &lt;p&gt;Also, do you think models specifically trained for a certain agent is the future? Are we going to see more agent specific models going forward and how big do you think is the effort to create these fine tunes? Will it depend on collaborations with big names the likes of Mistral or will Roo et al. be able to provide fine tunes on their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksfos8/why_has_no_one_been_talking_about_open_hands_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T02:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kss44x</id>
    <title>Tiny agents from hugging face is great for llama.cpp mcp agents</title>
    <updated>2025-05-22T14:28:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tiny agents have to be the easiest browsers control setup, you just the cli, a json, and a prompt definition. &lt;/p&gt; &lt;p&gt;- it uses main MCPs, like Playright, mcp-remote&lt;br /&gt; - works with local models via openai compatible server&lt;br /&gt; - model can controls the browser or local files without calling APIs&lt;/p&gt; &lt;p&gt;here's a tutorial form the MCP course &lt;a href="https://huggingface.co/learn/mcp-course/unit2/tiny-agents"&gt;https://huggingface.co/learn/mcp-course/unit2/tiny-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T14:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksklse</id>
    <title>I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image</title>
    <updated>2025-05-22T07:15:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt; &lt;img alt="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" src="https://external-preview.redd.it/emh3Y3JjbjlhYTJmMdq-zCDOPop6wDopQzw_Axrs5Q3Ewmi7BuHyc4moiH9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=334a76aa4fd56af8a4b415b1555c615a82e68a46" title="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to the official description, &lt;strong&gt;3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6as4adn9aa2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T07:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmiwz</id>
    <title>👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp; More Coming!</title>
    <updated>2025-05-22T09:34:51+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt; &lt;img alt="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" src="https://b.thumbs.redditmedia.com/qSkeoL3Zvn8J5J0e71_KbF_aotj9r_uZphIqZ9sA98I.jpg" title="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has released a new preview version of their Gemma 3n model on Hugging Face: google/gemma-3n-E4B-it-litert-preview&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20"&gt;https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some key takeaways from the model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Input:&lt;/strong&gt; This model is designed to handle text, image, video, and audio input, generating text outputs. The current checkpoint on Hugging Face supports text and vision input, with full multimodal features expected soon.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture:&lt;/strong&gt; Gemma 3n models feature a novel architecture that allows them to run with a smaller number of effective parameters (E2B and E4B variants mentioned). They also utilize a Matformer architecture for nesting multiple models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Resource Devices:&lt;/strong&gt; These models are specifically designed for efficient execution on low-resource devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Parameter Activation:&lt;/strong&gt; This technology helps reduce resource requirements, allowing the models to operate at an effective size of 2B and 4B parameters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Data:&lt;/strong&gt; Trained on a dataset of approximately 11 trillion tokens, including web documents, code, mathematics, images, and audio, with a knowledge cutoff of June 2024.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intended Uses:&lt;/strong&gt; Suited for tasks like content creation (text, code, etc.), chatbots, text summarization, and image/audio data extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preview Version:&lt;/strong&gt; Keep in mind this is a preview version, intended for use with Google AI Edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You'll need to agree to Google's usage license on Hugging Face to access the model files. You can find it by searching for google/gemma-3n-E4B-it-litert-preview on Hugging Face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T09:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kso7p2</id>
    <title>AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp; Radeon RX 9000 Series GPUs</title>
    <updated>2025-05-22T11:22:20+00:00</updated>
    <author>
      <name>/u/nostriluu</name>
      <uri>https://old.reddit.com/user/nostriluu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"&gt; &lt;img alt="AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp;amp; Radeon RX 9000 Series GPUs" src="https://external-preview.redd.it/ZrbQ75vRAB5hVtrNdq8cJcDVR-h2KRgOrR5RepitAdo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56d443870db73b9730484569753cff8b7852157" title="AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp;amp; Radeon RX 9000 Series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nostriluu"&gt; /u/nostriluu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-takes-a-major-leap-in-edge-ai-with-rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T11:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksjkhb</id>
    <title>Jan is now Apache 2.0</title>
    <updated>2025-05-22T06:03:22+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt; &lt;img alt="Jan is now Apache 2.0" src="https://external-preview.redd.it/URelWOcOKsdGwEnGYxMQqnu09GiloVzXPjQD9-QBbco.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f6ee4e949835d86b3d3ceaef317ab0dc1752b1" title="Jan is now Apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we've just changed &lt;a href="https://jan.ai/"&gt;Jan&lt;/a&gt;'s license. &lt;/p&gt; &lt;p&gt;Jan has always been open-source, but the AGPL license made it hard for many teams to actually use it. Jan is now licensed under Apache 2.0, a more permissive, industry-standard license that works inside companies as well.&lt;/p&gt; &lt;p&gt;What this means:&lt;/p&gt; &lt;p&gt;– You can bring Jan into your org without legal overhead&lt;br /&gt; – You can fork it, modify it, ship it&lt;br /&gt; – You don't need to ask permission&lt;/p&gt; &lt;p&gt;This makes Jan easier to adopt. At scale. In the real world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/menloresearch/jan/blob/dev/LICENSE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T06:03:22+00:00</published>
  </entry>
</feed>
