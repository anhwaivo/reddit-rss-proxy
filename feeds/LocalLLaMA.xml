<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-03T14:38:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lqo1bt</id>
    <title>I want to split a model to run a portion of it on client and run the remaining layers on server. Is that possible?</title>
    <updated>2025-07-03T12:14:47+00:00</updated>
    <author>
      <name>/u/crazycodemonkey</name>
      <uri>https://old.reddit.com/user/crazycodemonkey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.&lt;br /&gt; While I understand this doesn't exactly solve the privacy issue, this level of information loss is enough for my usecase.&lt;/p&gt; &lt;p&gt;My questions:&lt;br /&gt; 1. Is something like this even possible? Has anybody done something like this before?&lt;br /&gt; 2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazycodemonkey"&gt; /u/crazycodemonkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T12:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpvcb</id>
    <title>Best local TEXT EXTRACTION model 24GB/48GB?</title>
    <updated>2025-07-03T13:40:01+00:00</updated>
    <author>
      <name>/u/Otherwise-Tiger3359</name>
      <uri>https://old.reddit.com/user/Otherwise-Tiger3359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been liking Gemma3 but the text extraction performance is far, far behind any of the &amp;quot;chat&amp;quot; offerings. Can one do better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Tiger3359"&gt; /u/Otherwise-Tiger3359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T13:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq79xx</id>
    <title>FP8 fixed on VLLM for RTX Pro 6000 (and RTX 5000 desktop cards)</title>
    <updated>2025-07-02T21:02:46+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yay! Been waiting for this one for a while, guessing I'm not the only one? &lt;a href="https://github.com/vllm-project/vllm/pull/17280"&gt;https://github.com/vllm-project/vllm/pull/17280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On 70B I'm maxing out around 1400T/s on the Pro 6000 with 100 threads.&lt;/p&gt; &lt;p&gt;Quick install instructions if you want to try it:&lt;/p&gt; &lt;p&gt;mkdir vllm-src&lt;br /&gt; cd vllm-src&lt;br /&gt; python3 -m venv myenv&lt;br /&gt; source myenv/bin/activate&lt;br /&gt; pip install torch torchvision torchaudio --index-url &lt;a href="https://download.pytorch.org/whl/cu128"&gt;https://download.pytorch.org/whl/cu128&lt;/a&gt;&lt;br /&gt; git clone &lt;a href="https://github.com/huggingface/transformers.git"&gt;https://github.com/huggingface/transformers.git&lt;/a&gt;&lt;br /&gt; git clone &lt;a href="https://github.com/vllm-project/vllm.git"&gt;https://github.com/vllm-project/vllm.git&lt;/a&gt;&lt;br /&gt; cd transformers&lt;br /&gt; pip install -e .&lt;br /&gt; cd ../vllm&lt;br /&gt; python use_existing_torch.py&lt;br /&gt; pip install -r requirements/build.txt&lt;br /&gt; pip install -r requirements/cuda.txt&lt;br /&gt; pip install -e . --no-build-isolation&lt;br /&gt; vllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic&lt;br /&gt; vllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqhers</id>
    <title>Any updates on Llama models from Meta?</title>
    <updated>2025-07-03T05:18:42+00:00</updated>
    <author>
      <name>/u/True_Requirement_891</name>
      <uri>https://old.reddit.com/user/True_Requirement_891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while and llama maverick and scout are still shite. I have tried nearly every provider at this point. &lt;/p&gt; &lt;p&gt;Any updates if they're gonna launch any improvements to these models or any new reasoning models? &lt;/p&gt; &lt;p&gt;How are they fucking up this bad? Near unlimited money, resources, researchers. What are they doing wrong? &lt;/p&gt; &lt;p&gt;They weren't that far behind in the LLM race compared to Google and now they are like behind everyone at this point. &lt;/p&gt; &lt;p&gt;And any updates on Microsoft? They're not gonna do their own models &amp;quot;Big Ones&amp;quot; and are completely reliant on OpenAI?&lt;/p&gt; &lt;p&gt;Chinese companies are releasing models left and right... I tested Ernie models and they're better than Llama 4s&lt;/p&gt; &lt;p&gt;DeepSeek-V3-0324 seems to be the best non-reasoning open source LLM we have.&lt;/p&gt; &lt;p&gt;Are there even any projects that have attempted to improve Llama4s via fine-tuning it or other magical techniques we have? God it's so shite, it's comprehension abilities are just embarrassing. It feels like you can find a million models that are far better than llama 4s for almost anything. The only thing they seem to have is speed on VRAM constrained setups but what's the point when then responses are useless? It's a waste of resource at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/True_Requirement_891"&gt; /u/True_Requirement_891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T05:18:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqo8q0</id>
    <title>Llama.cpp after Ollama for industry grade softwares</title>
    <updated>2025-07-03T12:24:51+00:00</updated>
    <author>
      <name>/u/bull_bear25</name>
      <uri>https://old.reddit.com/user/bull_bear25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone &lt;/p&gt; &lt;p&gt;I am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application &lt;/p&gt; &lt;p&gt;Now, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bull_bear25"&gt; /u/bull_bear25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T12:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq7vjc</id>
    <title>I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use</title>
    <updated>2025-07-02T21:27:35+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"&gt; &lt;img alt="I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use" src="https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebf4e25308a0e578d32f61beb93e3128aaa19efe" title="I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gh20o4e63jaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1jyr</id>
    <title>Mamba-2 support in llama.cpp landed</title>
    <updated>2025-07-02T17:14:08+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt; &lt;img alt="Mamba-2 support in llama.cpp landed" src="https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d" title="Mamba-2 support in llama.cpp landed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T17:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqebbv</id>
    <title>Kwai-Keye/Keye-VL-8B-Preview · Hugging Face</title>
    <updated>2025-07-03T02:29:26+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"&gt; &lt;img alt="Kwai-Keye/Keye-VL-8B-Preview · Hugging Face" src="https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a674ea1c399ba022e42f0633ac66250ac99a0f9e" title="Kwai-Keye/Keye-VL-8B-Preview · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.01949"&gt;https://arxiv.org/abs/2507.01949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://kwai-keye.github.io/"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Kwai-Keye/Keye"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T02:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq8gjv</id>
    <title>Ubuntu 24.04: observing that nvidia-535 drivers run 20 tokens/sec faster than nvidia-570 drivers with no other changes in my vLLM setup</title>
    <updated>2025-07-02T21:52:28+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.&lt;/p&gt; &lt;p&gt;I was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using &lt;code&gt;apt&lt;/code&gt; to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.&lt;/p&gt; &lt;p&gt;Curious if anyone has seen similar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:52:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqmbh3</id>
    <title>Anyone here run llama4 scout/Maverick with 1 million to 10 million context?</title>
    <updated>2025-07-03T10:38:22+00:00</updated>
    <author>
      <name>/u/night0x63</name>
      <uri>https://old.reddit.com/user/night0x63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Anyone here run llama4 with 1 million to 10 million context?&lt;/h3&gt; &lt;p&gt;Just curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.&lt;/p&gt; &lt;p&gt;What are vram/ram requirements for 1m context? 10m context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/night0x63"&gt; /u/night0x63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqmmv2</id>
    <title>Best way to get an LLM to sound like me? Prompt eng or Finetune?</title>
    <updated>2025-07-03T10:57:32+00:00</updated>
    <author>
      <name>/u/RelevantPractice2074</name>
      <uri>https://old.reddit.com/user/RelevantPractice2074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.&lt;/p&gt; &lt;p&gt;My use case: Creating social content which sounds like me, not AI slop.&lt;/p&gt; &lt;p&gt;What's the best way to do this nowadays? Would appreciate any direction&lt;/p&gt; &lt;p&gt;Edit for more context: Right now I'm generating content with a powerful model, then I'm aiming to do the 'styling' in a final call.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelevantPractice2074"&gt; /u/RelevantPractice2074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:57:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqj3eq</id>
    <title>Sharing new inference engines I got to know recently</title>
    <updated>2025-07-03T07:04:38+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/cactus-compute/cactus"&gt;https://github.com/cactus-compute/cactus&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/jafioti/luminal"&gt;https://github.com/jafioti/luminal&lt;/a&gt; ( Rust )&lt;/p&gt; &lt;p&gt;Catus seems to start from fork of llama.cpp. (similar to Ollama)&lt;/p&gt; &lt;p&gt;Luminal is more interesting since it rebuild everything.&lt;br /&gt; GeoHot from Tinygrad is quite active in Luminal's Discord too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T07:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqx16</id>
    <title>Kyutai Unmute (incl. TTS) released</title>
    <updated>2025-07-03T14:25:11+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmute github: &lt;a href="https://github.com/kyutai-labs/unmute"&gt;https://github.com/kyutai-labs/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unmute blog: &lt;a href="https://kyutai.org/next/unmute"&gt;https://kyutai.org/next/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS blog with a demo: &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS weights: &lt;a href="https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29"&gt;https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;STT was released earlier so the whole component stack is now out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqndyy</id>
    <title>Hey r/LocalLLaMA! We made evolutionary model merging feasible on consumer GPUs – meet Mergenetic 🧬</title>
    <updated>2025-07-03T11:40:36+00:00</updated>
    <author>
      <name>/u/leviatan0</name>
      <uri>https://old.reddit.com/user/leviatan0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with &lt;strong&gt;Mergenetic&lt;/strong&gt;, an open-source library that makes &lt;em&gt;evolutionary&lt;/em&gt; merging practical without needing big hardware.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evolves high-quality LLM merges using evolutionary algorithms&lt;/li&gt; &lt;li&gt;Supports SLERP, TIES, DARE, Task Arithmetic, and more&lt;/li&gt; &lt;li&gt;Efficient: search happens in parameter space, not gradient needed&lt;/li&gt; &lt;li&gt;Modular, hackable, and built on familiar tools (&lt;code&gt;mergekit&lt;/code&gt;, &lt;code&gt;pymoo&lt;/code&gt;, &lt;code&gt;lm-eval-harness&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.&lt;/p&gt; &lt;p&gt;For details, check out our papers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ACL 2025 Demo: &lt;a href="https://arxiv.org/pdf/2505.11427"&gt;arxiv.org/abs/2505.11427&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ICML 2025: &lt;a href="https://arxiv.org/pdf/2502.10436"&gt;arxiv.org/abs/2502.10436&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/tommasomncttn/mergenetic"&gt;GitHub: tommasomncttn/mergenetic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or contributions — hope it’s useful to some of you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leviatan0"&gt; /u/leviatan0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T11:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqlsyb</id>
    <title>Yappp - Yet Another Poor Peasent Post</title>
    <updated>2025-07-03T10:06:40+00:00</updated>
    <author>
      <name>/u/needthosepylons</name>
      <uri>https://old.reddit.com/user/needthosepylons</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I wanted to share my experience and hear about yours.&lt;/p&gt; &lt;p&gt;Hardware : &lt;/p&gt; &lt;p&gt;GPU : 3060 12GB CPU : i5-3060 RAM : 32GB&lt;/p&gt; &lt;p&gt;Front-end : Koboldcpp + open-webui&lt;/p&gt; &lt;p&gt;Use cases : General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt; &lt;p&gt;I've been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. &lt;/p&gt; &lt;p&gt;GEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they're quite fast, and have a good ability to stick to the prompt. &lt;/p&gt; &lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it's variants. &lt;/p&gt; &lt;p&gt;What are your experiences? Do you use other models of the same range? &lt;/p&gt; &lt;p&gt;Good day everyone! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/needthosepylons"&gt; /u/needthosepylons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqxhq</id>
    <title>I have made a True Reasoning LLM</title>
    <updated>2025-07-03T14:25:42+00:00</updated>
    <author>
      <name>/u/moilanopyzedev</name>
      <uri>https://old.reddit.com/user/moilanopyzedev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt; &lt;p&gt;You can get it here&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/moelanoby/phi-3-M3-coder"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moilanopyzedev"&gt; /u/moilanopyzedev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqdcgr</id>
    <title>PrivateScribe.ai - a fully local, MIT licensed AI transcription platform</title>
    <updated>2025-07-03T01:40:31+00:00</updated>
    <author>
      <name>/u/SecondPathDev</name>
      <uri>https://old.reddit.com/user/SecondPathDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt; &lt;img alt="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" src="https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67733391cf800cb69df3f2bbf96c8c0dcd8a7ecb" title="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share my first open source project - PrivateScribe.ai.&lt;/p&gt; &lt;p&gt;I’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.&lt;/p&gt; &lt;p&gt;I’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts on the idea or things people would want for other use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SecondPathDev"&gt; /u/SecondPathDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://www.privatescribe.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T01:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnczx</id>
    <title>AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework</title>
    <updated>2025-07-03T11:39:08+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"&gt; &lt;img alt="AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework" src="https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b7cac54a1763a205a826aebda0fe3bb69d0bd91" title="AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AIDC-AI/Ovis-U1-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T11:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpm60</id>
    <title>[Upcoming Release &amp; Feedback] A new 4B &amp; 20B model, building on our SmallThinker work. Plus, a new hardware device to run them locally.</title>
    <updated>2025-07-03T13:28:43+00:00</updated>
    <author>
      <name>/u/yzmizeyu</name>
      <uri>https://old.reddit.com/user/yzmizeyu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We're the startup team behind some of the projects you might be familiar with, including &lt;strong&gt;PowerInfer (&lt;a href="https://github.com/SJTU-IPADS/PowerInfer"&gt;https://github.com/SJTU-IPADS/PowerInfer&lt;/a&gt;)&lt;/strong&gt; and &lt;strong&gt;SmallThinker (&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-3B-Preview"&gt;https://huggingface.co/PowerInfer/SmallThinker-3B-Preview&lt;/a&gt;)&lt;/strong&gt;. The feedback from this community has been crucial, and we're excited to give you a heads-up on our next open-source release coming in &lt;strong&gt;late July&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We're releasing two new MoE models, both of which we have &lt;strong&gt;pre-trained from scratch&lt;/strong&gt; with a structure specifically optimized for efficient inference on edge devices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A new 4B Reasoning Model:&lt;/strong&gt; An evolution of SmallThinker with significantly improved logic capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 20B Model:&lt;/strong&gt; Designed for high performance in a local-first environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We'll be releasing the &lt;strong&gt;full weights, a technical report, and parts of the training dataset&lt;/strong&gt; for both.&lt;/p&gt; &lt;p&gt;Our core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we've also been developing a dedicated edge device. It's a small, self-contained unit (&lt;strong&gt;around 10x7x1.5 cm&lt;/strong&gt;) capable of running the 20B model completely offline with a power draw of &lt;strong&gt;around 30W&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is still a work in progress, but it proves what's possible with full-stack optimization. We'd love to get your feedback on this direction:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;For a compact, private device like this, what are the most compelling use cases you can imagine?&lt;/li&gt; &lt;li&gt;For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?&lt;/li&gt; &lt;li&gt;Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We'll be in the comments to answer questions. We're incredibly excited to share our work and believe local AI is the future we're all building together&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yzmizeyu"&gt; /u/yzmizeyu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T13:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqbmwa</id>
    <title>DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp; 20% faster than R1</title>
    <updated>2025-07-03T00:15:16+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt; &lt;img alt="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" src="https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97a766bd7b9c921ab450ffb020d26db72a498fc7" title="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T00:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq5fqq</id>
    <title>I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source</title>
    <updated>2025-07-02T19:48:05+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt; &lt;img alt="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" src="https://preview.redd.it/nmerohq4miaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82000e0b8dd6c6f395384b8459f531f8884586e0" title="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmerohq4miaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:48:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqi863</id>
    <title>DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling</title>
    <updated>2025-07-03T06:08:33+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"&gt; &lt;img alt="DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling" src="https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01af29ebca7d21fb21f6786fc5df5242a0853781" title="DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33"&gt;https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/agentica-org/DeepSWE-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T06:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqkknh</id>
    <title>Jan now supports MCP servers as an experimental feature</title>
    <updated>2025-07-03T08:44:41+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"&gt; &lt;img alt="Jan now supports MCP servers as an experimental feature" src="https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413b9cacad71e68f248ebbfebbea2dd7080cf113" title="Jan now supports MCP servers as an experimental feature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, this is Emre from the Jan team. &lt;/p&gt; &lt;p&gt;We've been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.&lt;/p&gt; &lt;p&gt;How to try MCP in Jan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Settings -&amp;gt; General -&amp;gt; toggle &amp;quot;Experimental Features&amp;quot;&lt;/li&gt; &lt;li&gt;A new &amp;quot;MCP Servers&amp;quot; tab appears -&amp;gt; add or enable your server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick tip: To use MCP servers, make sure the model's Tools capability is enabled.&lt;/p&gt; &lt;p&gt;Full doc with screenshots: &lt;a href="https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan"&gt;https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.&lt;/p&gt; &lt;p&gt;Plus, since then we've pushed a few hot-fixes to smooth out model loading and MCP performance.&lt;/p&gt; &lt;p&gt;Other recent fixes &amp;amp; tweaks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).&lt;/li&gt; &lt;li&gt;We fixed a bug that caused some GGUF models to get stuck while loading.&lt;/li&gt; &lt;li&gt;Lighter UI polish and clearer error messages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this update, Jan now supports &lt;a href="https://huggingface.co/Menlo/Jan-nano-gguf"&gt;Jan-nano 4B &lt;/a&gt;as well, it's available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.&lt;/p&gt; &lt;p&gt;For the latest build, please update your Jan or &lt;a href="https://jan.ai/"&gt;download the latest&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8sdnjxd6emaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T08:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqh55j</id>
    <title>No love for these new models?</title>
    <updated>2025-07-03T05:03:21+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dots&lt;/p&gt; &lt;p&gt;Minimax&lt;/p&gt; &lt;p&gt;Hunyuan&lt;/p&gt; &lt;p&gt;Ernie&lt;/p&gt; &lt;p&gt;I’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.&lt;/p&gt; &lt;p&gt;Sorry, just wanted to put this out here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T05:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnwih</id>
    <title>I can't believe it actually runs - Qwen 235b @ 16GB VRAM</title>
    <updated>2025-07-03T12:07:58+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I decided to try my luck with Qwen 235b so downloaded Unsloth's Q2XL. I've got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).&lt;/p&gt; &lt;p&gt;My runtime args:&lt;/p&gt; &lt;p&gt;llama-cli -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;/p&gt; &lt;p&gt;Super simple user prompt because I wasn't expecting miracles:&lt;/p&gt; &lt;p&gt;tell me a joke&lt;/p&gt; &lt;p&gt;Result:&lt;br /&gt; 8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven't tried any actual work on it yet.&lt;/p&gt; &lt;p&gt;cli output blurb:&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 24.81 ms / 476 runs ( 0.05 ms per token, 19183.49 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 16979.96 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 1497.01 ms / 12 tokens ( 124.75 ms per token, 8.02 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 85040.21 ms / 463 runs ( 183.67 ms per token, 5.44 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 100251.11 ms / 475 tokens&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;p&gt;It looks like I'm only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T12:07:58+00:00</published>
  </entry>
</feed>
