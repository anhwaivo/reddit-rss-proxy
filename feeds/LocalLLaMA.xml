<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-27T23:37:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ma2j62</id>
    <title>Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?</title>
    <updated>2025-07-26T19:49:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"&gt; &lt;img alt="Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?" src="https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;amp;crop=smart&amp;amp;s=a4df17ec2e79efaeb8495d39a06eefe4fc80e5a6" title="Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T19:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mav8p7</id>
    <title>Is there a website which has a collection of all benchmarks perfomed for LLM models?</title>
    <updated>2025-07-27T19:30:03+00:00</updated>
    <author>
      <name>/u/Special_System_6627</name>
      <uri>https://old.reddit.com/user/Special_System_6627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically benchmark of benchmarks. AI companies generally just show the benchmarks which suits accordingly to them, and hiding others. Is there a place where I can all of the benchmarks, so that I can take an informed decision before using any LLM API or downloading any new models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_System_6627"&gt; /u/Special_System_6627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mav8p7/is_there_a_website_which_has_a_collection_of_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mav8p7/is_there_a_website_which_has_a_collection_of_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mav8p7/is_there_a_website_which_has_a_collection_of_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T19:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1malflg</id>
    <title>Building a quiet LLM machine for 24/7 use, is this setup overkill or smart?</title>
    <updated>2025-07-27T12:47:40+00:00</updated>
    <author>
      <name>/u/bardanaadam</name>
      <uri>https://old.reddit.com/user/bardanaadam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m putting together a PC mainly for running large language models like Qwen, LLaMA3, DeepSeek, etc. It’ll mostly be used for &lt;strong&gt;code generation tasks&lt;/strong&gt;, and I want it to run &lt;strong&gt;24/7&lt;/strong&gt;, quietly, in my home office.&lt;/p&gt; &lt;p&gt;Here’s what I’ve picked so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Case&lt;/strong&gt;: Lian Li O11D EVO XL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 9 7950X3D&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: MSI RTX 4090 Suprim Liquid X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: ASUS ProArt X670E-Creator&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 64GB DDR5 G.Skill Trident Z5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AIO Coolers&lt;/strong&gt;: 360mm for CPU, 240mm for GPU (built-in)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt;: Samsung 990 Pro 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU&lt;/strong&gt;: Corsair AX1600i Titanium (probably overkill, but wanted room to grow)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’m wondering:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone running something similar — how &lt;strong&gt;quiet&lt;/strong&gt; is it under load? Any tips to make it even quieter?&lt;/li&gt; &lt;li&gt;Can this handle models like &lt;strong&gt;Qwen2.5-32B&lt;/strong&gt; comfortably in 4-bit? Or am I dreaming?&lt;/li&gt; &lt;li&gt;I’m also thinking of renting the GPU out on &lt;a href="http://Vast.ai"&gt;&lt;strong&gt;Vast.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/ RunPod&lt;/strong&gt; when I’m not using it. Anyone making decent side income doing that?&lt;/li&gt; &lt;li&gt;Any parts you’d swap out or downscale without losing performance?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Goal is to have something powerful but also quiet enough to keep on 24/7 — and if it can earn a bit while idle, even better.&lt;/p&gt; &lt;p&gt;Appreciate any thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bardanaadam"&gt; /u/bardanaadam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1malflg/building_a_quiet_llm_machine_for_247_use_is_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1malflg/building_a_quiet_llm_machine_for_247_use_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1malflg/building_a_quiet_llm_machine_for_247_use_is_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T12:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1marx3v</id>
    <title>How can we simulate gemini deepthink with models like deepseek/qwen or other open models?</title>
    <updated>2025-07-27T17:18:06+00:00</updated>
    <author>
      <name>/u/True_Requirement_891</name>
      <uri>https://old.reddit.com/user/True_Requirement_891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's good hyper around gemini deep think. Can we simulate it using the DeepSeek models or Qwen?&lt;/p&gt; &lt;p&gt;Is that simply gemini 2.5 pro with a much higher thinking budget or it's using some branch of thoughts or Graph of thoughts behind the scenes using multiple parallel instances???? &lt;/p&gt; &lt;p&gt;Has anyone tested something like this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/True_Requirement_891"&gt; /u/True_Requirement_891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1marx3v/how_can_we_simulate_gemini_deepthink_with_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1marx3v/how_can_we_simulate_gemini_deepthink_with_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1marx3v/how_can_we_simulate_gemini_deepthink_with_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T17:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1max9qz</id>
    <title>Speculative decoding without a draft model (C#)</title>
    <updated>2025-07-27T20:53:02+00:00</updated>
    <author>
      <name>/u/DeProgrammer99</name>
      <uri>https://old.reddit.com/user/DeProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: faster grammar check and minor code edits without a draft model: a C# proof-of-concept.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dpmm99/ModelFreeSpeculation"&gt;https://github.com/dpmm99/ModelFreeSpeculation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a toy project built on LLamaSharp. It's a toy because it assumes the output will be nearly identical to the input--no particularly large added sequences and such. A better difference-tracking algorithm would make it more usable, and I think it could also be better if it fell back to a real draft model smartly when there are big differences. I'd been thinking about this since I saw a statement that &lt;strong&gt;a draft &amp;quot;model&amp;quot; isn't limited to LLMs&lt;/strong&gt;, and I remember it every time I accidentally click &amp;quot;Apply&amp;quot; in GitHub Copilot and watch it scan through a few hundred lines of code just to add one function, haha.&lt;/p&gt; &lt;p&gt;I tested it on two prompts using Phi-4-14B-Q4_K_M with &lt;strong&gt;8 draft tokens&lt;/strong&gt; per inference loop iteration on my RTX 4060 Ti using CUDA and &lt;a href="https://github.com/SciSharp/LLamaSharp/pull/1225"&gt;this pre-release of LLamaSharp&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For the spell-check prompt:&lt;/p&gt; &lt;p&gt;Duration: 7.39s, Tokens: 135, Tokens/sec: 18.28&lt;/p&gt; &lt;p&gt;Duration: 4.89s, Tokens: 135, Tokens/sec: 27.60 (88 accepted, 283 rejected) &lt;strong&gt;(+51%)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the code editing prompt:&lt;/p&gt; &lt;p&gt;Duration: 17.84s, Tokens: 328, Tokens/sec: 18.39&lt;/p&gt; &lt;p&gt;Duration: 10.40s, Tokens: 328, Tokens/sec: 31.55 (237 accepted, 473 rejected) &lt;strong&gt;(+71%)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Duration: 9.50s, Tokens: 328, Tokens/sec: 34.52 (250 draft tokens accepted; &lt;strong&gt;draft length 20&lt;/strong&gt;) &lt;strong&gt;(+88%)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was also thinking this approach could go nicely with a model fine-tuned for &lt;em&gt;applying&lt;/em&gt; code edits like &lt;a href="https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B"&gt;https://huggingface.co/models?other=base_model:quantized:microsoft/NextCoder-32B&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeProgrammer99"&gt; /u/DeProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1max9qz/speculative_decoding_without_a_draft_model_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T20:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mazi8m</id>
    <title>An LLM Focused Just on Debugging</title>
    <updated>2025-07-27T22:27:56+00:00</updated>
    <author>
      <name>/u/Sharp-Arachnid-8760</name>
      <uri>https://old.reddit.com/user/Sharp-Arachnid-8760</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found this paper recently and thought the idea was worth sharing.&lt;/p&gt; &lt;p&gt;It is a language model trained specifically for debugging rather than general-purpose code generation. It’s built to understand large codebases over time, using something called Adaptive Graph-Guided Retrieval to pull in relevant files, logs, and commit history when tracing bugs.&lt;/p&gt; &lt;p&gt;The model is trained on millions of real debugging examples like stack traces, test failures, and CI logs. Instead of just predicting code, it runs through a full debugging loop: retrieve context, propose fix, test, refine, and update memory.&lt;/p&gt; &lt;p&gt;A few standout points:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claims 65% success on real-world debugging tasks, compared to ~10% for GPT-4 or Claude&lt;/li&gt; &lt;li&gt;Retrieval seems to prioritize structural relationships between code, not just token similarity&lt;/li&gt; &lt;li&gt;Focus is on producing fixes, tests, and docs, not just autocomplete&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Honestly surprised we haven’t seen more models focus purely on debugging like this. Most tools still treat it like another code generation task. Would be interested to hear thoughts on how this compares to retrieval-augmented agents or if anyone’s explored similar approaches.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.12482"&gt;https://arxiv.org/abs/2507.12482&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharp-Arachnid-8760"&gt; /u/Sharp-Arachnid-8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mazi8m/an_llm_focused_just_on_debugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mazi8m/an_llm_focused_just_on_debugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mazi8m/an_llm_focused_just_on_debugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T22:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mazvnk</id>
    <title>How do you monitor your Ollama instance?</title>
    <updated>2025-07-27T22:44:42+00:00</updated>
    <author>
      <name>/u/ishbuggy</name>
      <uri>https://old.reddit.com/user/ishbuggy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running an ollama server as a container in unraid, but I am running up against some problems where models are failing for some use cases. I have several different clients connecting to the server. But I don't know the best way to monitor ollama, for example even just for token usage. But really I want to have some way to monitor what ollama is doing, how models are performing, and to help diagnose problems. But I am having trouble finding a good way to do it. How are you monitoring your ollama server and checking model performance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ishbuggy"&gt; /u/ishbuggy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T22:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1maixye</id>
    <title>I tried implementing the CRISP paper from Google Deepmind in Python</title>
    <updated>2025-07-27T10:26:29+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the weekend crafting this open-source PyTorch implementation of Google's &lt;a href="https://arxiv.org/pdf/2505.11471"&gt;CRISP paper (arXiv:2505.11471)&lt;/a&gt;. The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.&lt;/p&gt; &lt;p&gt;For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings &lt;em&gt;after&lt;/em&gt; training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering &lt;em&gt;during&lt;/em&gt; training to force the model to learn inherently &amp;quot;clusterable&amp;quot; representations.&lt;/p&gt; &lt;p&gt;The repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sigridjineth/crisp-py"&gt;https://github.com/sigridjineth/crisp-py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maixye/i_tried_implementing_the_crisp_paper_from_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:26:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma08e0</id>
    <title>Appreciation Post - Thank you unsloth team, and thank you bartowski</title>
    <updated>2025-07-26T18:14:13+00:00</updated>
    <author>
      <name>/u/fuutott</name>
      <uri>https://old.reddit.com/user/fuutott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?&lt;/p&gt; &lt;p&gt;Edit yeah and llama.cpp team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuutott"&gt; /u/fuutott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T18:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma8yua</id>
    <title>Local LLM is more important than ever</title>
    <updated>2025-07-27T00:40:57+00:00</updated>
    <author>
      <name>/u/NeedleworkerDull7886</name>
      <uri>https://old.reddit.com/user/NeedleworkerDull7886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt; &lt;img alt="Local LLM is more important than ever" src="https://a.thumbs.redditmedia.com/uE7wV-zcvDiT-3iVUwWcdDhkrRrgCwBg-OrGsKIspS0.jpg" title="Local LLM is more important than ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc"&gt;https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sam Altman admitting that ChatGPT will never protect your privacy &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeedleworkerDull7886"&gt; /u/NeedleworkerDull7886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T00:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma6b57</id>
    <title>New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples</title>
    <updated>2025-07-26T22:32:47+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"&gt; &lt;img alt="New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples" src="https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79243a5ca169d7972acf9a3bdc240df386129d25" title="New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are people's thoughts on Sapient Intelligence's recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1maipjy</id>
    <title>PowerInfer/SmallThinker-21BA3B-Instruct · Hugging Face</title>
    <updated>2025-07-27T10:11:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipjy/powerinfersmallthinker21ba3binstruct_hugging_face/"&gt; &lt;img alt="PowerInfer/SmallThinker-21BA3B-Instruct · Hugging Face" src="https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637" title="PowerInfer/SmallThinker-21BA3B-Instruct · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipjy/powerinfersmallthinker21ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maipjy/powerinfersmallthinker21ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mae4yz</id>
    <title>Wan 2.2 coming out Monday July 28th</title>
    <updated>2025-07-27T05:17:50+00:00</updated>
    <author>
      <name>/u/Comed_Ai_n</name>
      <uri>https://old.reddit.com/user/Comed_Ai_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/"&gt; &lt;img alt="Wan 2.2 coming out Monday July 28th" src="https://preview.redd.it/6fhk0wjppcff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dc0d6b8afe675c915a40e90b3006a47c5764036" title="Wan 2.2 coming out Monday July 28th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comed_Ai_n"&gt; /u/Comed_Ai_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6fhk0wjppcff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mae4yz/wan_22_coming_out_monday_july_28th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T05:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1maw5dy</id>
    <title>Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning</title>
    <updated>2025-07-27T20:07:24+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.16784"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maw5dy/beyond_context_limits_subconscious_threads_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maw5dy/beyond_context_limits_subconscious_threads_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T20:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1maywaw</id>
    <title>Devstral &amp; Magistral as adapters of Mistral</title>
    <updated>2025-07-27T22:01:31+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt; &lt;img alt="Devstral &amp;amp; Magistral as adapters of Mistral" src="https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ed0b082947f94ee079c2a6004328efe6c66fc9" title="Devstral &amp;amp; Magistral as adapters of Mistral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7"&gt;The initials of Devstral, Mistral, and Magistral as connected puzzle pieces&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tl;dr: title. Here are the weights: &lt;a href="https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision"&gt;Devstral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision"&gt;Magistral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA"&gt;Devstral-Small-2507-Rebased-Vision-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Mistral-Small-3.2 for the past few weeks. It's pretty solid, and the combination of vision and speed make it a really good pick for me, but...&lt;/p&gt; &lt;p&gt;I'm using sglang and it's really memory hungry which means it's hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I've tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at ~Q6-Q8, but meh.&lt;/p&gt; &lt;p&gt;Then I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for ~1.1x the VRAM? Yes, please! I tried &lt;a href="https://github.com/arcee-ai/mergekit#lora-extraction"&gt;mergekit&lt;/a&gt; but it requires the same architecture, so I'd either have to drop vision (which I also tried, and it seemed to work, but I don't like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it's actually pretty easy, you just have to copy the &lt;code&gt;model&lt;/code&gt; weights over the &lt;code&gt;language_model&lt;/code&gt; weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &lt;span class="md-spoiler-text"&gt;Yes, I know 2+ other attempts were made (&lt;em&gt;one by unsloth, from whom I stole the weights, lol&lt;/em&gt;) for the &lt;em&gt;exact&lt;/em&gt; same thing, and could've saved me a whole day of pain, but I only remembered about it ~5 mins ago, but this wasn't the core of what I wanted to do anyway so we'll conveniently call it a draw D:&lt;/span&gt;&lt;/p&gt; &lt;p&gt;With the &amp;quot;new&amp;quot; models in place, the next step was to try creating LoRAs again. Well, mergekit didn't work. I almost quit, but decided to search the web for another method and I ended up finding &lt;a href="https://github.com/thomasgauthier/LoRD"&gt;LoRD&lt;/a&gt;, the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn't even tell me why, I just get a generic error, but it's probably the vision parts, or 1+ of the modules (linear_1 / linear_2 / merging_layer / lm_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in &lt;a href="https://github.com/vllm-project/vllm/issues/18574"&gt;vLLM&lt;/a&gt;). In either case, it meant I couldn't run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.&lt;/p&gt; &lt;p&gt;If I'm not too lazy (which I'll likely be), I'll give this another go sometime, but now I'll just start my 761435 Karl Franz campaign.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T22:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1maptvc</id>
    <title>Drummer's Mixtral 4x3B v1 - A finetuned clown MoE experiment with Voxtral 3B!</title>
    <updated>2025-07-27T15:56:16+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maptvc/drummers_mixtral_4x3b_v1_a_finetuned_clown_moe/"&gt; &lt;img alt="Drummer's Mixtral 4x3B v1 - A finetuned clown MoE experiment with Voxtral 3B!" src="https://external-preview.redd.it/f52jZxJLyrUGUN-MtgsNp9MhYVmfObcQcPQZdRl80CA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bb469872360a699d06db5617b0f24cf8eea8d5f" title="Drummer's Mixtral 4x3B v1 - A finetuned clown MoE experiment with Voxtral 3B!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Mixtral-4x3B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maptvc/drummers_mixtral_4x3b_v1_a_finetuned_clown_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maptvc/drummers_mixtral_4x3b_v1_a_finetuned_clown_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T15:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1man0hu</id>
    <title>Qwen GSPO (Group Sequence Policy Optimization)</title>
    <updated>2025-07-27T14:00:25+00:00</updated>
    <author>
      <name>/u/koc_Z3</name>
      <uri>https://old.reddit.com/user/koc_Z3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen has introduced a new technique called &lt;strong&gt;GSPO&lt;/strong&gt; (Group Sequence Policy Optimization)&lt;/p&gt; &lt;p&gt;Put simply:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's a new method for training large language models&lt;/li&gt; &lt;li&gt;Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance&lt;/li&gt; &lt;li&gt;This approach makes training more &lt;strong&gt;stable&lt;/strong&gt; and less prone to crashes or errors, especially when used with large, modular models like &lt;strong&gt;MoE (Mixture of Experts)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The training process is &lt;strong&gt;simpler&lt;/strong&gt; and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage&lt;/li&gt; &lt;li&gt;The more compute you throw at it, the better the model becomes — it &lt;strong&gt;scales efficiently&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The latest &lt;strong&gt;Qwen3 models&lt;/strong&gt; (like those that can code or follow instructions) were trained using this method&lt;/li&gt; &lt;li&gt;Compared to the older &lt;strong&gt;GRPO&lt;/strong&gt; method, GSPO leads to &lt;strong&gt;faster convergence&lt;/strong&gt; (the model learns faster) and uses &lt;strong&gt;fewer resources&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2507.18071"&gt;https://huggingface.co/papers/2507.18071&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koc_Z3"&gt; /u/koc_Z3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T14:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1maxfeb</id>
    <title>What happened to the Yi models?</title>
    <updated>2025-07-27T20:59:49+00:00</updated>
    <author>
      <name>/u/GabryIta</name>
      <uri>https://old.reddit.com/user/GabryIta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember some of them were really solid, but it's been over a year since we've seen a new release.&lt;br /&gt; Is the team still active, or has the project quietly died?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabryIta"&gt; /u/GabryIta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T20:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1majfwi</id>
    <title>Are ~70B Models Going Out of Fashion?</title>
    <updated>2025-07-27T10:57:30+00:00</updated>
    <author>
      <name>/u/HvskyAI</name>
      <uri>https://old.reddit.com/user/HvskyAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt; &lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt; &lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt; &lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt; &lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt; &lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...&lt;/p&gt; &lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt; &lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt; &lt;p&gt;I suppose I'm partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt; &lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HvskyAI"&gt; /u/HvskyAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mab2i2</id>
    <title>Tencent releases Hunyuan3D World Model 1.0 - first open-source 3D world generation model</title>
    <updated>2025-07-27T02:28:05+00:00</updated>
    <author>
      <name>/u/pseudoreddituser</name>
      <uri>https://old.reddit.com/user/pseudoreddituser</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudoreddituser"&gt; /u/pseudoreddituser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/TencentHunyuan/status/1949288986192834718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T02:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1maq0hg</id>
    <title>Why hasn't LoRA gained more popularity?</title>
    <updated>2025-07-27T16:03:21+00:00</updated>
    <author>
      <name>/u/dabomb007</name>
      <uri>https://old.reddit.com/user/dabomb007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabomb007"&gt; /u/dabomb007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T16:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1maipzo</id>
    <title>A new 21B-A3B model that can run 30 token/s on i9 CPU</title>
    <updated>2025-07-27T10:11:52+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt; &lt;img alt="A new 21B-A3B model that can run 30 token/s on i9 CPU" src="https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637" title="A new 21B-A3B model that can run 30 token/s on i9 CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f"&gt;https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3"&gt;https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker"&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:11:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mao95d</id>
    <title>Running LLMs exclusively on AMD Ryzen AI NPU</title>
    <updated>2025-07-27T14:52:33+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM&lt;/strong&gt; — a fast, runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and other models &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;. No CPU or iGPU fallback — just lean, efficient, &lt;strong&gt;NPU-native inference&lt;/strong&gt;. Think &lt;strong&gt;Ollama&lt;/strong&gt;, but purpose-built and deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;server mode (REST API)&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deeply hardware-optimized&lt;/strong&gt;, NPU-only inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; support (e.g., 128K for LLaMA)&lt;/li&gt; &lt;li&gt;Over &lt;strong&gt;11× power efficiency&lt;/strong&gt; compared to iGPU/CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating quickly and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (on remote machine):&lt;/strong&gt; Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a &lt;strong&gt;remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM&lt;/strong&gt; — no installation needed. &lt;a href="https://open-webui.testdrive-fastflowlm.com/"&gt;Launch Demo&lt;/a&gt; &lt;strong&gt;Login:&lt;/strong&gt; &lt;code&gt;guest@flm.npu&lt;/code&gt; &lt;strong&gt;Password:&lt;/strong&gt; &lt;code&gt;0000&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT"&gt;youtube.com/@FastFlowLM-YT&lt;/a&gt; &lt;em&gt;→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Community:&lt;/strong&gt; &lt;a href="https://discord.gg/Sze3Qsv5"&gt;discord.gg/Sze3Qsv5&lt;/a&gt; &lt;em&gt;→ Join us to ask questions, report issues, or contribute ideas&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know what works, what breaks, and what you’d love to see next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T14:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mammv5</id>
    <title>Qwen3-235B-A22B 2507 is so good</title>
    <updated>2025-07-27T13:43:37+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. &lt;/p&gt; &lt;p&gt;The markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it's a bit worse than 2.5 flash but that's probably because it's smaller model. better at coding than flash too. &lt;/p&gt; &lt;p&gt;running unsloth Q8. I haven't tried the thinking one yet. what do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T13:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1majemr</id>
    <title>Suprise suprise!!</title>
    <updated>2025-07-27T10:55:19+00:00</updated>
    <author>
      <name>/u/GoodGuyLafarge</name>
      <uri>https://old.reddit.com/user/GoodGuyLafarge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"&gt; &lt;img alt="Suprise suprise!!" src="https://preview.redd.it/k64e9lwtdeff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d09af7edf96adcd3793cd8970c2cab58d53352b" title="Suprise suprise!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodGuyLafarge"&gt; /u/GoodGuyLafarge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k64e9lwtdeff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:55:19+00:00</published>
  </entry>
</feed>
