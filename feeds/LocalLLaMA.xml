<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-23T20:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kt1hmk</id>
    <title>Tried Sonnet 4, not impressed</title>
    <updated>2025-05-22T20:46:01+00:00</updated>
    <author>
      <name>/u/Marriedwithgames</name>
      <uri>https://old.reddit.com/user/Marriedwithgames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt; &lt;img alt="Tried Sonnet 4, not impressed" src="https://preview.redd.it/k68q6q65be2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7773c238e9148d1a369ca7c06ac85f64c5d87e5" title="Tried Sonnet 4, not impressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A basic image prompt failed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marriedwithgames"&gt; /u/Marriedwithgames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k68q6q65be2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktt3i8</id>
    <title>Google Veo 3 Computation Usage</title>
    <updated>2025-05-23T20:02:24+00:00</updated>
    <author>
      <name>/u/Spiritual-Neat889</name>
      <uri>https://old.reddit.com/user/Spiritual-Neat889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any asumptions what google veo 3 may cost in computation? &lt;/p&gt; &lt;p&gt;I just want to see if there is a chance of model becoming local available. Or how their price may develop over time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual-Neat889"&gt; /u/Spiritual-Neat889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kszxmj</id>
    <title>Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)</title>
    <updated>2025-05-22T19:43:04+00:00</updated>
    <author>
      <name>/u/RuairiSpain</name>
      <uri>https://old.reddit.com/user/RuairiSpain</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt; &lt;img alt="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" src="https://preview.redd.it/g91uyr7tyd2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4631f915329d465f3cf27d7c20d9ddc5663b1465" title="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RuairiSpain"&gt; /u/RuairiSpain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g91uyr7tyd2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T19:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktm1n7</id>
    <title>Sarvam-M a 24B open-weights hybrid reasoning model</title>
    <updated>2025-05-23T15:13:11+00:00</updated>
    <author>
      <name>/u/RealKingNish</name>
      <uri>https://old.reddit.com/user/RealKingNish</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm1n7/sarvamm_a_24b_openweights_hybrid_reasoning_model/"&gt; &lt;img alt="Sarvam-M a 24B open-weights hybrid reasoning model" src="https://preview.redd.it/8gk7kugnsj2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfc3a087396e0a4a8f0a79dc5b3427bd30d54414" title="Sarvam-M a 24B open-weights hybrid reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Link: &lt;a href="https://huggingface.co/sarvamai/sarvam-m"&gt;https://huggingface.co/sarvamai/sarvam-m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Info: It's a 2 staged post trained version of Mistral 24B on SFT and GRPO.&lt;/p&gt; &lt;p&gt;It's a hybrid reasoning model which means that both reasoning and non-reasoning models are fitted in same model. You can choose when to reason and when not.&lt;/p&gt; &lt;p&gt;If you wanna try you can either run it locally or from Sarvam's platform.&lt;/p&gt; &lt;p&gt;&lt;a href="https://dashboard.sarvam.ai/playground"&gt;https://dashboard.sarvam.ai/playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, they released detailed blog post on post training: &lt;a href="https://www.sarvam.ai/blogs/sarvam-m"&gt;https://www.sarvam.ai/blogs/sarvam-m&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealKingNish"&gt; /u/RealKingNish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gk7kugnsj2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm1n7/sarvamm_a_24b_openweights_hybrid_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm1n7/sarvamm_a_24b_openweights_hybrid_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:13:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktq54n</id>
    <title>Kanana 1.5 2.1B/8B, English/Korean bilingual by kakaocorp</title>
    <updated>2025-05-23T17:59:17+00:00</updated>
    <author>
      <name>/u/nananashi3</name>
      <uri>https://old.reddit.com/user/nananashi3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktq54n/kanana_15_21b8b_englishkorean_bilingual_by/"&gt; &lt;img alt="Kanana 1.5 2.1B/8B, English/Korean bilingual by kakaocorp" src="https://external-preview.redd.it/Q9Aj2m2UPlli1ePF9mPrz1K16Bxc0hfngw0SCUO5UfI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b05df0965bf6728f5812c07f2fac9dfd6f121c" title="Kanana 1.5 2.1B/8B, English/Korean bilingual by kakaocorp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nananashi3"&gt; /u/nananashi3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/kakaocorp/kanana-15-682d75c83b5f51f4219a17fb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktq54n/kanana_15_21b8b_englishkorean_bilingual_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktq54n/kanana_15_21b8b_englishkorean_bilingual_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T17:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt0zvd</id>
    <title>House passes budget bill that inexplicably bans state AI regulations for ten years</title>
    <updated>2025-05-22T20:26:06+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt; &lt;img alt="House passes budget bill that inexplicably bans state AI regulations for ten years" src="https://external-preview.redd.it/is2Xb-bjmFmGSvp-crWowCGBhCXFlH_gdhrRUHNXU_I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bb497ae5922ecf83e5c0a152d97d9c4b33aa5a5" title="House passes budget bill that inexplicably bans state AI regulations for ten years" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tech.yahoo.com/articles/house-passes-budget-bill-inexplicably-184936484.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktqgk0</id>
    <title>Tested Qwen3 all models on CPU (i5-10210U), RTX 3060 12GB, and RTX 3090 24GB</title>
    <updated>2025-05-23T18:12:05+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Model Testing Results (CPU + GPU)&lt;/p&gt; &lt;p&gt;Model | Hardware | Load | Answer | Speed (t/s)&lt;/p&gt; &lt;p&gt;------------------|--------------------------------------------|--------------------|---------------------|------------&lt;/p&gt; &lt;p&gt;Qwen3-0.6B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 31.65&lt;/p&gt; &lt;p&gt;Qwen3-1.7B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 14.87&lt;/p&gt; &lt;p&gt;Qwen3-4B | Laptop (i5-10210U, 16GB RAM) | CPU only | Correct (misleading)| 7.03&lt;/p&gt; &lt;p&gt;Qwen3-8B | Laptop (i5-10210U, 16GB RAM) | CPU only | Incorrect | 4.06&lt;/p&gt; &lt;p&gt;Qwen3-8B | Desktop (5800X, 32GB RAM, RTX 3060) | 100% GPU | Incorrect | 46.80&lt;/p&gt; &lt;p&gt;Qwen3-14B | Desktop (5800X, 32GB RAM, RTX 3060) | 94% GPU / 6% CPU | Correct | 19.35&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Laptop (i5-10210U, 16GB RAM) | CPU only | Correct | 3.27&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Desktop (5800X, 32GB RAM, RTX 3060) | 49% GPU / 51% CPU | Correct | 15.32&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B | Desktop (5800X, 64GB RAM, RTX 3090) | 100% GPU | Correct | 105.57&lt;/p&gt; &lt;p&gt;Qwen3-32B | Desktop (5800X, 64GB RAM, RTX 3090) | 100% GPU | Correct | 30.54&lt;/p&gt; &lt;p&gt;Qwen3-235B-A22B | Desktop (5800X, 128GB RAM, RTX 3090) | 15% GPU / 85% CPU | Correct | 2.43&lt;/p&gt; &lt;p&gt;Here is the full video of all tests: &lt;a href="https://youtu.be/kWjJ4F09-cU"&gt;https://youtu.be/kWjJ4F09-cU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T18:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktdisj</id>
    <title>GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding</title>
    <updated>2025-05-23T07:23:08+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"&gt; &lt;img alt="GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding" src="https://external-preview.redd.it/_qyQ5Nb0aZ0pjIERMz0EBymLna5bhwRL3S2vTvBvqUQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=978b5b8d9f71176e70ad9f69cf874f5d01401ac0" title="GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models is available at &lt;a href="https://github.com/jacklishufan/LaViDa"&gt;https://github.com/jacklishufan/LaViDa&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jacklishufan/LaViDa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T07:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktmdpo</id>
    <title>Spatial Reasoning is Hot 🔥🔥🔥🔥🔥🔥</title>
    <updated>2025-05-23T15:26:50+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktmdpo/spatial_reasoning_is_hot/"&gt; &lt;img alt="Spatial Reasoning is Hot 🔥🔥🔥🔥🔥🔥" src="https://b.thumbs.redditmedia.com/fe_4WvxiCT24ikMyRUc_EH5MAk7-ND1tup0fAClRAOg.jpg" title="Spatial Reasoning is Hot 🔥🔥🔥🔥🔥🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Notice the recent uptick in google search interest around &amp;quot;spatial reasoning.&amp;quot;&lt;/p&gt; &lt;p&gt;And now we have a fantastic new benchmark to better measure these capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SpatialScore:&lt;/strong&gt; &lt;a href="https://haoningwu3639.github.io/SpatialScore/"&gt;https://haoningwu3639.github.io/SpatialScore/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;SpatialScore&lt;/strong&gt; benchmarks offer a comprehensive assessment covering key spatial reasoning capabilities like: &lt;/p&gt; &lt;p&gt;obj counting&lt;/p&gt; &lt;p&gt;2D localization&lt;/p&gt; &lt;p&gt;3D distance estimation&lt;/p&gt; &lt;p&gt;This benchmark can help drive progress in adapting VLMs for embodied AI use cases in robotics, where perception and planning hinge on strong spatial understanding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktmdpo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktmdpo/spatial_reasoning_is_hot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktmdpo/spatial_reasoning_is_hot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiusw</id>
    <title>nanoVLM: The simplest repository to train your VLM in pure PyTorch</title>
    <updated>2025-05-23T12:54:55+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"&gt; &lt;img alt="nanoVLM: The simplest repository to train your VLM in pure PyTorch" src="https://external-preview.redd.it/k3XI6YWGCxh9L4PoRExljDZTmAkbUgwnwQi71BtdC9A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06978f4f95414bba1cfe00e253ce645b2a32d135" title="nanoVLM: The simplest repository to train your VLM in pure PyTorch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nanovlm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7whv</id>
    <title>AGI Coming Soon... after we master 2nd grade math</title>
    <updated>2025-05-23T01:47:36+00:00</updated>
    <author>
      <name>/u/SingularitySoooon</name>
      <uri>https://old.reddit.com/user/SingularitySoooon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt; &lt;img alt="AGI Coming Soon... after we master 2nd grade math" src="https://b.thumbs.redditmedia.com/eIAXh1BO-pSo8c3MXScDeH2kayk1IHs4BckFY-FL0QE.jpg" title="AGI Coming Soon... after we master 2nd grade math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pe2eeljssf2f1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f881b7ce4409013458c17fff08e8377a329cb9df"&gt;Claude 4 Sonnet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When will LLM master the classic &amp;quot;9.9 - 9.11&amp;quot; problem???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SingularitySoooon"&gt; /u/SingularitySoooon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktqsog</id>
    <title>"Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small" can't they just say they have fine tuned mistral small or it's kind of wrapper?</title>
    <updated>2025-05-23T18:25:47+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"&gt; &lt;img alt="&amp;quot;Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small&amp;quot; can't they just say they have fine tuned mistral small or it's kind of wrapper?" src="https://external-preview.redd.it/WzIS2JjZo2kST66vrrt4qEsayLue07AZ01pMdBT5Wtc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd18d82c4dcfc9c4f7f92bcffba8e25084b30453" title="&amp;quot;Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small&amp;quot; can't they just say they have fine tuned mistral small or it's kind of wrapper?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sarvam.ai/blogs/sarvam-m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktqsog/sarvamm_a_24b_openweights_hybrid_model_built_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T18:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiere</id>
    <title>A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG</title>
    <updated>2025-05-23T12:33:08+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt; &lt;img alt="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" src="https://preview.redd.it/bn39fvozzi2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9702ce1baab0703350e9800e0619c24d489b70eb" title="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This project demonstrates how to implement Cache-Augmented Generation (CAG) in an LLM and shows its performance gains compared to RAG. &lt;/p&gt; &lt;p&gt;Project Link: &lt;a href="https://github.com/ronantakizawa/cacheaugmentedgeneration"&gt;https://github.com/ronantakizawa/cacheaugmentedgeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CAG preloads document content into an LLM’s context as a precomputed key-value (KV) cache. &lt;/p&gt; &lt;p&gt;This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. &lt;/p&gt; &lt;p&gt;CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bn39fvozzi2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktsqit</id>
    <title>Best Vibe Code tools (like Cursor) but are free and use your own local LLM?</title>
    <updated>2025-05-23T19:46:58+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company.&lt;/p&gt; &lt;p&gt;Does anybody know of any good Vibe Coding tools, as good or better than Cursor, that run on your own local LLMs?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;EDIT: Especially tools that integrate with ollama's API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T19:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktpz29</id>
    <title>Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?</title>
    <updated>2025-05-23T17:52:15+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt; &lt;img alt="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" src="https://b.thumbs.redditmedia.com/n5NQk6PSosGh3XCgL-CRJ01dhOkMxqKYXWlOm95gUJw.jpg" title="Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sits on my office desk for running very large context prompts (50K words) with QwQ 32B. Gotta be offline because they have a lot of P.I.I.&lt;/p&gt; &lt;p&gt;Had it in a Mechanic Master c34plus (25L) but CPU fans (Scythe Grand Tornado 3,000rpm) kept ramping up because two 5090s were blasting the radiator in a confined space, and could only fit a 1300W PSU in that tiny case which meant heavy power limiting for the CPU and GPUs.&lt;/p&gt; &lt;p&gt;Paid $3,200 each for the 5090 FE's and would have paid more. Couldn't be happier and this rig turns what used to take me 8 hours into 5 minutes of prompt processing and inference + 15 minutes of editing to output complicated 15 page reports.&lt;/p&gt; &lt;p&gt;Anytime I show a coworker what it can do, they immediately throw money at me and tell me to build them a rig, so I tell them I'll get them 80% of the performance for about $2,200 and I've built two dual 3090 local Al rigs for such coworkers so far.&lt;/p&gt; &lt;p&gt;Frame is a 3D printed one from Etsy by ArcadeAdamsParts. There were some minor issues with it, but Adam was eager to address them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktpz29"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktpz29/anyone_on_oahu_want_to_let_me_borrow_an_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T17:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgxxa</id>
    <title>AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning</title>
    <updated>2025-05-23T11:15:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt; &lt;img alt="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" src="https://external-preview.redd.it/_aQtUZTp2VBwp5MK35YBXI25HOZhHuEgT9O1MgXLN7I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb4068255f83d79055a6f21dccc859d949b32f54" title="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktojxe</id>
    <title>So what are some cool projects you guys are running on you local llms?</title>
    <updated>2025-05-23T16:55:33+00:00</updated>
    <author>
      <name>/u/itzikhan</name>
      <uri>https://old.reddit.com/user/itzikhan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find good ideas to implement on my setup, or maybe get some inspiration to do something on my own &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itzikhan"&gt; /u/itzikhan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T16:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgvoe</id>
    <title>server audio input has been merged into llama.cpp</title>
    <updated>2025-05-23T11:12:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt; &lt;img alt="server audio input has been merged into llama.cpp" src="https://external-preview.redd.it/w-TAeYFPuT8QOBKphdcEnCVLkPeOPrOjKse263sRyos.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30ef27dc1e8b5fa00168ba96a589759da20990b" title="server audio input has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13714"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksyicp</id>
    <title>Introducing the world's most powerful model</title>
    <updated>2025-05-22T18:45:16+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt; &lt;img alt="Introducing the world's most powerful model" src="https://preview.redd.it/hqx8fzosod2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96d0c448070aead295d21d9be7e8fd395520a72b" title="Introducing the world's most powerful model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqx8fzosod2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T18:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktm0hd</id>
    <title>AI becoming too sycophantic? Noticed Gemini 2.5 praising me instead of solving the issue</title>
    <updated>2025-05-23T15:11:54+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I get the feeling that the trend of making AI more inclined towards flattery and overly focused on a user's feelings is somehow degrading its ability to actually solve problems. Is it just me? For instance, I've recently noticed that Gemini 2.5, instead of giving a direct solution, will spend time praising me, saying I'm using the right programming paradigms, blah blah blah, and that my code should generally work. In the end, it was no help at all. Qwen2 32B, on the other hand, just straightforwardly pointed out my error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktkhp8</id>
    <title>Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results</title>
    <updated>2025-05-23T14:08:12+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt; &lt;img alt="Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results" src="https://b.thumbs.redditmedia.com/DNvc1pfuOj2yfO0pW08qBgw58D4CJdgut3-YAYKWjzQ.jpg" title="Claude 4 (Sonnet) isn't great for document understanding tasks: some surprising results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finished benchmarking Claude 4 (Sonnet) across a range of document understanding tasks, and the results are… not that good. It's currently &lt;strong&gt;ranked 7th overall&lt;/strong&gt; on the leaderboard.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weak performance in OCR – Claude 4 lags behind even smaller models like GPT-4.1-nano and InternVL3-38B-Instruct.&lt;/li&gt; &lt;li&gt;Rotation sensitivity – We tested OCR robustness with slightly rotated images ([-5°, +5°]). Most large models had a 2–3% drop in accuracy. Claude 4 dropped 9%.&lt;/li&gt; &lt;li&gt;Poor on handwritten documents – Scored only 51.64%, while Gemini 2.0 Flash got 71.24%. It also struggled with handwritten datasets in other tasks like key information extraction.&lt;/li&gt; &lt;li&gt;Chart VQA and visual tasks – Performed decently but still behind Gemini, Claude 3.7, and GPT-4.5/o4-mini.&lt;/li&gt; &lt;li&gt;Long document understanding – Claude 3.7 Sonnet (reasoning:low) ranked 1st. Claude 4 Sonnet ranked 13th.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One bright spot: table extraction&lt;/strong&gt; – Claude 4 Sonnet is currently ranked 1st, narrowly ahead of Claude 3.7 Sonnet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/72zkmcyogj2f1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8fb9e86ca0bcfe129e25dab934d06818f7d638"&gt;https://preview.redd.it/72zkmcyogj2f1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8fb9e86ca0bcfe129e25dab934d06818f7d638&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://idp-leaderboard.org/"&gt;https://idp-leaderboard.org/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Codebase: &lt;a href="https://github.com/NanoNets/docext"&gt;https://github.com/NanoNets/docext&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How has everyone’s experience with the models been so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktkhp8/claude_4_sonnet_isnt_great_for_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktklo5</id>
    <title>Unmute by Kyutai: Make LLMs listen and speak</title>
    <updated>2025-05-23T14:12:46+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems nicely polished and apparently works with any LLM. Open-source in the coming weeks.&lt;/p&gt; &lt;p&gt;Demo uses Gemma 3 12B as base LLM (demo link in the blog post, reddit seems to auto-delete my post if I include it here).&lt;/p&gt; &lt;p&gt;If any Kyutai dev happens to lurk here, would love to hear about the memory requirements of the TTS &amp;amp; STT models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kyutai.org/2025/05/22/unmute.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktklo5/unmute_by_kyutai_make_llms_listen_and_speak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T14:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktoh78</id>
    <title>LLMI system I (not my money) got for our group</title>
    <updated>2025-05-23T16:52:23+00:00</updated>
    <author>
      <name>/u/SandboChang</name>
      <uri>https://old.reddit.com/user/SandboChang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt; &lt;img alt="LLMI system I (not my money) got for our group" src="https://preview.redd.it/lgjexuw8ak2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3260ccc53dd2f7cca5692637366920fd7a9928ec" title="LLMI system I (not my money) got for our group" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandboChang"&gt; /u/SandboChang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lgjexuw8ak2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktoh78/llmi_system_i_not_my_money_got_for_our_group/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T16:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiq99</id>
    <title>I accidentally too many P100</title>
    <updated>2025-05-23T12:48:51+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt; &lt;img alt="I accidentally too many P100" src="https://b.thumbs.redditmedia.com/IdF4SU4XHKp-_JI6o-Y6kol8-cLrv94jdBxKlq9CTYI.jpg" title="I accidentally too many P100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I had quite positive results with a P100 last summer, so when R1 came out, I decided to try if I could put 16 of them in a single pc... and I could.&lt;/p&gt; &lt;p&gt;Not the fastest think in the universe, and I am not getting awesome PCIE speed (2@4x). But it works, is still cheaper than a 5090, and I hope I can run stuff with large contexts.&lt;/p&gt; &lt;p&gt;I hoped to run llama4 with large context sizes, and scout runs almost ok, but llama4 as a model is abysmal. I tried to run Qwen3-235B-A22B, but the performance with llama.cpp is pretty terrible, and I haven't been able to get it working with the vllm-pascal (ghcr.io/sasha0552/vllm:latest).&lt;/p&gt; &lt;p&gt;If you have any pointers on getting Qwen3-235B to run with any sort of parallelism, or want me to benchmark any model, just say so!&lt;/p&gt; &lt;p&gt;The MB is a 2014 intel S2600CW with dual 8-core xeons, so CPU performance is rather low. I also tried to use MB with an EPYC, but it doesn't manage to allocate the resources to all PCIe devices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktiq99"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktlz3w</id>
    <title>96GB VRAM! What should run first?</title>
    <updated>2025-05-23T15:10:20+00:00</updated>
    <author>
      <name>/u/Mother_Occasion_8076</name>
      <uri>https://old.reddit.com/user/Mother_Occasion_8076</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt; &lt;img alt="96GB VRAM! What should run first?" src="https://preview.redd.it/co0zhh06sj2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64b43f0124c5d5b397b2efd848e6e83c1dcfcfdc" title="96GB VRAM! What should run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to make a fake company domain name to order this from a supplier. They wouldn’t even give me a quote with my Gmail address. I got the card though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother_Occasion_8076"&gt; /u/Mother_Occasion_8076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co0zhh06sj2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T15:10:20+00:00</published>
  </entry>
</feed>
