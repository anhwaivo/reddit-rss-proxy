<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-04T06:51:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n7vgjc</id>
    <title>Ex-Miner Turned Local LLM Enthusiast, now I have a Dilemma</title>
    <updated>2025-09-04T00:13:33+00:00</updated>
    <author>
      <name>/u/mslocox</name>
      <uri>https://old.reddit.com/user/mslocox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ex-miner here, now messing around with local LLMs. Kept my rig through the crypto craze, and it’s paid off. Got 5x RTX 3080 (10GB VRAM), 2x RTX 3060 (12GB), and a 3080 Ti (12GB), all running on 850W PSUs. Total VRAM’s like 86GB across 8 cards. All mine from day one, kept ‘em cool, maintained, no complaints.&lt;/p&gt; &lt;p&gt;Been at it since Mixtral 8x7B days, took a break, now I’m back with ComfyUI for diffusion stuff and LLMs for long story videos. Splitting tasks across GPUs nodes here, models there....... works pretty well.&lt;/p&gt; &lt;p&gt;Here’s the deal: snagged a 3090 (24GB VRAM) to test some ideas, and damn, it’s nice. Fits a whole ComfyUI diffusion model on one card, rest of the rig handles other stuff. Problem is, my 850W PSUs choke if I try more than one 3090. Also tried jamming all 8 GPUs together with PCIe risers back in the day and had some inestability problems. But I think that I should be okay doing some more testing. &lt;/p&gt; &lt;p&gt;So, I’m stuck thinking:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dump my setup and grab used 3090s? More VRAM per card (24GB) is tempting for big models, and I could maybe get 4x 3090s for ~96GB total. But my cards are clean, first-owner, and used 3090s might be beat to hell. I could use my 4 x 850W psu for the rig. Maybe adding some 3060 to the mix. &lt;/li&gt; &lt;li&gt;Tweak what I got? Maybe find a sweet spot for my 3080s/3060s/3080 Ti where it’s stable. Could pull a card or two for side experiments, maybe even EXO mining down the line if I feel like it.&lt;/li&gt; &lt;li&gt;Wait for next-gen cards? Heard recently of the 96GB VRAM from HUAWEI, but that’s probably a year out.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you all think? Anyone got a stable multi-GPU setup with 3080s or similar for LLMs/ComfyUI? Tips for risers not sucking? Worth selling my good cards for mined used 3090s? Or just keep tweaking, testing? Waiting for cheap big-VRAM cards worth it?&lt;/p&gt; &lt;p&gt;Hit me with your roasts and ideas. I am open to hear. Thank you so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mslocox"&gt; /u/mslocox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vgjc/exminer_turned_local_llm_enthusiast_now_i_have_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vgjc/exminer_turned_local_llm_enthusiast_now_i_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vgjc/exminer_turned_local_llm_enthusiast_now_i_have_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T00:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n75z15</id>
    <title>GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations</title>
    <updated>2025-09-03T05:01:51+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt; &lt;img alt="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" src="https://preview.redd.it/6c1jae9atvmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f69c39fa3f7051f8ad4c85418e9c6c975491e18b" title="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full benchmarking methodology here: &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;https://artificialanalysis.ai/methodology/intelligence-benchmarking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c1jae9atvmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ypio</id>
    <title>MoE models benchmarked on iGPU</title>
    <updated>2025-09-04T02:46:36+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any recommended MoE models? I was benchmarking models on my MiniPC AMD Ryzen 6800H with iGPU 680M. Test with llama.cpp Vulkan build: e92734d5 (6250)&lt;/p&gt; &lt;p&gt;Here are the &lt;em&gt;tg128&lt;/em&gt; results. &lt;/p&gt; &lt;p&gt;Models tested in this order:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;qwen2.5-coder-14b-instruct-q8_0.gguf Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-D_AU-Q4_k_m.gguf M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q3_k_m.gguf Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf DS4X8R1L3.1-Dp-Thnkr-UnC-24B-D_AU-Q4_k_m.gguf EXAONE-4.0-32B-Q4_K_M.gguf gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4.gguf openchat-3.6-8b-20240522.Q8_0.gguf Yi-1.5-9B.Q8_0.gguf Ministral-8B-Instruct-2410-Q8_0.gguf DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;T/S (avg ± std)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="left"&gt;14.62 GiB&lt;/td&gt; &lt;td align="left"&gt;14.77 B&lt;/td&gt; &lt;td align="left"&gt;3.65 ± 0.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2moe 57B.A14B Q4_K&lt;/td&gt; &lt;td align="left"&gt;2.34 GiB&lt;/td&gt; &lt;td align="left"&gt;4.09 B&lt;/td&gt; &lt;td align="left"&gt;25.09 ± 0.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 7B Q3_K&lt;/td&gt; &lt;td align="left"&gt;10.83 GiB&lt;/td&gt; &lt;td align="left"&gt;24.15 B&lt;/td&gt; &lt;td align="left"&gt;5.57 ± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;qwen3moe 30B.A3B Q4_K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;17.28 GiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;30.53 B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;28.48 ± 0.09&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K&lt;/td&gt; &lt;td align="left"&gt;14.11 GiB&lt;/td&gt; &lt;td align="left"&gt;24.94 B&lt;/td&gt; &lt;td align="left"&gt;3.81 ± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;exaone4 32B Q4_K&lt;/td&gt; &lt;td align="left"&gt;18.01 GiB&lt;/td&gt; &lt;td align="left"&gt;32.00 B&lt;/td&gt; &lt;td align="left"&gt;2.52 ± 0.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20B MXFP4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;11.27 GiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;20.91 B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;23.36 ± 0.04&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenChat-3.6-8B Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.95 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03B&lt;/td&gt; &lt;td align="left"&gt;5.60 ± 1.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Yi-1.5-9B Q8_0&lt;/td&gt; &lt;td align="left"&gt;8.74 GiB&lt;/td&gt; &lt;td align="left"&gt;8.83B&lt;/td&gt; &lt;td align="left"&gt;4.20 ± 1.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ministral-8B-Instruct Q8_0&lt;/td&gt; &lt;td align="left"&gt;7.94 GiB&lt;/td&gt; &lt;td align="left"&gt;8.02B&lt;/td&gt; &lt;td align="left"&gt;4.71 ± 1.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528-Qwen3-8B Q8_K_XL&lt;/td&gt; &lt;td align="left"&gt;10.08 GiB&lt;/td&gt; &lt;td align="left"&gt;8.19B&lt;/td&gt; &lt;td align="left"&gt;3.81 ± 1.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528-Qwen3-8B IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;4.26 GiB&lt;/td&gt; &lt;td align="left"&gt;8.19B&lt;/td&gt; &lt;td align="left"&gt;12.74 ± 1.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-8B IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;4.13 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03B&lt;/td&gt; &lt;td align="left"&gt;14.76 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: All models are running on RPC + Vulkan backend.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ngl&lt;/strong&gt;: The number of layers used for testing (99).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;code&gt;pp512&lt;/code&gt;: Prompt processing with 512 tokens.&lt;/li&gt; &lt;li&gt;&lt;code&gt;tg128&lt;/code&gt;: Text generation with 128 tokens.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;t/s&lt;/strong&gt;: Tokens per second, averaged with standard deviation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Clear winners: MoE models. I expect similar results to Ollama with ROCm.&lt;/p&gt; &lt;p&gt;1st Qwen3-Coder-30B-A3B-Instruct-Q4_K_M&lt;/p&gt; &lt;p&gt;2nd gpt-oss-20b-GGUF_gpt-oss-20b-mxfp4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T02:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7meyo</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T18:13:50+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/intel-arc-pro-b50-linux"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7meyo/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m1ig</id>
    <title>Mapping LLM Style and Range in Flash Fiction</title>
    <updated>2025-09-03T18:00:11+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt; &lt;img alt="Mapping LLM Style and Range in Flash Fiction" src="https://b.thumbs.redditmedia.com/u40TUFkM22cScfVpJOWkzRp8n5pZSql8r4GWBthStMo.jpg" title="Mapping LLM Style and Range in Flash Fiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Additional charts and analysis: &lt;a href="https://github.com/lechmazur/writing_styles"&gt;https://github.com/lechmazur/writing_styles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on 400 flash-fiction pieces of 600–800 words per LLM. Prompts include required elements to keep content varied.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7m1ig"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7m1ig/mapping_llm_style_and_range_in_flash_fiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7bqgm</id>
    <title>LangExtract by Google: many people don't know about this yet!</title>
    <updated>2025-09-03T11:02:22+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt; &lt;img alt="LangExtract by Google: many people don't know about this yet!" src="https://external-preview.redd.it/n9THNRvTBgabZmzyX_O8lEw2GxXkLfCbQBuYD0khQMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9ab8760012176bf18e519e0590b3f5f3d4bab" title="LangExtract by Google: many people don't know about this yet!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google/langextract"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7bqgm/langextract_by_google_many_people_dont_know_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T11:02:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7rp5v</id>
    <title>Who here has got a Mac Studio with 512 gigs RAM?</title>
    <updated>2025-09-03T21:34:03+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have questions for you guys. So many questions. What models you run and what token/sec you get? What is the context size you set, do you run local LLM for fun or you do development and trying to replace Claude. &lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7rp5v/who_here_has_got_a_mac_studio_with_512_gigs_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T21:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n81d1t</id>
    <title>[Level 0] Fine-tuned my first personal chatbot</title>
    <updated>2025-09-04T05:06:17+00:00</updated>
    <author>
      <name>/u/FastCommission2913</name>
      <uri>https://old.reddit.com/user/FastCommission2913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt; &lt;img alt="[Level 0] Fine-tuned my first personal chatbot" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="[Level 0] Fine-tuned my first personal chatbot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up my first LLM fine-tuning project and wanted to share the experience since I learned a ton. Used Unsloth + `cognitivecomputations/dolphin-2.9-llama3-8b` with around 1400 custom examples about myself, trained on Colab's free T4 GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How I learnt:&lt;/strong&gt; I knew the basics of LoRA and QLoRA since we were never taught the practical. I am a self taught with a medical condition. Rest I followed the steps of ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Generated dataset using ChatGPT by providing it with my personal info (background, interests, projects, etc.). Formatted as simple question-answer pairs in JSONL. Used LoRA with r=16, trained for 300 steps (~20 minutes), ended with loss around 0.74.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/39hnvx6zl2nf1.png?width=2394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0d0c1bcdd0ea06139760b8817ac64939070008c"&gt;This is what my current dataset looks like.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Model went from generic &amp;quot;I'm an AI assistant created by...&amp;quot; to actually knowing I'm Sohaib Ahmed, ..... grad from ...., into anime (1794 watched according to my Anilist), gaming (Genshin Impact, ZZZ), and that I built InSightAI library with minimal PyPI downloads. Responses sound natural and match my personality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;: Llama 3.1 8B base model was solid. Dataset quality mattered more than quantity. Unsloth made everything stupid fast and memory-efficient.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues hit&lt;/strong&gt;: Tried Mistral 7B first but got incomplete responses (&amp;quot;I am and I do&amp;quot;). Safety triggers still override on certain phrases - asking about &amp;quot;abusive language&amp;quot; makes it revert to generic safety mode instead of answering as me. Occasionally hallucinates experiences I never had when answering general knowledge questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Next steps&lt;/strong&gt;: &amp;quot;I don't know&amp;quot; boundary examples to fix the hallucination issue. How do I make it so that it says &amp;quot;I don't know&amp;quot; for other general purpose questions? How can I improve it further?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Level 1 (based on my idiotic knowledge): I want to learn how can I make the text summarization personalized. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Final model actually passes the &amp;quot;tell me about yourself&amp;quot; test convincingly. Pretty solid for a first attempt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Colab notebook:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing"&gt;https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Confusions:&lt;/strong&gt; I don't know much on hosting/ deploying a Local LLM. Following are my specs: &lt;strong&gt;MacBook Pro with Apple M4 chip, 16GB RAM, and an Apple M4 GPU with 10 cores&lt;/strong&gt;. I only know that I can run any LLM &amp;lt; 16GB but don't know any good yet to do the tool calling and all that stuff. I want to make something with it.&lt;/p&gt; &lt;p&gt;So, sorry in advance if my Colab Notebook's code is messy. Any advice would be a appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastCommission2913"&gt; /u/FastCommission2913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T05:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n82ndz</id>
    <title>Finally: 3090 Successor: 5070 Ti super 24Gb 800$</title>
    <updated>2025-09-04T06:24:02+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt; &lt;img alt="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" src="https://external-preview.redd.it/kT4ohg_saogl0QowFisFMgdjPOl3cV1Xjwbw3qji8TU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01c103360d1456c04311f988c3089b01de5157d0" title="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306"&gt;https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9ii4qrzfV5w"&gt;https://www.youtube.com/watch?v=9ii4qrzfV5w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If they are well compressed in terms of energy consumption, then now it will be possible to assemble a rig with 100 gigabytes of VRAM without kilowatts of energy consumption, and we shouldn’t forget about the new FP4 formats&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T06:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7xgm5</id>
    <title>How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s</title>
    <updated>2025-09-04T01:47:21+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt; &lt;img alt="How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s" src="https://a.thumbs.redditmedia.com/GfszssUYE2rt8pcGlvaw8ZJeriVVINMv1TxqRk17Y84.jpg" title="How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Completely useless but thought I would share :D)&lt;/p&gt; &lt;p&gt;This was just a fun experiment to see how fast I could run LLMs with WiFi interconnect and, well, I have to say it's quite a bit slower than I thought...&lt;/p&gt; &lt;p&gt;I set up two machines with 1x5090 each; then installed the latest vLLM on each, and also installed Ray on each of them. Then once you &lt;a href="https://docs.ray.io/en/latest/cluster/cli.html"&gt;start ray on one machine and connect to it with the other,&lt;/a&gt; you can run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-0.6B --max-model-len 1024 --tensor-parallel-size 1 --pipeline-parallel-size 2 --host 0.0.0.0 --port 8181 --enable-reasoning --reasoning-parser deepseek_r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lo and behold, the mighty Qwen3 0.6B running at 8.4 t/s split across 2 5090s!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6vwu78dhy1nf1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fadb7a4ead8e56a8f3270f038608585ce8b8ed2"&gt;Open WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not only is the model bad, but also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs way slower than just CPU.&lt;/li&gt; &lt;li&gt;Ray &amp;amp; vLLM need a bit of tweaking to get running correctly&lt;/li&gt; &lt;li&gt;vLLM will throw a bunch of random errors along the way ;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T01:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n802an</id>
    <title>Article: Evolution of GPU Programming. From Smart Pixels to the Backbone of an AI-driven World</title>
    <updated>2025-09-04T03:55:01+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A light technical read on the history of GPU programming, full of memes and nostalgia. From writing pixel shaders in GLSL to implementing real-time 3D scanning algorithms in OpenCL, to optimizing deep learning models in PyTorch and TensorFlow, to bleeding-edge technologies like Flash Attention. Don't expect a deep technical content. However, it is not trivial either.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/data-science-collective/evolution-of-gpu-programming-8de112bd798e"&gt;Link to the article on Medium&lt;/a&gt; (best formatting)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/evolution-of-gpu-programming"&gt;Non-medium article&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Safe for work in an open-minded environment&lt;/strong&gt; (Wojak and mildly suggestive memes, game screenshots)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n802an/article_evolution_of_gpu_programming_from_smart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n802an/article_evolution_of_gpu_programming_from_smart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n802an/article_evolution_of_gpu_programming_from_smart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n82epy</id>
    <title>Qwen3 14b failing to load at 128k on RTX 3090 and 32 GB RAM.</title>
    <updated>2025-09-04T06:09:07+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I missing here? The model itself is just 9 gigs. I am trying unsloth’s version at full GPU offload in LM Studio. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82epy/qwen3_14b_failing_to_load_at_128k_on_rtx_3090_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82epy/qwen3_14b_failing_to_load_at_128k_on_rtx_3090_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n82epy/qwen3_14b_failing_to_load_at_128k_on_rtx_3090_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T06:09:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jmiz</id>
    <title>Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!</title>
    <updated>2025-09-03T16:31:18+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt; &lt;img alt="Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!" src="https://external-preview.redd.it/uylRGwq1HYqH_9GVZQwtt7vjMGVse2R0k3BHHixg9iQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd30ec44d7dcdb6ab67aaebd28d83444619fea7e" title="Drummer's Skyfall 31B v4 · A Mistral 24B upscaled to 31B with more creativity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd also like to take this opportunity to share some benchmarks for Cydonia 24B v4.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.1/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-31B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jmiz/drummers_skyfall_31b_v4_a_mistral_24b_upscaled_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ilou</id>
    <title>Switzerland launches its own open source model</title>
    <updated>2025-09-03T15:54:28+00:00</updated>
    <author>
      <name>/u/ananas_tacos</name>
      <uri>https://old.reddit.com/user/ananas_tacos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt; &lt;img alt="Switzerland launches its own open source model" src="https://external-preview.redd.it/vqsLOQwLzSpFCY0wZKGHoR70wxX3Zo0oDI880u-Ya_o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659f786877c6c3ce79ac169974bd64f43e3484fc" title="Switzerland launches its own open source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ananas_tacos"&gt; /u/ananas_tacos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.engadget.com/ai/switzerland-launches-its-own-open-source-ai-model-133051578.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7ilou/switzerland_launches_its_own_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T15:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7jfpt</id>
    <title>Qwen3 30B A3B Thinking 2507 Hybrid !!</title>
    <updated>2025-09-03T16:24:34+00:00</updated>
    <author>
      <name>/u/Not4Fame</name>
      <uri>https://old.reddit.com/user/Not4Fame</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt; &lt;img alt="Qwen3 30B A3B Thinking 2507 Hybrid !!" src="https://external-preview.redd.it/3-YBimUSWKbnR7AwkACpdqNr5hKT1fY59SClnp7z_yM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5bdb4b82debdc147a98f0aa03728d4703fe317e" title="Qwen3 30B A3B Thinking 2507 Hybrid !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, with some creative merge from YOYO-AI, and some love from me, now you have Qwen3 30B A3B Thinking 2507 in hybrid mode, just like the old hybrid mode, but 2507 weights. First give the creator some love &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507/discussions"&gt;here&lt;/a&gt; and next, read my instructions and get the chat template &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507-Q4_K_M-GGUF/discussions/1"&gt;here&lt;/a&gt; finally, go and download the model &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;No coffee needed, whatever I do, I do for love, not for fame ;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i6jep5s67zmf1.png?width=1151&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00577ea14074f247cf2491ae18a0fd5bf3cbfbb4"&gt;Qwen3 30B A3B Thinking 2507 Hybrid !&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; OOPS, I've linked wrong models, sorry&lt;/p&gt; &lt;p&gt;Model is this &lt;a href="https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-Mixture-2507"&gt;YOYO-AI/Qwen3-30B-A3B-Mixture-2507 · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and GGUF is this &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF"&gt;mradermacher/Qwen3-30B-A3B-Mixture-2507-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Moar Edit: I've also corrected the links in post body as well now, my comment explaining the usage and the chat template, sadly, is written under wrong model, but hey, nobody is perfect :)&lt;/p&gt; &lt;p&gt;Even Moar Edit: I've also moved my comment under the correct model now and linked this post's link correctly to it, so all links are now correct, phew... (yeah I'm stoned... so ? :P)&lt;/p&gt; &lt;p&gt;Moar Bonus Edit:&lt;/p&gt; &lt;p&gt;YOYO-AI also has &lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A3B-CoderThinking-YOYO-linear-GGUF"&gt;this&lt;/a&gt; interesting merge, Qwen3 coder 2507 with Qwen3 Thinking 2507 merge, which in my experience makes the coder somewhat less rigid and a bit less resilient/more instruction following etc. I recommend you check that out too !&lt;/p&gt; &lt;p&gt;Moar Flex&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/466yn0x6tzmf1.png?width=1148&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98ad91f42fc6e0c0ebee2c9af31b05c74a20761b"&gt;Qwen3 30B A3B Hybrid Running at 196 tk/s on a 5090&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not4Fame"&gt; /u/Not4Fame &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7jfpt/qwen3_30b_a3b_thinking_2507_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vqn8</id>
    <title>Thoughts on Intel Arc Pro B50 x4 = 64GB of VRAM for $1400 and 280W Power Draw?</title>
    <updated>2025-09-04T00:26:25+00:00</updated>
    <author>
      <name>/u/79215185-1feb-44c6</name>
      <uri>https://old.reddit.com/user/79215185-1feb-44c6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For new cards that is some of the best $/GB of VRAM you can get, and it's also the best VRAM/w you can get and because they're x8 cards you can run them off of a x16 splitter right? How are x16 splitters? I assume you'd need some external PCIe power.&lt;/p&gt; &lt;p&gt;Is this realistic? Does me making this thread basically prevent this card from ever be obtainable? &lt;em&gt;Am I stupid?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/79215185-1feb-44c6"&gt; /u/79215185-1feb-44c6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T00:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7g0c2</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark w/ Leading Models</title>
    <updated>2025-09-03T14:15:58+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" src="https://b.thumbs.redditmedia.com/-7S29tlnbmvmCdgKuNNinsiLgi6LQ83T8Ar-ZV3lCVs.jpg" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, big thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for creating the original German &lt;strong&gt;Wer wird Millionär?&lt;/strong&gt; Benchmark and open-sourcing it. &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;After speaking, we said it would be fun to run the same benchmark on a set of leading models, and that's what we did here. &lt;/p&gt; &lt;p&gt;The rules and data stayed the same, 45 rounds, each with 15 multiple-choice questions from easy to hard. One wrong answer ends the program and you keep the current winnings. No lifelines. Answers are single letters A–D. same public WWM question corpus used in the original. &lt;a href="https://github.com/GerritKainz/wer_wird_millionaer"&gt;https://github.com/GerritKainz/wer_wird_millionaer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Questions remain in German for inference, but we included parallel English text so non-German readers can follow along. See fragen_antworten_en.json in the repo. Scripts to run many programs quickly and rebuild results from per-model outputs (millionaire-run.py, rebuild_leaderboard.py). We’ll attach a screenshot of the leaderboard instead of pasting a table here. same scoring and structure as the original, packaged for quick reruns.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Jose-Sabater/millionaire-bench-opper"&gt;https://github.com/Jose-Sabater/millionaire-bench-opper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Again thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for the idea and groundwork. If you try more models or tweak settings, feel free to open a PR or drop results in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7g0c2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zfj5</id>
    <title>built and trained this 103M MoE from scratch - went good</title>
    <updated>2025-09-04T03:21:57+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt; &lt;img alt="built and trained this 103M MoE from scratch - went good" src="https://preview.redd.it/vqtopd08g2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b650bedc8fac15fe5bfe8adb9d475eb26ed8f5bb" title="built and trained this 103M MoE from scratch - went good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made this model a few weeks ago and experimented with SFT and LoRA. &lt;/p&gt; &lt;p&gt;technical report - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf&lt;/a&gt;&lt;br /&gt; you could find the full source code and weights here - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vqtopd08g2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7wh65</id>
    <title>VibeVoice Gone?</title>
    <updated>2025-09-04T01:00:53+00:00</updated>
    <author>
      <name>/u/atrfx</name>
      <uri>https://old.reddit.com/user/atrfx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like the GitHub page and the huggingface page are gone. The huggingface only has the 1.5B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope still has it (for now) &lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/summary"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atrfx"&gt; /u/atrfx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T01:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7l5kg</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T17:27:29+00:00</updated>
    <author>
      <name>/u/levian_</name>
      <uri>https://old.reddit.com/user/levian_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt; &lt;img alt="Intel launches Arc Pro B50 graphics card at $349" src="https://preview.redd.it/357rwwhaizmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=066c5073e108f00168fb16f32dcc905e00df9cae" title="Intel launches Arc Pro B50 graphics card at $349" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial review, source:&lt;a href="https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349"&gt;https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/levian_"&gt; /u/levian_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/357rwwhaizmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mien</id>
    <title>Best current NSFW TTS model?</title>
    <updated>2025-09-03T18:17:17+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one? And how to use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7z5kl</id>
    <title>Did M$ take down VibeVoice repo??</title>
    <updated>2025-09-04T03:08:12+00:00</updated>
    <author>
      <name>/u/x0rchidia</name>
      <uri>https://old.reddit.com/user/x0rchidia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt; &lt;img alt="Did M$ take down VibeVoice repo??" src="https://preview.redd.it/vsnyimd3e2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277cc5e5dbf4b6c8f03d5f05352e5f7de6a92598" title="Did M$ take down VibeVoice repo??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure if I missed something, but &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; is a 404 now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0rchidia"&gt; /u/x0rchidia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vsnyimd3e2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uocj</id>
    <title>PSA: Make sure your API ports aren't exposed to the open internet</title>
    <updated>2025-09-03T23:38:13+00:00</updated>
    <author>
      <name>/u/nooclear</name>
      <uri>https://old.reddit.com/user/nooclear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are about 1,100 exposed Ollama servers out there according to this blog post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama"&gt;https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, if you see the prompt &amp;quot;What is 2+2?&amp;quot; in your logs, it was Cisco.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nooclear"&gt; /u/nooclear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T23:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zk45</id>
    <title>VibeVoice RIP? What do you think?</title>
    <updated>2025-09-04T03:28:29+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt; &lt;img alt="VibeVoice RIP? What do you think?" src="https://preview.redd.it/un6uilkoh2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39144e5e650c4ae66ef8205b6d09c62f6427edad" title="VibeVoice RIP? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two weeks, I had been working hard to try and contribute to OpenSource AI by creating the VibeVoice nodes for ComfyUI. I’m glad to see that my contribution has helped quite a few people:&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A short while ago, Microsoft suddenly deleted its official VibeVoice repository on GitHub. As of the time I’m writing this, the reason is still unknown (or at least I don’t know it).&lt;/p&gt; &lt;p&gt;At the same time, Microsoft also removed the VibeVoice-Large and VibeVoice-Large-Preview models from HF. For now, they are still available here: &lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, for those who have already downloaded and installed my nodes and the models, they will continue to work. Technically, I could decide to embed a copy of VibeVoice directly into my repo, but first I need to understand why Microsoft chose to remove its official repository. My hope is that they are just fixing a few things and that it will be back online soon. I also hope there won’t be any changes to the usage license...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE: I have released a new 1.0.9 version that embed VibeVoice. No longer requires external VibeVoice installation.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/un6uilkoh2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
