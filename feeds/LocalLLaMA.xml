<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-26T12:11:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lkcpk4</id>
    <title>MCP in LM Studio</title>
    <updated>2025-06-25T17:58:34+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"&gt; &lt;img alt="MCP in LM Studio" src="https://external-preview.redd.it/xgG5hj5Fs1PBuG048NliXZrJKETHuOiQipJujsnBkY8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=387668f505f292a153588da355a3524c7291548b" title="MCP in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.17"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkcpk4/mcp_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkmjdk</id>
    <title>Deep Research with local LLM and local documents</title>
    <updated>2025-06-26T00:42:40+00:00</updated>
    <author>
      <name>/u/tomkod</name>
      <uri>https://old.reddit.com/user/tomkod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;There are several Deep Research type projects which use local LLM that scrape the web, for example&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SakanaAI/AI-Scientist"&gt;https://github.com/SakanaAI/AI-Scientist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/langchain-ai/local-deep-researcher"&gt;https://github.com/langchain-ai/local-deep-researcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/TheBlewish/Automated-AI-Web-Researcher-Ollama"&gt;https://github.com/TheBlewish/Automated-AI-Web-Researcher-Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and I'm sure many more...&lt;/p&gt; &lt;p&gt;But I have my own knowledge and my own data. I would like an LLM research/scientist to use only my local documents, not scrape the web. Or, if it goes to the web, then I would like to provide the links myself (that I know provide legitimate info).&lt;/p&gt; &lt;p&gt;Is there a project with such capability?&lt;/p&gt; &lt;p&gt;Side note: I hope auto-mod is not as restrictive as before, I tried posting this several times in the past few weeks/months with different wording, with and without links, with no success...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tomkod"&gt; /u/tomkod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmjdk/deep_research_with_local_llm_and_local_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmjdk/deep_research_with_local_llm_and_local_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmjdk/deep_research_with_local_llm_and_local_documents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T00:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lksrw1</id>
    <title>Building an English-to-Malayalam AI dubbing platform – Need suggestions on tools &amp; model stack!</title>
    <updated>2025-06-26T06:14:22+00:00</updated>
    <author>
      <name>/u/Educational-Tart-494</name>
      <uri>https://old.reddit.com/user/Educational-Tart-494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a dubbing platform that takes &lt;strong&gt;English audio (from films/interviews/etc)&lt;/strong&gt; and generates &lt;strong&gt;Malayalam dubbed audio&lt;/strong&gt; — not just subtitles, but proper translated speech.&lt;/p&gt; &lt;p&gt;Here's what I'm currently thinking for the pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;ASR&lt;/strong&gt; – Using Whisper to convert English audio to English text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MT&lt;/strong&gt; – Translating English → Malayalam (maybe using Meta's NLLB or IndicTrans2?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS&lt;/strong&gt; – Converting Malayalam text into natural Malayalam speech (gTTS for now, exploring Coqui or others)&lt;/li&gt; &lt;li&gt;&lt;p&gt;Include voice cloning or syncing audio back to video (maybe using Wav2Lip?).&lt;/p&gt; &lt;p&gt;I'd love your suggestions on:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Better open-source models for &lt;strong&gt;English→Malayalam translation&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Malayalam &lt;strong&gt;TTS engines&lt;/strong&gt; that sound more human/natural&lt;/li&gt; &lt;li&gt;Any end-to-end pipelines/tools you know for &lt;strong&gt;dubbing&lt;/strong&gt; workflows&lt;/li&gt; &lt;li&gt;Any major bottlenecks I should expect?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also curious if anyone has tried &lt;strong&gt;localizing AI content for Indian languages&lt;/strong&gt; — what worked, what flopped?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Tart-494"&gt; /u/Educational-Tart-494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lksrw1/building_an_englishtomalayalam_ai_dubbing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lksrw1/building_an_englishtomalayalam_ai_dubbing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lksrw1/building_an_englishtomalayalam_ai_dubbing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T06:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkorvb</id>
    <title>How to run local LLMs from USB flash drive</title>
    <updated>2025-06-26T02:31:52+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to see if I could run a local LLM straight from a USB flash drive without installing anything on the computer.&lt;/p&gt; &lt;p&gt;This is how I did it:&lt;/p&gt; &lt;p&gt;* Formatted a 64GB USB drive with exFAT&lt;/p&gt; &lt;p&gt;* Downloaded Llamafile, renamed the file, and moved it to the USB&lt;/p&gt; &lt;p&gt;* Downloaded GGUF model from Hugging Face&lt;/p&gt; &lt;p&gt;* Created simple .bat files to run the model&lt;/p&gt; &lt;p&gt;Tested Qwen3 8B (Q4) and Qwen3 30B (Q4) MoE and both ran fine.&lt;/p&gt; &lt;p&gt;No install, no admin access.&lt;/p&gt; &lt;p&gt;I can move between machines and just run it from the USB drive.&lt;/p&gt; &lt;p&gt;If you're curious the full walkthrough is here&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/sYIajNkYZus"&gt;https://youtu.be/sYIajNkYZus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkorvb/how_to_run_local_llms_from_usb_flash_drive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkorvb/how_to_run_local_llms_from_usb_flash_drive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkorvb/how_to_run_local_llms_from_usb_flash_drive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T02:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkuc7y</id>
    <title>Collaboration between 2 or more LLM's TypeScript Project</title>
    <updated>2025-06-26T07:56:30+00:00</updated>
    <author>
      <name>/u/RiverRatt</name>
      <uri>https://old.reddit.com/user/RiverRatt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a project using TypeScript as a front and backend and have a Geforce RTX 4090.&lt;/p&gt; &lt;p&gt;If any of you guys think you might want to see the repo files let me know and I will post a link to it. Kinda neat to watch them chat to each other back and forth.&lt;/p&gt; &lt;p&gt;It uses node-llama-cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/wOVZapv.png"&gt;imgur screenshot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RiverRatt"&gt; /u/RiverRatt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkuc7y/collaboration_between_2_or_more_llms_typescript/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkuc7y/collaboration_between_2_or_more_llms_typescript/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkuc7y/collaboration_between_2_or_more_llms_typescript/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T07:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lknv7t</id>
    <title>Dual 5090 FE temps great in H6 Flow</title>
    <updated>2025-06-26T01:47:29+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lknv7t/dual_5090_fe_temps_great_in_h6_flow/"&gt; &lt;img alt="Dual 5090 FE temps great in H6 Flow" src="https://external-preview.redd.it/l5PbcIu7jCcNQxBTA9ZYKjNZvR8xIXDsnBuRhGsNN38.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc0a066becc68cd55a94d86756e87455813edd1" title="Dual 5090 FE temps great in H6 Flow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See the screenshots for for GPU temps and vram load and GPU utilization. First pic is complete idle. Higher GPU load pic is during prompt processing of 39K token prompt. Other closeup pic is during inference output on LM Studio with QwQ 32B Q4.&lt;/p&gt; &lt;p&gt;450W power limit applied to both GPUs coupled with 250 MHz overclock.&lt;/p&gt; &lt;p&gt;Top GPU not much hotter than bottom one surprisingly.&lt;/p&gt; &lt;p&gt;Had to do a lot of customization in the thermalright trcc software to get the GPU HW info I wanted showing. &lt;/p&gt; &lt;p&gt;I had these components in an open frame build but changed my mind because I wanted wanted physical protection for the expensive components in my office with other coworkers and janitors. And for dust protection even though it hadn't really been a problem in my my very clean office environment. &lt;/p&gt; &lt;p&gt;33 decibels idle at 1m away 37 decibels under under inference load and it's actually my PSU which is the loudest. Fans all set to &amp;quot;silent&amp;quot; profile in BIOS&lt;/p&gt; &lt;p&gt;Fidget spinners as GPU supports&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/2qwR9C"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DhVmP6/intel-core-i9-13900k-3-ghz-24-core-processor-bx8071513900k"&gt;Intel Core i9-13900K 3 GHz 24-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/R9kH99/thermalright-mjolnir-vision-360-argb-69-cfm-liquid-cpu-cooler-mjolnir-vision-360-white-argb"&gt;Thermalright Mjolnir Vision 360 ARGB 69 CFM Liquid CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$106.59 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/LYM48d/asus-rog-maximus-z790-hero-atx-lga1700-motherboard-rog-maximus-z790-hero"&gt;Asus ROG MAXIMUS Z790 HERO ATX LGA1700 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$522.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/VnpQzy/teamgroup-t-create-expert-32-gb-2-x-16-gb-ddr5-7200-cl34-memory-ctcwd532g7200hc34adc01"&gt;TEAMGROUP T-Create Expert 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$110.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/bF9wrH/crucial-t705-1-tb-m2-2280-pcie-50-x4-nvme-solid-state-drive-ct1000t705ssd3"&gt;Crucial T705 1 TB M.2-2280 PCIe 5.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$142.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/QD2j4D/nvidia-founders-edition-geforce-rtx-5090-32-gb-video-card-geforce-rtx-5090-founders-edition"&gt;NVIDIA Founders Edition GeForce RTX 5090 32 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$3200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/QD2j4D/nvidia-founders-edition-geforce-rtx-5090-32-gb-video-card-geforce-rtx-5090-founders-edition"&gt;NVIDIA Founders Edition GeForce RTX 5090 32 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$3200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/8QMMnQ/nzxt-h6-flow-atx-mid-tower-case-cc-h61fw-01"&gt;NZXT H6 Flow ATX Mid Tower Case&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$94.97 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/7tZzK8/evga-supernova-1600-g-1600-w-80-gold-certified-fully-modular-atx-power-supply-220-gp-1600-x1"&gt;EVGA SuperNOVA 1600 G+ 1600 W 80+ Gold Certified Fully Modular ATX Power Supply&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$299.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Scythe Grand Tornado 120mm 3,000rpm LCP 3-pack&lt;/td&gt; &lt;td align="left"&gt;$46.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$8024.52&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-06-25 21:30 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lknv7t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lknv7t/dual_5090_fe_temps_great_in_h6_flow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lknv7t/dual_5090_fe_temps_great_in_h6_flow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T01:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk63od</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T13:45:52+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Free license gets you access to Gemini 2.5 Pro and its massive 1 million token context window. To ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lklzav</id>
    <title>Tips that might help you using your LLM to do language translation.</title>
    <updated>2025-06-26T00:15:52+00:00</updated>
    <author>
      <name>/u/0ffCloud</name>
      <uri>https://old.reddit.com/user/0ffCloud</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After using LLM translation for production work(Korean&amp;lt;-&amp;gt;English&amp;lt;-&amp;gt;Chinese) for some time and got some experiences. I think I can share some idea that might help you improve your translation quality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Give it context, detailed context.&lt;/li&gt; &lt;li&gt;If it is a text, tells it what this text is about. Briefly.&lt;/li&gt; &lt;li&gt;If it is a conversation, assign name to each person. Prompt the model what it he/she doing, and insert context along the way. Give it the whole conversation, not individual line.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt the model to repeat the original text before translating.&lt;/strong&gt; This will drastically reduce the hallucination, especially if it's a non-thinking model.&lt;/li&gt; &lt;li&gt;Prompt it to analysis each section or even individual sentence. Sometimes they might pick the wrong word in the translation result, but give you the correct one in the analysis.&lt;/li&gt; &lt;li&gt;If the model is not fine tuned to a certain format, don't prompt it to input/output in that format. This will reduce the quality of translation by a lot, especially in small model.&lt;/li&gt; &lt;li&gt;Try to translate it into English first, this is especially true for general model without the fine tuning.&lt;/li&gt; &lt;li&gt;Assert how good the model is in the language by giving it some simple task in the source/target language. If it can't understand the task, it can't translate that.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A lot of these advice will eats a lot of context window, but it's the price to pay if you want high quality translation.&lt;/p&gt; &lt;p&gt;Now, for my personal experience:&lt;/p&gt; &lt;p&gt;For the translation task, I like Gemini Pro the most, I literally had a wow moment when I fist saw the result. It even understand the subtle tone change in the Korean conversation and knows why. For the first time I don't have to do any editing/polishing on the output and could just copy and paste. It gets every merit correctly with an original content.&lt;/p&gt; &lt;p&gt;The local counterpart Gemma 3 12/27b QAT is also pretty good. It might missed a few in-joke but as a local model without fine tuning, most of time it's gets the meaning correct and &amp;quot;good enough&amp;quot;. But it's really sensitive to the system prompt, if you don't prompt it correctly it will hallucinate to hell.&lt;/p&gt; &lt;p&gt;Qwen 3 32b q4k-xl is meh unless it's being fine tuned(even QwQ 32b is better than Qwen3 32b). &amp;quot;Meh&amp;quot; means it sometime gets the meaning of the sentence wrong in about 1 of 10, often with wrong words being used.&lt;/p&gt; &lt;p&gt;Deepseek R1-0528 671b FP8 is also meh, for its size it has greater vocabulary but otherwise the result isn't really better than Gemma3.&lt;/p&gt; &lt;p&gt;ChatGPT 4o/o3 as a online model is okay-ish, it can get the meaning correctly but often loses the merit, as a result it often need polishing. It also seems to have less data on Korean. O3 seems to have some regression on translation. I don't have access to o4.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0ffCloud"&gt; /u/0ffCloud &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lklzav/tips_that_might_help_you_using_your_llm_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lklzav/tips_that_might_help_you_using_your_llm_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lklzav/tips_that_might_help_you_using_your_llm_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T00:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk9ime</id>
    <title>Cydonia 24B v3.1 - Just another RP tune (with some thinking!)</title>
    <updated>2025-06-25T15:59:23+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"&gt; &lt;img alt="Cydonia 24B v3.1 - Just another RP tune (with some thinking!)" src="https://external-preview.redd.it/is5dxEtYQGcop66xpu9863OAeD17dNWUu8NQ03Wo_4I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8faa3dfb676a1ca98bc1f6a3369598de92ef4fba" title="Cydonia 24B v3.1 - Just another RP tune (with some thinking!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Serious Note: This was really scheduled to be released today... Such awkward timing!&lt;/p&gt; &lt;p&gt;This official release incorporated Magistral weights through merging. It is able to think thanks to that. &lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v3k-GGUF"&gt;Cydonia 24B v3k&lt;/a&gt; is a proper Magistral tune but not thoroughly tested.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;No claims of superb performance. No fake engagements of any sort (At least I hope not. Please feel free to delete comments / downvote the post if you think it's artificially inflated). No weird sycophants.&lt;/p&gt; &lt;p&gt;Just a moistened up Mistral 24B 3.1, a little dumb but quite fun and easy to use! Finetuned to &lt;em&gt;hopefully&lt;/em&gt; specialize on one single task: Your Enjoyment.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk9ime/cydonia_24b_v31_just_another_rp_tune_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T15:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk12th</id>
    <title>New Mistral Small 3.2 actually feels like something big. [non-reasoning]</title>
    <updated>2025-06-25T09:26:23+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt; &lt;img alt="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b"&gt;https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my experience, it ranges far above its size.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T09:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lktqz9</id>
    <title>Unusual use cases of local LLMs that don't require programming</title>
    <updated>2025-06-26T07:17:02+00:00</updated>
    <author>
      <name>/u/leuchtetgruen</name>
      <uri>https://old.reddit.com/user/leuchtetgruen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you use your local llms for that is not a standard use case (chatting, code generation, [E]RP)?&lt;/p&gt; &lt;p&gt;What I'm looking for is something like this: I use OpenWebUIs RAG feature in combination with Ollama to automatically generate cover letters for job applications. It has my CV as knowledge and I just paste the job description. It will generate a cover letter for me, that I then can continue to work on. But it saves me 80% of the time that I'd usually need to write a cover letter. &lt;/p&gt; &lt;p&gt;I created a &amp;quot;model&amp;quot; in OpenWebUI that has in it's system prompt the instruction to create a cover letter for the job description it's given. I gave this model access to the CV via RAG. I use Gemma3:12b as the model and it works quite well. I do all of this in German. &lt;/p&gt; &lt;p&gt;I think that's not something that comes to your mind immediately but it also didn't require any programming using LangChain or other things.&lt;/p&gt; &lt;p&gt;So my question is: Do you use any combination of standard tools in a use case that is a bit &amp;quot;out of the box&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leuchtetgruen"&gt; /u/leuchtetgruen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T07:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkixss</id>
    <title>Getting an LLM to set its own temperature: OpenAI-compatible one-liner</title>
    <updated>2025-06-25T22:01:59+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"&gt; &lt;img alt="Getting an LLM to set its own temperature: OpenAI-compatible one-liner" src="https://external-preview.redd.it/eGFpenhxOTlhNTlmMTzexiqj7MHOyelTArwBqWdVto7F0MAAs0_5qkS8tdr3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5712d62002d73b1e2e2644d94f511d87e132b75f" title="Getting an LLM to set its own temperature: OpenAI-compatible one-liner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure many seen the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt;ThermoAsk: getting an LLM to set its own temperature&lt;/a&gt; by u/&lt;a href="https://www.reddit.com/user/tycho_brahes_nose_/"&gt;tycho_brahes_nose_&lt;/a&gt; from earlier today. &lt;/p&gt; &lt;p&gt;So did I and the idea sounded very intriguing (thanks to OP!), so I spent some time to make it work with any OpenAI-compatible UI/LLM.&lt;/p&gt; &lt;p&gt;You can run it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run \ -e &amp;quot;HARBOR_BOOST_OPENAI_URLS=http://172.17.0.1:11434/v1&amp;quot; \ -e &amp;quot;HARBOR_BOOST_OPENAI_KEYS=sk-ollama&amp;quot; \ -e &amp;quot;HARBOR_BOOST_MODULES=autotemp&amp;quot; \ -p 8004:8000 \ ghcr.io/av/harbor-boost:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you don't use Ollama or have configured an auth for it - adjust the &lt;code&gt;URLS&lt;/code&gt; and &lt;code&gt;KEYS&lt;/code&gt; env vars as needed.&lt;/p&gt; &lt;p&gt;This service has OpenAI-compatible API on its own, so you can connect to it from any compatible client via URL/Key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://localhost:8004/v1 sk-boost &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kjxowr99a59f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T22:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkmp5s</id>
    <title>Open source has a similar tool like google cli released today?</title>
    <updated>2025-06-26T00:50:17+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source has a similar tool like google cli released today? ... because just tested that and OMG that is REALLY SOMETHING.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T00:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyo2p</id>
    <title>Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)</title>
    <updated>2025-06-25T06:44:26+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt; &lt;img alt="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" src="https://external-preview.redd.it/MDRyeGJ6bmJvMDlmMdx7LrexgFcEoZTqX8Yp_PzSREeGDqUB-Qd2XY93v_7d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71ebd7c03a7ccb476c3ff52d6b9e5cc00e65722" title="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I'd like to introduce our latest model: &lt;strong&gt;Jan-nano-128k&lt;/strong&gt; - this model is fine-tuned on &lt;strong&gt;Jan-nano&lt;/strong&gt; (which is a qwen3 finetune), improve performance when enable YaRN scaling &lt;strong&gt;(instead of having degraded performance)&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can uses tools continuously, repeatedly. &lt;/li&gt; &lt;li&gt;It can perform deep research &lt;strong&gt;VERY VERY DEEP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extremely persistence (please pick the right MCP as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, we are not trying to beat Deepseek-671B models, we just want to see how far this current model can go. To our surprise, &lt;strong&gt;it is going very very far.&lt;/strong&gt; Another thing, we have spent all the resource on this version of Jan-nano so.... &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We pushed back the technical report release! But it's coming ...sooon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-128k"&gt;https://huggingface.co/Menlo/Jan-nano-128k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;strong&gt;We are converting the GGUF check in comment section&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model will require &lt;strong&gt;YaRN Scaling&lt;/strong&gt; supported from inference engine, we already configure it in the model, but your inference engine will need to be able to handle YaRN scaling. Please run the model in l&lt;strong&gt;lama.server or Jan app&lt;/strong&gt; (these are from our team, we tested them, just it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; - jan-nano-v0.4-with-MCP: 80.7&lt;br /&gt; &lt;strong&gt;- jan-nano-128k-with-MCP: 83.2&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/909kwwnbo09f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lku0lo</id>
    <title>Is there any dedicated subreddits for neural network audio/voice/music generation?</title>
    <updated>2025-06-26T07:34:42+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought I'd ask here for recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T07:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkht3t</id>
    <title>Typos in the prompt lead to worse results</title>
    <updated>2025-06-25T21:15:53+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone knows that LLMs are great at ignoring all of your typos and still respond correctly - mostly. It &lt;a href="https://news.mit.edu/2025/llms-factor-unrelated-information-when-recommending-medical-treatments-0623"&gt;was now discovered&lt;/a&gt; that the response accuracy drops by around 8% when there are typos, upper/lower-case usage, or even extra white spaces in the prompt. There's also some degradation when not using precise language. (&lt;a href="https://dl.acm.org/doi/pdf/10.1145/3715275.3732121"&gt;paper&lt;/a&gt;, &lt;a href="https://github.com/abinithago/medium-is-message"&gt;code&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;A while ago it was found that &lt;a href="https://www.reddit.com/r/ChatGPTPro/comments/18xxyr8/comment/kg8nvjq/?context=3"&gt;tipping $50&lt;/a&gt; lead to better answers. The LLMs apparently generalized that people who offered a monetary incentive got higher quality results. Maybe the LLMs also generalized, that lower quality texts get lower-effort responses. Or those prompts simply didn't sufficiently match the high-quality medical training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkohrx</id>
    <title>With Unsloth's model's, what do the things like K, K_M, XL, etc mean?</title>
    <updated>2025-06-26T02:17:54+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking here: &lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"&gt;https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I understand the quant parts, but what do the differences in these specifically mean:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4bit: &lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;IQ4_NL&lt;/li&gt; &lt;li&gt;Q4_K_S&lt;/li&gt; &lt;li&gt;Q4_0&lt;/li&gt; &lt;li&gt;Q4_1&lt;/li&gt; &lt;li&gt;Q4_K_M&lt;/li&gt; &lt;li&gt;Q4_K_XL&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Could somebody please break down each, what it means? I'm a bit lost on this. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T02:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkifu8</id>
    <title>Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B.</title>
    <updated>2025-06-25T21:41:21+00:00</updated>
    <author>
      <name>/u/nero10578</name>
      <uri>https://old.reddit.com/user/nero10578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"&gt; &lt;img alt="Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B." src="https://external-preview.redd.it/bSYUJ_kisf3lxijdNPv6SmJ0R61X4277NoocNI2k1XI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6154edc4c489a4618af2112c22325225277cb6c9" title="Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nero10578"&gt; /u/nero10578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/DS-R1-Distill-70B-ArliAI-RpR-v4-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkijb5</id>
    <title>Open-source realtime 3D manipulator (minority report style)</title>
    <updated>2025-06-25T21:45:19+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"&gt; &lt;img alt="Open-source realtime 3D manipulator (minority report style)" src="https://external-preview.redd.it/aDdxYnZ0NmE4NTlmMfJKDYQsVfkIjJ_s4x_6JULCYI76ypQLK241aQ2pa_y3.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62a3edde8273227155f03fb45297589b6d41c361" title="Open-source realtime 3D manipulator (minority report style)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;demo link: &lt;a href="https://huggingface.co/spaces/stereoDrift/3d-model-playground"&gt;https://huggingface.co/spaces/stereoDrift/3d-model-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b03bkt6a859f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkc5mr</id>
    <title>LM Studio now supports MCP!</title>
    <updated>2025-06-25T17:37:55+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read the announcement: &lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/mcp"&gt;lmstudio.ai/blog/mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkv8vd</id>
    <title>MUVERA: Making multi-vector retrieval as fast as single-vector search</title>
    <updated>2025-06-26T08:57:50+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt; &lt;img alt="MUVERA: Making multi-vector retrieval as fast as single-vector search" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="MUVERA: Making multi-vector retrieval as fast as single-vector search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkh3og</id>
    <title>Introducing: The New BS Benchmark</title>
    <updated>2025-06-25T20:48:12+00:00</updated>
    <author>
      <name>/u/Turdbender3k</name>
      <uri>https://old.reddit.com/user/Turdbender3k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt; &lt;img alt="Introducing: The New BS Benchmark" src="https://preview.redd.it/4b2ufnhcy49f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfd8525d5b8c8bc0411893fe54cdd82fd4431a59" title="Introducing: The New BS Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a bs detector benchmark?^^ what if we can create questions that defy any logic just to bait the llm into a bs answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turdbender3k"&gt; /u/Turdbender3k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b2ufnhcy49f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkr9k7</id>
    <title>AMD can't be THAT bad at LLMs, can it?</title>
    <updated>2025-06-26T04:44:33+00:00</updated>
    <author>
      <name>/u/tojiro67445</name>
      <uri>https://old.reddit.com/user/tojiro67445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I recently upgraded from a Nvidia 3060 (12GB) to a AMD 9060XT (16GB) and running local models with the new GPU is effectively unusable. I knew Nvidia/CUDA dominate this space, but the difference is so shockingly bad that I feel like I must be doing something wrong. AMD can't possibly be THAT bad at this, right?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; I actually don't really use LLMs for anything, but they are adjacent to my work on GPU APIs so I like to keep tabs on how things evolve in that space. Call it academic curiosity. In any case, I usually dip in every few months, try a couple of newer local models, and get a feel for what they can and can't do.&lt;/p&gt; &lt;p&gt;I had a pretty good sense for the limits of my previous Nvidia GPU, and would get maybe ~10T/s with quantized 12B models running with koboldcpp. Nothing spectacular but it was fine for my needs.&lt;/p&gt; &lt;p&gt;This time around I decided to switch teams and get an AMD GPU, and I've been genuinely happy with it! Runs the games I throw at it great (because 1440p at 60FPS is perfectly fine IMO). But I was kind of shocked when I spun up koboldcpp with a model I had run earlier and was getting... ~1T/s??? A literal order of magnitude slower than with a GPU nearly 5 years older.&lt;/p&gt; &lt;p&gt;For context, I tried it with kobaldcpp_nocuda on Windows 11, Vulkan backend, gemma-3-12b-it-q4_0 as the model. Seems to load OK:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = false) load_tensors: relocated tensors: 0 of 627 load_tensors: offloading 48 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 49/49 layers to GPU load_tensors: Vulkan0 model buffer size = 7694.17 MiB load_tensors: Vulkan_Host model buffer size = 1920.00 MiB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the output is dreadful.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Processing Prompt [BLAS] (1024 / 1024 tokens) Generating (227 / 300 tokens) (EOS token triggered! ID:106) [20:50:09] CtxLimit:1251/4096, Amt:227/300, Init:0.00s, Process:21.43s (47.79T/s), Generate:171.62s (1.32T/s), Total:193.05s ====== Note: Your generation speed appears rather slow. You can try relaunching KoboldCpp with the high priority toggle (or --highpriority) to see if it helps. ====== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Spoiler alert: &lt;code&gt;--highpriority&lt;/code&gt; does not help.&lt;/p&gt; &lt;p&gt;So my question is am I just doing something wrong, or is AMD just really truly this terrible at the whole AI space? I know that most development in this space is done with CUDA and I'm certain that accounts for some of it, but in my experience devs porting CUDA code over to another GPU environment like Vulkan tend to come back with things like &amp;quot;initial release is 15% slower than the CUDA version because we haven't implemented these 20 vendor-specific extensions yet&amp;quot;, not 10x slower implementations. I also don't think that using a ROCm backend (should it ever get around to supporting the 9000 series on Windows) is magically going to give me a 10x boost. Vulkan is hard, y'all, but it's not THAT hard.&lt;/p&gt; &lt;p&gt;Anyone else have experience with the newer AMD cards that either confirms what I'm seeing or indicates I'm doing something wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tojiro67445"&gt; /u/tojiro67445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T04:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkbiva</id>
    <title>Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge.</title>
    <updated>2025-06-25T17:13:56+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt; &lt;img alt="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." src="https://preview.redd.it/11rgwmzvv39f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7039783722436b51c07b3fedff7d641b7b004cd" title="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/11rgwmzvv39f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lko09j</id>
    <title>Google's CLI DOES use your prompting data</title>
    <updated>2025-06-26T01:54:24+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt; &lt;img alt="Google's CLI DOES use your prompting data" src="https://preview.redd.it/j1km6ff1h69f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=183f6cf57cbd408bb1e17247c8aba72d8086d1a3" title="Google's CLI DOES use your prompting data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1km6ff1h69f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T01:54:24+00:00</published>
  </entry>
</feed>
