<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-30T14:35:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jnbd4v</id>
    <title>Grok Deep Search (Local)</title>
    <updated>2025-03-30T12:29:15+00:00</updated>
    <author>
      <name>/u/Asleep_Aerie_4591</name>
      <uri>https://old.reddit.com/user/Asleep_Aerie_4591</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was really impressed with how well Grok‚Äôs deep search works for reading and searching. I was wondering if it's possible to replicate something similar using local models or tools.&lt;/p&gt; &lt;p&gt;Has anyone tried this? Would love to hear your thoughts!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep_Aerie_4591"&gt; /u/Asleep_Aerie_4591 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbd4v/grok_deep_search_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbd4v/grok_deep_search_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbd4v/grok_deep_search_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqlii</id>
    <title>I Made a simple online tokenizer for any Hugging Face model</title>
    <updated>2025-03-29T16:50:57+00:00</updated>
    <author>
      <name>/u/Tweed_Beetle</name>
      <uri>https://old.reddit.com/user/Tweed_Beetle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;When I'm experimenting with different open models from Hugging Face, I often want to know how many tokens my prompts or texts actually are &lt;em&gt;for that specific model's tokenizer&lt;/em&gt;. It felt clunky to do this locally every time, and online tools seemed non-existent apart from OpenAI's tokenizer.&lt;/p&gt; &lt;p&gt;So I built a little web tool to help with this: &lt;strong&gt;Tokiwi&lt;/strong&gt; -&amp;gt; &lt;a href="https://tokiwi.dev"&gt;https://tokiwi.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just paste text and give it any HF repo ID (like &lt;code&gt;google/gemma-3-27b-it&lt;/code&gt;, &lt;code&gt;deepseek-ai/DeepSeek-V3-0324&lt;/code&gt;, your own fine-tune if it's public, etc.) and it shows the token count and the tokens themselves. It can also handle gated models if you give it an HF access token.&lt;/p&gt; &lt;p&gt;Wondering if this might be useful to others here. Let me know what you think! Any feedback is appreciated.&lt;/p&gt; &lt;p&gt;Thank you for your time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tweed_Beetle"&gt; /u/Tweed_Beetle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmruim</id>
    <title>[Build] A Beautiful Contradiction</title>
    <updated>2025-03-29T17:46:01+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt; &lt;img alt="[Build] A Beautiful Contradiction" src="https://external-preview.redd.it/cjdycmFoa3Mxb3JlMXAQIb2sMeAVglK21tTmFYitWWLXfsVRBH8Hkw8Jz_5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e9b78cea2f802b090ea4497620a669bf9afa261" title="[Build] A Beautiful Contradiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing my absolute contradiction of a local LLM rig - I found a 2019 Mac Pro outer shell for sale on eBay for $250 and wanted room to upsize my ITX build so I said fuck it and thus, a monstrosity was born. &lt;/p&gt; &lt;p&gt;Specs in the comments, hate welcomed üôè &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/irhtg3os1ore1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T17:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jml2w8</id>
    <title>Nemotron-49B uses 70% less KV cache compare to source Llama-70B</title>
    <updated>2025-03-29T12:21:40+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While studying how much KV cache major models uses using formula and empirically running it with llama.cpp if possible, I found that the Nemotron models are not only 30% smaller in model size, KV cache is also 70% less. Overall, it is 38% VRAM saving if you run at 128k context.&lt;/p&gt; &lt;p&gt;This is because the non-self attention layers doesn't have any KV cache at all. For Nemotron-49B, 31 out of 80 layers are non-self attention. For 51B, 26 out of 80 layers.&lt;/p&gt; &lt;p&gt;So if you are into 128k context and have 48GB VRAM, Nemotron can run at Q5_K_M at 128k with unquantized KV cache. On the other hand, QwQ can only run at IQ3_M due to 32GB KV cache.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Other things I learned:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;gemma-3 is pretty bad at KV cache while running with llama.cpp but this is because llama.cpp doesn't implement interleaved sliding window attention that can reduce KV cache to one sixth. (probably HF's transformers is the only one that support iSWA?)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek should make smaller MLA models that fit in 24GB or 48GB VRAM. This will blow the competition out of the water for local long context use.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T12:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvwi5</id>
    <title>GMKtec announces imminent availability of Strix Halo EVO-X2 mini PC</title>
    <updated>2025-03-29T20:50:00+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GMKtec-announces-imminent-availability-of-Strix-Halo-EVO-X2-mini-PC.989734.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwi5/gmktec_announces_imminent_availability_of_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvwi5/gmktec_announces_imminent_availability_of_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndl16</id>
    <title>What is your interview Assignment for AI Engineers ?</title>
    <updated>2025-03-30T14:27:08+00:00</updated>
    <author>
      <name>/u/Sarcinismo</name>
      <uri>https://old.reddit.com/user/Sarcinismo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt; &lt;p&gt;How do you evaluate AI (or ML) engineers these days? What kind of interview assignments or exercises do you use to assess their skills?&lt;/p&gt; &lt;p&gt;I‚Äôm specifically looking for engineers who can build AI agents using ‚ÄîLLMs, multi-agent frameworks, LLM observability tools, evals, and so on. Not really looking for folks focused on model training or deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sarcinismo"&gt; /u/Sarcinismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndl16/what_is_your_interview_assignment_for_ai_engineers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndl16/what_is_your_interview_assignment_for_ai_engineers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndl16/what_is_your_interview_assignment_for_ai_engineers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:27:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndqwo</id>
    <title>Assessing facial recognition performance of vision-LLMs</title>
    <updated>2025-03-30T14:34:54+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndqwo/assessing_facial_recognition_performance_of/"&gt; &lt;img alt="Assessing facial recognition performance of vision-LLMs" src="https://a.thumbs.redditmedia.com/2ibJa2kgT5keu_PFiJ5ci5XzpPqojDdb3k8vcixt6J0.jpg" title="Assessing facial recognition performance of vision-LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought it'd be interesting to assess face recognition performance of vision LLMs. Even though it wouldn't be wise to use a vision LLM to do face rec when there are dedicated models, I'll note that:&lt;/p&gt; &lt;p&gt;- it gives us a way to measure the gap between dedicated vision models and LLM approaches, to assess how close we are to 'vision is solved'.&lt;/p&gt; &lt;p&gt;- lots of jurisdictions have regulations around face rec system, so it is important to know if vision LLMs are becoming capable face rec systems.&lt;/p&gt; &lt;p&gt;I measured performance of multiple models on multiple datasets (AgeDB30, LFW, CFP). As a baseline, I used arface-resnet-100. Note that as there are 24,000 pair of images, I did not benchmark the more costly commercial APIs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j1h3buoc8ure1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a277f3b1cf211d390d9963a1d981e418dbb9da43"&gt;https://preview.redd.it/j1h3buoc8ure1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a277f3b1cf211d390d9963a1d981e418dbb9da43&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Samples&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6klg1g7j8ure1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07591185d4535c67c2bdbe0129bd6dcd97b8079b"&gt;https://preview.redd.it/6klg1g7j8ure1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07591185d4535c67c2bdbe0129bd6dcd97b8079b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;: &lt;/p&gt; &lt;p&gt;- Most vision LLMs are very far from even a several year old resnet-100. &lt;/p&gt; &lt;p&gt;- All models perform better than random chance.&lt;/p&gt; &lt;p&gt;- The google models (Gemini, Gemma) perform best.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndqwo/assessing_facial_recognition_performance_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndqwo/assessing_facial_recognition_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndqwo/assessing_facial_recognition_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmxdgg</id>
    <title>SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs</title>
    <updated>2025-03-29T21:58:54+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.07657"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmxdgg/splitquantv2_enhancing_lowbit_quantization_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn9206</id>
    <title>A good model to listen to me rant on niche topics?</title>
    <updated>2025-03-30T09:47:53+00:00</updated>
    <author>
      <name>/u/Mynameisjeff121</name>
      <uri>https://old.reddit.com/user/Mynameisjeff121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve had a good time with people‚Äôs suggestions in here when I was looking for models for different purposes, so I was hoping I could get help here again.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a model that‚Äôll hear me rant on niche video game/ fiction universes and ask questions about it. The few models I‚Äôve tested either derail too much or don‚Äôt really care about listening.&lt;/p&gt; &lt;p&gt;The searchbar on the huggingface site wasn‚Äôt that useful since models usually use tags on searches and I‚Äôm not that good on searching models. I‚Äôm kinda desperate now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mynameisjeff121"&gt; /u/Mynameisjeff121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9206/a_good_model_to_listen_to_me_rant_on_niche_topics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T09:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmpjeu</id>
    <title>SOTA 3d?</title>
    <updated>2025-03-29T16:03:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt; &lt;img alt="SOTA 3d?" src="https://external-preview.redd.it/ErYaOL2J__P1a1nSZoN5VkFh-_pWwoLL-ogamC2v0BM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16974cbb9664a3f501eea6ca32995ed70308e190" title="SOTA 3d?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/VAST-AI/TripoSG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn10lx</id>
    <title>Gemini 2.5 Pro unusable for coding?</title>
    <updated>2025-03-30T00:56:48+00:00</updated>
    <author>
      <name>/u/hyperknot</name>
      <uri>https://old.reddit.com/user/hyperknot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something really strange is going on with Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;On one hand, it's supposedly the smartest coding model ever made. But on the other hand, I ask it to add one single parameter, and instead of a simple 2-line diff, it generates a 35-line one where it randomly changes logic, removes a time.sleep() from an API call pagination loop, and is generally just totally &amp;quot;drunk&amp;quot; about what I asked it to do. It's somehow both pedantic and drunk at the same time.&lt;/p&gt; &lt;p&gt;Every other model, even much smaller ones, can easily make the 2-line change and leave everything else alone.&lt;/p&gt; &lt;p&gt;I'm wondering how this thing beat the Aider leaderboard. Did something change since the launch?&lt;/p&gt; &lt;p&gt;Setting temp to 0.0 doesn't help either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyperknot"&gt; /u/hyperknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn10lx/gemini_25_pro_unusable_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T00:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvsm3</id>
    <title>Local, GPU-Accelerated AI Characters with C#, ONNX &amp; Your LLM (Speech-to-Speech)</title>
    <updated>2025-03-29T20:44:56+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;Persona Engine&lt;/strong&gt;, an open-source project I built for creating interactive AI characters. Think VTuber tech meets your local AI stack.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice Input:&lt;/strong&gt; Listens via mic (Whisper.net ASR).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LLM:&lt;/strong&gt; Connects to any &lt;strong&gt;OpenAI-compatible API&lt;/strong&gt; (perfect for Ollama, LM Studio, etc., via LiteLLM perhaps). Personality defined in personality.txt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Output:&lt;/strong&gt; Advanced TTS pipeline + optional &lt;strong&gt;Real-time Voice Cloning (RVC)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live2D Avatar:&lt;/strong&gt; Animates your character.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Spout Output:&lt;/strong&gt; Direct feed to OBS/streaming software.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Deep Dive:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Everything Runs Locally:&lt;/strong&gt; The ASR, TTS, RVC, and rendering are all done on your machine. Point it at your local LLM, and the whole loop stays offline.&lt;/li&gt; &lt;li&gt;C# &lt;strong&gt;Powered:&lt;/strong&gt; The entire engine is built in &lt;strong&gt;C# on .NET 9&lt;/strong&gt;. This involved rewriting a lot of common Python AI tooling/pipelines, but gives us great performance and lovely async/await patterns for managing all the concurrent tasks (listening, thinking, speaking, rendering).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ONNX Runtime Under the Hood:&lt;/strong&gt; I leverage ONNX for the AI models (Whisper, TTS components, RVC). &lt;strong&gt;Theoretically,&lt;/strong&gt; this means it could target different execution providers (DirectML for AMD/Intel, CoreML, CPU). &lt;strong&gt;However,&lt;/strong&gt; the current build and included dependencies are optimized and primarily tested for &lt;strong&gt;NVIDIA CUDA/cuDNN&lt;/strong&gt; for maximum performance, especially with RVC. Getting other backends working would require compiling/sourcing the appropriate ONNX Runtime builds and potentially some code adjustments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Platform Potential:&lt;/strong&gt; Being C#/.NET means it could run on Linux/macOS, but you'd need to handle platform-specific native dependencies (like PortAudio, Spout alternatives e.g., Syphon) and compile things yourself. Windows is the main supported platform right now via the releases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo (Code &amp;amp; Releases):&lt;/strong&gt; &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Demo Video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=4V2DgI7OtHE"&gt;https://www.youtube.com/watch?v=4V2DgI7OtHE&lt;/a&gt; (forgive the cheesiness, I was having a bit of fun with capcut)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Heads-up:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For the pre-built releases: &lt;strong&gt;Requires NVIDIA GPU + correctly installed CUDA/cuDNN&lt;/strong&gt; for good performance. The README has a detailed guide for this.&lt;/li&gt; &lt;li&gt;Configure appsettings.json with your LLM endpoint/model.&lt;/li&gt; &lt;li&gt;Using standard LLMs? Grab personality_example.txt from the repo root as a starting point for personality.txt (requires prompt tuning!).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Excited to share this with a community that appreciates running things locally and diving into the tech! Let me know what you think or if you give it a spin. üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmvsm3/local_gpuaccelerated_ai_characters_with_c_onnx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T20:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqqxz</id>
    <title>First time testing: Qwen2.5:72b -&gt; Ollama Mac + open-webUI -&gt; M3 Ultra 512 gb</title>
    <updated>2025-03-29T16:57:54+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt; &lt;img alt="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" src="https://b.thumbs.redditmedia.com/GHJGnHixtYfi5hcwQIzYQveJXry9-u0b_5OgRRmDegc.jpg" title="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using it. Tested with the qwen2.5:72b, I add in the gallery the results of the first run. I would appreciate any comment that could help me to improve it. I also, want to thanks the community for the patience answering some doubts I had before buying this machine. I'm just beginning. &lt;/p&gt; &lt;p&gt;Doggo is just a plus!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jmqqxz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmjq5h</id>
    <title>Finally someone's making a GPU with expandable memory!</title>
    <updated>2025-03-29T10:54:13+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a RISC-V gpu with SO-DIMM slots, so don't get your hopes up just yet, but it's &lt;em&gt;something&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/"&gt;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://bolt.graphics/"&gt;https://bolt.graphics/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jncig2</id>
    <title>Any alternatives to the new 4o Multi-Modal Image capabilities?</title>
    <updated>2025-03-30T13:34:31+00:00</updated>
    <author>
      <name>/u/janusr</name>
      <uri>https://old.reddit.com/user/janusr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new 4o native image capabilities are quite impressing. Are there any open alternatives which allow similar native image input and output?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/janusr"&gt; /u/janusr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jncig2/any_alternatives_to_the_new_4o_multimodal_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnb3cl</id>
    <title>MacBook M3, 24GB ram. What's best for LLM engine?</title>
    <updated>2025-03-30T12:12:33+00:00</updated>
    <author>
      <name>/u/Familyinalicante</name>
      <uri>https://old.reddit.com/user/Familyinalicante</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like in title. I am in process of moving from windows laptop to MacBook Air M3, 24GB ram. I use it for local development in vscode and need to connect to local LLM. I've installed Ollama and it works but of course it's slower than my 3080ti16GB in windows laptop. It's not real problem because for my purpose I can leave laptop for hours to see result (that's the main reason for transition because windows laptop crash after an hour or so and worked loudly like steam engine). My question is if Ollama is fist class citizen in Apple or there's much better solution. I dont do any bleeding edge thing and use standard models like llama, Gemma, deepseek for my purpose. I used to Ollama and use it in such manner that all my projects connect to Ollama server on localhost. I know about LMstudio but didn't use it a lot as Ollama was sufficient. So, is Ollama ok or there much faster solutions, like 30% faster or more? Or there's a special configuration for Ollama in Apple beside installing it actually?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familyinalicante"&gt; /u/Familyinalicante &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnb3cl/macbook_m3_24gb_ram_whats_best_for_llm_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmttah</id>
    <title>Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak.</title>
    <updated>2025-03-29T19:13:59+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rvhj7wnchore1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmx0ih</id>
    <title>Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?</title>
    <updated>2025-03-29T21:41:45+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt; &lt;img alt="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" src="https://external-preview.redd.it/1DvBQgPBbFWMlcok52huGfBv7vgJ1oQojIIBOC8IpDA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50ef4e8c8eec3ec397e0751d55c871986cab02e" title="Someone created a highly optimized RDNA3 kernel that outperforms RocBlas by 60% on 7900XTX. How can I implement this and would it significantly benefit LLM inference?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seb-v.github.io/optimization/update/2025/01/20/Fast-GPU-Matrix-multiplication.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmx0ih/someone_created_a_highly_optimized_rdna3_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T21:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbhdl</id>
    <title>I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ‚ò∫Ô∏è</title>
    <updated>2025-03-30T12:36:19+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had similar experience with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmyvpd</id>
    <title>Moondream 2025-03-27 Release</title>
    <updated>2025-03-29T23:10:50+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt; &lt;img alt="Moondream 2025-03-27 Release" src="https://external-preview.redd.it/GtrXq5esaL1vBtb6j5XRN12_1xTaHr3DjPq8-x_uFDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149650fb9eeb4a3684aaac7092e35ed112f39db9" title="Moondream 2025-03-27 Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-03-27-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmyvpd/moondream_20250327_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T23:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmtkgo</id>
    <title>4x3090</title>
    <updated>2025-03-29T19:02:48+00:00</updated>
    <author>
      <name>/u/zetan2600</name>
      <uri>https://old.reddit.com/user/zetan2600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt; &lt;img alt="4x3090" src="https://preview.redd.it/zi8ghi2ifore1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaa2ef7723a30f4134fa44b42f76a17aa5ba357" title="4x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the only benefit of multiple GPUs concurrency of requests? I have 4x3090 but still seem limited to small models because it needs to fit in 24G vram. &lt;/p&gt; &lt;p&gt;AMD threadripper pro 5965wx 128 PCIe lanes ASUS ws pro wrx80 256G ddr4 3200 8 channels Primary PSU Corsair i1600 watt Secondary PSU 750watt 4 gigabyte 3090 turbos Phanteks Enthoo Pro II case Noctua industrial fans Artic cpu cooler&lt;/p&gt; &lt;p&gt;I am using vllm with tensor parallism of 4. I see all 4 cards loaded up and utilized evenly but doesn't seem any faster than 2 GPUs. &lt;/p&gt; &lt;p&gt;Currently using Qwen/Qwen2.5-14B-Instruct-AWQ with good success paired with Cline. &lt;/p&gt; &lt;p&gt;Will a nvlink bridge help? How can I run larger models? &lt;/p&gt; &lt;p&gt;14b seems really dumb compared to Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zetan2600"&gt; /u/zetan2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi8ghi2ifore1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn9klk</id>
    <title>This is the Reason why I am Still Debating whether to buy RTX5090!</title>
    <updated>2025-03-30T10:27:32+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt; &lt;img alt="This is the Reason why I am Still Debating whether to buy RTX5090!" src="https://a.thumbs.redditmedia.com/fsn9OVlHRAb11iT2p_HWV3Lsw8YzibhfHmPiywKjW70.jpg" title="This is the Reason why I am Still Debating whether to buy RTX5090!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11"&gt;https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T10:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnd6px</id>
    <title>LLMs over torrent</title>
    <updated>2025-03-30T14:08:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt; &lt;img alt="LLMs over torrent" src="https://preview.redd.it/8z6t2vvu3ure1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ade8fa1e4ff10e2d71461fdb60f942583a4d442f" title="LLMs over torrent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just messing around with an idea - serving LLM models over torrent. I‚Äôve uploaded Qwen2.5-VL-3B-Instruct to a seedbox sitting in a neutral datacenter in the Netherlands (hosted via Feralhosting).&lt;/p&gt; &lt;p&gt;If you wanna try it out, grab the torrent file here and load it up in any torrent client:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent"&gt;http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment - no promises about uptime, speed, or anything really. It might work, it might not ü§∑&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Some random thoughts / open questions: 1. Only models with redistribution-friendly licenses (like Apache-2.0) can be shared this way. Qwen is cool, Mistral too. Stuff from Meta or Google gets more legally fuzzy - might need a lawyer to be sure. 2. If we actually wanted to host a big chunk of available models, we‚Äôd need a ton of seedboxes. Huggingface claims they store 45PB of data üòÖ üìé &lt;a href="https://huggingface.co/docs/hub/storage-backends"&gt;https://huggingface.co/docs/hub/storage-backends&lt;/a&gt; 3. Binary deduplication would help save space. Bonus points if we can do OTA-style patch updates to avoid re-downloading full models every time. 4. Why bother? AI‚Äôs getting more important, and putting everything in one place feels a bit risky long term. Torrents could be a good backup layer or alt-distribution method.&lt;/p&gt; &lt;p&gt;‚∏ª&lt;/p&gt; &lt;p&gt;Anyway, curious what people think. If you‚Äôve got ideas, feedback, or even some storage/bandwidth to spare, feel free to join the fun. Let‚Äôs see what breaks üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8z6t2vvu3ure1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnc9rd</id>
    <title>It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x</title>
    <updated>2025-03-30T13:21:39+00:00</updated>
    <author>
      <name>/u/madaerodog</name>
      <uri>https://old.reddit.com/user/madaerodog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt; &lt;img alt="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" src="https://b.thumbs.redditmedia.com/xZgN0CnCg9_dkwL0g3ohDgCJu3nIHgZs9DZKGJ0a-FQ.jpg" title="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaerodog"&gt; /u/madaerodog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnc9rd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32 bit is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
</feed>
