<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-19T22:24:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m394zh</id>
    <title>new models from NVIDIA: OpenReasoning-Nemotron 32B/14B/7B/1.5B</title>
    <updated>2025-07-18T17:53:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenReasoning-Nemotron-32B is a large language model (LLM) which is a derivative of Qwen2.5-32B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning about math, code and science solution generation. The model supports a context length of 64K tokens. The OpenReasoning model is available in the following sizes: 1.5B, 7B and 14B and 32B. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial research use.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T17:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41f79</id>
    <title>Any idea when llama 4 behemoth will be released?</title>
    <updated>2025-07-19T17:09:20+00:00</updated>
    <author>
      <name>/u/Shubham_Garg123</name>
      <uri>https://old.reddit.com/user/Shubham_Garg123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven't heard any updates regarding this model since a few months..&lt;/p&gt; &lt;p&gt;Was it much stronger than they expected and they decided not to release it publicly? 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shubham_Garg123"&gt; /u/Shubham_Garg123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41f79/any_idea_when_llama_4_behemoth_will_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41f79/any_idea_when_llama_4_behemoth_will_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m41f79/any_idea_when_llama_4_behemoth_will_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m42gid</id>
    <title>Build advice: Consumer AI workstation with RTX 3090 + dual MI50s for LLM inference and Stable Diffusion (~$5k budget)</title>
    <updated>2025-07-19T17:51:59+00:00</updated>
    <author>
      <name>/u/neighbornugs</name>
      <uri>https://old.reddit.com/user/neighbornugs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for feedback on a mixed-use AI workstation build. Work is pushing me to get serious about local AI/model training or I'm basically toast career-wise, so trying to build something capable but not break the bank.&lt;/p&gt; &lt;p&gt;Planned specs:&lt;/p&gt; &lt;p&gt;CPU: Ryzen 9 9950X3D&lt;/p&gt; &lt;p&gt;Mobo: X870E (eyeing ASUS ROG Crosshair Hero for expansion)&lt;/p&gt; &lt;p&gt;RAM: 256GB DDR5-6000&lt;/p&gt; &lt;p&gt;GPUs: 1x RTX 3090 + 2x MI50 32GB&lt;/p&gt; &lt;p&gt;Use case split: RTX 3090 for Stable Diffusion, dual MI50s for LLM inference&lt;/p&gt; &lt;p&gt;Main questions:&lt;/p&gt; &lt;p&gt;MI50 real-world performance? I've got zero hands-on experience with them but the 32GB VRAM each for ~$250 on eBay seems insane value. How's ROCm compatibility these days for inference?&lt;/p&gt; &lt;p&gt;Can this actually run 70B models? With 64GB across the MI50s, should handle Llama 70B + smaller models simultaneously right?&lt;/p&gt; &lt;p&gt;Coding/creative writing performance? Main LLM use will be code assistance and creative writing (scripts, etc). Are the MI50s fast enough or will I be frustrated coming from API services?&lt;/p&gt; &lt;p&gt;Goals:&lt;/p&gt; &lt;p&gt;Keep under $5k initially but want expansion path&lt;/p&gt; &lt;p&gt;Handle Stable Diffusion without compromise (hence the 3090)&lt;/p&gt; &lt;p&gt;Run multiple LLM models for different users/tasks&lt;/p&gt; &lt;p&gt;Learn fine-tuning and custom models for work requirements&lt;/p&gt; &lt;p&gt;Alternatives I'm considering:&lt;/p&gt; &lt;p&gt;Just go dual RTX 3090s and call it a day, but the MI50 value proposition is tempting if they actually work well&lt;/p&gt; &lt;p&gt;Mac Studio M3 Ultra 256GB - saw one on eBay for $5k. Unified memory seems appealing but worried about AI ecosystem limitations vs CUDA&lt;/p&gt; &lt;p&gt;Mac Studio vs custom build thoughts? The 256GB unified memory on the Mac seems compelling for large models, but I'm concerned about software compatibility for training/fine-tuning. Most tutorials assume CUDA/PyTorch setup. Would I be limiting myself with Apple Silicon for serious AI development work?&lt;/p&gt; &lt;p&gt;Anyone running MI50s for LLM work? Is ROCm mature enough or am I setting myself up for driver hell? The job pressure is real so I need something that works reliably, not a weekend project that maybe runs sometimes.&lt;/p&gt; &lt;p&gt;Budget flexibility exists if there's a compelling reason to spend more, but I'm trying to be smart about price/performance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neighbornugs"&gt; /u/neighbornugs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m45sh1</id>
    <title>I am really hoping the openai IMO announcement will motivate the open source community to match it</title>
    <updated>2025-07-19T20:11:08+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think the chances are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m45sh1/i_am_really_hoping_the_openai_imo_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m45sh1/i_am_really_hoping_the_openai_imo_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m45sh1/i_am_really_hoping_the_openai_imo_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m40yo6</id>
    <title>any lovable and bolt alternative open source?</title>
    <updated>2025-07-19T16:50:17+00:00</updated>
    <author>
      <name>/u/yuval052</name>
      <uri>https://old.reddit.com/user/yuval052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi i love playing with those stuff create stuff for fun, but i have 0 code knowledge. i want to use api of openai or or anthropic . is there any open source that its like lovable and bolt but i use openai api and results are good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuval052"&gt; /u/yuval052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m40yo6/any_lovable_and_bolt_alternative_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m40yo6/any_lovable_and_bolt_alternative_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m40yo6/any_lovable_and_bolt_alternative_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T16:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3jogm</id>
    <title>4k local image gen</title>
    <updated>2025-07-19T01:22:24+00:00</updated>
    <author>
      <name>/u/kor34l</name>
      <uri>https://old.reddit.com/user/kor34l</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"&gt; &lt;img alt="4k local image gen" src="https://preview.redd.it/dulis7vegqdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11887cbf80f7af36eb4f0e9abe4330534f8e6b5a" title="4k local image gen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built an AI Wallpaper Generator that creates ultra-high-quality 4K wallpapers automatically with weather integration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After months of development, I've created a comprehensive AI wallpaper system that generates stunning 4K desktop backgrounds using multiple AI models. &lt;strong&gt;&lt;em&gt;The system just hit v4.2.0&lt;/em&gt;&lt;/strong&gt; with a completely rewritten SDXL pipeline that produces much higher quality photorealistic images.&lt;/p&gt; &lt;p&gt;It is flexible and simple enough to be used for ALL your image gen needs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Choose from FLUX.1-dev, DALL-E 3, GPT-Image-1, or SDXL with Juggernaut XL v9 + multi-LoRA stacking. Each model has its own optimized pipeline for maximum quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weather Integration&lt;/strong&gt;: Real-time weather data automatically influences artistic themes and moods. Rainy day? You get atmospheric, moody scenes. Sunny weather? Bright, vibrant landscapes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced Pipeline&lt;/strong&gt;: Generates at optimal resolution, upscales to 8K using Real-ESRGAN, then downsamples to perfect 4K for incredible detail and quality. No compromises - time and storage don't matter, only final quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smart Theme System&lt;/strong&gt;: 60+ curated themes across 10 categories including Nature, Urban, Space, Anime, and more. Features &amp;quot;chaos mode&amp;quot; for completely random combinations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intelligent Prompting&lt;/strong&gt;: Uses DeepSeek-r1:14b locally to generate creative, contextual prompts tailored to each model's strengths and current weather conditions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automated Scheduling&lt;/strong&gt;: Set-and-forget cron integration for daily wallpaper changes. Wake up to a new masterpiece every morning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage Options:&lt;/strong&gt; - &lt;code&gt;./ai-wallpaper generate&lt;/code&gt; - Default FLUX generation - &lt;code&gt;./ai-wallpaper generate --model sdxl&lt;/code&gt; - Use specific model&lt;br /&gt; - &lt;code&gt;./ai-wallpaper generate --random-model&lt;/code&gt; - Weighted random model selection - &lt;code&gt;./ai-wallpaper generate --save-stages&lt;/code&gt; - Save intermediate processing stages - &lt;code&gt;./ai-wallpaper generate --theme cyberpunk&lt;/code&gt; - Force specific theme - &lt;code&gt;./ai-wallpaper generate --prompt &amp;quot;custom prompt&amp;quot;&lt;/code&gt; - Direct prompt override - &lt;code&gt;./ai-wallpaper generate --random-params&lt;/code&gt; - Randomize generation parameters - &lt;code&gt;./ai-wallpaper generate --seed 42&lt;/code&gt; - Reproducible generation - &lt;code&gt;./ai-wallpaper generate --no-wallpaper&lt;/code&gt; - Generate only, don't set wallpaper - &lt;code&gt;./ai-wallpaper test --model flux&lt;/code&gt; - Test specific model - &lt;code&gt;./ai-wallpaper config --show&lt;/code&gt; - Display current configuration - &lt;code&gt;./ai-wallpaper models --list&lt;/code&gt; - Show all available models with status - &lt;code&gt;./setup_cron.sh&lt;/code&gt; - Automated daily wallpaper scheduling&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recent v4.2.0 Updates:&lt;/strong&gt; - &lt;strong&gt;&lt;em&gt;Completely rewritten SDXL pipeline&lt;/em&gt;&lt;/strong&gt; with Juggernaut XL v9 base model - Multi-LoRA stacking system with automatic theme-based selection - Enhanced negative prompts - Photorealistic prompt enhancement with DSLR camera modifiers - Optimized settings: 80+ steps, CFG 8.0, ensemble base/refiner pipeline&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Specs:&lt;/strong&gt; - &lt;strong&gt;Models&lt;/strong&gt;: FLUX.1-dev (24GB VRAM), DALL-E 3 (API), GPT-Image-1 (API), SDXL+LoRA (16GB VRAM) - &lt;strong&gt;Quality&lt;/strong&gt;: Maximum settings across all models - no speed optimizations - &lt;strong&gt;Output&lt;/strong&gt;: Native 4K (3840x2160) with professional color grading - &lt;strong&gt;Architecture&lt;/strong&gt;: Modular Python system with YAML configuration - &lt;strong&gt;Desktop&lt;/strong&gt;: XFCE4 multi-monitor/workspace support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; - NVIDIA GPU (RTX 3090 recommended for SDXL) - FLUX works off CPU entirely, if GPU is weak - Python 3.10+ with virtual environment - OpenAI API key (for DALL-E/GPT models)&lt;/p&gt; &lt;p&gt;The system is completely open source and designed to be &amp;quot;fail loud&amp;quot; - every error is verbose and clear, making it easy to troubleshoot. All configuration is in YAML files, and the modular architecture makes it simple to add new models or modify existing pipelines.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/expectbugs/ai-wallpaper"&gt;https://github.com/expectbugs/ai-wallpaper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The system handles everything from installation to daily automation. Check the README.md for complete setup instructions, model comparisons, and configuration options. &lt;/p&gt; &lt;p&gt;Would love feedback from the community! I'm excited to see what others create with it.&lt;/p&gt; &lt;p&gt;The documentation (and most of this post) were written by AI, the legacy monolithic fat scripts in the legacy directory where I started, were also written largly by AI. The complete system was made with a LOT of tools and a lot of manual effort and bugfixing and refactoring, plus, of course, AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kor34l"&gt; /u/kor34l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dulis7vegqdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T01:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3no1m</id>
    <title>Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy.</title>
    <updated>2025-07-19T04:53:01+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"&gt; &lt;img alt="Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy." src="https://preview.redd.it/wz3nkrm3hrdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52fcc39aab4ecdb879243bef5a4cc3e8c57f0e44" title="Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;How the Forensic Linguistics Analysis Works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built this using established computational linguistics techniques for authorship attribution - the same methods used in legal cases and academic research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Corpus Building&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compiled 76 documents (14M characters) of verified Trump statements from debates, speeches, tweets, and press releases&lt;/li&gt; &lt;li&gt;Cleaned the data to remove metadata while preserving actual speech patterns&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Stylometric Feature Extraction&lt;/strong&gt; The system extracts 4 categories of linguistic &amp;quot;fingerprints&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lexical Features&lt;/strong&gt;: Average word length, vocabulary richness, hapax legomena ratio (words used only once), Yule's K diversity measure&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntactic Features&lt;/strong&gt;: Part-of-speech distributions, dependency parsing patterns, sentence complexity scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Features&lt;/strong&gt;: 768-dimension embeddings from the STAR authorship attribution model (AIDA-UPM/star)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stylistic Features&lt;/strong&gt;: Modal verb usage, passive voice frequency, punctuation patterns, function word ratios&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Similarity Calculation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compares the disputed text against all corpus documents using cosine similarity and Jensen-Shannon divergence&lt;/li&gt; &lt;li&gt;Generates weighted scores across all four linguistic dimensions&lt;/li&gt; &lt;li&gt;The 89.6% syntactic similarity is particularly significant - sentence structure patterns are neurologically hardwired and hardest to fake&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Why This Matters&lt;/strong&gt; Syntactic patterns emerge from deep cognitive structures. You can consciously change topic or vocabulary, but your underlying grammatical architecture remains consistent. The high syntactic match (89.6%) combined with moderate lexical match (47.2%) suggests same author writing in a different context.&lt;/p&gt; &lt;p&gt;The system correctly identified this as &amp;quot;probably same author&amp;quot; with 66.1% overall confidence - which is forensically significant for disputed authorship cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wz3nkrm3hrdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m44tnz</id>
    <title>Keras vs Transformers fine tuning</title>
    <updated>2025-07-19T19:30:35+00:00</updated>
    <author>
      <name>/u/Ok-Refrigerator6609</name>
      <uri>https://old.reddit.com/user/Ok-Refrigerator6609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to ML and fine tuning.&lt;/p&gt; &lt;p&gt;Recently I've tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn't convert it to pytorch because the conversion script by Keras doesn't support the gemma 3 yet and so I abandoned this project because of that.&lt;/p&gt; &lt;p&gt;I then tried fine tuning with transformers and even though I've tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.&lt;/p&gt; &lt;p&gt;I learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.&lt;/p&gt; &lt;p&gt;I'm wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.&lt;/p&gt; &lt;p&gt;Thank you in advance&lt;/p&gt; &lt;p&gt;Sorry, this post &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Refrigerator6609"&gt; /u/Ok-Refrigerator6609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T19:30:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3wogu</id>
    <title>Are there any quants of larger models 48 VRAM + 96 RAM can run, which are better than just 32B models?</title>
    <updated>2025-07-19T13:48:31+00:00</updated>
    <author>
      <name>/u/West_Investigator258</name>
      <uri>https://old.reddit.com/user/West_Investigator258</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. &lt;/p&gt; &lt;p&gt;However most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can't even shove more RAM into it to run 200gb quants. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West_Investigator258"&gt; /u/West_Investigator258 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m390kj</id>
    <title>DGAF if it’s dumber. It’s mine.</title>
    <updated>2025-07-18T17:48:14+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt; &lt;img alt="DGAF if it’s dumber. It’s mine." src="https://preview.redd.it/8dnb7bl76odf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca95154378d607f250ca4e5e26488394250116bf" title="DGAF if it’s dumber. It’s mine." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dnb7bl76odf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T17:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qpxz</id>
    <title>What are the most intriguing AI papers of 2025</title>
    <updated>2025-07-19T08:00:18+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been keeping up with AI research in 2025, and DeepSeek R1 really stands out to me as game-changing. What other papers from this year do you consider to be truly revolutionary?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T08:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3tk92</id>
    <title>I love local models</title>
    <updated>2025-07-19T11:06:55+00:00</updated>
    <author>
      <name>/u/TweeMansLeger</name>
      <uri>https://old.reddit.com/user/TweeMansLeger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"&gt; &lt;img alt="I love local models" src="https://preview.redd.it/k7ebpl1nctdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22285a6e5642636636a349ad5e51158d2aa60f71" title="I love local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TweeMansLeger"&gt; /u/TweeMansLeger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k7ebpl1nctdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T11:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xp21</id>
    <title>ChatSong, a lightweight, local LLM chat tool that's a single executable file</title>
    <updated>2025-07-19T14:33:28+00:00</updated>
    <author>
      <name>/u/Suitable-Patience916</name>
      <uri>https://old.reddit.com/user/Suitable-Patience916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"&gt; &lt;img alt="ChatSong, a lightweight, local LLM chat tool that's a single executable file" src="https://preview.redd.it/jcc7hsejdudf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b46aa1f23372af44a62d97c0c4858e30eac7768" title="ChatSong, a lightweight, local LLM chat tool that's a single executable file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built a lightweight LLM API invocation tool that requires no installation, just a single executable file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Truly Portable: It's a single executable file, no installation required.&lt;/li&gt; &lt;li&gt;Bring Your Own Model: Customize models and prompts easily through a config file.&lt;/li&gt; &lt;li&gt;Save &amp;amp; Share: Export entire conversations as clean, single-file HTML pages.&lt;/li&gt; &lt;li&gt;Model Hopping: Switch between models in the same conversation.&lt;/li&gt; &lt;li&gt;Web-Aware: Can perform a web search or pull text from a URL to use as context for its answers.&lt;/li&gt; &lt;li&gt;File Upload: Drop in a PDF, TXT, or even a ZIP file to chat with your documents.&lt;/li&gt; &lt;li&gt;Code-Friendly: Proper Markdown rendering and syntax highlighting for code blocks.&lt;/li&gt; &lt;li&gt;Cost-Aware: Tracks token usage and lets you limit the conversation history sent with each request, which is a huge token saver.&lt;/li&gt; &lt;li&gt;Incognito Mode: For all your top-secret conversations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/jingangdidi/chatsong"&gt;https://github.com/jingangdidi/chatsong&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Patience916"&gt; /u/Suitable-Patience916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcc7hsejdudf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:33:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3iv6s</id>
    <title>any idea how to open source that?</title>
    <updated>2025-07-19T00:42:15+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt; &lt;img alt="any idea how to open source that?" src="https://preview.redd.it/x9e7q7z59qdf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd978d7cb888d92fdfc0a24134a57d1d3821cd08" title="any idea how to open source that?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9e7q7z59qdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T00:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3ssb2</id>
    <title>ARC AGI 3 is stupid</title>
    <updated>2025-07-19T10:17:53+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the first game, first level of 8, I completed the level after wasting a lot of time trying to figure out what functionality the spacebar and mouse clicks had. None, it turned out. On the second level, I got completely stuck, then read in another thread that you have to move on and off the first shape several times to loop through available shapes until hitting the target shape. I would never in a millioin years have figured this out because I would never consider anyone would make an intelligence test this stupid.&lt;/p&gt; &lt;p&gt;ARC AGI 1 and 2 were fine, well designed. But this 3 version is a test of stupid persistence, not intelligence.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T10:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m39uqi</id>
    <title>I made a 1000 hour NSFW TTS dataset</title>
    <updated>2025-07-18T18:20:34+00:00</updated>
    <author>
      <name>/u/hotroaches4liferz</name>
      <uri>https://old.reddit.com/user/hotroaches4liferz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can find and listen to the dataset on huggingface: &lt;a href="https://huggingface.co/datasets/setfunctionenvironment/testnew"&gt;https://huggingface.co/datasets/setfunctionenvironment/testnew&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The sample rate of all audio is 24,000 kHz&lt;/p&gt; &lt;p&gt;Stats:&lt;/p&gt; &lt;p&gt;Total audio files/samples: 556,667&lt;/p&gt; &lt;p&gt;Total duration: 1024.71 hours (3688949 seconds)&lt;/p&gt; &lt;p&gt;Average duration: 6.63 seconds&lt;/p&gt; &lt;p&gt;Shortest clip: 0.41 seconds&lt;/p&gt; &lt;p&gt;Longest clip: 44.97 seconds (all audio &amp;gt;45 seconds removed)&lt;/p&gt; &lt;p&gt;more and more TTS models are releasing and improving, the size of these models are decreasing some even being 0.5b 0.7b or 0.1b parameters but unfortunately they all dont have NSFW capability. It is a shame there are so many NSFW LLM finetunes out there but none exist for text to speech, so if anyone at all has the compute to finetune one of the existing TTS models (kokoro, zonos, F5, chatterbox, orpheus) on my dataset that would be very appreciated as I would like to try it 🙏🙏🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hotroaches4liferz"&gt; /u/hotroaches4liferz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T18:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3wnnm</id>
    <title>What's New in Agent Leaderboard v2?</title>
    <updated>2025-07-19T13:47:25+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"&gt; &lt;img alt="What's New in Agent Leaderboard v2?" src="https://preview.redd.it/bwu8hq345udf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=994610c0a05c8b64924cfefbf8e0691a9c5619ef" title="What's New in Agent Leaderboard v2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Here is a quick TL;DR 👇&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;GPT-4.1&lt;/strong&gt; tops with 62% Action Completion (AC) overall.&lt;br /&gt; ⚡ &lt;strong&gt;Gemini 2.5&lt;/strong&gt; Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).&lt;br /&gt; 💸 &lt;strong&gt;GPT-4.1&lt;/strong&gt;-mini is &lt;em&gt;most cost-effective&lt;/em&gt; at $0.014/session vs. GPT-4.1’s $0.068.&lt;br /&gt; 🏭 No single model dominates across industries.&lt;br /&gt; 🤖 &lt;strong&gt;Grok 4&lt;/strong&gt; didn't lead in any metric.&lt;br /&gt; 🧩 Reasoning models &lt;em&gt;underperform&lt;/em&gt; compared to non-reasoning ones.&lt;br /&gt; 🆕 &lt;strong&gt;Kimi’s K2&lt;/strong&gt; leads &lt;em&gt;open-source models&lt;/em&gt; with 0.53 AC, 0.90 TSQ, and $0.039/session.&lt;/p&gt; &lt;p&gt;Link Below:&lt;/p&gt; &lt;p&gt;[Blog]: &lt;a href="https://galileo.ai/blog/agent-leaderboard-v2"&gt;https://galileo.ai/blog/agent-leaderboard-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Agent v2 Live Leaderboard]: &lt;a href="https://huggingface.co/spaces/galileo-ai/agent-leaderboard"&gt;https://huggingface.co/spaces/galileo-ai/agent-leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwu8hq345udf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:47:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3sgr1</id>
    <title>WordPecker: Open Source Personalized Duolingo</title>
    <updated>2025-07-19T09:57:11+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt; &lt;img alt="WordPecker: Open Source Personalized Duolingo" src="https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b297127eab67178a51943bc8535bfc9dfb9f671" title="WordPecker: Open Source Personalized Duolingo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/baturyilmaz/wordpecker-app"&gt;https://github.com/baturyilmaz/wordpecker-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5fximscazsdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T09:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41bj1</id>
    <title>A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created</title>
    <updated>2025-07-19T17:05:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt; &lt;img alt="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" src="https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c89159d8d8c6aba292f37a10b0a43f8493d0366" title="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the extensie discussion &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;about UTCP&lt;/a&gt; last week, the authors of UTCP created an RFC for it.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.&lt;/p&gt; &lt;p&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.&lt;/p&gt; &lt;p&gt;UTCP does this by specifying a &amp;quot;manual&amp;quot;, where a tool provider publishes a standardized description of its &amp;quot;tools&amp;quot; together with the necessary information to call them (named in the following &amp;quot;transport&amp;quot;, previously known as &amp;quot;provider&amp;quot;).&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Discussion issue: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Current RFC: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3yzes</id>
    <title>Localllama’s (first?) IFTA - I’ll Fine-Tune Anything</title>
    <updated>2025-07-19T15:28:11+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.&lt;/p&gt; &lt;p&gt;Premise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. &lt;/p&gt; &lt;p&gt;Rules are:&lt;/p&gt; &lt;p&gt;Has to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. &lt;/p&gt; &lt;p&gt;The output of the training will be shared publicly on HF for the benefit of the community. &lt;/p&gt; &lt;p&gt;What do you say, interested? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3n89p</id>
    <title>(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models</title>
    <updated>2025-07-19T04:28:21+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt; &lt;img alt="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" src="https://preview.redd.it/edxmilbhdrdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed39c860785fb34d8104df720311441abac8087" title="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.&lt;/p&gt; &lt;p&gt;“Text data generated by the model is NOT considered as a derivative work.”&lt;/p&gt; &lt;p&gt;Hopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edxmilbhdrdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46gtn</id>
    <title>Can we finally "index" a code project?</title>
    <updated>2025-07-19T20:40:18+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I understand how &amp;quot;tooling&amp;quot; works w/ newer LLMs now, I can take a large code project and &amp;quot;index&amp;quot; it in such a way that an LLM can &amp;quot;search&amp;quot; it like a database and answer questions regarding the source code?&lt;/p&gt; &lt;p&gt;This is my #1 need at the moment, being able to get quick answers about my code base that's quite large. I don't need a coder so much as I need a local LLM that can be API and Source-Code &amp;quot;aware&amp;quot; and can help me in the biggest bottlenecks that myself and most senior engineers face: &amp;quot;Now where the @#$% did that line of code that does that one thing??&amp;quot; or &amp;quot;Given the class names i've used so far, what's a name for this NEW class that stays consistent with the other names&amp;quot; and finally &amp;quot;What's the thousand-mile view of this class/script's purpose?&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks in advance! I'm fairly new so my terminology could certainly be outdated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xgjo</id>
    <title>Dual GPU set up was surprisingly easy</title>
    <updated>2025-07-19T14:23:11+00:00</updated>
    <author>
      <name>/u/m-gethen</name>
      <uri>https://old.reddit.com/user/m-gethen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt; &lt;img alt="Dual GPU set up was surprisingly easy" src="https://a.thumbs.redditmedia.com/FZ5L51GTZo6IrqOEds48bUmd3srrQbWvmNjPPEfS1l0.jpg" title="Dual GPU set up was surprisingly easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. &lt;/p&gt; &lt;p&gt;Both in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. &lt;/p&gt; &lt;p&gt;Current spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold&lt;/p&gt; &lt;p&gt;Will be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.&lt;/p&gt; &lt;p&gt;Any tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m-gethen"&gt; /u/m-gethen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m3xgjo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3vqom</id>
    <title>A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality</title>
    <updated>2025-07-19T13:03:43+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.11851"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:03:43+00:00</published>
  </entry>
</feed>
