<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-31T16:49:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1idt9xz</id>
    <title>Watch this SmolAgent save me over 100 hours of work.</title>
    <updated>2025-01-30T18:08:42+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt; &lt;img alt="Watch this SmolAgent save me over 100 hours of work." src="https://external-preview.redd.it/eXpvaDN2aXY4NmdlMaIWY-pKRTEFed4oaflr_50jeaU7y6AfPZ2q49QYyqUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ea3707bc7016b8ca7da0ee5c72fef8602edfdba" title="Watch this SmolAgent save me over 100 hours of work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2gcviv86ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:08:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iduk3b</id>
    <title>Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)</title>
    <updated>2025-01-30T19:02:02+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt; &lt;img alt="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" src="https://preview.redd.it/gazbvr6gi6ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6de32bcaa9f8ae8ff3f2ab317c2401bd2f5b73" title="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gazbvr6gi6ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie7db5</id>
    <title>Chris Manning (top 3 NLP/Machine Learning researchers in the world) believes the Deepseek 6m dollar training costs due to the optimizations discussed in their paper</title>
    <updated>2025-01-31T05:03:02+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While a lot of the things discussed in the Deepseek paper have been verified, what has garnered the most skepticism is the training cost. &lt;/p&gt; &lt;p&gt;Chris manning, whose highly regarded as one of the top 3-5 NLP researchers in the world, gave a talk yesterday, which was live tweeted&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700131884490762"&gt;https://x.com/atroyn/status/1884700131884490762&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;deepseek have succeeded at producing models with large numbers of experts (256 in v3). combined with multi-head latent attention, plus training in fb8, dramatically reduces training costs. @chrmanning buys the $6M training compute cost.&amp;quot;&lt;/p&gt; &lt;p&gt;He buys the 6 million dollar training cost claimed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T05:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido3fn</id>
    <title>Are there Â½ million people capable of running locally 685B params models?</title>
    <updated>2025-01-30T14:25:02+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_Â½_million_people_capable_of_running/"&gt; &lt;img alt="Are there Â½ million people capable of running locally 685B params models?" src="https://b.thumbs.redditmedia.com/nUAmR_7owY5oJQcrzV0vL3H93-ccvgV-SDlaKg3CSyw.jpg" title="Are there Â½ million people capable of running locally 685B params models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ido3fn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_Â½_million_people_capable_of_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_Â½_million_people_capable_of_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebozu</id>
    <title>DeepSeek-R1 Now Live With NVIDIA NIM</title>
    <updated>2025-01-31T10:16:20+00:00</updated>
    <author>
      <name>/u/Lacy_Hall</name>
      <uri>https://old.reddit.com/user/Lacy_Hall</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebozu/deepseekr1_now_live_with_nvidia_nim/"&gt; &lt;img alt="DeepSeek-R1 Now Live With NVIDIA NIM" src="https://external-preview.redd.it/KJ7uKiPBYxGxiVvh_MM2oHJDEG-PLE0KeC8jxiR32Mk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5adf72a14ff4723a91cfd9823ed1d2f1fcc2ae7f" title="DeepSeek-R1 Now Live With NVIDIA NIM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lacy_Hall"&gt; /u/Lacy_Hall &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/deepseek-r1-nim-microservice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebozu/deepseekr1_now_live_with_nvidia_nim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iebozu/deepseekr1_now_live_with_nvidia_nim/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T10:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie8jea</id>
    <title>What the fuck is abbas manðŸ—¿ðŸ’”</title>
    <updated>2025-01-31T06:16:37+00:00</updated>
    <author>
      <name>/u/Fun-Property-5964</name>
      <uri>https://old.reddit.com/user/Fun-Property-5964</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"&gt; &lt;img alt="What the fuck is abbas manðŸ—¿ðŸ’”" src="https://preview.redd.it/bi8mqxkuu9ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82fac435f8a5f0ee1b5ec8398cb5ea3b6c1952d" title="What the fuck is abbas manðŸ—¿ðŸ’”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Property-5964"&gt; /u/Fun-Property-5964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bi8mqxkuu9ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T06:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny3w</id>
    <title>Mistral Small 3</title>
    <updated>2025-01-30T14:17:56+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt; &lt;img alt="Mistral Small 3" src="https://preview.redd.it/kj3s0jvr35ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317aadc49155a8df1074618844c589ea3d2753d" title="Mistral Small 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kj3s0jvr35ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6qpi</id>
    <title>Which is the best NSFW llm?</title>
    <updated>2025-01-31T04:26:37+00:00</updated>
    <author>
      <name>/u/NebulaNinja_779</name>
      <uri>https://old.reddit.com/user/NebulaNinja_779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Frankly speaking i am looking to build an chat app for adult talking. It's a test project of mine. So if you know which one should i use, please let me know &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NebulaNinja_779"&gt; /u/NebulaNinja_779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6qpi/which_is_the_best_nsfw_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6qpi/which_is_the_best_nsfw_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6qpi/which_is_the_best_nsfw_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T04:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idva1j</id>
    <title>Welcome back, Le Mistral!</title>
    <updated>2025-01-30T19:31:40+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt; &lt;img alt="Welcome back, Le Mistral!" src="https://preview.redd.it/4td7dsrjn6ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcee379bd06ff66ce0c2532f18c365ea37c8d6d1" title="Welcome back, Le Mistral!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4td7dsrjn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieihjr</id>
    <title>What the hell do people expect?</title>
    <updated>2025-01-31T16:20:03+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release of R1 I saw so many &amp;quot;But it can't talk about tank man!&amp;quot;, &amp;quot;But it's censored!&amp;quot;, &amp;quot;But it's from the chinese!&amp;quot; posts. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are all censored. And for R1 in particular... I don't want to discuss chinese politics (or politics at all) with my LLM. That's not my use-case and I don't think I'm in a minority here.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would happen if it was not censored the way it is? The guy behind it would probably have disappeared by now.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They all give a fuck about data privacy as much as they can. Else we wouldn't have ever read about samsung engineers not being allowed to use GPT for processor development anymore.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;IMHO it's not worse or better than the rest (non self-hosted) and the negative media reports are 1:1 the same like back in the days when Zen was released by AMD and all Intel could do was cry like &amp;quot;But it's just cores they glued together!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4brg</id>
    <title>DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked</title>
    <updated>2025-01-31T02:16:09+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt; &lt;img alt="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" src="https://external-preview.redd.it/FoBRfbFJiqbPvZWr-1-_kZti4liFY86vCxy63rbFeaE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8563af2625a34ede2f3818cc4a25bab8f7cf54c0" title="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thehackernews.com/2025/01/deepseek-ai-database-exposed-over-1.html?m=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T02:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie94yy</id>
    <title>Tool calling support landed in llama.cpp today!</title>
    <updated>2025-01-31T06:59:16+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of the popular open models are supported: generic + native for Llama, Functionary, Hermes, Mistral, Firefunction, DeepSeek&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/9639"&gt;https://github.com/ggerganov/llama.cpp/pull/9639&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T06:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieh01f</id>
    <title>Fully open source codebase to train SOTA VLMs</title>
    <updated>2025-01-31T15:15:11+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Andi from multimodal team at Hugging Face.&lt;/p&gt; &lt;p&gt;Today we're open-sourcing the codebase used to train SmolVLM from scratch on 256 H100s&lt;br /&gt; Inspired by our team's effort to open-source DeepSeek's R1 training, we are releasing the training and evaluation code on top of the weights&lt;br /&gt; Now you can train any of our SmolVLMsâ€”or create your own custom VLMs!&lt;/p&gt; &lt;p&gt;Go check it out:&lt;br /&gt; &lt;a href="https://github.com/huggingface/smollm/tree/main/vision"&gt;https://github.com/huggingface/smollm/tree/main/vision&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie0a8u</id>
    <title>QWEN just launched their chatbot website</title>
    <updated>2025-01-30T23:03:37+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt; &lt;img alt="QWEN just launched their chatbot website" src="https://preview.redd.it/vzgzfrhlp7ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa67cbeae08d4c800e7b5dc088c0330556268f" title="QWEN just launched their chatbot website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link: &lt;a href="https://chat.qwenlm.ai/"&gt;https://chat.qwenlm.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vzgzfrhlp7ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T23:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebs6c</id>
    <title>Coauthors of DeepSeek researchers. Can u spot Meta?</title>
    <updated>2025-01-31T10:22:53+00:00</updated>
    <author>
      <name>/u/osint_for_good</name>
      <uri>https://old.reddit.com/user/osint_for_good</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"&gt; &lt;img alt="Coauthors of DeepSeek researchers. Can u spot Meta?" src="https://preview.redd.it/9gdffiyl2bge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2aa09755c24832c4f07acbe500235ee5b176845" title="Coauthors of DeepSeek researchers. Can u spot Meta?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/osint_for_good"&gt; /u/osint_for_good &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9gdffiyl2bge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T10:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv7yb</id>
    <title>Marc Andreessen on Anthropic CEO's Call for Export Controls on China</title>
    <updated>2025-01-30T19:29:13+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt; &lt;img alt="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" src="https://preview.redd.it/wlsi25dcn6ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d695bb3258d357570ad11762d15df689f13fe2a8" title="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wlsi25dcn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtkll</id>
    <title>Interview with Deepseek Founder: We wonâ€™t go closed-source. We believe that establishing a robust technology ecosystem matters more.</title>
    <updated>2025-01-30T18:20:59+00:00</updated>
    <author>
      <name>/u/deoxykev</name>
      <uri>https://old.reddit.com/user/deoxykev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt; &lt;img alt="Interview with Deepseek Founder: We wonâ€™t go closed-source. We believe that establishing a robust technology ecosystem matters more." src="https://external-preview.redd.it/VCPkBGJsVaggWY7c9V20KQQGCJhrF411vyVYUsHeuns.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=495bbbb03e5ebeff92050c2a71f7e340cb4bbebc" title="Interview with Deepseek Founder: We wonâ€™t go closed-source. We believe that establishing a robust technology ecosystem matters more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deoxykev"&gt; /u/deoxykev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5tls</id>
    <title>If you can't afford to run R1 locally, then being patient is your best action.</title>
    <updated>2025-01-31T03:35:25+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pause for a minute and read &lt;a href="https://simonwillison.net/2024/Dec/9/llama-33-70b/"&gt;I can now run a GPT-4 class model on my laptop&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It only take &lt;em&gt;20 months&lt;/em&gt; for smaller model that can run on consumer hardware to surpass bigger older models.&lt;/p&gt; &lt;p&gt;Yes, it feels like an eternity for internet user. But 1.5 years is small for human lifespan. Don't believe me? Llama 1 is almost 2 years old! (Released on February 24, 2023)&lt;/p&gt; &lt;p&gt;In the next 20 months, there will be small model that are better than R1.&lt;/p&gt; &lt;p&gt;Just like patient gamer save money waiting for steam sale, we save money by waiting for better, more efficient smaller model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T03:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefhfj</id>
    <title>Mistral Small 3 24B GGUF quantization Evaluation results</title>
    <updated>2025-01-31T14:03:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt; &lt;img alt="Mistral Small 3 24B GGUF quantization Evaluation results" src="https://external-preview.redd.it/6gL76ZMrrBOOoaB0ogD-JcHTREOFmGNz2SY3hi5tJtE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=506e8acadec458fdbc5263aab62e7fe23bff3e73" title="Mistral Small 3 24B GGUF quantization Evaluation results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c"&gt;https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1"&gt;https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab"&gt;https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please note that the purpose of this test is to check if the model's intelligence will be significantly affected at low quantization levels, rather than evaluating which gguf is the best.&lt;/p&gt; &lt;p&gt;Regarding Q6_K-lmstudio: This model was downloaded from the lmstudio hf repo and uploaded by bartowski. However, this one is a static quantization model, while others are dynamic quantization models from bartowski's own repo.&lt;/p&gt; &lt;p&gt;gguf: &lt;a href="https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF"&gt;https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Backend: &lt;a href="https://www.ollama.com/"&gt;https://www.ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation tool: &lt;a href="https://github.com/chigkim/Ollama-MMLU-Pro"&gt;https://github.com/chigkim/Ollama-MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation config: &lt;a href="https://pastebin.com/mqWZzxaH"&gt;https://pastebin.com/mqWZzxaH&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T14:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1idz487</id>
    <title>'we're in this bizarre world where the best way to learn about llms... is to read papers by chinese companies. i do not think this is a good state of the world' - us labs keeping their architectures and algorithms secret is ultimately hurting ai development in the us.' - Dr Chris Manning</title>
    <updated>2025-01-30T22:13:22+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700560500416881"&gt;https://x.com/atroyn/status/1884700560500416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieaiq4</id>
    <title>Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source.</title>
    <updated>2025-01-31T08:44:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt; &lt;img alt="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." src="https://external-preview.redd.it/eMItb2dkatZwZPt-N-o-ODncWuvwGgn8w91JWJRsEcg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8507bb5e6a8a03ef08478289b8275d62d0245cc" title="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/lightblue/r1-multilingual-679c890166ac0a84e83e38fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6gv0</id>
    <title>Itâ€™s time to lead guys</title>
    <updated>2025-01-31T04:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt; &lt;img alt="Itâ€™s time to lead guys" src="https://preview.redd.it/4r69mh9f89ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f3c997deb132531af541fbe7a279f1544512cbb" title="Itâ€™s time to lead guys" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4r69mh9f89ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T04:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iehstw</id>
    <title>GPU pricing is spiking as people rush to self-host deepseek</title>
    <updated>2025-01-31T15:50:54+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt; &lt;img alt="GPU pricing is spiking as people rush to self-host deepseek" src="https://preview.redd.it/599a10y9pcge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60ac26bd7a2d3395eefcaee8fbf07a28102792f" title="GPU pricing is spiking as people rush to self-host deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/599a10y9pcge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefan2</id>
    <title>Idea: "Can I Run This LLM?" Website</title>
    <updated>2025-01-31T13:55:03+00:00</updated>
    <author>
      <name>/u/Dangerous_Bunch_3669</name>
      <uri>https://old.reddit.com/user/Dangerous_Bunch_3669</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt; &lt;img alt="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" src="https://preview.redd.it/l344q42n4cge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d54b955e3b1b2cf6f6d117e19782d25f8f4603c8" title="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have and idea. You know how websites like Can You Run It let you check if a game can run on your PC, showing FPS estimates and hardware requirements?&lt;/p&gt; &lt;p&gt;What if there was a similar website for LLMs? A place where you could enter your hardware specs and see:&lt;/p&gt; &lt;p&gt;Tokens per second, VRAM &amp;amp; RAM requirements etc.&lt;/p&gt; &lt;p&gt;It would save so much time instead of digging through forums or testing models manually. &lt;/p&gt; &lt;p&gt;Does something like this exist already? ðŸ¤”&lt;/p&gt; &lt;p&gt;I would pay for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Bunch_3669"&gt; /u/Dangerous_Bunch_3669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l344q42n4cge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T13:55:03+00:00</published>
  </entry>
</feed>
