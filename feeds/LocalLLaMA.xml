<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-20T12:27:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1muslis</id>
    <title>I tried to get 600 dollars "deep think" for local models by making them argue with each other for hours. It's slow, but it's interesting</title>
    <updated>2025-08-19T19:35:53+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking a lot about how we, as people, develop ideas. It's rarely a single, brilliant flash of insight. Our minds are shaped by the countless small interactions we have throughout the day—a conversation here, an article there. This environment of constant, varied input seems just as important as the act of thinking itself.&lt;/p&gt; &lt;p&gt;I wanted to see if I could recreate a small-scale version of that &amp;quot;soup&amp;quot; required for true insight, for local LLMs. The result is a project I'm calling &lt;strong&gt;Network of Agents (NoA)&lt;/strong&gt;, and I wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full README &lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core idea is to treat AI agents like individual neurons in a larger network. You give the network a difficult problem, and a whole team of agents, each with a different &amp;quot;personality&amp;quot; and skillset, starts working on it. They pass their ideas from one layer to the next, building on each other's work to come up with a final, combined solution.&lt;/p&gt; &lt;p&gt;Here’s the part that I'm most curious about. I was inspired by the concept of backpropagation in neural networks. It's a numerical algorithm, of course, but I wondered if the core principle could be applied qualitatively. What if, instead of sending back a numerical error signal, you sent back a &amp;quot;reflection&amp;quot;?&lt;/p&gt; &lt;p&gt;After the network produces a solution, a &amp;quot;critique&amp;quot; agent reviews it and provides criticism. This feedback is then used to automatically re-write the core system prompts of the agents that contributed. The goal is for the network to &amp;quot;learn&amp;quot; from its mistakes over multiple cycles, refining not just its answers, but its own internal structure and approach.&lt;/p&gt; &lt;p&gt;The whole thing is designed to run locally on modest hardware. I've been running it on my laptop with the streets local legend (qwen 30b a3b 2507 instruct lol). It allows the machine to just sit and &amp;quot;think&amp;quot; about a problem for a very long time. The algorithm does really well in problems where creativity and insight override pure precision. It can come up with new frameworks for the social sciences for instance. Physics and math not so much. Looking into opensource gemini-cli to give each &amp;quot;neuron&amp;quot; an execution environment so it can code, but that would be future tense. It certainly adds a lot more complexity.&lt;/p&gt; &lt;p&gt;The obvious trade-off here is speed. It’s the opposite of instantaneous. A 6-layer network with 6 agents per layer, running for 20 cycles, can easily take 12 hours to complete. You're trading quick computation for a slow, iterative process of refinement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is where I'd love to get some community input.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My long-term vision is to go beyond a single machine. I dream of building a P2P networking layer that would allow multiple users to connect their instances of the micro-app. We could create a shared, distributed network where our machines could collaborate to tackle truly massive problems.&lt;/p&gt; &lt;p&gt;However, my background is in Python and AI, and I'm not an expert in distributed systems. &lt;strong&gt;If you're someone who knows about peer-to-peer networking and this idea sounds at all interesting to you, I would genuinely love to hear from you and potentially collaborate.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It’s an open-source experiment, and I’d be grateful for any thoughts, feedback, or ideas you might have.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full repo and documentation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/my-personal-essays-about-ai/blob/main/beyond-agi.md"&gt;The place im coming from theoretically &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv2y08</id>
    <title>NVIDIA-Nemotron-Nano-9B-v2 "Better than GPT-5" at LiveCodeBench?</title>
    <updated>2025-08-20T02:40:57+00:00</updated>
    <author>
      <name>/u/randomqhacker</name>
      <uri>https://old.reddit.com/user/randomqhacker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt; &lt;img alt="NVIDIA-Nemotron-Nano-9B-v2 &amp;quot;Better than GPT-5&amp;quot; at LiveCodeBench?" src="https://external-preview.redd.it/GiqzTuyH_eElt0yVAuFWAuvHSRjIIaLz2aN8rPQ0Z8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=482f8ac96c3cea37e27a2f7f5ecd766197607428" title="NVIDIA-Nemotron-Nano-9B-v2 &amp;quot;Better than GPT-5&amp;quot; at LiveCodeBench?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/c9n1vpdl83kf1.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e9ac6a8836d8f4b25e04fb899612dffcad6bf8"&gt;Pikachu surprised a 9B \&amp;quot;beats GPT-5\&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pruned from a 12B and further trained by Nvidia. Lots of the dataset is open source as well! But better that GPT-5 and GLM 4.5 Air at LiveCodeBench? Really?&lt;/p&gt; &lt;p&gt;I will be taking this one for a spin...&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b%2Cgpt-4-1%2Cgpt-oss-20b%2Cgpt-5-minimal%2Co4-mini%2Co3%2Cgpt-5-medium%2Cgpt-5%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cgemini-2-5-flash-reasoning%2Cclaude-4-sonnet-thinking%2Cmagistral-small%2Cdeepseek-r1%2Cgrok-4%2Csolar-pro-2-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Cnvidia-nemotron-nano-9b-v2-reasoning%2Ckimi-k2%2Cexaone-4-0-32b-reasoning%2Cglm-4-5-air%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning"&gt;https://artificialanalysis.ai/evaluations/livecodebench?models=gpt-oss-120b%2Cgpt-4-1%2Cgpt-oss-20b%2Cgpt-5-minimal%2Co4-mini%2Co3%2Cgpt-5-medium%2Cgpt-5%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cgemini-2-5-flash-reasoning%2Cclaude-4-sonnet-thinking%2Cmagistral-small%2Cdeepseek-r1%2Cgrok-4%2Csolar-pro-2-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Cnvidia-nemotron-nano-9b-v2-reasoning%2Ckimi-k2%2Cexaone-4-0-32b-reasoning%2Cglm-4-5-air%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomqhacker"&gt; /u/randomqhacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv2y08/nvidianemotronnano9bv2_better_than_gpt5_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T02:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv60jv</id>
    <title>A simple script to make two llms talk to each other. Currently getting gpt-oss to talk to gemma3</title>
    <updated>2025-08-20T05:20:28+00:00</updated>
    <author>
      <name>/u/simplan</name>
      <uri>https://old.reddit.com/user/simplan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;import urllib.request import json import random import time from collections import deque MODEL_1 = &amp;quot;gemma3:27b&amp;quot; MODEL_2 = &amp;quot;gpt-oss:20b&amp;quot; OLLAMA_API_URL = &amp;quot;http://localhost:11434/api/generate&amp;quot; INSTRUCTION = ( &amp;quot;You are in a conversation. &amp;quot; &amp;quot;Reply with ONE short sentence only, but mildly interesting.&amp;quot; &amp;quot;Do not use markdown, formatting, or explanations. &amp;quot; &amp;quot;Always keep the conversation moving forward.&amp;quot; ) def reframe_history(history, current_model): &amp;quot;&amp;quot;&amp;quot;Reframe canonical history into 'me:'/'you:' for model input.&amp;quot;&amp;quot;&amp;quot; reframed = [] for line in history: speaker, text = line.split(&amp;quot;:&amp;quot;, 1) if speaker == current_model: reframed.append(f&amp;quot;me:{text}&amp;quot;) else: reframed.append(f&amp;quot;you:{text}&amp;quot;) return reframed def ollama_generate(model, history): prompt = &amp;quot;\n&amp;quot;.join(reframe_history(history[-5:], model)) data = {&amp;quot;model&amp;quot;: model, &amp;quot;prompt&amp;quot;: prompt, &amp;quot;system&amp;quot;: INSTRUCTION, &amp;quot;stream&amp;quot;: False} req = urllib.request.Request( OLLAMA_API_URL, data=json.dumps(data).encode(&amp;quot;utf-8&amp;quot;), headers={&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;}, ) with urllib.request.urlopen(req) as response: resp_json = json.loads(response.read().decode(&amp;quot;utf-8&amp;quot;)) reply = resp_json.get(&amp;quot;response&amp;quot;, &amp;quot;&amp;quot;).strip() # Trim to first sentence only if &amp;quot;.&amp;quot; in reply: reply = reply.split(&amp;quot;.&amp;quot;)[0] + &amp;quot;.&amp;quot; return reply def main(): topics = [&amp;quot;Hi&amp;quot;] start_message = random.choice(topics) # canonical history with real model names history = deque([f&amp;quot;{MODEL_1}: {start_message}&amp;quot;], maxlen=20) print(&amp;quot;Starting topic:&amp;quot;) print(f&amp;quot;{MODEL_1}: {start_message}&amp;quot;) turn = 0 while True: if turn % 2 == 0: model = MODEL_2 else: model = MODEL_1 reply = ollama_generate(model, list(history)) line = f&amp;quot;{model}: {reply}&amp;quot; print(line) history.append(line) turn += 1 time.sleep(1) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplan"&gt; /u/simplan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv60jv/a_simple_script_to_make_two_llms_talk_to_each/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv60jv/a_simple_script_to_make_two_llms_talk_to_each/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv60jv/a_simple_script_to_make_two_llms_talk_to_each/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvaiy4</id>
    <title>What do you think about Artificial analysis intelligence index?</title>
    <updated>2025-08-20T09:59:05+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate using a particular benchmark because it often is bad, but i like to rely on theirs because it aggregates on many bemchmarks so I feel it's harder to benchmaxx on all fronts. Is it still shit? Is there a better way than vibes? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvaiy4/what_do_you_think_about_artificial_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvaiy4/what_do_you_think_about_artificial_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvaiy4/what_do_you_think_about_artificial_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T09:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv1nqp</id>
    <title>How does OpenRouter track the category people use LLM for?</title>
    <updated>2025-08-20T01:40:31+00:00</updated>
    <author>
      <name>/u/skyline159</name>
      <uri>https://old.reddit.com/user/skyline159</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"&gt; &lt;img alt="How does OpenRouter track the category people use LLM for?" src="https://preview.redd.it/2qi6k7dow2kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a023c09808d9bc5c0c1b2af98dabefce0b46d63e" title="How does OpenRouter track the category people use LLM for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does that mean they read the content of our requests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skyline159"&gt; /u/skyline159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2qi6k7dow2kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1nqp/how_does_openrouter_track_the_category_people_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T01:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1munvj6</id>
    <title>The new design in DeepSeek V3.1</title>
    <updated>2025-08-19T16:47:11+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just pulled the &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;V3.1-Base&lt;/a&gt; configs and compared to V3-Base&lt;br /&gt; They add four new special tokens&lt;br /&gt; &amp;lt;｜search▁begin｜&amp;gt; (id: 128796)&lt;br /&gt; &amp;lt;｜search▁end｜&amp;gt; (id: 128797)&lt;br /&gt; &amp;lt;think&amp;gt; (id: 128798)&lt;br /&gt; &amp;lt;/think&amp;gt; (id: 128799)&lt;br /&gt; And I noticed that V3.1 on the web version actively searches even when the search button is turned off, unless explicitly instructed &amp;quot;do not search&amp;quot; in the prompt.&lt;br /&gt; would this be related to the design of the special tokens mentioned above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6cjq</id>
    <title>NVIDIA-Nemotron-Nano-9B-v2 vs Qwen/Qwen3-Coder-30B</title>
    <updated>2025-08-20T05:39:39+00:00</updated>
    <author>
      <name>/u/Ok-Pattern9779</name>
      <uri>https://old.reddit.com/user/Ok-Pattern9779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing both NVIDIA-Nemotron-Nano-9B-v2 and Qwen3-Coder-30B in coding tasks (specifically Go and JavaScript), and here’s what I’ve noticed:&lt;/p&gt; &lt;p&gt;When the project codebase is provided as context, Nemotron-Nano-9B-v2 consistently outperforms Qwen3-Coder-30B. It seems to leverage the larger context better and gives more accurate completions/refactors.&lt;/p&gt; &lt;p&gt;When the project codebase is not given (e.g., one-shot prompts or isolated coding questions), Qwen3-Coder-30B produces better results. Nemotron struggles without detailed context.&lt;/p&gt; &lt;p&gt;Both models were tested running in FP8 precision.&lt;/p&gt; &lt;p&gt;So in short:&lt;/p&gt; &lt;p&gt;With full codebase → Nemotron wins&lt;/p&gt; &lt;p&gt;One-shot prompts → Qwen wins&lt;/p&gt; &lt;p&gt;Curious if anyone else has tried these side by side and seen similar results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pattern9779"&gt; /u/Ok-Pattern9779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6cjq/nvidianemotronnano9bv2_vs_qwenqwen3coder30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukwq6</id>
    <title>🤗 DeepSeek-V3.1-Base</title>
    <updated>2025-08-19T15:01:07+00:00</updated>
    <author>
      <name>/u/newsletternew</name>
      <uri>https://old.reddit.com/user/newsletternew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v3.1 base model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsletternew"&gt; /u/newsletternew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv71iz</id>
    <title>nvidia/canary-1b-v2</title>
    <updated>2025-08-20T06:21:59+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"&gt; &lt;img alt="nvidia/canary-1b-v2" src="https://external-preview.redd.it/iwfz--wu3rGmlqaXtgvbftUiGYG_9ACNfyhUIFyWWG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58860fc844d8319708093d930be8b459dd6edb43" title="nvidia/canary-1b-v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/canary-1b-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv71iz/nvidiacanary1bv2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv0ph0</id>
    <title>So if you want something as close as Claude to run locally do you have to spend $10k?</title>
    <updated>2025-08-20T00:56:58+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it have to be the M4 Max or one of those most expensive GPUs by NVidia and AMD? I am obsessed with the idea of locally hosted LLM that can act as my coding buddy and I keep updating it as it improves or new version comes like qwen3 coder. &lt;/p&gt; &lt;p&gt;But the initial setup is too much expensive that I think if it is worth it to spend that much amount of money when the technology is rapidly evolving and tomorrow or in a couple of months that 10 grand investment looks like dust. We're having more software evolution than hardware. Software is pretty much free but the hardware costs more than kidneys. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv0ph0/so_if_you_want_something_as_close_as_claude_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T00:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq72y</id>
    <title>Deepseek v3.1 scores 71.6% on aider – non-reasoning sota</title>
    <updated>2025-08-19T18:09:56+00:00</updated>
    <author>
      <name>/u/Similar-Cycle8413</name>
      <uri>https://old.reddit.com/user/Similar-Cycle8413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` - dirname: 2025-08-19-17-08-33--deepseek-v3.1 test_cases: 225 model: deepseek/deepseek-chat edit_format: diff commit_hash: 32faf82 pass_rate_1: 41.3 pass_rate_2: 71.6 pass_num_1: 93 pass_num_2: 161 percent_cases_well_formed: 95.6 error_outputs: 13 num_malformed_responses: 11 num_with_malformed_responses: 10 user_asks: 63 lazy_comments: 0 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 1 prompt_tokens: 2239930 completion_tokens: 551692 test_timeouts: 8 total_tests: 225 command: aider --model deepseek/deepseek-chat date: 2025-08-19 versions: 0.86.2.dev seconds_per_case: 134.0 total_cost: 1.0112&lt;/p&gt; &lt;p&gt;costs: $0.0045/test-case, $1.01 total, $1.01 projected ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Cycle8413"&gt; /u/Similar-Cycle8413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvbzvh</id>
    <title>Qwen 30B Instruct vs GPT-OSS 20B for real life coding</title>
    <updated>2025-08-20T11:20:38+00:00</updated>
    <author>
      <name>/u/Mobile_Ice1759</name>
      <uri>https://old.reddit.com/user/Mobile_Ice1759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Would like some opinions besides benchmarks for those 2 models (or maybe additional one) from people who use it for production applications. Web (PHP/JS), iOS (Swift). As Im GPU poor and have 1x3090 these are the best local options for me now.&lt;/p&gt; &lt;p&gt;Both models sucks with the whole codebases (qwen cli, aider), so I'm making some summaries which then I give to it along with some context.&lt;/p&gt; &lt;p&gt;Naturally GPT works a bit faster, but I encounter a problem where I have to switch models for different problems, like UI or back-end, even though they are not consistently better versus each other. I'm looking for anyone who can get me along the way with models parameters, workflow, etc with going on this setup.&lt;/p&gt; &lt;p&gt;Mostly all my problems are solved via paid services, but there are 2 projects now, where I can't/won't share data and trying to think of solution without spending half a budget on making a lab or purchasing cloud gpu.&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mobile_Ice1759"&gt; /u/Mobile_Ice1759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T11:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv01ls</id>
    <title>Daily driving GLM 4.5 for 10 days, kinda insane how good it is at half the size of other frontier models</title>
    <updated>2025-08-20T00:27:19+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been running GLM 4.5 (355B) locally for about 10 days, and it’s basically replaced my old setup. I used to juggle GPT-4o/4.1 for general tasks and o3 for heavier reasoning, but after GPT-5 struggled with long research paper convos I moved to GLM 4.5, and it covers both use cases in one.&lt;/p&gt; &lt;p&gt;Using Unsloth’s GGUF builds in llama.cpp: ud-iq2_m for 128k context (K/V cache q8) and ud-q2_k_xl when 30k okay. What’s impressive is how well it holds up under 2-bit quant + cache quant. I expected obvious degradation, but it’s solid. Non-thinking mode feels like GPT-4o in fluency, and thinking mode sits between o4-mini-high and o3—better than most open weights I’ve tried at this scale.&lt;/p&gt; &lt;p&gt;Benchmarks aside, real usage matches the claims. I’ve thrown workloads at it I’d normally reserve for GPT-4.1 or o3, and it keeps up or beats them, without the usual quantization artifacts. Knowledge depth and overall competence are ahead of GPT-4.1/4o in many areas, and it’s one of the most stock, uncensored releases I’ve seen.&lt;/p&gt; &lt;p&gt;Running on RTX 6000 Ada + 6000 Pro Blackwell with a llama.cpp fork for GLM 4.5 tool support. Frontend is Open WebUI with some filter functions to toggle thinking on/off, plus a fixed chat template so /nothink only applies to the last message (avoids re-prefill issues).&lt;/p&gt; &lt;p&gt;First time I’ve daily-driven a local model this large and felt it genuinely outclass closed ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv4kwc</id>
    <title>gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks</title>
    <updated>2025-08-20T04:02:02+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious results. &lt;a href="https://arxiv.org/pdf/2508.12461"&gt;https://arxiv.org/pdf/2508.12461&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The gpt-oss-120 was interesting, but I am beyond perplexed how they decided to compare it against other much larger models like some sort of apples to apples comparison. Like, fr -DeepSeek-R1!? 70B Dense? Even Scout at 17B active is much bigger. I mean, wth:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;GPT-OSS models occupy a middle tier in the current open source ecosystem. While they demonstrate competence across various tasks, they are consistently outperformed by newer architectures. Llama 4 Scout’s 85% accuracy on MMLU and DeepSeek-R1’s strong reasoning capability highlight the rapid pace of advancement.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4kwc/gptoss20b_consistently_outperforms_gptoss120b_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T04:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv4et3</id>
    <title>DeepSeek V3.1 BASE Q4_K_M available</title>
    <updated>2025-08-20T03:53:13+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm making imatrix calculations from Q4_K_M so figured might as well upload it in the meantime for anyone who wants to use it&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF"&gt;https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As noted in the model card, it's good to keep in mind this is a &lt;em&gt;BASE&lt;/em&gt; model&lt;/p&gt; &lt;p&gt;Typically to use base models for general conversation, you want to feed it a couple of turns to teach if what a conversation looks like&lt;/p&gt; &lt;p&gt;I simply gave it a system message and a couple turns of each user/assistant and it seemed capable:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./llama-cli -m /models/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-00001-of-00011.gguf -p &amp;quot;You are a helpful assistant.&amp;lt;User&amp;gt;Hello, who are you?&amp;lt;Assistant&amp;gt;I am DeepSeek, a helpful AI assistant.&amp;lt;User&amp;gt;How are you today?&amp;lt;Assistant&amp;gt;I'm doing well! Is there anything I can assist you with?&amp;lt;User&amp;gt;Can you explain the laws of thermodynamics?&amp;lt;Assistant&amp;gt;&amp;quot; -no-cnv -ngl 0 --reverse-prompt &amp;quot;&amp;lt;User&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;Sure, here's a brief explanation of the laws of thermodynamics: 1. Zeroth Law of Thermodynamics: If two thermodynamic systems are each in thermal equilibrium with a third system, then they are in thermal equilibrium with each other. 2. First Law of Thermodynamics: The total energy of an isolated system is constant; energy can be transformed from one form to another, but cannot be created or destroyed. 3. Second Law of Thermodynamics: The entropy of an isolated system not in equilibrium will tend to increase over time, approaching a maximum value at equilibrium. 4. Third Law of Thermodynamics: As the temperature of a system approaches absolute zero, the entropy of the system approaches a minimum value. Would you like more details on any of these laws?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yes, I am using &lt;code&gt;&amp;lt;User&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;Assistant&amp;gt;&lt;/code&gt; as opposed to the special tokens &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;, for some reason this seems to be more stable? Using the proper tokens tended to result in endless incoherent generation.&lt;/p&gt; &lt;p&gt;Can't comment on quality in any way, but figured someone would want to play early! Feel free to share multi-turn prompts that give good results, mine are likely far from ideal but at least they seem to work :)&lt;/p&gt; &lt;p&gt;384GB btw...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7zdl</id>
    <title>Deepseek V3.1 is bad at creative writing, way worse than 0324</title>
    <updated>2025-08-20T07:19:09+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've tried 3.1 on chat.deepseek.com, and boy it is very very bad at conversation and creative writing; it does not understand prompt nuances V3 0324 does, it has very high slop cliche output, and generally feels like switch from Mistral Small 2409 to 2501. &lt;/p&gt; &lt;p&gt;Let me know your impression.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T07:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv41oq</id>
    <title>Editing iconic photographs with editing model</title>
    <updated>2025-08-20T03:34:44+00:00</updated>
    <author>
      <name>/u/ThunderBR2</name>
      <uri>https://old.reddit.com/user/ThunderBR2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"&gt; &lt;img alt="Editing iconic photographs with editing model" src="https://b.thumbs.redditmedia.com/UVgmNC7ap44VT4P5wG4pwKhnnLO0hcrS_MgvAlRFePM.jpg" title="Editing iconic photographs with editing model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThunderBR2"&gt; /u/ThunderBR2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv41oq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv41oq/editing_iconic_photographs_with_editing_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6wwe</id>
    <title>nvidia/parakeet-tdt-0.6b-v3 (now multilingual)</title>
    <updated>2025-08-20T06:14:15+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt; &lt;img alt="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" src="https://external-preview.redd.it/12PzLvQjZXrvyzotsfsH7vxtU3vJRsRc5ZD3WiNviO0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a6ac17d880f0cd5d00920d58cc8f2aa4530205" title="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;parakeet-tdt-0.6b-v3 is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription. It extends the parakeet-tdt-0.6b-v2 model by expanding language support from English to 25 European languages. The model automatically detects the language of the audio and transcribes it without requiring additional prompting. It is part of a series of models that leverage the Granary [1, 2] multilingual corpus as their primary training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1muxbqj</id>
    <title>Understanding DeepSeek-V3.1-Base Updates at a Glance</title>
    <updated>2025-08-19T22:32:24+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt; &lt;img alt="Understanding DeepSeek-V3.1-Base Updates at a Glance" src="https://preview.redd.it/mqcnus8py1kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=489f12b61d95cf93fb3d2849e85440310d44f38d" title="Understanding DeepSeek-V3.1-Base Updates at a Glance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek officially released DeepSeek-V3.1-Base a few hours ago. The model card has not been uploaded yet, so performance data is not available. &lt;/p&gt; &lt;p&gt;I have directly reviewed the model's configuration files, tokenizer, and other data, and combined this with test data published by the community to create a summary for everyone. &lt;/p&gt; &lt;p&gt;This should give you a quick overview of what has been updated in DeepSeek-V3.1-Base. Please point out any errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqcnus8py1kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T22:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv1c96</id>
    <title>PSA: before spending 5k€ on GPUs, you might want to test the models online first</title>
    <updated>2025-08-20T01:25:56+00:00</updated>
    <author>
      <name>/u/e79683074</name>
      <uri>https://old.reddit.com/user/e79683074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can do so on &lt;a href="https://lmarena.ai/?mode=direct"&gt;https://lmarena.ai/?mode=direct&lt;/a&gt; or any other place you know. Local models have come a huge, long way since the first Llama appearances, and the amount of progress done is unbelievable.&lt;/p&gt; &lt;p&gt;However, don't expect to be able to unsub from Gemini\ChatGPT\Claude soon. Test them first, before you spend money on hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/e79683074"&gt; /u/e79683074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T01:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base · Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7kk2</id>
    <title>Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528</title>
    <updated>2025-08-20T06:54:36+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt; &lt;img alt="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" src="https://b.thumbs.redditmedia.com/H3XSTIYwTWYE6zf6TJkmBkaHUW2BDiBZVxdTdMDbz8s.jpg" title="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://github.com/cpldcpu/LRMTokenEconomy"&gt;here &lt;/a&gt;for more background information on the evaluation.&lt;/p&gt; &lt;p&gt;It appears they significantly reduced overthinking for prompts that can can be answered from model knowledge and math problems. There are still some cases where it creates very long CoT though for logic puzzles.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv7kk2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3hcr</id>
    <title>GPT 4.5 vs DeepSeek V3.1</title>
    <updated>2025-08-20T03:06:43+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt; &lt;img alt="GPT 4.5 vs DeepSeek V3.1" src="https://preview.redd.it/5c3gbyx3c3kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcee32647c7f5d8e6d02cf6e6eb7d06bb63cacab" title="GPT 4.5 vs DeepSeek V3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5c3gbyx3c3kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
