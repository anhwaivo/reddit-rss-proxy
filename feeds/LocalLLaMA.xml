<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-26T07:55:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kvnu5k</id>
    <title>Gemma-3-27b quants?</title>
    <updated>2025-05-26T06:49:12+00:00</updated>
    <author>
      <name>/u/MAXFlRE</name>
      <uri>https://old.reddit.com/user/MAXFlRE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm running Gemma-3-27b Q6_K_L with 45/67 offload to GPU(3090) at about 5 t/s. It is borderline useful at this speed. I wonder would Q4_QAT quant be the like the same evaluation performance (model quality) just faster. Or maybe I should aim for Q8 (I could afford second 3090 so I might have a better speed and longer context with higher quant) but wondering if one could really notice the difference (except speed). What upgrade/sidegrade vector do you think would be preferable? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MAXFlRE"&gt; /u/MAXFlRE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnu5k/gemma327b_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnu5k/gemma327b_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnu5k/gemma327b_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvegc1</id>
    <title>Used or New Gamble</title>
    <updated>2025-05-25T22:08:06+00:00</updated>
    <author>
      <name>/u/thehoffau</name>
      <uri>https://old.reddit.com/user/thehoffau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aussie madlad here. &lt;/p&gt; &lt;p&gt;The second hand market in AU is pretty small, there are the odd 3090s running around but due to distance they are always a risk in being a) a scam b) damaged in freight c) broken at time of sale.&lt;/p&gt; &lt;p&gt;The 7900xtx new and a 3090 used are about the same price. Reading this group for months the XTX seems to get the job done for most things (give or take 10% and feature delay?) &lt;/p&gt; &lt;p&gt;I have a threadripper system that's CPU/ram can do LLMs okay and I can easily slot in two GPU which is the medium term plan. I was initially looking at 2 X A4000(16gb) but am now looking at long term either 2x3090 or 2xXTX&lt;/p&gt; &lt;p&gt;It's a pretty sizable investment to loose out on and I'm stuck in a loop. Risk second hand for NVIDIA or safe for AMD?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thehoffau"&gt; /u/thehoffau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvegc1/used_or_new_gamble/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvcq04</id>
    <title>Qwen2.5-VL and Gemma 3 settings for OCR</title>
    <updated>2025-05-25T20:50:19+00:00</updated>
    <author>
      <name>/u/dzdn1</name>
      <uri>https://old.reddit.com/user/dzdn1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working with using VLMs to OCR handwriting (think journals, travel logs). I get much better results than traditional OCR, which pretty much fails completely even with tools meant to do better with handwriting. &lt;/p&gt; &lt;p&gt;However, results are inconsistent, and changing parameters like temp, repeat-penalty and others affect the results, but in unpredictable ways (to a newb like myself).&lt;/p&gt; &lt;p&gt;Gemma 3 (12B) with default settings just makes a whole new narrative seemingly loosely inspired by the text on the page. I have not found settings to improve this. &lt;/p&gt; &lt;p&gt;Qwen2.5-VL (7B) does much better, getting even words I can barely read, but requires a detailed and kind of randomly pieced together prompt and system prompt, and changing it in minor ways can break it, making it skip sections, lose accuracy on some letters, etc. which I think makes it unreliable for long-term use.&lt;/p&gt; &lt;p&gt;Additionally, llama.cpp I believe shrinks the image to 1024 max for Qwen (because much larger quickly floods RAM). I am working on trying to use more sophisticated downscaling and sharpening edges, etc. but this does not seem to be improving the results.&lt;/p&gt; &lt;p&gt;Has anyone gotten these or other models to work well with freeform handwriting and if so, do you have any advice for settings to use?&lt;/p&gt; &lt;p&gt;I have seen how these new VLMs can finally help with handwriting in a way previously unimagined, but I am having trouble getting out to the &amp;quot;next step.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dzdn1"&gt; /u/dzdn1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvcq04/qwen25vl_and_gemma_3_settings_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv762l</id>
    <title>RTX PRO 6000 96GB plus Intel Battlemage 48GB feasible?</title>
    <updated>2025-05-25T16:51:41+00:00</updated>
    <author>
      <name>/u/SteveRD1</name>
      <uri>https://old.reddit.com/user/SteveRD1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OK, this may be crazy but I wanted to run it by you all.&lt;/p&gt; &lt;p&gt;Can you combine a RTX PRO 6000 96GB (with all the Nvidia CUDA goodies) with a (relatively) cheap Intel 48GB GPUs for extra VRAM?&lt;/p&gt; &lt;p&gt;So you have 144GB VRAM available, but you have all the capabilities of Nvidia on your main card driving the LLM inferencing?&lt;/p&gt; &lt;p&gt;This idea sounds too good to be true....what am I missing here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SteveRD1"&gt; /u/SteveRD1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv762l/rtx_pro_6000_96gb_plus_intel_battlemage_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnta6</id>
    <title>What would be the best LLM to have for analyzing PDFs?</title>
    <updated>2025-05-26T06:47:36+00:00</updated>
    <author>
      <name>/u/newbreed69</name>
      <uri>https://old.reddit.com/user/newbreed69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bassically, i want to dump a few hundreds of pages of PDFs into an LLM, and get the LLM to refer back to them when i have a question&lt;/p&gt; &lt;p&gt;Or would a paid LLM be better? if so, what one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newbreed69"&gt; /u/newbreed69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnta6/what_would_be_the_best_llm_to_have_for_analyzing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnta6/what_would_be_the_best_llm_to_have_for_analyzing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnta6/what_would_be_the_best_llm_to_have_for_analyzing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuy45r</id>
    <title>Gemma 3n Architectural Innovations - Speculation and poking around in the model.</title>
    <updated>2025-05-25T08:59:16+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt; &lt;img alt="Gemma 3n Architectural Innovations - Speculation and poking around in the model." src="https://external-preview.redd.it/EZj1oQJN95Oq7YDpbSs0ORxFqLi-24Ocse4J7I4sO0A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1052f0a2169e9b937ad3af8d86cab69bb6b8b09b" title="Gemma 3n Architectural Innovations - Speculation and poking around in the model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it-litert-preview"&gt;Gemma 3n&lt;/a&gt; is a new member of the Gemma family with free weights that was released during Google I/O. It's dedicated to on-device (edge) inference and supports image and text input, with audio input. Google has released an app that can be used for inference on the phone.&lt;/p&gt; &lt;p&gt;What is clear from&lt;a href="https://ai.google.dev/gemma/docs/gemma-3n"&gt; the documentation&lt;/a&gt;, is that this model is stuffed to the brim with architectural innovations: Per-Layer Embedding (PLE), MatFormer Architecture, Conditional Parameter Loading.&lt;/p&gt; &lt;p&gt;Unfortunately, there is no paper out for the model yet. I assume that this will follow at some point, but so far I had some success poking around in the model file. I thought I'd share my findings so far, maybe someone else has more insights?&lt;/p&gt; &lt;p&gt;The provided .task file is actually a ZIP container of tflite models. It can be unpacked with ZIP.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PREFILL_DECODE&lt;/td&gt; &lt;td align="left"&gt;2.55 GB&lt;/td&gt; &lt;td align="left"&gt;Main language model component for text generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_PER_LAYER_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;1.23 GB&lt;/td&gt; &lt;td align="left"&gt;Per-layer embeddings from the transformer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_EMBEDDER&lt;/td&gt; &lt;td align="left"&gt;259 MB&lt;/td&gt; &lt;td align="left"&gt;Input embeddings&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ENCODER&lt;/td&gt; &lt;td align="left"&gt;146 MB&lt;/td&gt; &lt;td align="left"&gt;Vision Encoding&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TF_LITE_VISION_ADAPTER&lt;/td&gt; &lt;td align="left"&gt;17 MB&lt;/td&gt; &lt;td align="left"&gt;Adapts vision embeddings for the language model?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TOKENIZER_MODEL&lt;/td&gt; &lt;td align="left"&gt;4.5 MB&lt;/td&gt; &lt;td align="left"&gt;Tokenizer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;METADATA&lt;/td&gt; &lt;td align="left"&gt;56 bytes&lt;/td&gt; &lt;td align="left"&gt;general metadata&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The TFlite models can be opened in a network visualizer like &lt;a href="http://netron.app"&gt;netron.app&lt;/a&gt; to display the content.&lt;/p&gt; &lt;p&gt;The model uses an inner dimension of 2048 and has 35 transformer blocks. Tokenizer size is 262144.&lt;/p&gt; &lt;p&gt;First, one interesting find it that is uses learned residual connections. This paper seems to be related to this: &lt;a href="https://arxiv.org/abs/2411.07501v3"&gt;https://arxiv.org/abs/2411.07501v3&lt;/a&gt; (LAuReL: Learned Augmented Residual Layer)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0"&gt;https://preview.redd.it/tvl3od2v3w2f1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cee0588c6b82700ab02e3eadd02a13d7c9b7af0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The FFN is projecting from 2048 to 16384 with a GeGLU activation. This is an unusually wide ratio. I assume that some part of these parameters can be selectively turned on and off to implement the Matformer architecture. It is not clear how this is implemented in the compute graph though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036"&gt;https://preview.redd.it/foq74ff15w2f1.png?width=605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdf24aec8aaa93efe3c968fd93b62b1439af0036&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A very interesting part is the per-layer embedding. The file TF_LITE_PER_LAYER_EMBEDDER contains very large lookup tables (262144x256x35) that will output a 256 embedding for every layer depending on the input token. Since this is essentially a lookup table, it can be efficiently processed even on the CPU. This is an extremely interesting approach to adding more capacity to the model without increasing FLOPS.&lt;/p&gt; &lt;p&gt;The embeddings are applied in an operation that follows the FFN and are used as a gate to a low rank projection. The residual stream is downprojected to 256, multiplied with the embedding and then projected up to 2048 again. It's a bit like a token-selective LoRA. In addition there is a gating operation that controls the overall weighting of this stream.&lt;/p&gt; &lt;p&gt;I am very curious for further information. I was not able to find any paper on this aspect of the model. Hopefully, google will share more information.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab"&gt;https://preview.redd.it/lchxfc6w6w2f1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a612976c324a28f34dee305b2c64f8b911a2cab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f"&gt;https://preview.redd.it/wca7kzfq5w2f1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3fd2195e4829bf47c8f6d0e2d6fef2c133e1d2f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuy45r/gemma_3n_architectural_innovations_speculation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T08:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvceya</id>
    <title>I need a text only browser python library</title>
    <updated>2025-05-25T20:36:15+00:00</updated>
    <author>
      <name>/u/Somerandomguy10111</name>
      <uri>https://old.reddit.com/user/Somerandomguy10111</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"&gt; &lt;img alt="I need a text only browser python library" src="https://preview.redd.it/bd5haso3oz2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e16001d2ec9ed0b0439505bab505eaf6fdd9fed1" title="I need a text only browser python library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an open source AI agent framework with search and eventually web interaction capabilities. To do that I need a browser. While it could be conceivable to just forward a screenshot of the browser it would be much more efficient to introduce the page into the context as text.&lt;/p&gt; &lt;p&gt;Ideally I'd have something like lynx which you see in the screenshot, but as a python library. Like Lynx above it should conserve the layout, formatting and links of the text as good as possible. Just to cross a few things off:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lynx: While it looks pretty much ideal, it's a terminal utility. It'll be pretty difficult to integrate with Python.&lt;/li&gt; &lt;li&gt;HTML get requests: It works for some things but some websites require a Browser to even load the page. Also it doesn't look great&lt;/li&gt; &lt;li&gt;Screenshot the browser: As discussed above, it's possible. But not very efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Have you faced this problem? If yes, how have you solved it? I've come up with a selenium driven Browser Emulator but it's pretty rough around the edges and I don't really have time to go into depth on that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Somerandomguy10111"&gt; /u/Somerandomguy10111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bd5haso3oz2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvceya/i_need_a_text_only_browser_python_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvj34f</id>
    <title>Jetson Orin AGX 32gb</title>
    <updated>2025-05-26T02:07:58+00:00</updated>
    <author>
      <name>/u/randylush</name>
      <uri>https://old.reddit.com/user/randylush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can’t get this dumb thing to use the GPU with Ollama. As far as I can tell not many people are using it, and the mainline of llama.cpp is often broken, and some guy has a fork for the Jetson devices. I can get the whole ollama stack running but it’s dog slow and nothing shows up on Nvidia-smi. I’m trying Qwen3-30b-a3b. That seems to run just great on my 3090. Would I ever expect the Jetson to match its performance?&lt;/p&gt; &lt;p&gt;The software stack is also hot garbage, it seems like you can only install nvidia’s OS using their SDK manager. There is no way I’d ever recommend this to anyone. This hardware could have so much potential but Nvidia couldn’t be bothered to give it an understandable name let alone a sensible software stack. &lt;/p&gt; &lt;p&gt;Anyway, is anyone having success with this for basic LLM work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randylush"&gt; /u/randylush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj34f/jetson_orin_agx_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj34f/jetson_orin_agx_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj34f/jetson_orin_agx_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T02:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv2gb2</id>
    <title>Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops</title>
    <updated>2025-05-25T13:23:47+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt; &lt;img alt="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" src="https://external-preview.redd.it/GusyhEpTmXh7oXULalG-maSvDCVfQxTdBP1AMHOUr_A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b7731e91f8b244e977d425e9ede836b7d78861c" title="Qualcomm discrete NPU (Qualcomm AI 100) in upcoming Dell workstation laptops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://uk.pcmag.com/laptops/158095/dell-ditches-the-gpu-for-an-ai-chip-in-this-bold-new-workstation-laptop"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv2gb2/qualcomm_discrete_npu_qualcomm_ai_100_in_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T13:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnf46</id>
    <title>QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</title>
    <updated>2025-05-26T06:22:26+00:00</updated>
    <author>
      <name>/u/Fancy_Fanqi77</name>
      <uri>https://old.reddit.com/user/Fancy_Fanqi77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt; &lt;img alt="QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning" src="https://external-preview.redd.it/4fgEuTMOx_oXXqA0kQVQE7o892NJc01radrTqR_KFkw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=084020eb1aeecf203d2fdf8aa8277a361260d5b2" title="QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pekhw95jl23f1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ce794b4e65f4ab48d59fe760345c3dce91d329b"&gt;https://preview.redd.it/pekhw95jl23f1.png?width=1480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ce794b4e65f4ab48d59fe760345c3dce91d329b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B"&gt;🤗 QwenLong-L1-32B&lt;/a&gt; is the first long-context Large Reasoning Model (LRM) trained with reinforcement learning for long-context document reasoning tasks. Experiments on seven long-context DocQA benchmarks demonstrate that &lt;strong&gt;QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking&lt;/strong&gt;, demonstrating leading performance among state-of-the-art LRMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Fanqi77"&gt; /u/Fancy_Fanqi77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnf46/qwenlongl1_towards_longcontext_large_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvc9ri</id>
    <title>I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more</title>
    <updated>2025-05-25T20:30:08+00:00</updated>
    <author>
      <name>/u/erdaltoprak</name>
      <uri>https://old.reddit.com/user/erdaltoprak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"&gt; &lt;img alt="I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more" src="https://external-preview.redd.it/NmVqMXp0bWRqejJmMd5vTxA1ciqILnZ5a_nUE62vdQvPjJEI_nmjQVTxndSt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4640fca024f61ffbd3fba74333acfc27e740ea7d" title="I wrote an automated setup script for my Proxmox AI VM that installs Nvidia CUDA Toolkit, Docker, Python, Node, Zsh and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a script (&lt;a href="https://gist.github.com/erdaltoprak/cdc1ec4056b81a9da540229dcde3aa0b"&gt;available on Github here&lt;/a&gt;) that automates the setup of a fresh Ubuntu 24.04 server for AI/ML development work. It handles the complete installation and configuration of Docker, ZSH, Python (via pyenv), Node (via n), NVIDIA drivers and the NVIDIA Container Toolkit, basically everything you need to get a GPU accelerated development environment up and running quickly&lt;/p&gt; &lt;p&gt;This script reflects my personal setup preferences and hardware, so if you want to customize it for your own needs, I highly recommend reading through the script and understanding what it does before running it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erdaltoprak"&gt; /u/erdaltoprak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e006utmdjz2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9ri/i_wrote_an_automated_setup_script_for_my_proxmox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnt5u</id>
    <title>Best Uncensored model for 42GB of VRAM</title>
    <updated>2025-05-26T06:47:23+00:00</updated>
    <author>
      <name>/u/KeinNiemand</name>
      <uri>https://old.reddit.com/user/KeinNiemand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current best uncensored model for &amp;quot;Roleplay&amp;quot;.&lt;br /&gt; Well Not really roleplay in the sense that I'm roleplaying with an AI character with a character card and all that. Usually I'm more doing like some sort of choose your own adventure or text adventure thing where I give the AI some basic prompt about the world, let it generate and then I tell it what I want my character to do, there's some roleplay involved but it's not the typical me downloading or making a character card and then roleplaying with a singular AI character.&lt;br /&gt; I care more about how well the AI (in terms of creativity) does with short, relatively basic prompts then how well it performs when all my prompts are long, elaborate and well written. &lt;/p&gt; &lt;p&gt;I've got 42GB of VRAM (1 5090 + 1 3080 10GB), so it should probably a 70B model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KeinNiemand"&gt; /u/KeinNiemand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnt5u/best_uncensored_model_for_42gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuwrll</id>
    <title>👀 BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You’ve Been Waiting For.</title>
    <updated>2025-05-25T07:24:39+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt; &lt;img alt="👀 BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You’ve Been Waiting For." src="https://b.thumbs.redditmedia.com/0FqyiVWdrxZTgD89ug8SMDskK09zDRwfDx-Rn8gc8zk.jpg" title="👀 BAGEL-7B-MoT: The Open-Source GPT-Image-1 Alternative You’ve Been Waiting For." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e"&gt;https://preview.redd.it/sw3eao9cqv2f1.jpg?width=3000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c753fae3901f5a15249aa73803dbfbed0b8f77e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ByteDance has unveiled &lt;strong&gt;BAGEL-7B-MoT&lt;/strong&gt;, an open-source multimodal AI model that rivals OpenAI's proprietary &lt;strong&gt;GPT-Image-1&lt;/strong&gt; in capabilities. With 7 billion active parameters (14 billion total) and a Mixture-of-Transformer-Experts (MoT) architecture, BAGEL offers advanced functionalities in text-to-image generation, image editing, and visual understanding—all within a single, unified model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Capabilities:&lt;/strong&gt; BAGEL seamlessly integrates text, image, and video processing, eliminating the need for multiple specialized models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Image Editing:&lt;/strong&gt; Supports free-form editing, style transfer, scene reconstruction, and multiview synthesis, often producing more accurate and contextually relevant results than other open-source models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emergent Abilities:&lt;/strong&gt; Demonstrates capabilities such as chain-of-thought reasoning and world navigation, enhancing its utility in complex tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Performance:&lt;/strong&gt; Outperforms models like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards and delivers text-to-image quality competitive with specialist generators like SD3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Comparison with GPT-Image-1:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;BAGEL-7B-MoT&lt;/th&gt; &lt;th align="left"&gt;GPT-Image-1&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source (Apache 2.0)&lt;/td&gt; &lt;td align="left"&gt;Proprietary (requires OpenAI API key)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multimodal Capabilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Text-to-image, image editing, visual understanding&lt;/td&gt; &lt;td align="left"&gt;Primarily text-to-image generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mixture-of-Transformer-Experts&lt;/td&gt; &lt;td align="left"&gt;Diffusion-based model&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Self-hostable on local hardware&lt;/td&gt; &lt;td align="left"&gt;Cloud-based via OpenAI API&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Emergent Abilities&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free-form image editing, multiview synthesis, world navigation&lt;/td&gt; &lt;td align="left"&gt;Limited to text-to-image generation and editing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Installation and Usage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Developers can access the model weights and implementation on Hugging Face. For detailed installation instructions and usage examples, the GitHub repository is available.&lt;/p&gt; &lt;p&gt;BAGEL-7B-MoT represents a significant advancement in multimodal AI, offering a versatile and efficient solution for developers working with diverse media types. Its open-source nature and comprehensive capabilities make it a valuable tool for those seeking an alternative to proprietary models like GPT-Image-1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuwrll/bagel7bmot_the_opensource_gptimage1_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T07:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvn51x</id>
    <title>Vector Space - Llama running locally on Apple Neural Engine</title>
    <updated>2025-05-26T06:04:10+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt; &lt;img alt="Vector Space - Llama running locally on Apple Neural Engine" src="https://external-preview.redd.it/chjzdPRsgKclskhkJtHyR9G4ascbrd4AkBc1D_gB_VY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3aac9a23637bf6c47f86301fe243a6a9117af54b" title="Vector Space - Llama running locally on Apple Neural Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6yhsn8x0g23f1.png?width=1368&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=42f7f189fdda7cabc2dd3055a55917468a9beba9"&gt;Llama 3.2 1B Full Precision (float16) running on iPhone 14 Pro Max&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Core ML is Apple’s official way to run Machine Learning models on device, and also appears to be the only way to engage the Neural Engine, which is a powerful NPU installed on every iPhone/iPad that is capable of performing tens of billions of computations per second.&lt;/p&gt; &lt;p&gt;In recent years, Apple has improved support for Large Language Models (and other transformer-based models) to run on device by introducing Stateful models, quantizations, etc. Despite these improvements, developers still face hurdles and a steep learning curve if they try to incorporate a large language model on-device. This leads to an (often paid) network API call for even the most basic AI-functions. For this reason, an Agentic AI often has to charge tens of dollars per month while still limiting usage for the user.&lt;/p&gt; &lt;p&gt;I have founded the Vector Space project to conquer the above issues. My Goal is two folds:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enable users to use AI (marginally) freely and smoothly&lt;/li&gt; &lt;li&gt;Enable small developers o build agentic apps without cost, without having to understand how AI works under the hood, and without having to worry about API key safety.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kvn51x/video/kagsls50h23f1/player"&gt;Llama 3.2 1B Full Precision (float16) on the Vector Space App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To achieve the above goals, Vector Space will provide&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Architecture and tools that can convert models to Core ML format that can be run on Apple Neural Engine.&lt;/li&gt; &lt;li&gt;Swift Package that can run performant model inference.&lt;/li&gt; &lt;li&gt;App for users to directly download and manage model on Device, and for developers and enthusiasts to try out different models directly on iPhone. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My goal is NOT to:&lt;/p&gt; &lt;p&gt;Completely replace server-based AI, where models with hundreds of billions of parameters can be hosted, with context length of hundreds of k. Online models will still excel at complex tasks. However, it is also important to note that not every user is asking AI to do programing and math challenges. &lt;/p&gt; &lt;p&gt;Current Progress:&lt;/p&gt; &lt;p&gt;I have already preliminarily supported Llama 3.2 1B in full precision. The Model runs on ANE and supports MLState.&lt;/p&gt; &lt;p&gt;I am pleased to release the TestFlight Beta of the App mentioned in goal #3 above so you can try it out directly on your iPhone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you decide to try out the TestFlight version, please note the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We do NOT collect any information about your chat messages. It remains completely on device and/or in your iCloud.&lt;/li&gt; &lt;li&gt;The first model load into memory (after downloading) will take about 1-2 minutes. Subsequent load will only take a couple seconds.&lt;/li&gt; &lt;li&gt;Chat history would not persist across app launches.&lt;/li&gt; &lt;li&gt;I cannot guarantee the downloaded app will continue work when I release the next update. You might need to delete and redownload the app when an update is released in the future.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Next Step:&lt;/p&gt; &lt;p&gt;I will be working on a quantized version of Llama 3.2 1B that is expected to have significant inference speed improvement. I will then provide a much wider selection of models available for download.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvn51x/vector_space_llama_running_locally_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvjwiz</id>
    <title>Implemented a quick and dirty iOS app for the new Gemma3n models</title>
    <updated>2025-05-26T02:54:28+00:00</updated>
    <author>
      <name>/u/sid9102</name>
      <uri>https://old.reddit.com/user/sid9102</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sid9102"&gt; /u/sid9102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sid9102/gemma3n-ios"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvjwiz/implemented_a_quick_and_dirty_ios_app_for_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvjwiz/implemented_a_quick_and_dirty_ios_app_for_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T02:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvoobg</id>
    <title>If only its true...</title>
    <updated>2025-05-26T07:44:42+00:00</updated>
    <author>
      <name>/u/Famous-Associate-436</name>
      <uri>https://old.reddit.com/user/Famous-Associate-436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/YouJiacheng/status/1926885863952159102"&gt;https://x.com/YouJiacheng/status/1926885863952159102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek-v3-0526, some guy saw this on changelog&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Associate-436"&gt; /u/Famous-Associate-436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvoobg/if_only_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T07:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvd0jr</id>
    <title>M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores)</title>
    <updated>2025-05-25T21:03:03+00:00</updated>
    <author>
      <name>/u/procraftermc</name>
      <uri>https://old.reddit.com/user/procraftermc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently got the M3 Ultra Mac Studio (96 GB RAM, 60 core GPU). Here's its performance.&lt;/p&gt; &lt;p&gt;I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarking Results&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name &amp;amp; Size&lt;/th&gt; &lt;th align="left"&gt;Time to First Token (s)&lt;/th&gt; &lt;th align="left"&gt;Tokens / Second&lt;/th&gt; &lt;th align="left"&gt;Input Context Size (tokens)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 0.6b (bf16)&lt;/td&gt; &lt;td align="left"&gt;18.21&lt;/td&gt; &lt;td align="left"&gt;78.61&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30b-a3b (8-bit)&lt;/td&gt; &lt;td align="left"&gt;67.74&lt;/td&gt; &lt;td align="left"&gt;34.62&lt;/td&gt; &lt;td align="left"&gt;40240&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 3 27B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;108.15&lt;/td&gt; &lt;td align="left"&gt;29.55&lt;/td&gt; &lt;td align="left"&gt;30869&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LLaMA4 Scout 17B-16E (4-bit)&lt;/td&gt; &lt;td align="left"&gt;111.33&lt;/td&gt; &lt;td align="left"&gt;33.85&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Large 123B (4-bit)&lt;/td&gt; &lt;td align="left"&gt;900.61&lt;/td&gt; &lt;td align="left"&gt;7.75&lt;/td&gt; &lt;td align="left"&gt;32705&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Additional Information&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Input was 30,000 - 40,000 tokens of Lorem Ipsum text&lt;/li&gt; &lt;li&gt;Model was reloaded with no prior caching&lt;/li&gt; &lt;li&gt;After caching, prompt processing (time to first token) dropped to almost zero&lt;/li&gt; &lt;li&gt;Prompt processing times on input &amp;lt;10,000 tokens was also workably low&lt;/li&gt; &lt;li&gt;Interface used was LM Studio&lt;/li&gt; &lt;li&gt;All models were 4-bit &amp;amp; MLX except Qwen3 0.6b and Qwen3 30b-a3b (they were bf16 and 8bit, respectively)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Token speeds were generally good, especially for MoE's like Qen 30b and Llama4. Of course, time-to-first-token was quite high as expected.&lt;/p&gt; &lt;p&gt;Loading models was way more efficient than I thought, I could load Mistral Large (4-bit) with 32k context using only ~70GB VRAM.&lt;/p&gt; &lt;p&gt;Feel free to request benchmarks for any model, I'll see if I can download and benchmark it :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/procraftermc"&gt; /u/procraftermc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T21:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuzk3t</id>
    <title>Online inference is a privacy nightmare</title>
    <updated>2025-05-25T10:39:04+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand how big tech just convinced people to hand over so much stuff to be processed in plain text. Cloud storage at least can be all encrypted. But people have got comfortable sending emails, drafts, their deepest secrets, all in the open on some servers somewhere. Am I crazy? People were worried about posts and likes on social media for privacy but this is magnitudes larger in scope. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kuzk3t/online_inference_is_a_privacy_nightmare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T10:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvmrgu</id>
    <title>nvidia/AceReason-Nemotron-7B · Hugging Face</title>
    <updated>2025-05-26T05:40:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"&gt; &lt;img alt="nvidia/AceReason-Nemotron-7B · Hugging Face" src="https://external-preview.redd.it/E-gtAbU28GDV0NLa926rL1ecWFn9v0jJKe5iqmUNFzo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52002294beca04a31b018ffdca2c01eba72b139a" title="nvidia/AceReason-Nemotron-7B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvmrgu/nvidiaacereasonnemotron7b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T05:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv6jjk</id>
    <title>Fine-tuning HuggingFace SmolVLM (256M) to control the robot</title>
    <updated>2025-05-25T16:24:19+00:00</updated>
    <author>
      <name>/u/Complex-Indication</name>
      <uri>https://old.reddit.com/user/Complex-Indication</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt; &lt;img alt="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" src="https://external-preview.redd.it/b29vMmxwbTNmeTJmMSsIw5Jo6JnOhGNxnxMg56RLkadqgoRaNULw7zamXe9N.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d86f3549101d3f35fb75d562c3bd121473237386" title="Fine-tuning HuggingFace SmolVLM (256M) to control the robot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with tiny LLMs and VLMs for a while now, perhaps some of your saw my earlier post here about running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1g9seqf/a_tiny_language_model_260k_params_is_running/"&gt;LLM on ESP32 for Dalek&lt;/a&gt; Halloween prop. This time I decided to use HuggingFace really tiny (256M parameters!) SmolVLM to control robot just from camera frames. The input is a prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward. Based on the image choose one action: forward, left, right, back. If there is an obstacle blocking the view, choose back. If there is an obstacle on the left, choose right. If there is an obstacle on the right, choose left. If there are no obstacles, choose forward.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and an image from Raspberry Pi Camera Module 2. The output is text.&lt;/p&gt; &lt;p&gt;The base model didn't work at all, but after collecting some data (200 images) and fine-tuning with LORA, it actually (to my surprise) started working!&lt;/p&gt; &lt;p&gt;Currently the model runs on local PC and the data is exchanged between Raspberry Pi Zero 2 and the PC over local network. I know for a fact I can run SmolVLM fast enough on Raspberry Pi 5, but I was not able to do it due to power issues (Pi 5 is very power hungry), so I decided to leave it for the next video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Complex-Indication"&gt; /u/Complex-Indication &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9s2q9nm3fy2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kv6jjk/finetuning_huggingface_smolvlm_256m_to_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T16:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvc9w6</id>
    <title>Cheapest Ryzen AI Max+ 128GB yet at $1699. Ships June 10th.</title>
    <updated>2025-05-25T20:30:17+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvc9w6/cheapest_ryzen_ai_max_128gb_yet_at_1699_ships/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T20:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvf8d2</id>
    <title>Nvidia RTX PRO 6000 Workstation 96GB - Benchmarks</title>
    <updated>2025-05-25T22:46:05+00:00</updated>
    <author>
      <name>/u/fuutott</name>
      <uri>https://old.reddit.com/user/fuutott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Posting here as it's something I would like to know before I acquired it. No regrets.&lt;/p&gt; &lt;p&gt;RTX 6000 PRO 96GB @ 600W - Platform w5-3435X rubber dinghy rapids&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;zero context input - &amp;quot;who was copernicus?&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40K token input 40000 tokens of lorem ipsum - &lt;a href="https://pastebin.com/yAJQkMzT"&gt;https://pastebin.com/yAJQkMzT&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;model settings : flash attention enabled - &lt;strong&gt;128K context&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LM Studio 0.3.16 beta - cuda 12 runtime 1.33.0&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Zero Context (tok/sec)&lt;/th&gt; &lt;th&gt;First Token (s)&lt;/th&gt; &lt;th&gt;40K Context (tok/sec)&lt;/th&gt; &lt;th&gt;First Token 40K (s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;llama-3.3-70b-instruct@q8_0 64000 context Q8 KV cache (81GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;9.72&lt;/td&gt; &lt;td&gt;0.45&lt;/td&gt; &lt;td&gt;3.61&lt;/td&gt; &lt;td&gt;66.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;gigaberg-mistral-large-123b@Q4_K_S 64000 context Q8 KV cache (90.8GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;18.61&lt;/td&gt; &lt;td&gt;0.14&lt;/td&gt; &lt;td&gt;11.01&lt;/td&gt; &lt;td&gt;71.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;meta/llama-3.3-70b@q4_k_m (84.1GB VRAM)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;28.56&lt;/td&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;18.14&lt;/td&gt; &lt;td&gt;33.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b@&lt;strong&gt;BF16&lt;/strong&gt; 40960 context&lt;/td&gt; &lt;td&gt;21.55&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;16.24&lt;/td&gt; &lt;td&gt;19.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-32b-128k@q8_k_xl&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;33.01&lt;/td&gt; &lt;td&gt;0.17&lt;/td&gt; &lt;td&gt;21.73&lt;/td&gt; &lt;td&gt;20.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;gemma-3-27b-instruct-qat@Q4_0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;45.25&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;td&gt;&lt;strong&gt;45.44&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;15.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwq-32b@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;53.18&lt;/td&gt; &lt;td&gt;0.07&lt;/td&gt; &lt;td&gt;33.81&lt;/td&gt; &lt;td&gt;18.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;deepseek-r1-distill-qwen-32b@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;53.91&lt;/td&gt; &lt;td&gt;0.07&lt;/td&gt; &lt;td&gt;33.48&lt;/td&gt; &lt;td&gt;18.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Llama-4-Scout-17B-16E-Instruct@Q4_K_M (Q8 KV cache)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;68.22&lt;/td&gt; &lt;td&gt;0.08&lt;/td&gt; &lt;td&gt;46.26&lt;/td&gt; &lt;td&gt;30.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;google_gemma-3-12b-it-Q8_0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;68.47&lt;/td&gt; &lt;td&gt;0.06&lt;/td&gt; &lt;td&gt;53.34&lt;/td&gt; &lt;td&gt;11.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m – my beloved&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;79.00&lt;/td&gt; &lt;td&gt;0.03&lt;/td&gt; &lt;td&gt;51.71&lt;/td&gt; &lt;td&gt;11.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m – &lt;strong&gt;400W CAP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;78.02&lt;/td&gt; &lt;td&gt;0.11&lt;/td&gt; &lt;td&gt;49.78&lt;/td&gt; &lt;td&gt;14.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistral-small-3.1-24b-instruct-2503@q4_k_m – &lt;strong&gt;300W CAP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;69.02&lt;/td&gt; &lt;td&gt;0.12&lt;/td&gt; &lt;td&gt;39.78&lt;/td&gt; &lt;td&gt;18.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-14b-128k@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;107.51&lt;/td&gt; &lt;td&gt;0.22&lt;/td&gt; &lt;td&gt;61.57&lt;/td&gt; &lt;td&gt;10.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-30b-a3b-128k@q8_k_xl&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;122.95&lt;/td&gt; &lt;td&gt;0.25&lt;/td&gt; &lt;td&gt;64.93&lt;/td&gt; &lt;td&gt;7.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;qwen3-8b-128k@q4_k_m&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;153.63&lt;/td&gt; &lt;td&gt;0.06&lt;/td&gt; &lt;td&gt;79.31&lt;/td&gt; &lt;td&gt;8.42&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuutott"&gt; /u/fuutott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvf8d2/nvidia_rtx_pro_6000_workstation_96gb_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-25T22:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvj0nt</id>
    <title>New LocalLLM Hardware complete</title>
    <updated>2025-05-26T02:04:11+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"&gt; &lt;img alt="New LocalLLM Hardware complete" src="https://b.thumbs.redditmedia.com/ZBd82qXPCpl5Y406kiGaXRlPIUh1aiKasaz1i4H363A.jpg" title="New LocalLLM Hardware complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I spent this last week at Red Hats conference with this hardware sitting at home waiting for me. Finally got it put together. The conference changed my thought on what I was going to deploy but interest in everyone's thoughts.&lt;/p&gt; &lt;p&gt;The hardware is an AMD Ryzen 7 5800x with 64GB of ram, 2x 3909Ti that my best friend gave me (2x 4.0x8) with a 500gb boot and 4TB nvme. &lt;/p&gt; &lt;p&gt;The rest of the lab isal also available for ancillary things.&lt;/p&gt; &lt;p&gt;At the conference, I shifted my session from Ansible and Openshift to as much vLLM as I could and it's gotten me excited for IT Work for the first time in a while. &lt;/p&gt; &lt;p&gt;Currently still setting thingd up - got the Qdrant DB installed on the proxmox cluster in the rack. Plan to use vLLM/ HF with Open-WebUI for a GPT front end for the rest of the family with RAG, TTS/STT and maybe even Home Assistant voice.&lt;/p&gt; &lt;p&gt;Any recommendations? Ivr got nvidia-smi working g and both gpus are detected. Got them power limited ton300w each with the persistence configured (I have a 1500w psu but no need to blow a breaker lol). Im coming from my M3 Ultra Mac Studio running Ollama, that's really for my music studio - wanted to separate out the functions.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kvj0nt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvj0nt/new_localllm_hardware_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T02:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvknlo</id>
    <title>Speechless: Speech Instruction Training Without Speech for Low Resource Languages</title>
    <updated>2025-05-26T03:36:39+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"&gt; &lt;img alt="Speechless: Speech Instruction Training Without Speech for Low Resource Languages" src="https://preview.redd.it/ju7kqbqjq13f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24c19692c9e98b21bf15c0e3f564d6800ac0fa76" title="Speechless: Speech Instruction Training Without Speech for Low Resource Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it’s me from &lt;strong&gt;Menlo Research&lt;/strong&gt; again 👋. Today I want to share some news + a new model!&lt;/p&gt; &lt;p&gt;Exciting news - our paper &lt;em&gt;“SpeechLess”&lt;/em&gt; just got accepted to &lt;strong&gt;Interspeech 2025&lt;/strong&gt;, and we’ve finished the camera-ready version! 🎉&lt;/p&gt; &lt;p&gt;The idea came out of a challenge we faced while building a speech instruction model - we didn’t have enough speech instruction data for our use case. That got us thinking: Could we train the model entirely using synthetic data?&lt;/p&gt; &lt;p&gt;That’s how &lt;strong&gt;SpeechLess&lt;/strong&gt; was born.&lt;br /&gt; &lt;strong&gt;Method Overview (with diagrams in the paper):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Convert real speech → discrete tokens (train a quantizer)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Convert text → discrete tokens (train SpeechLess to simulate speech tokens from text)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Use this pipeline (text → synthetic speech tokens) to train a LLM on speech instructions- just like training any other language model.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Training on fully synthetic speech tokens is surprisingly effective - performance holds up, and it opens up new possibilities for building speech systems in &lt;strong&gt;low-resource settings&lt;/strong&gt; where collecting audio data is difficult or expensive.&lt;/p&gt; &lt;p&gt;We hope this helps other teams in similar situations and inspires more exploration of synthetic data in speech applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; - Paper: &lt;a href="https://arxiv.org/abs/2502.14669"&gt;https://arxiv.org/abs/2502.14669&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Speechless Model: &lt;a href="https://huggingface.co/Menlo/Speechless-llama3.2-v0.1"&gt;https://huggingface.co/Menlo/Speechless-llama3.2-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Dataset: &lt;a href="https://huggingface.co/datasets/Menlo/Ichigo-pretrain-tokenized-v0.1"&gt;https://huggingface.co/datasets/Menlo/Ichigo-pretrain-tokenized-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- LLM: &lt;a href="https://huggingface.co/Menlo/Ichigo-llama3.1-8B-v0.5"&gt;https://huggingface.co/Menlo/Ichigo-llama3.1-8B-v0.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Github: &lt;a href="https://github.com/menloresearch/ichigo"&gt;https://github.com/menloresearch/ichigo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju7kqbqjq13f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvknlo/speechless_speech_instruction_training_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T03:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvnti4</id>
    <title>Open-source project that use LLM as deception system</title>
    <updated>2025-05-26T06:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone 👋&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on that I think you'll find really interesting. It's called Beelzebub, an open-source honeypot framework that uses LLMs to create incredibly realistic and dynamic deception environments.&lt;/p&gt; &lt;p&gt;By integrating LLMs, it can mimic entire operating systems and interact with attackers in a super convincing way. Imagine an SSH honeypot where the LLM provides plausible responses to commands, even though nothing is actually executed on a real system.&lt;/p&gt; &lt;p&gt;The goal is to keep attackers engaged for as long as possible, diverting them from your real systems and collecting valuable, real-world data on their tactics, techniques, and procedures. We've even had success capturing real threat actors with it!&lt;/p&gt; &lt;p&gt;I'd love for you to try it out, give it a star on GitHub, and maybe even contribute! Your feedback,&lt;br /&gt; especially from an LLM-centric perspective, would be incredibly valuable as we continue to develop it.&lt;/p&gt; &lt;p&gt;You can find the project here:&lt;/p&gt; &lt;p&gt;👉 GitHub:&lt;a href="https://github.com/mariocandela/beelzebub"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think in the comments! Do you have ideas for new LLM-powered honeypot features?&lt;/p&gt; &lt;p&gt;Thanks for your time! 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvnti4/opensource_project_that_use_llm_as_deception/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T06:48:01+00:00</published>
  </entry>
</feed>
