<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-01T11:22:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kzrfop</id>
    <title>Getting sick of companies cherry picking their benchmarks when they release a new model</title>
    <updated>2025-05-31T07:36:35+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get why they do it. They need to hype up their thing etc. But cmon a bit of academic integrity would go a long way. Every new model comes with the claim that it outcompetes older models that are 10x their size etc. Like, no. Maybe I'm an old man shaking my fist at clouds here I don't know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T07:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l09i8f</id>
    <title>What are the top creative writing models ?</title>
    <updated>2025-05-31T22:33:14+00:00</updated>
    <author>
      <name>/u/TheArchivist314</name>
      <uri>https://old.reddit.com/user/TheArchivist314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone I wanted to know what are the top models that are good at creative writing. I'm looking for ones I can run on my card. I've got a 4070. It has 12GB of Vram. I've got 64GB of normal ram. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheArchivist314"&gt; /u/TheArchivist314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l09i8f/what_are_the_top_creative_writing_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T22:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l06g4l</id>
    <title>The SRE’s Guide to High Availability Open WebUI Deployment Architecture</title>
    <updated>2025-05-31T20:15:14+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"&gt; &lt;img alt="The SRE’s Guide to High Availability Open WebUI Deployment Architecture" src="https://external-preview.redd.it/XpbGGkJKPGpF-WdM9CwPHoy0zCWwEbDV6ozBsv9F_h8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b02d3e90330be8f56dd819511015b3b554630c8" title="The SRE’s Guide to High Availability Open WebUI Deployment Architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on my real world experiences running Open WebUI for thousands of concurrent users, this guide covers the best practices for deploying stateless Open WebUI containers (Kubernetes Pods, Swarm services, ECS etc), Redis and external embeddings, vector databases and put all that behind a load balancer that understands long-lived WebSocket upgrades.&lt;/p&gt; &lt;p&gt;When you’re ready to graduate from single container deployment to a distributed HA architecture for Open WebUI, this is where you should start! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://taylorwilsdon.medium.com/the-sres-guide-to-high-availability-open-webui-deployment-architecture-2ee42654eced"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l06g4l/the_sres_guide_to_high_availability_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T20:15:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0m2yd</id>
    <title>How to execute commands by llm or how to switch back and forth llm to tool/function call?</title>
    <updated>2025-06-01T10:50:20+00:00</updated>
    <author>
      <name>/u/InsideResolve4517</name>
      <uri>https://old.reddit.com/user/InsideResolve4517</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to execute commands by llm or how to switch back and forth llm to tool/function call? (sorry if question is not clear itself)&lt;/p&gt; &lt;p&gt;I will try to cover my requirement.&lt;/p&gt; &lt;p&gt;I am developing my personal assistant. So assuming I am giving command to llm&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q: &amp;quot;What is the time now?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llm answer: (internally: user asked time but I don't know time but I know I have function or something I can execute that function get_current_time)&lt;br /&gt; get_current_time: The time is 12:12AM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q: &amp;quot;What is my battery percentage?&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llm: llm will think and it will try to match if it can give answer to it or not and it will then find function like (get_battery_percentage)&lt;br /&gt; get_battery_percentage: Current battery percentage is 15%&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q: Please run system update command&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llm: I need to understand what type of system architacture os etc is(get_system_info(endExecution=false))&lt;/p&gt; &lt;p&gt;get_system_info: it will return system info&lt;br /&gt; (since endExecution is false which should be deciced by llm then I will not return system info and end command. Instead I will pas that response again to llm then now llm will take over next)&lt;br /&gt; llm: function return is passed to llm&lt;/p&gt; &lt;p&gt;then llm gets the system like it's ubuntu and using apt so I for this it's sudo apt update&lt;/p&gt; &lt;p&gt;so it will either retured to user or pass to (terminal_call) with command.&lt;/p&gt; &lt;p&gt;assume for now it's returned command&lt;/p&gt; &lt;p&gt;so at the end&lt;/p&gt; &lt;p&gt;llm will say:&lt;/p&gt; &lt;p&gt;To update your system please run sudo apt update in command prompt&lt;/p&gt; &lt;p&gt;so I want to make mini assistant which will run in my local system with local llm (ollama interface) but I am struggling with back and forth switching to tool and again taking over by llm.&lt;/p&gt; &lt;p&gt;I am okay if on each take over I need another llm prompt execution&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideResolve4517"&gt; /u/InsideResolve4517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m2yd/how_to_execute_commands_by_llm_or_how_to_switch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m2yd/how_to_execute_commands_by_llm_or_how_to_switch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m2yd/how_to_execute_commands_by_llm_or_how_to_switch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T10:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzw65c</id>
    <title>AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market</title>
    <updated>2025-05-31T12:41:19+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt; &lt;img alt="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" src="https://external-preview.redd.it/bnCoi_QMP0ucYNmMDpD8YzNjydtxrrZkZROQJhXvr2s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=981e9ab36322decfefbeb6831d8e913c9f0d6692" title="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-pro-385-spotted-on-geekbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T12:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l02hmq</id>
    <title>deepseek/deepseek-r1-0528-qwen3-8b stuck on infinite tool loop. Any ideas?</title>
    <updated>2025-05-31T17:23:44+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've downloaded the official Deepseek distillation from their official sources and it does seem a touch smarter. However, when using tools, it often gets stuck forever trying to use them. Do you know why this is going on, and if we have any workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l01bfe</id>
    <title>Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!</title>
    <updated>2025-05-31T16:34:06+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt; &lt;img alt="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" src="https://external-preview.redd.it/ZG81Yjhkenk2NTRmMdgqNWupVXy_ZPAevb2tTQhA9R_THDnUrLckbufzOiAz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6b4e7c264f8e0ba31013dc39fa0ffa3f2a2d820" title="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I have spent a couple weekends giving the tiny Qwen3 0.6B model the ability to show off its underutilized tool calling abilities by using remote MCP servers. I am pleasantly surprised at how well it can chain tools. Additionally, I gave it the option to limit how much it can think to avoid the &amp;quot;overthinking&amp;quot; issue reasoning models (especially Qwen) can have. &lt;a href="https://muellerzr.github.io/til/end_thinking.html"&gt;This implementation was largely inspired by a great article from Zach Mueller outlining just that.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Also, this project is an adaptation of &lt;a href="https://github.com/huggingface/transformers.js-examples/tree/main/qwen3-webgpu"&gt;Xenova's Qwen3 0.6 WebGPU code in transformers.js-examples&lt;/a&gt;, it was a solid starting point to work with Qwen3 0.6B. &lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;HF Space Link: &lt;a href="https://huggingface.co/spaces/callbacked/Qwen3-MCP"&gt;https://huggingface.co/spaces/callbacked/Qwen3-MCP&lt;/a&gt;&lt;br /&gt; Repo: &lt;a href="https://github.com/callbacked/qwen3-mcp"&gt;https://github.com/callbacked/qwen3-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Footnote: With Qwen3 8B having a distillation from R1-0528, I really hope we can see that trickle down to other models including Qwen3 0.6B. Seeing how much more intelligent the other models can get off of R1-0528 would be a cool thing see in action!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r495cezy654f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T16:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0guyk</id>
    <title>What's the best setup/llm for writing fast code?</title>
    <updated>2025-06-01T05:03:30+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested how automated the process of writing the fastest code possible can be. Say I want code to multiply two 1000 by 1000 matrices as quickly as possible for example. Ideally the setup would produce code, time it on my machine, modify the code and repeat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0guyk/whats_the_best_setupllm_for_writing_fast_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0guyk/whats_the_best_setupllm_for_writing_fast_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0guyk/whats_the_best_setupllm_for_writing_fast_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T05:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0e4jl</id>
    <title>I'm tired of windows awful memory management how is the performance of LLM and AI tasks in Ubuntu? Windows takes 8+ gigs of ram idle and that's after debloating.</title>
    <updated>2025-06-01T02:29:33+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Windows isnt horrible for AI but god its so resource inefficient, for example if I train a wan 1.3b lora it will take 50+ gigs of ram unless I do something like launch Doom The Dark Ages and play on my other GPU then WSL ram usage drops and stays at 30 gigs. Why? No clue windows is the worst at memory management. When I use Ubuntu on my old server idle memory usage is 2gb max. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0e4jl/im_tired_of_windows_awful_memory_management_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0e4jl/im_tired_of_windows_awful_memory_management_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0e4jl/im_tired_of_windows_awful_memory_management_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T02:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0j9r8</id>
    <title>Prebuilt PC vs DIY 5090</title>
    <updated>2025-06-01T07:38:29+00:00</updated>
    <author>
      <name>/u/henrygatech</name>
      <uri>https://old.reddit.com/user/henrygatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0j9r8/prebuilt_pc_vs_diy_5090/"&gt; &lt;img alt="Prebuilt PC vs DIY 5090" src="https://external-preview.redd.it/tpiY2KNJA8Hoz09nmlhVjJx1UKfVzlzUwEhk5dBrcDM.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e055f890da2b034199db051097096c4421f040a" title="Prebuilt PC vs DIY 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks to micro center Santa Clara, I got lucky to bought an HP OMEN 45L prebuilt: Ultra 9 285K, RTX 5090 (OEM), 64GB DDR5, 2TB SSD, 360mm liquid cooling.&lt;/p&gt; &lt;p&gt;As well as a 5090 Founders Edition.&lt;/p&gt; &lt;p&gt;Background: • Have some prev ML/DL knowledge and exposure, but haven’t been hands-on in a while • Looking to get back into deep learning, both for learning and side projects&lt;/p&gt; &lt;p&gt;Use case: • ML learning/ Re-implementing papers • Local LLM, fine-tuning, LoRA • 4K gaming • Maybe dual-GPU in the future, but still figuring things out&lt;/p&gt; &lt;p&gt;The OMEN prebuild is quiet, stable, and ready to go — but have concerns on limited upgrade flexibility (BIOS, PSU, airflow).&lt;/p&gt; &lt;p&gt;Would you suggest stick to the prebuilt or spend time for a custom built with the 5090 fe?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henrygatech"&gt; /u/henrygatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.microcenter.com/product/693699/hp-omen-45l-gt22-3090-gaming-pc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0j9r8/prebuilt_pc_vs_diy_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0j9r8/prebuilt_pc_vs_diy_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T07:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1l033vh</id>
    <title>Best models to try on 96gb gpu?</title>
    <updated>2025-05-31T17:49:48+00:00</updated>
    <author>
      <name>/u/sc166</name>
      <uri>https://old.reddit.com/user/sc166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX pro 6000 Blackwell arriving next week. What are the top local coding and image/video generation models I can try? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sc166"&gt; /u/sc166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l05wpz</id>
    <title>Has anyone managed to get a non Google AI to run</title>
    <updated>2025-05-31T19:51:20+00:00</updated>
    <author>
      <name>/u/Gabrielmorrow</name>
      <uri>https://old.reddit.com/user/Gabrielmorrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"&gt; &lt;img alt="Has anyone managed to get a non Google AI to run" src="https://preview.redd.it/8yt7shdl964f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=739e2d5a72bad15d28de3d6a9992852dc9157d9f" title="Has anyone managed to get a non Google AI to run" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the new Google edge gallery app? I'm wondering if deepseek or a version of it can be ran locally with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gabrielmorrow"&gt; /u/Gabrielmorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8yt7shdl964f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l05wpz/has_anyone_managed_to_get_a_non_google_ai_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T19:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l049hr</id>
    <title>Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)</title>
    <updated>2025-05-31T18:38:56+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"&gt; &lt;img alt="Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)" src="https://external-preview.redd.it/a2RzcmN3MGp3NTRmMQcy6PVwRQbV7yy14JYjj4jOMAMqB9rDPOOSK6pFaFzH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c24fbad79aeb47f5f19e401647c9bc11e9bcde0c" title="Demo Video of AutoBE, Backend Vibe Coding Agent Achieving 100% Compilation Success (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;AutoBE: Backend Vibe Coding Agent Achieving 100% Compilation Success&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Github Repository: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Playground Website: &lt;a href="https://stackblitz.com/github/wrtnlabs/autobe-playground-stackblitz"&gt;https://stackblitz.com/github/wrtnlabs/autobe-playground-stackblitz&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo Result (Generated backend applications by AutoBE) &lt;ul&gt; &lt;li&gt;&lt;a href="https://stackblitz.com/edit/autobe-demo-bbs"&gt;Bullet-in Board System&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://stackblitz.com/edit/autobe-demo-shopping"&gt;E-Commerce&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I previously posted about this same project on Reddit, but back then the Prisma (ORM) agent side only had around 70% success rate.&lt;/p&gt; &lt;p&gt;The reason was that the error messages from the Prisma compiler for AI-generated incorrect code were so unintuitive and hard to understand that even I, as a human, struggled to make sense of them. Consequently, the AI agent couldn't perform proper corrections based on these cryptic error messages.&lt;/p&gt; &lt;p&gt;However, today I'm back with AutoBE that truly achieves 100% compilation success. I solved the problem of Prisma compiler's unhelpful and unintuitive error messages by directly building the Prisma AST (Abstract Syntax Tree), implementing validation myself, and creating a custom code generator.&lt;/p&gt; &lt;p&gt;This approach bypasses the original Prisma compiler's confusing error messaging altogether, enabling the AI agent to generate consistently compilable backend code.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Introducing AutoBE: The Future of Backend Development&lt;/p&gt; &lt;p&gt;We are immensely proud to introduce AutoBE, our revolutionary open-source vibe coding agent for backend applications, developed by Wrtn Technologies.&lt;/p&gt; &lt;p&gt;The most distinguished feature of AutoBE is its exceptional 100% success rate in code generation. AutoBE incorporates built-in TypeScript and Prisma compilers alongside OpenAPI validators, enabling automatic technical corrections whenever the AI encounters coding errors. Furthermore, our integrated review agents and testing frameworks provide an additional layer of validation, ensuring the integrity of all AI-generated code.&lt;/p&gt; &lt;p&gt;What makes this even more remarkable is that backend applications created with AutoBE can seamlessly integrate with our other open-source projects—Agentica and AutoView—to automate AI agent development and frontend application creation as well. In theory, this enables complete full-stack application development through vibe coding alone.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Alpha Release: 2025-06-01&lt;/li&gt; &lt;li&gt;Beta Release: 2025-07-01&lt;/li&gt; &lt;li&gt;Official Release: 2025-08-01&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;AutoBE currently supports comprehensive requirements analysis and derivation, database design, and OpenAPI document generation (API interface specification). All core features will be completed by the beta release, while the integration with Agentica and AutoView for full-stack vibe coding will be finalized by the official release.&lt;/p&gt; &lt;p&gt;We eagerly anticipate your interest and support as we embark on this exciting journey.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f2df0y0jw54f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l049hr/demo_video_of_autobe_backend_vibe_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T18:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0ct34</id>
    <title>Is there an alternative to LM Studio with first class support for MLX models?</title>
    <updated>2025-06-01T01:17:14+00:00</updated>
    <author>
      <name>/u/ksoops</name>
      <uri>https://old.reddit.com/user/ksoops</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using LM Studio for the last few months on my Macs due to it's first class support for MLX models (they implemented a very nice &lt;a href="https://github.com/lmstudio-ai/mlx-engine"&gt;MLX engine&lt;/a&gt; which supports adjusting context length etc.&lt;/p&gt; &lt;p&gt;While it works great, there are a few issues with it:&lt;br /&gt; - it doesn't work behind a company proxy, which means it's a pain in the ass to update the MLX engine etc when there is a new release, on my work computers&lt;/p&gt; &lt;p&gt;- it's closed source, which I'm not a huge fan of&lt;/p&gt; &lt;p&gt;I can run the MLX models using `mlx_lm.server` and using open-webui or Jan as the front end; but running the models this way doesn't allow for adjustment of context window size (as far as I know)&lt;/p&gt; &lt;p&gt;Are there any other solutions out there? I keep scouring the internet for alternatives once a week but I never find a good alternative.&lt;/p&gt; &lt;p&gt;With the unified memory system in the new mac's and how well the run local LLMs, I'm surprised to find lack of first class support Apple's MLX system.&lt;/p&gt; &lt;p&gt;(Yes, there is quite a big performance improvement, as least for me! I can run the MLX version Qwen3-30b-a3b at 55-65 tok/sec, vs ~35 tok/sec with the GGUF versions)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ksoops"&gt; /u/ksoops &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ct34/is_there_an_alternative_to_lm_studio_with_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ct34/is_there_an_alternative_to_lm_studio_with_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ct34/is_there_an_alternative_to_lm_studio_with_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T01:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzv322</id>
    <title>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</title>
    <updated>2025-05-31T11:41:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T11:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0ig7q</id>
    <title>Which model is suitable for e-mail classification / labeling?</title>
    <updated>2025-06-01T06:44:25+00:00</updated>
    <author>
      <name>/u/surveypoodle</name>
      <uri>https://old.reddit.com/user/surveypoodle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to automatically add labels my to e-mails like &lt;code&gt;spam&lt;/code&gt;, &lt;code&gt;scam&lt;/code&gt;, &lt;code&gt;cold-email&lt;/code&gt;, &lt;code&gt;marketing&lt;/code&gt;, &lt;code&gt;resume&lt;/code&gt;, &lt;code&gt;proposal&lt;/code&gt;, &lt;code&gt;meeting-request&lt;/code&gt;, etc. to see how effective it is at keeping my mailbox organized. I need it to be self-hostable and I don't mind if it is slow.&lt;/p&gt; &lt;p&gt;What is a suitable model for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surveypoodle"&gt; /u/surveypoodle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ig7q/which_model_is_suitable_for_email_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ig7q/which_model_is_suitable_for_email_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0ig7q/which_model_is_suitable_for_email_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T06:44:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0bc5j</id>
    <title>OpenWebUI vs LibreChat?</title>
    <updated>2025-05-31T23:59:50+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;These are the two most popular Chat UI tools for LLMs. Have you tried them?&lt;/p&gt; &lt;p&gt;Which one do you think is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0bc5j/openwebui_vs_librechat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0bc5j/openwebui_vs_librechat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0bc5j/openwebui_vs_librechat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T23:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l05hpu</id>
    <title>llama-server, gemma3, 32K context *and* speculative decoding on a 24GB GPU</title>
    <updated>2025-05-31T19:32:46+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp keeps cooking! Draft model support with SWA landed this morning and early tests show up to 30% improvements in performance. Fitting it all on a single 24GB GPU was tight. The 4b as a draft model had a high enough acceptance rate to make a performance difference. Generating code had the best speed ups and creative writing got slower. &lt;/p&gt; &lt;p&gt;Tested on dual 3090s: &lt;/p&gt; &lt;h3&gt;4b draft model&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt&lt;/th&gt; &lt;th&gt;n&lt;/th&gt; &lt;th&gt;tok/sec&lt;/th&gt; &lt;th&gt;draft_n&lt;/th&gt; &lt;th&gt;draft_accepted&lt;/th&gt; &lt;th&gt;ratio&lt;/th&gt; &lt;th&gt;Δ %&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;create a one page html snake game in javascript&lt;/td&gt; &lt;td&gt;1542&lt;/td&gt; &lt;td&gt;49.07&lt;/td&gt; &lt;td&gt;1422&lt;/td&gt; &lt;td&gt;956&lt;/td&gt; &lt;td&gt;0.67&lt;/td&gt; &lt;td&gt;26.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;write a snake game in python&lt;/td&gt; &lt;td&gt;1904&lt;/td&gt; &lt;td&gt;50.67&lt;/td&gt; &lt;td&gt;1709&lt;/td&gt; &lt;td&gt;1236&lt;/td&gt; &lt;td&gt;0.72&lt;/td&gt; &lt;td&gt;31.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;write a story about a dog&lt;/td&gt; &lt;td&gt;982&lt;/td&gt; &lt;td&gt;33.97&lt;/td&gt; &lt;td&gt;1068&lt;/td&gt; &lt;td&gt;282&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;-14.4%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Scripts and configurations can be found on &lt;a href="https://github.com/mostlygeek/llama-swap/wiki/gemma3-27b-100k-context"&gt;llama-swap's wiki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama-swap config: &lt;/p&gt; &lt;p&gt;```yaml macros: &amp;quot;server-latest&amp;quot;: /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap&lt;/p&gt; &lt;p&gt;# quantize KV cache to Q8, increases context but # has a small effect on perplexity # &lt;a href="https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt; &amp;quot;q8-kv&amp;quot;: &amp;quot;--cache-type-k q8_0 --cache-type-v q8_0&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;gemma3-args&amp;quot;: | --model /path/to/models/gemma-3-27b-it-q4_0.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95&lt;/p&gt; &lt;p&gt;models: # fits on a single 24GB GPU w/ 100K context # requires Q8 KV quantization &amp;quot;gemma&amp;quot;: env: # 3090 - 35 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 11.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1&amp;quot; cmd: | ${server-latest} ${q8-kv} ${gemma3-args} --ctx-size 102400 --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# single GPU w/ draft model (lower context) &amp;quot;gemma-fit&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot; cmd: | ${server-latest} ${q8-kv} ${gemma3-args} --ctx-size 32000 --ctx-size-draft 32000 --model-draft /path/to/models/gemma-3-4b-it-q4_0.gguf --draft-max 8 --draft-min 4&lt;/p&gt; &lt;p&gt;# Requires 30GB VRAM for 100K context and non-quantized cache # - Dual 3090s, 38.6 tok/sec # - Dual P40s, 15.8 tok/sec &amp;quot;gemma-full&amp;quot;: env: # 3090 - 38 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 15.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-ea4&amp;quot; cmd: | ${server-latest} ${gemma3-args} --ctx-size 102400 --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf #-sm row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# Requires: 35GB VRAM for 100K context w/ 4b model # with 4b as a draft model # note: --mmproj not compatible with draft models&lt;/p&gt; &lt;p&gt;&amp;quot;gemma-draft&amp;quot;: env: # 3090 - 38 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; cmd: | ${server-latest} ${gemma3-args} --ctx-size 102400 --model-draft /path/to/models/gemma-3-4b-it-q4_0.gguf --ctx-size-draft 102400 --draft-max 8 --draft-min 4 ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l05hpu/llamaserver_gemma3_32k_context_and_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T19:32:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l06f7r</id>
    <title>Most powerful &lt; 7b parameters model at the moment?</title>
    <updated>2025-05-31T20:14:03+00:00</updated>
    <author>
      <name>/u/ventilador_liliana</name>
      <uri>https://old.reddit.com/user/ventilador_liliana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know which is the best model less than 7b currently available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ventilador_liliana"&gt; /u/ventilador_liliana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l06f7r/most_powerful_7b_parameters_model_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T20:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0m8r0</id>
    <title>104k-Token Prompt in a 110k-Token Context with DeepSeek-R1-0528-UD-IQ1_S – Benchmark &amp; Impressive Results</title>
    <updated>2025-06-01T11:00:46+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Prompt:&lt;/strong&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Command (on Windows):&lt;/strong&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Runescape_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; - Tips: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kysms8"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kysms8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Answer (first time I see a model provide such a good answer):&lt;/strong&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Hardware:&lt;/strong&gt; &lt;code&gt; i9-7980XE - 4.2Ghz on all cores 256GB DDR4 F4-3200C14Q2-256GTRS - XMP enabled 1x 5090 (x16) 1x 3090 (x16) 1x 3090 (x8) Prime-X299-A-II &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark results:&lt;/strong&gt; ``` llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.07 ms / 106524 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.22 ms / 106524 tokens ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The questions:&lt;/strong&gt; 1. Would 1x RTX PRO 6000 Blackwell or even 2x RTX PRO 6000 Blackwell significantly improve these metrics without any other hardware upgrade? (knowing that there would still be CPU offloading) 2. Would a different CPU, motherboard and RAM improve these metrics? 3. How to significantly improve prompt processing speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0jcoa</id>
    <title>How many parameters does R1 0528 have?</title>
    <updated>2025-06-01T07:44:01+00:00</updated>
    <author>
      <name>/u/Sudden-Albatross-733</name>
      <uri>https://old.reddit.com/user/Sudden-Albatross-733</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"&gt; &lt;img alt="How many parameters does R1 0528 have?" src="https://b.thumbs.redditmedia.com/WwyP2AGTFovCSAcu4ERv7Xhjnr7t6sRjYVGA0rP1BDI.jpg" title="How many parameters does R1 0528 have?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found conflicting info online, some articles say it's 685b and some say 671b, which is correct? huggingface also shows 685b (look at the attached screenshot) BUT it shows that even for the old one, which I know for sure was 671b. anyone know which is correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Albatross-733"&gt; /u/Sudden-Albatross-733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0jcoa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0jcoa/how_many_parameters_does_r1_0528_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T07:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0cg8b</id>
    <title>AMD RX 9080 XT ES engineering sample, up to 32 GB of VRAM.</title>
    <updated>2025-06-01T00:57:59+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/AMD-RX-9080-XT-ES-engineering-sample-could-rival-RTX-5080-Super.1027707.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0cg8b/amd_rx_9080_xt_es_engineering_sample_up_to_32_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0cg8b/amd_rx_9080_xt_es_engineering_sample_up_to_32_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T00:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzzshu</id>
    <title>Google lets you run AI models locally</title>
    <updated>2025-05-31T15:29:15+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/"&gt;https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzsa70</id>
    <title>China is leading open source</title>
    <updated>2025-05-31T08:35:25+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt; &lt;img alt="China is leading open source" src="https://preview.redd.it/6stw9ivzw24f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87af4f2951867765dd0c43808b34253b587103b5" title="China is leading open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6stw9ivzw24f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0l1fx</id>
    <title>OpenAI to release open-source model this summer - everything we know so far</title>
    <updated>2025-06-01T09:40:49+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Tweet (March 31th 2025)&lt;/em&gt;&lt;br /&gt; &lt;a href="https://x.com/sama/status/1906793591944646898"&gt;https://x.com/sama/status/1906793591944646898&lt;/a&gt;&lt;br /&gt; [...] We are planning to release our first open-weigh language model since GPT-2. We've been thinking about this for a long time but other priorities took precedence. Now it feels important to do [...]&lt;/p&gt; &lt;p&gt;&lt;em&gt;TED2025 (April 11th 2025)&lt;/em&gt;&lt;br /&gt; &lt;a href="https://youtu.be/5MWT_doo68k?t=473"&gt;https://youtu.be/5MWT_doo68k?t=473&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Question:&lt;/strong&gt; How much were you shaken up by the arrival of DeepSeek?&lt;br /&gt; &lt;strong&gt;Sam Altman's response:&lt;/strong&gt; I think open-source has an important place. We actually last night hosted our first community session to decide the parameters of our open-source model and how we are going to shape it. We are going to do a very powerful open-source model. I think this is important. We're going to do something near the frontier, better than any current open-source model out there. There will be people who use this in ways that some people in this room maybe you or I don't like. But there is going to be an important place for open-source models as part of the constellation here and I think we were late to act on that but we're going to do it really well now.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tweet (April 25th 2025)&lt;/em&gt;&lt;br /&gt; &lt;a href="https://x.com/actualananda/status/1915909779886858598"&gt;https://x.com/actualananda/status/1915909779886858598&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Question:&lt;/strong&gt; Open-source model when daddy?&lt;br /&gt; &lt;strong&gt;Sam Altman's response:&lt;/strong&gt; heat waves.&lt;br /&gt; The lyric 'late nights in the middle of June' from Glass Animals' 'Heat Waves' has been interpreted as a cryptic hint at a model release in June.&lt;/p&gt; &lt;p&gt;&lt;em&gt;OpenAI CEO Sam Altman testifies on AI competition before Senate committee (May 8th 2025)&lt;/em&gt;&lt;br /&gt; &lt;a href="https://youtu.be/jOqTg1W_F5Q?t=4741"&gt;https://youtu.be/jOqTg1W_F5Q?t=4741&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Question:&lt;/strong&gt; &amp;quot;How important is US leadership in either open-source or closed AI models?&lt;br /&gt; &lt;strong&gt;Sam Altman's response:&lt;/strong&gt; I think it's quite important to lead in both. We realize that OpenAI can do more to help here. So, we're going to release an open-source model that we believe will be the leading model this summer because we want people to build on the US stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0l1fx/openai_to_release_opensource_model_this_summer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0l1fx/openai_to_release_opensource_model_this_summer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0l1fx/openai_to_release_opensource_model_this_summer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T09:40:49+00:00</published>
  </entry>
</feed>
