<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-27T21:48:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iz6dik</id>
    <title>Phi-4 mini</title>
    <updated>2025-02-27T03:25:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"&gt; &lt;img alt="Phi-4 mini" src="https://external-preview.redd.it/HbA7FdS3-3IKs2fSROiouEFOClJB8eMuRXERbnX8pEE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=025db9aba033f1ba1b3e39d55a1e2c1c7e214393" title="Phi-4 mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T03:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1izoau2</id>
    <title>GPT 4.5 System Card</title>
    <updated>2025-02-27T19:32:58+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoau2/gpt_45_system_card/"&gt; &lt;img alt="GPT 4.5 System Card" src="https://external-preview.redd.it/iKjVEll_93gXLlselpXdio1HWIWv_aR9HX5IGUGU20Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5259520ed53b84eba4c9a86fd5956ca31bd4d7ef" title="GPT 4.5 System Card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/reach-vb/GPT-4.5-System-Card/blob/main/gpt-4-5-system-card.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoau2/gpt_45_system_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izoau2/gpt_45_system_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T19:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1izffwq</id>
    <title>made a real time voice agent with FastRTC, smolagents, and hugging face inference providers</title>
    <updated>2025-02-27T13:13:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"&gt; &lt;img alt="made a real time voice agent with FastRTC, smolagents, and hugging face inference providers" src="https://external-preview.redd.it/bndlY2Uzdzlsb2xlMTJQN53bP0jkQd6Ha0hKebDSrLH7l6BLz9vlfya9ye5X.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7bdc41ab88b3ba079e37f395a3b00b045bc67e1" title="made a real time voice agent with FastRTC, smolagents, and hugging face inference providers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zx9o67w9lole1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1izq45w</id>
    <title>benchmark ðŸŽ‰gpt 4.5 vs grok 3 vs Sonnet 3.5</title>
    <updated>2025-02-27T20:50:07+00:00</updated>
    <author>
      <name>/u/palyer69</name>
      <uri>https://old.reddit.com/user/palyer69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izq45w/benchmark_gpt_45_vs_grok_3_vs_sonnet_35/"&gt; &lt;img alt="benchmark ðŸŽ‰gpt 4.5 vs grok 3 vs Sonnet 3.5" src="https://b.thumbs.redditmedia.com/ElcbkEbauVI2Eo6CGTQl4jZvSYMJkE0qw2TPftXdZGQ.jpg" title="benchmark ðŸŽ‰gpt 4.5 vs grok 3 vs Sonnet 3.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palyer69"&gt; /u/palyer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1izq45w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izq45w/benchmark_gpt_45_vs_grok_3_vs_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izq45w/benchmark_gpt_45_vs_grok_3_vs_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1izl9qh</id>
    <title>Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)</title>
    <updated>2025-02-27T17:28:22+00:00</updated>
    <author>
      <name>/u/gkamer8</name>
      <uri>https://old.reddit.com/user/gkamer8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izl9qh/generate_a_wiki_for_your_research_topic_sourcing/"&gt; &lt;img alt="Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)" src="https://external-preview.redd.it/8t_8ma6bYc6oGtpAhaHj5jBbi1LLrwxpqhbhCStrt4M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=508f8dd999d0ba0a52a79d22e4d9bb42f2259f47" title="Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkamer8"&gt; /u/gkamer8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goodreasonai/nichey"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izl9qh/generate_a_wiki_for_your_research_topic_sourcing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izl9qh/generate_a_wiki_for_your_research_topic_sourcing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T17:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1izd62d</id>
    <title>Everyoneâ€™s saying AGI is just around the corner, but honestly, what even is AGI to you?</title>
    <updated>2025-02-27T10:58:54+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like one of those things where the definition keeps shifting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T10:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1izgxd6</id>
    <title>It ain't much but it's mine Xeon E5-2690 v4 2X P-104-100 8GB 1X GTX-1080 128GB DDR4 RAM</title>
    <updated>2025-02-27T14:24:09+00:00</updated>
    <author>
      <name>/u/Artur-Ochowiak</name>
      <uri>https://old.reddit.com/user/Artur-Ochowiak</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artur-Ochowiak"&gt; /u/Artur-Ochowiak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4798ziwxole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izgxd6/it_aint_much_but_its_mine_xeon_e52690_v4_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izgxd6/it_aint_much_but_its_mine_xeon_e52690_v4_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T14:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbw2g</id>
    <title>Are we becoming more or less dependent on CUDA as time goes on?</title>
    <updated>2025-02-27T09:26:44+00:00</updated>
    <author>
      <name>/u/Fringolicious</name>
      <uri>https://old.reddit.com/user/Fringolicious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at my next GPU and seriously considering a 7900 XTX - 24GB VRAM, decent price, not catching on fire and readily available.&lt;/p&gt; &lt;p&gt;Question is, will this be a massive problem for running models etc locally? I know I've enabled CUDA support and used CUDA flags on a bunch of things recently for my 3070, so would it be a massive deal to not have CUDA? Are we moving in the direction of less reliance on CUDA over time or more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fringolicious"&gt; /u/Fringolicious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz2syr</id>
    <title>By the time Deepseek does make an actual R1 Mini, I won't even notice</title>
    <updated>2025-02-27T00:26:02+00:00</updated>
    <author>
      <name>/u/Cerebral_Zero</name>
      <uri>https://old.reddit.com/user/Cerebral_Zero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because everyone keeps referring to these distil models as R1 while ignoring the words distil or what foundation model it's finetuned on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cerebral_Zero"&gt; /u/Cerebral_Zero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T00:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm8k6</id>
    <title>Phi-4-Mini performance metrics on Intel PCs</title>
    <updated>2025-02-27T18:07:59+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt; &lt;img alt="Phi-4-Mini performance metrics on Intel PCs" src="https://external-preview.redd.it/pOPuW_iNdIa10dhRtc1cNC57z3SNqjd2uyv5Nypw2CQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8bb08e89753226b2ff85d39d197ef70f2b7abc" title="Phi-4-Mini performance metrics on Intel PCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intel posted &lt;a href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-4-small-language-models.html"&gt;an article&lt;/a&gt; with inference speed benchmarks of Phi-4-Mini (4-bit weights + OpenVINO hardware acceleration) running on a couple of their chips. &lt;/p&gt; &lt;p&gt;It's cool to see hard performance data with an SLM announcement for once. (At least, it's saving my team from one on-device benchmark ðŸ˜…) &lt;/p&gt; &lt;p&gt;On an Asus Zenbook S 14, which has an Intel Core Ultra 9 inside with 32GB RAM, they're getting ~30 toks/s for 1024 tokens in/out.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kl5e00430qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc6f692ccdd0d4293f87668a6d4471439c92dae7"&gt;https://preview.redd.it/kl5e00430qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc6f692ccdd0d4293f87668a6d4471439c92dae7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exciting to see the progress with local inference on typical consumer hardware :) &lt;/p&gt; &lt;p&gt;They also ran a benchmark on a PC with an Core i9-149000K and a discrete Arc B580 GPU, which was hitting &amp;gt;90 toks/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0mrilz70qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=040651dffabfde774b87c8571af6d53fe050393d"&gt;https://preview.redd.it/y0mrilz70qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=040651dffabfde774b87c8571af6d53fe050393d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqhfw</id>
    <title>Any theories on what's going on here for this coding benchmark?</title>
    <updated>2025-02-27T21:05:52+00:00</updated>
    <author>
      <name>/u/__eita__</name>
      <uri>https://old.reddit.com/user/__eita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt; &lt;img alt="Any theories on what's going on here for this coding benchmark?" src="https://preview.redd.it/uc4k9x64yqle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3fc22fdd5fdbf678a6a10b6d9392048ae32f70" title="Any theories on what's going on here for this coding benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why a reasoning model would perform way better for swe-bench verified while performing poorly for swe-lancer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__eita__"&gt; /u/__eita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4k9x64yqle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T21:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1izazyk</id>
    <title>Kokoro TTS 1.1</title>
    <updated>2025-02-27T08:18:53+00:00</updated>
    <author>
      <name>/u/incognataa</name>
      <uri>https://old.reddit.com/user/incognataa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incognataa"&gt; /u/incognataa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T08:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz54du</id>
    <title>DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm</title>
    <updated>2025-02-27T02:20:47+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt; &lt;img alt="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" src="https://external-preview.redd.it/8TUylBdHG6G-RlVpDiMU8uIUEktXyGwSKK-R9XNOIZE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de464dc178abc09c8c46cb861ec265373d57c26" title="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DualPipe is an innovative bidirectional pipeline parallism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DualPipe"&gt;https://github.com/deepseek-ai/DualPipe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b"&gt;https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T02:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz9fpc</id>
    <title>Phi Model Family: The rise of The Small Language Models (SLMs)!</title>
    <updated>2025-02-27T06:24:39+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt; &lt;img alt="Phi Model Family: The rise of The Small Language Models (SLMs)!" src="https://preview.redd.it/1218qwefkmle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0047de65b619a85493b7afdeb217512daf64f0b" title="Phi Model Family: The rise of The Small Language Models (SLMs)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1218qwefkmle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T06:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz1fv4</id>
    <title>Microsoft announces Phi-4-multimodal and Phi-4-mini</title>
    <updated>2025-02-26T23:22:15+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt; &lt;img alt="Microsoft announces Phi-4-multimodal and Phi-4-mini" src="https://external-preview.redd.it/QxVX6RZwkbebYL7yNK-C4tfRXCDplq8w2ZjdBvIh-2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4f4d685b401cb23df7ee1e63ea0579a77eea2bc" title="Microsoft announces Phi-4-multimodal and Phi-4-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T23:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm42j</id>
    <title>What is Aider?</title>
    <updated>2025-02-27T18:03:05+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt; &lt;img alt="What is Aider?" src="https://preview.redd.it/6kgjr75i1qle1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a96cf91fad0adc3fea1292d175b92cbc64800ec" title="What is Aider?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, what is Aider? Is it a model? Or a benchmark? Or a cli? Or a browser extension? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kgjr75i1qle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm9pu</id>
    <title>I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted.</title>
    <updated>2025-02-27T18:09:20+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt; &lt;img alt="I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." src="https://external-preview.redd.it/bWhwMHFib2gycWxlMYR5n1H2-KAhKVY699t1y87JffT7MDLWeztGuBbhiJoR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ce3d64ee8a302c6a140ce04850b2b4f5ed3d45c" title="I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rs0obaoh2qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbmbb</id>
    <title>Perplexity R1 1776 performs worse than DeepSeek R1 for complex problems.</title>
    <updated>2025-02-27T09:06:25+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity claims the reasoning abilities of R1 1776 are not affected by the decensoring process, but after testing it in &lt;a href="https://github.com/fairydreaming/lineage-bench/"&gt;lineage-bench&lt;/a&gt; I found that for very complex problems there are significant differences in the model performance.&lt;/p&gt; &lt;p&gt;Below you can see benchmark results for different problem sizes:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;lineage-8&lt;/th&gt; &lt;th align="left"&gt;lineage-16&lt;/th&gt; &lt;th align="left"&gt;lineage-32&lt;/th&gt; &lt;th align="left"&gt;lineage-64&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1&lt;/td&gt; &lt;td align="left"&gt;0.965&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.945&lt;/td&gt; &lt;td align="left"&gt;0.780&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 1776&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.975&lt;/td&gt; &lt;td align="left"&gt;0.675&lt;/td&gt; &lt;td align="left"&gt;0.205&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While for lineage-8 and lineage-16 problem sizes the model performance matches or even exceeds the original DeepSeek R1, for lineage-32 we can already observe difference in scores, while for lineage-64 R1 1776 score reached random guessing level.&lt;/p&gt; &lt;p&gt;So it looks like Perplexity claims about reasoning abilities not being affected by the decensoring process are not true.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We also ensured that the modelâ€™s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Edit: here's one example prompt for lineage-64 and the model output generated in Perplexity Labs playground in case anyone is interested: &lt;a href="https://pastebin.com/EPy06bqp"&gt;https://pastebin.com/EPy06bqp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also Perplexity staff noticed my findings and are looking into the problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1izdrsd</id>
    <title>vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days</title>
    <updated>2025-02-27T11:38:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt; &lt;img alt="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" src="https://preview.redd.it/wnphfz5s4ole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d23c35465c203ce3194ca69901bcbe56c0961102" title="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wnphfz5s4ole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T11:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1izfy2d</id>
    <title>LLaDA - Large Language Diffusion Model (weights + demo)</title>
    <updated>2025-02-27T13:36:28+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HF Demo:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;https://huggingface.co/spaces/multimodalart/LLaDA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Base&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;https://arxiv.org/abs/2502.09992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Diffusion LLMs are looking promising for alternative architecture. Some lab also recently announced a proprietary one (inception) which you could test, it can generate code quite well. &lt;/p&gt; &lt;p&gt;This stuff comes with the promise of parallelized token generation.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;LLaDA predicts all masked tokens simultaneously during each step of the reverse process.&amp;quot; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we wouldn't need super high bandwidth for fast t/s anymore. It's not memory bandwidth bottlenecked, it has a compute bottleneck. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1izoyxk</id>
    <title>A diffusion based 'small' coding LLM that is 10x faster in token generation than transformer based LLMs (apparently 1000 tok/s on H100)</title>
    <updated>2025-02-27T20:01:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Karpathy post: &lt;a href="https://xcancel.com/karpathy/status/1894923254864978091"&gt;https://xcancel.com/karpathy/status/1894923254864978091&lt;/a&gt; (covers some interesting nuance about transformer vs diffusion for image/video vs text)&lt;/p&gt; &lt;p&gt;Artificial analysis comparison: &lt;a href="https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig"&gt;https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo video: &lt;a href="https://xcancel.com/InceptionAILabs/status/1894847919624462794"&gt;https://xcancel.com/InceptionAILabs/status/1894847919624462794&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The chat link (down rn, probably over capacity) &lt;a href="https://chat.inceptionlabs.ai/"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's interesting here is that this thing generates all tokens at once and then goes through refinements as opposed to transformer based one token at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ize4n0</id>
    <title>Dual 5090FE</title>
    <updated>2025-02-27T12:01:15+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt; &lt;img alt="Dual 5090FE" src="https://preview.redd.it/defh49ux8ole1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=843767edca5506f54e0bdb3a8b57d7e022c97a89" title="Dual 5090FE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/defh49ux8ole1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmsrl</id>
    <title>Building a robot that can see, hear, talk, and dance. Powered by on-device AI!</title>
    <updated>2025-02-27T18:31:16+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt; &lt;img alt="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" src="https://external-preview.redd.it/Y3JjNzRwc2k2cWxlMe__omCO_n66cYU7Fe7wXFz05iYznG-U5sQ5kSodSfXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3b1aca9ec3aa181f1b6214eda1a89059c04a5ab" title="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h13dgnsi6qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1izpgm9</id>
    <title>GPT-4.5 cost</title>
    <updated>2025-02-27T20:22:13+00:00</updated>
    <author>
      <name>/u/Timotheeee1</name>
      <uri>https://old.reddit.com/user/Timotheeee1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izpgm9/gpt45_cost/"&gt; &lt;img alt="GPT-4.5 cost" src="https://preview.redd.it/gpyxg6x8qqle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b08f75fe9165e4c062b38dc03b08ca3fc4b43b60" title="GPT-4.5 cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timotheeee1"&gt; /u/Timotheeee1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gpyxg6x8qqle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izpgm9/gpt45_cost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izpgm9/gpt45_cost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1izf4zf</id>
    <title>Pythagoras : i should've guessed first hand ðŸ˜© !</title>
    <updated>2025-02-27T12:59:00+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt; &lt;img alt="Pythagoras : i should've guessed first hand ðŸ˜© !" src="https://preview.redd.it/m3vrfaz8jole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e5f34f13dda8627c1b31cbceebdf6cfb503c19e" title="Pythagoras : i should've guessed first hand ðŸ˜© !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3vrfaz8jole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:00+00:00</published>
  </entry>
</feed>
