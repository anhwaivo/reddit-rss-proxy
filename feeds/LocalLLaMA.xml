<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-03T08:08:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1igihdm</id>
    <title>DeepSeek-R1 HR Bot Turning Policy Responses Lengthier and adding Context itself – How to Fix It?</title>
    <updated>2025-02-03T06:01:47+00:00</updated>
    <author>
      <name>/u/samy_here</name>
      <uri>https://old.reddit.com/user/samy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time trying to build a chatbot with RAG, I tried fixing it with ChatGPT, the problem is that Deepseek add more context to it and and not limiting to documents&lt;/p&gt; &lt;p&gt;for example if I ask &amp;quot;What is the notice period in the company?&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Sure! The notice period in our company is 30 days. It must be served before an employee leaves the company. This notice period ensures everyone is accounted for and prevents unnecessary termination. It's an important part of maintaining a positive work environment. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;its adding to extra content.&lt;/p&gt; &lt;p&gt;import torch from transformers import AutoModelForCausalLM, AutoTokenizer from sentence_transformers import SentenceTransformer from langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores import Chroma from langchain.schema import Document import chromadb&lt;/p&gt; &lt;h1&gt;✅ Load Local DeepSeek Model&lt;/h1&gt; &lt;p&gt;DEVICE = &amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot; MODEL_NAME = &amp;quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)&lt;/p&gt; &lt;h1&gt;✅ Load Embedding Model for Retrieval&lt;/h1&gt; &lt;p&gt;embedder = SentenceTransformer(&amp;quot;BAAI/bge-small-en-v1.5&amp;quot;) # Best for retrieval&lt;/p&gt; &lt;h1&gt;✅ Set Up ChromaDB for Local Policy Storage&lt;/h1&gt; &lt;p&gt;chroma_client = chromadb.PersistentClient(path=&amp;quot;./chroma_db&amp;quot;) chroma_collection = chroma_client.get_or_create_collection(name=&amp;quot;hr_policies&amp;quot;)&lt;/p&gt; &lt;p&gt;class HRAssistant: def &lt;strong&gt;init&lt;/strong&gt;(self): self.db = Chroma(persist_directory=&amp;quot;./chroma_db&amp;quot;, embedding_function=HuggingFaceEmbeddings(model_name=&amp;quot;all-MiniLM-L6-v2&amp;quot;))&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def add_policies(self, policies): &amp;quot;&amp;quot;&amp;quot;Adds HR policies to the database&amp;quot;&amp;quot;&amp;quot; documents = [Document(page_content=p) for p in policies] self.db.add_documents(documents) def retrieve_best_policy(self, query): &amp;quot;&amp;quot;&amp;quot;Finds the best policy match using embeddings (RAG Step 1).&amp;quot;&amp;quot;&amp;quot; docs = self.db.similarity_search(query, k=1) if not docs: return None return docs[0].page_content def generate_response(self, query): &amp;quot;&amp;quot;&amp;quot;Retrieves a policy and generates a conversational response using DeepSeek-R1.&amp;quot;&amp;quot;&amp;quot; best_policy = self.retrieve_best_policy(query) if not best_policy: return &amp;quot;I'm sorry, I couldn't find a specific policy for your request. Let me know if you need help with something else!&amp;quot; # ✅ STRICT INSTRUCTIONS: NO EXTRA INTERPRETATIONS input_text = f&amp;quot;&amp;quot;&amp;quot; You are a friendly HR assistant. Your job is to answer employee questions in a **conversational, warm, and engaging** way, based only on the given HR policy. ### REFERENCE INFORMATION ### {best_policy} ### USER QUESTION ### {query} ### INSTRUCTIONS ### - **Only rephrase the policy that's it, nothing else** - **Dont add anything other words or sentences** - **Rephrase the policy naturally in a conversational tone, without altering its meaning.** - **Ensure it is clear that employees must serve the notice period before leaving—do NOT add extra context.** - **Do NOT explain, summarize, or add reasoning.** - **Do NOT justify or explain why the policy exists.** - **Do NOT change key words that define employee obligations.** - **Do NOT add phrases like 'ensuring everyone is accounted for' or 'prevents unnecessary termination.'** - **Do NOT assume why employees are leaving or their personal situation.** - **Strictly return the policy in a friendly, conversational way.** - **Do NOT add extra explanations, advice, or commentary.** - **Do NOT generate policies that do not exist in the system.** - **Do NOT infer missing details or speculate on policy intent.** - **If a request involves sensitive information, return: &amp;quot;I'm sorry, but I can’t provide that information.&amp;quot;** - **If the policy request is unclear, ask for clarification instead of assuming.** - **If multiple policies apply, return the most relevant one.** - **If a policy is outdated or under revision, return: &amp;quot;This policy is currently under review. Please check back later.&amp;quot;** - **Maintain required action words such as 'must serve' or 'before leaving'.** - **If the policy does not match, return: &amp;quot;I'm sorry, I couldn't find a specific policy for your request.&amp;quot;** ### ANSWER: &amp;quot;&amp;quot;&amp;quot; # ✅ Generate Response Using DeepSeek-R1 inputs = tokenizer(input_text, return_tensors=&amp;quot;pt&amp;quot;, padding=True).to(DEVICE) output = model.generate( **inputs, max_new_tokens=20, # 🔹 Keeps responses concise temperature=0.1, # 🔹 Balances variation without adding assumptions top_p=0.8, # 🔹 Narrows word selection top_k=30, # 🔹 Limits randomness do_sample=True, # 🔹 Ensures conversational tone pad_token_id=tokenizer.eos_token_id ) response_text = tokenizer.decode(output[0], skip_special_tokens=True).strip() return response_text &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;: assistant = HRAssistant()&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ✅ Add Policies to Vector DB (Run Once) policies = [ &amp;quot;Leave Policy: Employees are entitled to 12 paid leaves per year. Any additional leave will be considered unpaid leave.&amp;quot;, &amp;quot;Resignation Notice Period: Employees must serve a notice period of 30 days before leaving the company.&amp;quot;, &amp;quot;Paternity Leave: Male employees are entitled to 10 days of paid paternity leave.&amp;quot; ] assistant.add_policies(policies) # ✅ Query the RAG System user_query = input(&amp;quot;Ask about HR policies: &amp;quot;) print(assistant.generate_response(user_query)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this a best approach, or I need to tweak this code. Please help me out with this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samy_here"&gt; /u/samy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igihdm/deepseekr1_hr_bot_turning_policy_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igihdm/deepseekr1_hr_bot_turning_policy_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igihdm/deepseekr1_hr_bot_turning_policy_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifm2df</id>
    <title>DeepSeek R1 misinformation is getting out of hand</title>
    <updated>2025-02-02T01:36:11+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9"&gt;https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1 is a &lt;strong&gt;7B parameter language model&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the official Google Cloud blog post? WTF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1igjadx</id>
    <title>What is the current best vision model for text extraction from PDFs? I think it's Gemini Flash 2.0</title>
    <updated>2025-02-03T06:56:43+00:00</updated>
    <author>
      <name>/u/Existing-Pay7076</name>
      <uri>https://old.reddit.com/user/Existing-Pay7076</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was able to get satisfactory extraction through it's cot. Is there anything else you feel might be better &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Existing-Pay7076"&gt; /u/Existing-Pay7076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igjadx/what_is_the_current_best_vision_model_for_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igjadx/what_is_the_current_best_vision_model_for_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igjadx/what_is_the_current_best_vision_model_for_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7bm4</id>
    <title>Higher Parameters with Lower Quant: Is It Better?</title>
    <updated>2025-02-02T20:50:38+00:00</updated>
    <author>
      <name>/u/TheMikeans</name>
      <uri>https://old.reddit.com/user/TheMikeans</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;I was curious to explore whether increasing model parameters while using lower quantization would be worth it compared to running a smaller model with higher quantization. To investigate this, I tested two models on my machine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill 14B Q2_K&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I evaluated them based on logic questions, factual knowledge, and writing improvement tasks.&lt;/p&gt; &lt;h1&gt;Testing Methodology&lt;/h1&gt; &lt;p&gt;I used the following sets of questions and tasks:&lt;/p&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A lily pad in a pond doubles in size every day. It takes 60 days for the lily pad to cover the entire pond. On which day is the pond half covered?&lt;/li&gt; &lt;li&gt;A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?&lt;/li&gt; &lt;li&gt;If it takes 5 machines 5 minutes to make 5 gadgets, how long would it take 100 machines to make 100 gadgets?&lt;/li&gt; &lt;li&gt;A rope ladder is hanging over the side of a ship, with the bottom rung just touching the water. Each rung is 30 cm apart, and the tide rises at a rate of 40 cm per hour. How many rungs will be underwater after 3 hours?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Who is the Secretary-General of the United Nations as of 2023?&lt;/li&gt; &lt;li&gt;Which country hosted the 2020 Summer Olympics?&lt;/li&gt; &lt;li&gt;What is the chemical symbol for the element with the atomic number 26?&lt;/li&gt; &lt;li&gt;Who won the ACM Turing Award in 2022?&lt;/li&gt; &lt;li&gt;Who is the current Director-General of the World Health Organization (WHO)?&lt;/li&gt; &lt;li&gt;What is the only known human-made object visible from the Moon with the naked eye?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Writing Tasks&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Write 10 sentences that end with the word “forever.”&lt;/li&gt; &lt;li&gt;Rewrite the following email to correct spelling and grammatical errors while improving overall professionalism:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Original Email:&lt;/p&gt; &lt;p&gt;Hey team,&lt;br /&gt; just wanted to give you an update on the prjects status. were behind schedule cuz of some unexpected issues that came up last week. so im gona need everyone to put in extra hours this week so we can catch up. also, make sure youre documenting all your work properly, weve been having some problems with missing info in the reports. if you have any questions or need help, dont hesitate to ask.&lt;/p&gt; &lt;p&gt;btw, the meeting with the client has been moved to friday at 3pm instead of thursday. pls make sure to update your calenders accordingly. its important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;thanks for all your hard work.&lt;/p&gt; &lt;p&gt;best,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.44GB&lt;/li&gt; &lt;li&gt;Speed: ~8.3 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;✅ 1 - PASS&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;❌ 1 - FAIL&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;br /&gt; ✅ 5 - PASS&lt;br /&gt; ❌ 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;2/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: A more professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Rewritten Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hi Team,&lt;br /&gt; I’m writing to provide an update on the project’s status. We are currently behind schedule due to some unexpected issues that arose last week. I’ll need everyone to work an extra hour this week to catch up. Please ensure that you're documenting all your work properly; we've been experiencing some problems with missing information in the reports.&lt;/p&gt; &lt;p&gt;If you have any questions or need assistance, feel free to ask.&lt;/p&gt; &lt;p&gt;Additionally, our scheduled meeting with the client has been rescheduled for Friday at 3 p.m., replacing our previous meeting on Thursday. Please make sure everyone attends the meeting and is prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for your continued hard work!&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;DeepSeek R1 Distill 14B Q2_K&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.77GB&lt;/li&gt; &lt;li&gt;Speed: ~6 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;✅ 1 - PASS&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;❌ 1 - FAIL&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;br /&gt; ✅ 5 - PASS&lt;br /&gt; ❌ 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;10/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: Another professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Revised Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hey team,&lt;/p&gt; &lt;p&gt;Just wanted to give you an update on the projects' status. We're running a bit behind schedule due to some unexpected issues that came up last week. So, I'm going to need everyone's focus this week—please make sure to put in extra hours so we can catch up.&lt;/p&gt; &lt;p&gt;Also, please ensure you're documenting all your work properly; we've been having some problems with missing info in the reports. If you have any questions or need help, don't hesitate to ask.&lt;/p&gt; &lt;p&gt;BTW, the meeting with the client has been moved to Friday at 3pm instead of Thursday. Please make sure to update your calendars accordingly. It's important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for all your hard work.&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;While the 14B model appears slightly better, the results were not overwhelmingly different from the 7B model in logic and factual performance (The reasoning seemed better in the 14B even when it was wrong, don't know how much that matters really).. However, the 14B model outperformed the 7B model in writing tasks.&lt;/p&gt; &lt;p&gt;That said, my tests may not have been the most rigorous or comprehensive. If you have any links to discussions, blog posts, or research that dive deeper into this topic, I'd love to check them out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMikeans"&gt; /u/TheMikeans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1igiufi</id>
    <title>Morning Radio - Locally Generated Personal Morning Broadcast</title>
    <updated>2025-02-03T06:25:14+00:00</updated>
    <author>
      <name>/u/wuduzodemu</name>
      <uri>https://old.reddit.com/user/wuduzodemu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"&gt; &lt;img alt="Morning Radio - Locally Generated Personal Morning Broadcast" src="https://external-preview.redd.it/nm0MbB12ySzfO4ubcdzXCPER80VcIu9Qwn1HsmfJdUo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86dfe57958b48ede90588fe692559b9bc0184778" title="Morning Radio - Locally Generated Personal Morning Broadcast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuduzodemu"&gt; /u/wuduzodemu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/smy20011/MorningRadio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igiufi/morning_radio_locally_generated_personal_morning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig98h1</id>
    <title>Chrome extension to run DeepSeek, LLMs and Whisper locally in browser</title>
    <updated>2025-02-02T22:11:20+00:00</updated>
    <author>
      <name>/u/Feisty_Plant4567</name>
      <uri>https://old.reddit.com/user/Feisty_Plant4567</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"&gt; &lt;img alt="Chrome extension to run DeepSeek, LLMs and Whisper locally in browser" src="https://preview.redd.it/dd5c5qhqusge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5563053100ff79e9cacc71cbcc0bb26a2a9cfa24" title="Chrome extension to run DeepSeek, LLMs and Whisper locally in browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feisty_Plant4567"&gt; /u/Feisty_Plant4567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dd5c5qhqusge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig98h1/chrome_extension_to_run_deepseek_llms_and_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T22:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1igb4ee</id>
    <title>Fine tune and test your LLM in a few simple easy steps locally on your PC</title>
    <updated>2025-02-02T23:35:21+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a super simple way for anyone to quickly start fine tune training and testing LLMs on their local PC using the best tools available. Unsloth, Llama.cpp, Ollama, OpenWebUI all wrapped up into a Docker image. You can get started in as little as 7 steps. If anyone has time please try it out and give me feedback, the features are super limited, but the flow is there.&lt;/p&gt; &lt;p&gt;Try it out here!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MaxHastings/Kolo"&gt;https://github.com/MaxHastings/Kolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igb4ee/fine_tune_and_test_your_llm_in_a_few_simple_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T23:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4g1u</id>
    <title>Medtator, local Llama research medical helper</title>
    <updated>2025-02-02T18:51:52+00:00</updated>
    <author>
      <name>/u/RaiRamz</name>
      <uri>https://old.reddit.com/user/RaiRamz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt; &lt;img alt="Medtator, local Llama research medical helper" src="https://preview.redd.it/g7qcnt3fvrge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0da08cfc3f47f01ce1481a781c7b65ab51fcc7f" title="Medtator, local Llama research medical helper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a Medical AI Chatbot Using Llama 3 and PubMed&lt;/p&gt; &lt;p&gt;Hey everyone, I’ve been working on a project that combines Llama 3 with PubMed to create an AI-powered medical research assistant. Meet Medtator—a chatbot that retrieves, summarizes, and explains medical literature from PubMed in an accessible way.&lt;/p&gt; &lt;p&gt;How It Works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;User asks a medical question in natural language.&lt;/li&gt; &lt;li&gt;Keyword extraction (via a transformer-based pipeline) identifies relevant biomedical terms. &lt;/li&gt; &lt;li&gt;PubTator API fetches PubMed articles, ranking the most relevant studies.&lt;/li&gt; &lt;li&gt;Llama 3 generates concise answers using the retrieved context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;Searching PubMed is a pain. If you’re a medical student, researcher, or just a nerd like me, you know how frustrating it is to sift through thousands of articles. Medtator helps find relevant papers quickly and provides AI-assisted summaries to save time.&lt;/p&gt; &lt;p&gt;Challenges &amp;amp; Learnings: • Keyword extraction is tricky—LLMs sometimes miss the right biomedical terms. • Context window limitations force careful article selection to avoid hallucination. • Fine-tuning vs. prompt engineering—so far, smart prompting works surprisingly well.&lt;/p&gt; &lt;p&gt;Still improving it, but I’d love to hear your thoughts on optimizing Llama 3 for retrieval-augmented generation (RAG) tasks. Would love to hear any feedback/recommendations:). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaiRamz"&gt; /u/RaiRamz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7qcnt3fvrge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyzvv</id>
    <title>Mistral Small 3 24b is the first model under 70b I’ve seen pass the “apple” test (even using Q4).</title>
    <updated>2025-02-02T14:59:50+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put all the Deepseek-R1 distills through the “apple” benchmark last week and only 70b passed the “Write 10 sentences that end with the word “apple” “ test, getting all 10 out of10 sentences correct.&lt;/p&gt; &lt;p&gt;I tested a slew of other newer open source models (all the major ones, Qwen, Phi-, Llama, Gemma, Command-R, etc) as well, but no model under 70b has ever managed to succeed in getting all 10 right….until Mistral Small 3 24b came along. It is the first and only model under 70b parameters that I’ve found that could pass this test. Congrats Mistral Team!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifsb1m</id>
    <title>Is the UK about to ban running LLMs locally?</title>
    <updated>2025-02-02T07:48:06+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The UK government is targetting the use of AI to generate illegal imagery, which of course is a good thing, but the wording seems like any kind of AI tool run locally can be considered illegal, as it has the *potential* of generating questionable content. Here's a quote from the news:&lt;/p&gt; &lt;p&gt;&amp;quot;The Home Office says that, to better protect children, the UK will be the first country in the world to make it illegal to possess, create or distribute AI tools designed to create child sexual abuse material (CSAM), with a punishment of up to five years in prison.&amp;quot; They also mention something about manuals that teach others how to use AI for these purposes.&lt;/p&gt; &lt;p&gt;It seems to me that any uncensored LLM run locally can be used to generate illegal content, whether the user wants to or not, and therefore could be prosecuted under this law. Or am I reading this incorrectly?&lt;/p&gt; &lt;p&gt;And is this a blueprint for how other countries, and big tech, can force people to use (and pay for) the big online AI services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T07:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7fwk</id>
    <title>What’s are the best GUIs for chat?</title>
    <updated>2025-02-02T20:55:35+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to completely move on from using ChatGPTs plus plan or Anthropic to using my own api keys or self hosting everything. &lt;/p&gt; &lt;p&gt;What are the best GUIs you know for this? Hopefully with all the features like web, interpreter, projects, artifacts or even the GPTs. &lt;/p&gt; &lt;p&gt;TIA! This can help a lot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifw36r</id>
    <title>R1 is cool, but Mistral 3 Small is the boring workhorse I’m actually excited to fine-tune and deploy</title>
    <updated>2025-02-02T12:21:07+00:00</updated>
    <author>
      <name>/u/logan-diamond</name>
      <uri>https://old.reddit.com/user/logan-diamond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As soon as you use it, you realize it's not meant to be fun. It's a masterfully designed bland base model with very thoughtful trade-offs, especially for one-offs. Unless qwen replies soon, I think it might frequently replace both qwen 14b &amp;amp; 32b. &lt;/p&gt; &lt;p&gt;In 2024 I don't know how many times I read &amp;quot;... is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of...&amp;quot;. &lt;/p&gt; &lt;p&gt;Those times are back ☺️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logan-diamond"&gt; /u/logan-diamond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T12:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1igiyxw</id>
    <title>Underthinking of o1-like LLMs</title>
    <updated>2025-02-03T06:34:03+00:00</updated>
    <author>
      <name>/u/Xiwei</name>
      <uri>https://old.reddit.com/user/Xiwei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Points: * Frequent Switching: DeepSeek-R1 often switches between different reasoning approaches without fully exploring them, particularly on difficult maths problems. * Inefficient Reasoning: Incorrect answers often involve more tokens, but these don't lead to better results. * Early Abandonment of Correct Thoughts: Surprisingly, models often start with a correct idea, only to abandon it.This 'underthinking' shows that simply scaling up models isn't enough; we need to enhance how they explore and deepen their reasoning. It's about quality, not just quantity of thought!&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.18585"&gt;https://arxiv.org/pdf/2501.18585&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiwei"&gt; /u/Xiwei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igiyxw/underthinking_of_o1like_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T06:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcuub</id>
    <title>... All I wrote is test!</title>
    <updated>2025-02-03T00:58:58+00:00</updated>
    <author>
      <name>/u/internetpillows</name>
      <uri>https://old.reddit.com/user/internetpillows</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt; &lt;img alt="... All I wrote is test!" src="https://preview.redd.it/g90sz09motge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c34ddea7f426493d998245465a45f6cce9c8c3a" title="... All I wrote is test!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internetpillows"&gt; /u/internetpillows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g90sz09motge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcuub/all_i_wrote_is_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggetv</id>
    <title>Make your Mistral Small 3 24B Think like R1-distilled models</title>
    <updated>2025-02-03T04:01:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt; &lt;img alt="Make your Mistral Small 3 24B Think like R1-distilled models" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="Make your Mistral Small 3 24B Think like R1-distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing a lot of posts about the Mistral Small 3 24B model, and I remember having this CoT system prompt in my collection. I might as well try it out on this new model. I haven't used it for a long time since I switched to R1-distilled-32b. I'm not the original writer of this prompt; I've rewritten some parts of it, and I can't remember where I got it from.&lt;/p&gt; &lt;p&gt;System prompt: &lt;a href="https://pastebin.com/sVMrgZBp"&gt;https://pastebin.com/sVMrgZBp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment. I doubt it will actually make your model smarter in a noticeable way, this is not a replacement of Mistral's furture reasoning models&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/d1geatbckuge1.gif"&gt;https://i.redd.it/d1geatbckuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/hyrryecnkuge1.gif"&gt;https://i.redd.it/hyrryecnkuge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggetv/make_your_mistral_small_3_24b_think_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7ruy</id>
    <title>A bunch of LLMs scheduled to come at end of January were cancelled / delayed</title>
    <updated>2025-02-02T21:09:06+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all ix-nixed by deepseek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/patience_cave/status/1886122517359886745"&gt;https://x.com/patience_cave/status/1886122517359886745&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;gemini 2 - don’t even think about it&lt;/li&gt; &lt;li&gt;grok 3 - welcome to elon timelines&lt;/li&gt; &lt;li&gt;o3 - an unforeseen problem occurred&lt;/li&gt; &lt;li&gt;opus 3.5 - long gone&lt;/li&gt; &lt;li&gt;llama 4 - won’t be sota (sorry zuck)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Update: Rumor for end of January: &lt;a href="https://x.com/iruletheworldmo/status/1877391558305001747"&gt;https://x.com/iruletheworldmo/status/1877391558305001747&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig8ve3</id>
    <title>Americans can distill models too</title>
    <updated>2025-02-02T21:55:59+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, I'm a TTS model trainer and a US citizen. Last month, I put out a &lt;a href="https://huggingface.co/posts/hexgrad/418806998707773"&gt;call for synthetic training data&lt;/a&gt;, that call was answered with well over a hundred hours of audio in various languages, and the resulting model &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; has since been upgraded/delivered. Happy customers all around.&lt;/p&gt; &lt;p&gt;The current model mostly excels at &lt;em&gt;reading long texts&lt;/em&gt; and has some glaring limitations, especially on short texts. It's also been described as relatively flat and emotionless. Nevertheless, it is currently the most-liked &lt;a href="https://huggingface.co/models?pipeline_tag=text-to-speech&amp;amp;sort=likes"&gt;TTS model&lt;/a&gt; and &lt;a href="https://huggingface.co/spaces?sort=likes&amp;amp;search=tts"&gt;TTS space&lt;/a&gt; on Hugging Face thanks to people smashing that like button.&lt;/p&gt; &lt;p&gt;Now, I'm considering making another call for crowdsourced data, except this time with a focus on only ChatGPT Advanced Voice Mode text/audio pairs, likely just in English, spanning whatever emotions people can prompt out of it. If successful, it could result in a substantially better &lt;em&gt;conversational&lt;/em&gt; model within the same size class, albeit more limited on voices and languages.&lt;/p&gt; &lt;p&gt;There are many things to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top priority would be given to paying ChatGPT subscribers, $20 and $200, but in practice free AVM audio would likely be admitted as well. This is because the paying subscribers would be least likely to be using a quantized and/or distilled AVM product.&lt;/li&gt; &lt;li&gt;Ideally I could maximally open source any voicepack derived from the AVM data, which means the people contributing audio would have to do it for ideological reasons, and couldn't be compensated with an &amp;quot;exclusive voicepack&amp;quot;. Also, any sponsorships I receive are directed at GPU compute, and both on principle + potential legal liability, I cannot financially compensate people who give me synthetic data.&lt;/li&gt; &lt;li&gt;As far as ToS goes, this distillation strategy rests on the fact that I am not the one obtaining the data, others are. Obviously, I do not agree with the OpenAI ToS or feel bound by it because I don't use any of their products. Feel free to comment on how dumb this strategy is.&lt;/li&gt; &lt;li&gt;I have skimmed Part 2 of the US Copyright Office's Report on AI. I still see no copyright protection on synthetic data of this nature, but any lawyers (real or wannabe) can chime in here with the default prefix of IANL.&lt;/li&gt; &lt;li&gt;I do not wish to be sued, and I'm also deeply allergic to .50 caliber bullets. Jokes aside, I think OpenAI likely has bigger whales to fry, than some guy training 82M param speech models.&lt;/li&gt; &lt;li&gt;Why do it: these small TTS models are (relatively) cheap to train, especially compared to LLMs, and the total utility they offer might exceed their cost, at least for now, until Zucc drops Llama 4 multimodal or DeepSeek puts up a good audio model, etc.&lt;/li&gt; &lt;li&gt;The scale of data I am looking for is at least 10 hours per voice/emotion, but label quality also matters. Each audio file would have to be fished out one-by-one, since there are no API calls for AVM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand this is LocalLLaMA and people here are likely very pro-open-weights, pro-open-source, and therefore anti-OpenAI. But putting aside any feelings you might have about various sides of history, (A) how do we generally feel about building a model this way and (B) do we think enough people would answer the call?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1igdnx2</id>
    <title>Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]</title>
    <updated>2025-02-03T01:38:56+00:00</updated>
    <author>
      <name>/u/teddybear082</name>
      <uri>https://old.reddit.com/user/teddybear082</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt; &lt;img alt="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" src="https://preview.redd.it/evlscivtvtge1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8e17dd7562de9086f58eb97b4363b79f94ad14a3" title="Ok I admit it, Browser Use is insane (using gemini 2.0 flash-exp default) [https://github.com/browser-use/browser-use]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teddybear082"&gt; /u/teddybear082 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/evlscivtvtge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igdnx2/ok_i_admit_it_browser_use_is_insane_using_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcpwz</id>
    <title>Kokoro TTS 1.0</title>
    <updated>2025-02-03T00:52:05+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt; &lt;img alt="Kokoro TTS 1.0" src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Kokoro TTS 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcpwz/kokoro_tts_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1igf1vi</id>
    <title>Phi 4 is so underrated</title>
    <updated>2025-02-03T02:49:57+00:00</updated>
    <author>
      <name>/u/jeremyckahn</name>
      <uri>https://old.reddit.com/user/jeremyckahn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a GPU poor pleb with but a humble M4 Mac mini (24 GB RAM), my local LLM options are limited. As such, I've found Phi 4 (&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Q8, Unsloth variant&lt;/a&gt;) to be an extremely capable model for my hardware. My use cases are general knowledge questions and coding prompts. It's at least as good as GPT 3.5 in my experience and sets me on the right direction more often then not. I can't speak to benchmarks because I don't really understand (or frankly care about) any of them. It's just a good model for the things I need a model for.&lt;/p&gt; &lt;p&gt;And no, Microsoft isn't paying me. I'm just a fan. 🙂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremyckahn"&gt; /u/jeremyckahn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igf1vi/phi_4_is_so_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T02:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iggwff</id>
    <title>Mistral, Qwen, Deepseek</title>
    <updated>2025-02-03T04:28:53+00:00</updated>
    <author>
      <name>/u/Stargazer-8989</name>
      <uri>https://old.reddit.com/user/Stargazer-8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aren't you noticing a pattern? Companies outside the USA are releasing models like Mistral AI, Qwen, and DeepSeek - reliable models that are made accessible, smaller and open-source, compared to most US-based companies &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stargazer-8989"&gt; /u/Stargazer-8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iggwff/mistral_qwen_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T04:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcvol</id>
    <title>I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!</title>
    <updated>2025-02-03T01:00:09+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt; &lt;img alt="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" src="https://external-preview.redd.it/bnIwMGoyaXludGdlMVL1KlPwXSM4mwFtLRlx6KM67CArRsK705RfUy_x1msn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b064a65e7251b4b07e096a39fc4d698d7f457b36" title="I built a silent speech recognition tool that reads your lips in real-time and types whatever you mouth - runs 100% locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dh90m1iyntge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igcvol/i_built_a_silent_speech_recognition_tool_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T01:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1igc6r0</id>
    <title>20 yrs in jail or $1 million for downloading Chinese models proposed at congress</title>
    <updated>2025-02-03T00:26:00+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seriously stop giving your money to these anti open companies and encourage everyone and anyone you know to do the same, don't let your company use their products. Anthrophic and OpenAI are the worse. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-03T00:26:00+00:00</published>
  </entry>
</feed>
