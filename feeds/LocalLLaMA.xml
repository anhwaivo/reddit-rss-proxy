<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-11T13:52:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lx3p4i</id>
    <title>How do I force the LLM to respond shortly?</title>
    <updated>2025-07-11T10:47:42+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It understands it in the beginning, but as conversation increases, it starts becoming a paragraph spewing machine. &lt;/p&gt; &lt;p&gt;Only way I can think of is to re-run responses on a 2nd AI conversation and ask it to re-write it shortly, then channel it back to the conversation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3p4i/how_do_i_force_the_llm_to_respond_shortly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3p4i/how_do_i_force_the_llm_to_respond_shortly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3p4i/how_do_i_force_the_llm_to_respond_shortly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T10:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4zpr</id>
    <title>Blackwell FP8 W8A8 NVFP4 support discussion</title>
    <updated>2025-07-11T11:58:34+00:00</updated>
    <author>
      <name>/u/Kitchen-Year-8434</name>
      <uri>https://old.reddit.com/user/Kitchen-Year-8434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Context here: WSLv2, Win11, Blackwell Pro 6000 workstation.&lt;/p&gt; &lt;p&gt;I've beaten my head against the wall with W8A8 FP8 support and kind of loosely eyed NVFP4 from a distance, fully expecting it to be a nightmare. Like may of you I've seen on here, I went through the gauntlet and very specific hell of trying to build vllm + flash-attention + flashinfer from HEAD on nightly pytorch to get W8A8 support only to have things blow up in my face. Partial CUTLASS support, lack of Gemma-3 vision support, flash-attention version failures when combined with certain models, flashinfer failures, etc.&lt;/p&gt; &lt;p&gt;So my question to the community: has anyone gotten FP8 support working in Blackwell and lived to tell the tale? What about TensorRT-LLM w/NVFP4 support? If so - got any pointers for how to do it?&lt;/p&gt; &lt;p&gt;Fully acknowledging that vllm Blackwell enablement isn't done: &lt;a href="https://github.com/vllm-project/vllm/issues/18153"&gt;link&lt;/a&gt;, but should be done enough to work at this point?&lt;/p&gt; &lt;p&gt;Ideally we could get a set of gists together on github to automate the setup of both environments that we all collaborate on to unstick this, assuming I'm not just completely failing at something obvious.&lt;/p&gt; &lt;p&gt;Part of the problem as well seems to be in model choice; I've been specifically trying to get a Gemma-3-27b + Devstral-Small stack together and going for various Roo pipeline steps, and it seems like running those newer models in the TensorRT-LLM ecosystem is extra painful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Year-8434"&gt; /u/Kitchen-Year-8434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx78bk</id>
    <title>When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. "Considering Elon Musk's views" is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political.</title>
    <updated>2025-07-11T13:42:01+00:00</updated>
    <author>
      <name>/u/lostlifon</name>
      <uri>https://old.reddit.com/user/lostlifon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"&gt; &lt;img alt="When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. &amp;quot;Considering Elon Musk's views&amp;quot; is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political." src="https://external-preview.redd.it/Z3N0OTZ4Z24wOWNmMWs9zsnwyBbwxMH0ZvInVPpBqNaLm12SmXXjJes3XWMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4a4f4c30fcd1df9e99ea2ac2bd6d004202cce27" title="When asked about Israel v Palestine, Grok 4 searches through twitter and other sources for Elon Musk's views so it can align with them. &amp;quot;Considering Elon Musk's views&amp;quot; is the summary of the CoT task shown at 0:50. Grok 4 does NOT do this for any other question, controversial or political." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First seen by ramez on twitter - &lt;a href="https://x.com/ramez/status/1943431212766294413"&gt;https://x.com/ramez/status/1943431212766294413&lt;/a&gt; and the video is from Jeremy Howard confirming this behaviour - &lt;a href="https://x.com/jeremyphoward/status/1943444549696917714"&gt;https://x.com/jeremyphoward/status/1943444549696917714&lt;/a&gt; &lt;/p&gt; &lt;p&gt;For other questions, it doesn't do this - &lt;a href="https://x.com/jeremyphoward/status/1943543291129270366"&gt;https://x.com/jeremyphoward/status/1943543291129270366&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostlifon"&gt; /u/lostlifon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2cdl5xgn09cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx78bk/when_asked_about_israel_v_palestine_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx3u8s</id>
    <title>EuroEval: The robust European language model benchmark.</title>
    <updated>2025-07-11T10:56:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered this really cool project, EuroEval, which has LLM benchmarks of many open-weights models in different European languages (ðŸ‡©ðŸ‡° Danish, ðŸ‡³ðŸ‡± Dutch, ðŸ‡¬ðŸ‡§ English, ðŸ‡«ðŸ‡´ Faroese, ðŸ‡«ðŸ‡® Finnish, ðŸ‡«ðŸ‡· French, ðŸ‡©ðŸ‡ª German, ðŸ‡®ðŸ‡¸ Icelandic, ðŸ‡®ðŸ‡¹ Italian, ðŸ‡³ðŸ‡´ Norwegian, ðŸ‡ªðŸ‡¸ Spanish, ðŸ‡¸ðŸ‡ª Swedish).&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;EuroEval is a language model benchmarking framework that supports evaluating all types of language models out there: encoders, decoders, encoder-decoders, base models, and instruction tuned models. EuroEval has been battle-tested for more than three years and are the standard evaluation benchmark for many companies, universities and organisations around Europe.&lt;/p&gt; &lt;p&gt;Check out the &lt;a href="https://euroeval.com/leaderboards"&gt;leaderboards&lt;/a&gt; to see how different language models perform on a wide range of tasks in various European languages. The leaderboards are updated regularly with new models and new results. All benchmark results have been computed using the associated &lt;a href="https://euroeval.com/python-package"&gt;EuroEval Python package&lt;/a&gt;, which you can use to replicate all the results. It supports all models on the &lt;a href="https://huggingface.co/models"&gt;Hugging Face Hub&lt;/a&gt;, as well as models accessible through 100+ different APIs, including models you are hosting yourself via, e.g., &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; or &lt;a href="https://lmstudio.ai/"&gt;LM Studio&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The idea of EuroEval grew out of the development of Danish language model RÃ¸BÃ†RTa in 2021, when we realised that there was no standard way to evaluate Danish language models. It started as a hobby project including Danish, Swedish and Norwegian, but has since grown to include 12+ European languages.&lt;/p&gt; &lt;p&gt;EuroEval is maintained by &lt;a href="https://www.saattrupdan.com/"&gt;Dan Saattrup Smart&lt;/a&gt; from the &lt;a href="https://alexandra.dk/"&gt;Alexandra Institute&lt;/a&gt;, and is funded by the EU project &lt;a href="https://trustllm.eu/"&gt;TrustLLM&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Leaderboard: &lt;a href="https://euroeval.com/leaderboards/"&gt;https://euroeval.com/leaderboards/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Source code: &lt;a href="https://github.com/EuroEval/EuroEval"&gt;https://github.com/EuroEval/EuroEval&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://euroeval.com/leaderboards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3u8s/euroeval_the_robust_european_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3u8s/euroeval_the_robust_european_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T10:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6yer</id>
    <title>Skywork/Skywork-R1V3-38B Â· Hugging Face</title>
    <updated>2025-07-11T13:30:10+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"&gt; &lt;img alt="Skywork/Skywork-R1V3-38B Â· Hugging Face" src="https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef" title="Skywork/Skywork-R1V3-38B Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Skywork-R1V 3.0: an open source model that beats close source models on multi-modal reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6yer/skyworkskyworkr1v338b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx3jtc</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-11T10:38:36+00:00</updated>
    <author>
      <name>/u/Better-Armadillo1371</name>
      <uri>https://old.reddit.com/user/Better-Armadillo1371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Armadillo1371"&gt; /u/Better-Armadillo1371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx3jtc/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T10:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwe5y8</id>
    <title>mistralai/Devstral-Small-2507</title>
    <updated>2025-07-10T14:29:19+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt; &lt;img alt="mistralai/Devstral-Small-2507" src="https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba" title="mistralai/Devstral-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4ya7</id>
    <title>New OSS project: llamac-lab or a pure C runtime for LLaMA models, made for the edge</title>
    <updated>2025-07-11T11:56:30+00:00</updated>
    <author>
      <name>/u/rvnllm</name>
      <uri>https://old.reddit.com/user/rvnllm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing my new madness, really, not much to say about it, as its very early. &lt;/p&gt; &lt;p&gt;So the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So what it is (or going to be if I don't disappear into the abyss):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Started hacking on this today. &lt;/p&gt; &lt;p&gt;A &lt;strong&gt;pure C (or I try to keep it that way)&lt;/strong&gt; runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.&lt;/p&gt; &lt;p&gt;So let me introduce &lt;code&gt;llamac-lab&lt;/code&gt;, a work-in-progress open-source runtime for LLaMA-based models.&lt;/p&gt; &lt;p&gt;Think llama.cpp, but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flattened into straight-up C (no C++ or STL baggage)&lt;/li&gt; &lt;li&gt;Optimized for &lt;strong&gt;minimal memory use&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Dead simple to embed into &lt;strong&gt;any stack&lt;/strong&gt; (Rust, Python, or LUA or anything else that can interface with C)&lt;/li&gt; &lt;li&gt;Born for &lt;strong&gt;edge devices&lt;/strong&gt;, MCUs, and other weird places LLMs don't usually go&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.&lt;/p&gt; &lt;p&gt;Note: Itâ€™s super early. No model loading yet. No inference. Just early scaffolding and dreams.&lt;br /&gt; But if you're into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!&lt;/p&gt; &lt;p&gt;Repo: [llamac](&lt;a href="https://github.com/llamac-lab/llamac"&gt;https://github.com/llamac-lab/llamac&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Note: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rvnllm"&gt; /u/rvnllm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4hxt</id>
    <title>SmolTalk2: The dataset behind SmolLM3's dual reasoning</title>
    <updated>2025-07-11T11:32:19+00:00</updated>
    <author>
      <name>/u/loubnabnl</name>
      <uri>https://old.reddit.com/user/loubnabnl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt; &lt;img alt="SmolTalk2: The dataset behind SmolLM3's dual reasoning" src="https://external-preview.redd.it/guh_Ilc0qa1iubfc8zva4ecZtUzKUNZdEcOEXQmd36A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fb36eb92fed67dfe7cf18375f3d278d7d7f435b" title="SmolTalk2: The dataset behind SmolLM3's dual reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960"&gt;https://preview.redd.it/02jo0wkc98cf1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bbe4b4e754526d79553aea53066d6a49f492960&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, we're following up on the SmolLM3 release with the full dataset we used for post-training the model. It includes high-quality open datasets and new ones we created to balance model performance in dual reasoning + address the scarcity of reasoning datasets in certain domains such as multi-turn conversations, multilinguality, and alignment.&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk2"&gt;https://huggingface.co/datasets/HuggingFaceTB/smoltalk2&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We hope you will build great models on top of it ðŸš€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loubnabnl"&gt; /u/loubnabnl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4hxt/smoltalk2_the_dataset_behind_smollm3s_dual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:32:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwl9ai</id>
    <title>The New Nvidia Model is Really Chatty</title>
    <updated>2025-07-10T19:07:49+00:00</updated>
    <author>
      <name>/u/SpyderJack</name>
      <uri>https://old.reddit.com/user/SpyderJack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt; &lt;img alt="The New Nvidia Model is Really Chatty" src="https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64406c81316cc6f179ed0040ddcdc5047147395f" title="The New Nvidia Model is Really Chatty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpyderJack"&gt; /u/SpyderJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8bnc2od6i3cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx66on</id>
    <title>Issues with Qwen 3 Embedding models (4B and 0.6B)</title>
    <updated>2025-07-11T12:56:05+00:00</updated>
    <author>
      <name>/u/IndependentApart5556</name>
      <uri>https://old.reddit.com/user/IndependentApart5556</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt; &lt;img alt="Issues with Qwen 3 Embedding models (4B and 0.6B)" src="https://b.thumbs.redditmedia.com/YgiW1tXulPbYxegiQD_ZWMXklDrQz0iXGKWh4ebIFjE.jpg" title="Issues with Qwen 3 Embedding models (4B and 0.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I'm currently facing a weird issue.&lt;br /&gt; I was testing different embedding models, with the goal being to integrate the best local one in a django application. &lt;/p&gt; &lt;p&gt;Architecture is as follows : &lt;/p&gt; &lt;p&gt;- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations&lt;/p&gt; &lt;p&gt;- My PC for the django application, running the codebase &lt;/p&gt; &lt;p&gt;I use CosineDistance to test the models. The functionality is a semantic search. &lt;/p&gt; &lt;p&gt;I noticed the following : &lt;/p&gt; &lt;p&gt;- Using the text-embedding-3-large model, (OAI API) gives great results&lt;br /&gt; - Using Nomic embedding model gives great results also&lt;br /&gt; - Using Qwen embedding models give very bad results, as if the encoding wouldn't make any sense. &lt;/p&gt; &lt;p&gt;i'm using a aembed() method to call the embedding models, and I declare them using : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;OpenAIEmbeddings( model=model_name, check_embedding_ctx_length=False, base_url=base_url, api_key=api_key, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b"&gt;OpenAI cosine distance test results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e"&gt;LM Studio Nomic cosine distance test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742"&gt;LM Studio Qwen 3 cosine distance test &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just can't figure out what's going on. Qwen 3 is supposed to be among the best models.&lt;br /&gt; Can someone give advice ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentApart5556"&gt; /u/IndependentApart5556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2j1l</id>
    <title>I built a tool to run Humanity's Last Exam on your favorite local models!</title>
    <updated>2025-07-11T09:33:40+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt; &lt;img alt="I built a tool to run Humanity's Last Exam on your favorite local models!" src="https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81757699f7df25dfc08d7353f703eb730b727ec6" title="I built a tool to run Humanity's Last Exam on your favorite local models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4"&gt;https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;in the last few weeks, I've spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: &lt;em&gt;good&lt;/em&gt; tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.&lt;/p&gt; &lt;p&gt;Thus, I've built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?&lt;/p&gt; &lt;p&gt;My tool supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;both &lt;strong&gt;vision-based&lt;/strong&gt; and &lt;strong&gt;text-only&lt;/strong&gt; prompting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;automatic judging&lt;/strong&gt; by a third-party model not involved in answering the exam questions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;question randomization&lt;/strong&gt; and only testing for a small subset of HLE&lt;/li&gt; &lt;li&gt;export of the results to &lt;strong&gt;machine-readable JSON&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;running &lt;strong&gt;several evaluations&lt;/strong&gt; for different models all in one go&lt;/li&gt; &lt;li&gt;support for &lt;strong&gt;external Ollama instances&lt;/strong&gt; with Bearer Authentication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The entire source code is on GitHub! &lt;a href="https://github.com/mags0ft/hle-eval-ollama"&gt;https://github.com/mags0ft/hle-eval-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To anyone new to HLE (Humanity's Last Exam)&lt;/strong&gt;, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we're only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)&lt;/p&gt; &lt;p&gt;My project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.&lt;/p&gt; &lt;p&gt;I'd love to get some feedback, so don't hesitate to comment! Have fun trying it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5n8c</id>
    <title>FYI Qwen3 235B A22B IQ4_XS works with 128 GB DDR5 + 8GB VRAM in Windows</title>
    <updated>2025-07-11T12:30:36+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Disclaimers: Nothing new here especially given the recent posts, but was supposed to report back at &lt;a href="/u/Evening_Ad6637"&gt;u/Evening_Ad6637&lt;/a&gt; et al. Furthermore, i am a total noob and do local LLM via LM Studio on Windows 11, so no fancy ik_llama.cpp etc., as it is just so convenient.)&lt;/p&gt; &lt;p&gt;I finally received 2x64 GB DDR5 5600 MHz Sticks (Kingston &lt;a href="https://www.kingston.com/datasheets/KF556C36BBE-8.pdf"&gt;Datasheet&lt;/a&gt;) giving me 128 GB RAM on my ITX Build. I did load the EXPO0 timing profile giving CL36 etc.&lt;br /&gt; This is complemented by a Low Profile RTX 4060 with 8 GB, all controlled by a Ryzen 9 7950X (any CPU would do).&lt;/p&gt; &lt;p&gt;Through LM Studio, I downloaded and ran both unsloth's 128K Q3_K_XL quant (103.7 GB) as well as managed to run the &lt;strong&gt;IQ4_XS&lt;/strong&gt; quant (125.5 GB) on a freshly restarted windows machine. (Haven't tried crashing or stress testing it yet, it currently works without issues).&lt;br /&gt; I left all model settings untouched and increased the context to ~17000. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Time to first token&lt;/strong&gt; on a prompt about a Berlin neighborhood took &lt;strong&gt;around 10 sec, then 3.3-2.7 tps.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I can try to provide any further information or run prompts for you and return the response as well as times. Just wanted to update you that this works. Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5n8c/fyi_qwen3_235b_a22b_iq4_xs_works_with_128_gb_ddr5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4a3t</id>
    <title>Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces</title>
    <updated>2025-07-11T11:20:42+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"&gt; &lt;img alt="Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces" src="https://external-preview.redd.it/OI2iID36iwvBkHarjg-7dH6Sebf2j49Fh1hND_ikvDI.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb8719e3b1cc5345fd3b0ddd04b3379b419c55" title="Prime Intellect on X: Releasing SYNTHETIC-2: our open dataset of 4m verified reasoning traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/PrimeIntellect/status/1943424561116045389"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4a3t/prime_intellect_on_x_releasing_synthetic2_our/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx10ja</id>
    <title>With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?</title>
    <updated>2025-07-11T07:51:04+00:00</updated>
    <author>
      <name>/u/GyozaHoop</name>
      <uri>https://old.reddit.com/user/GyozaHoop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt; &lt;p&gt;But with LLMs like Gemini that support 1M-token context, isnâ€™t building a RAG system somewhat extra?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GyozaHoop"&gt; /u/GyozaHoop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T07:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwx50s</id>
    <title>2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs</title>
    <updated>2025-07-11T03:57:38+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href="https://arxiv.org/pdf/2507.07145"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwsrx7</id>
    <title>Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp</title>
    <updated>2025-07-11T00:20:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt; &lt;img alt="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" src="https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a" title="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt; &lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href="https://www.ibm.com/think/topics/mixture-of-experts"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-ai-platform/Bamba-9B-v1"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13550"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2dw4</id>
    <title>Is a heavily quantised Q235b any better than Q32b?</title>
    <updated>2025-07-11T09:24:06+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt; &lt;p&gt;I'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.&lt;/p&gt; &lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:24:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwztnp</id>
    <title>Granite-speech-3.3-8b</title>
    <updated>2025-07-11T06:33:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt; &lt;img alt="Granite-speech-3.3-8b" src="https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a3cfa1633e9a330cab59c33f8413530288842b0" title="Granite-speech-3.3-8b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T06:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4qhp</id>
    <title>Moonshot AI about to release their 1T parameters model?</title>
    <updated>2025-07-11T11:45:02+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt; &lt;img alt="Moonshot AI about to release their 1T parameters model?" src="https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f19a5b1a2112e5c15ddbc66a6e92a07eecb3c7" title="Moonshot AI about to release their 1T parameters model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kts1w8a7g8cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6dcm</id>
    <title>llama2.c running on the original 2007 iPhone</title>
    <updated>2025-07-11T13:04:10+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt; &lt;img alt="llama2.c running on the original 2007 iPhone" src="https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d84dc04e7624cefc75d18c603d35424468ce1db" title="llama2.c running on the original 2007 iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3u6728ask8cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwta86</id>
    <title>AMD's Pull Request for llama.cpp: Enhancing GPU Support</title>
    <updated>2025-07-11T00:45:46+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt; &lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br /&gt; Discussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14624"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2hn2</id>
    <title>Uncensored LLM ranking for roleplay?</title>
    <updated>2025-07-11T09:31:05+00:00</updated>
    <author>
      <name>/u/mikemend</name>
      <uri>https://old.reddit.com/user/mikemend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.&lt;/p&gt; &lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikemend"&gt; /u/mikemend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx62hd</id>
    <title>Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain "cutlass"</title>
    <updated>2025-07-11T12:50:38+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5awq</id>
    <title>Friendly reminder that Grok 3 should be now open-sourced</title>
    <updated>2025-07-11T12:13:48+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt; &lt;img alt="Friendly reminder that Grok 3 should be now open-sourced" src="https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg" title="Friendly reminder that Grok 3 should be now open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx5awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:13:48+00:00</published>
  </entry>
</feed>
