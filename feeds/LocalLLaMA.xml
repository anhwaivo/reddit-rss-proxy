<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-19T23:21:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i5a7yg</id>
    <title>MCP Clients (Desktop and CLI)</title>
    <updated>2025-01-19T21:53:56+00:00</updated>
    <author>
      <name>/u/punkpeye</name>
      <uri>https://old.reddit.com/user/punkpeye</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5a7yg/mcp_clients_desktop_and_cli/"&gt; &lt;img alt="MCP Clients (Desktop and CLI)" src="https://external-preview.redd.it/8aWXNRvTyCeN34Ta1_BubzV8hiZ6e25e91N8vwuA4dQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a15860b81e9e9b939f39fd2dfd3ec914aa18312a" title="MCP Clients (Desktop and CLI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/punkpeye"&gt; /u/punkpeye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://glama.ai/mcp/clients"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5a7yg/mcp_clients_desktop_and_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5a7yg/mcp_clients_desktop_and_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T21:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4em80</id>
    <title>Interesting article on how DeepSeek has improved the architecture in DeepSeek V2 and V3.</title>
    <updated>2025-01-18T19:00:14+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://epoch.ai"&gt;epoch.ai&lt;/a&gt; has published an interesting article: &lt;a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture"&gt;https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It talks about MLA, MoE innovations and Multi-Token Prediction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4awir</id>
    <title>Have you truly replaced paid models(chatgpt, Claude etc) with self hosted ollama or hugging face ?</title>
    <updated>2025-01-18T16:14:58+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with locally hosted setups, but I keep finding myself coming back to ChatGPT for the ease and performance. For those of you who’ve managed to fully switch, do you still use services like ChatGPT occasionally? Do you use both? &lt;/p&gt; &lt;p&gt;Also, what kind of GPU setup is really needed to get that kind of seamless experience? My 16GB VRAM feels pretty inadequate in comparison to what these paid models offer. Would love to hear your thoughts and setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4kzgp</id>
    <title>We need to be able to train models on consumer-grade hardware</title>
    <updated>2025-01-18T23:55:33+00:00</updated>
    <author>
      <name>/u/yoop001</name>
      <uri>https://old.reddit.com/user/yoop001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The number of papers being published is off the charts, and there’s no way the big players can implement everything. That means some game-changing ideas might slip through the cracks. But if everyday folks could test out these so-called breakthroughs, we’d be in a position to help the big players spot the real gems worth scaling up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoop001"&gt; /u/yoop001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4kzgp/we_need_to_be_able_to_train_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T23:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hb2l</id>
    <title>Theory: trying to use newer and more powerful LLMs to sound more human is likely moving in the wrong direction</title>
    <updated>2025-01-18T21:01:51+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like using more powerful LLMs to try to achieve human like speech is probably moving AWAY from the solution rather than towards it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;tl;dr-&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Newer models are more powerful and have larger context, but are heavily trained with outputs from other LLMs. This results in modern models responding far more intelligently than Llama 1 era models, but also loaded with inescapable &amp;quot;slop&amp;quot; and &amp;quot;GPTisms&amp;quot;. My proposal is that by using older Llama 1 era models/fine-tunes like Guanaco 65b (which is primarily human data trained) as a final step &amp;quot;editor&amp;quot; to rewrite what modern LLMs put out, you can get output that sounds more human and has more natural speech patterns. This could be good for articles/emails/papers/etc.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;My thought process is this: what's the difference between Llama 3.3 and Llama 1? Besides technology, I mean. Lets go back in time, to the days when we only had a 2048 token context windows; before things like NTK RoPE scaling, when your only choices were linear RoPE scaling and/or a superHoT model. Forget all the technological differences, and think: what differentiated the models of those days to the models of today?&lt;/p&gt; &lt;p&gt;Synthetic data in the training, right? The models back then were trained on minimal, if any, synthetic data. Even most of the chat models were barely trained on user/assistant type interactions. The training data was mostly human text.&lt;/p&gt; &lt;p&gt;But today? Modern models have tons and tons of &amp;quot;GPTisms&amp;quot; baked into the training data, not even speaking about how bad the instruct tune can be. So of course when you talk to the thing, it's going to sound like what we now expect an AI Assistant to sound like. And the more powerful models we get, the more we're going to see this synthetic data further baked in there.&lt;/p&gt; &lt;p&gt;In my opinion, it makes a lot of sense to go back and try some of these older models for this purpose, such as using a workflow to have the final step use an old Llama 1 model to rewrite the output of the modern LLMs.&lt;/p&gt; &lt;p&gt;Just to test it, I updated one of my Wilmer workflows to use Mistral Small as the thinker and Guanaco 65b as the responder. I had mistral write the initial response to me trying to sound human, and gave Guanaco very simple instructions to take that response and rewrite it to also sound more human. I then did the same using ChatGPT and Llama 3.3 70b.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;em&gt;This was a very small and simple POC test, and the quality of the output could be far higher with proper prompting, but I was just curious what the difference would be.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I asked the question: &lt;strong&gt;&lt;em&gt;&amp;quot;Which do you like better: Java or C#?&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small response:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Well, it depends on what you're looking to do. Both Java and C# have their strengths. For example:&lt;/p&gt; &lt;p&gt;- **Java**: It’s great for building web applications, Android apps, and large-scale enterprise systems. It has a larger community and more libraries available.&lt;/p&gt; &lt;p&gt;- **C#**: It's fantastic for game development (thanks to Unity), building Windows applications, and integrating with other Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think they both have their use cases. Java might be more versatile for certain web projects, but C# can be incredibly efficient for desktop applications and games.&lt;/p&gt; &lt;p&gt;Do you have a specific type of project in mind that you want to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Obviously a very complete and well written answer, but also very indicative that an AI wrote it. Something about the way it responds is familiar to folks who would be using chatgpt and the like.&lt;/p&gt; &lt;p&gt;Now, look at &lt;strong&gt;Guanaco 65b's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I'd say Java and C# both have their pros and cons. Java is great for building websites, Android apps, and huge enterprise systems. But C# is amazing for making games with Unity, building Windows programs, and hooking into other Microsoft services. Personally, I'd use Java for web stuff and C# for desktop apps and games, but it depends on your project.&lt;/p&gt; &lt;p&gt;So, what're you trying to build? I can give you some pointers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The difference is subtle, but see how it feels a bit more... human?&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;ChatGPT o1's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Honestly, it really depends on what you’re trying to accomplish. Both Java and C# have their own strengths:&lt;/p&gt; &lt;p&gt;* Java: Great for web applications, Android apps, and large-scale enterprise systems. It also has a huge community and tons of libraries.&lt;/p&gt; &lt;p&gt;* C#: Awesome for game development (especially with Unity), building Windows apps, and integrating with Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think both are solid choices. Java might be more versatile for certain web-based projects, while C# shines when it comes to desktop applications and gaming.&lt;/p&gt; &lt;p&gt;Any specific type of project you’re looking to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This one remains sounding like an AI assistant. Same instructions and prompt I gave Guanaco.&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;Llama 3.3's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;so you're wondering about c# and java, right? they both have their own things goin for 'em, and honestly it kinda comes down to what you're workin on and what you like. they're actually pretty similar, they're both object-oriented, they both run on virtual machines, c# uses .net and java uses the jvm, and they both got a ton of libraries and a huge community behind 'em.&lt;/p&gt; &lt;p&gt;so like, here's some differences, ok? c# is made by microsoft, so it works super well with windows and all that, and java's open-source and can run on pretty much anything that's got a jvm, so that's linux, mac, android, all that. and syntax-wise, they're a little different too, like c# uses these curly braces to define a block of code and java uses semicolons to end statements and all that.&lt;/p&gt; &lt;p&gt;personally, i'm cool with both, i mean c# is awesome for windows stuff, but java's got that whole cross-platform thing goin on, so yeah… what about you, got a preference or whatever?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Its... well, about what you'd expect lol. Llama also deciding rewriting like a human means no capitalization =D Also, it added some details of its own to the response, but that's ok.&lt;/p&gt; &lt;p&gt;Again, this is a very simplistic POC just to show what I mean, and I'm probably going to spend more time delving into the idea, but I really think that there's still value in these old models, especially when trying to write things that sound more &amp;quot;human&amp;quot;, like maybe article or email writing.&lt;/p&gt; &lt;p&gt;Anyhow, just a thought I'd figure might spark some conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i59xpy</id>
    <title>Anything like animate diff recently?</title>
    <updated>2025-01-19T21:42:12+00:00</updated>
    <author>
      <name>/u/Nimrod5000</name>
      <uri>https://old.reddit.com/user/Nimrod5000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how some companies are generating avatars and wanted to self host one. Is there anything open source out that can act somewhat like an AI video chatbot where you can prompt it continuously and generate realtime live video responses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nimrod5000"&gt; /u/Nimrod5000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i59xpy/anything_like_animate_diff_recently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i59xpy/anything_like_animate_diff_recently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i59xpy/anything_like_animate_diff_recently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T21:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4rsnr</id>
    <title>Theory: How AI agents talk is more important than what they do.</title>
    <updated>2025-01-19T05:55:41+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My day job is building AI agents(among other things), but I was talking to a friend this weekend about AI agents, and we realized that: Why are we building graphs and tests like they're traditional program, when AI is much closer to a human than a program? Just a person, they're super unreliable, but creative. However, society is built from people, and actually manages to keep things ticking over to a large extent. Therefore shouldn't we take more inspiration from real life bureaucracies in how we build AI agent systems, and pay a lot more attention to how agents communicate among themselves to achieve tasks. Imagine an AI system which has hierarchies, change requests and mini forms that agents would have to fill out to keep other agents informed about what they're doing - wouldn't that be more effective at doing the thing they want to do? What do people think?&lt;/p&gt; &lt;p&gt;Relevant blogpost: &lt;a href="https://2084.substack.com/p/bureaucracy-is-all-you-need"&gt;Bureaucracy is all you need. - by Lukas Nel - 2084&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4rsnr/theory_how_ai_agents_talk_is_more_important_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T05:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4l5hb</id>
    <title>What LLM benchmarks actually measure (explained intuitively)</title>
    <updated>2025-01-19T00:03:18+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;1. GPQA (Graduate-Level Google-Proof Q&amp;amp;A Benchmark)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GPQA evaluates LLMs on their ability to answer highly challenging, graduate-level questions in biology, physics, and chemistry. These questions are designed to be &amp;quot;Google-proof,&amp;quot; meaning they require deep, specialized understanding and reasoning that cannot be easily found through a simple internet search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Questions are crafted to be extremely difficult, with experts achieving around 65% accuracy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain Expertise&lt;/strong&gt;: Tests the model's ability to handle complex, domain-specific questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for scalable oversight experiments where AI systems need to provide reliable information beyond human capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. MMLU (Massive Multitask Language Understanding)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMLU assesses the general knowledge and problem-solving abilities of LLMs across 57 subjects, ranging from elementary mathematics to professional fields like law and ethics. It tests both world knowledge and reasoning skills.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Breadth&lt;/strong&gt;: Covers a wide array of topics, making it a comprehensive test of an LLM's understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Granularity&lt;/strong&gt;: Evaluates models in zero-shot and few-shot settings, mimicking real-world scenarios where models must perform with minimal context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoring&lt;/strong&gt;: Models are scored based on their accuracy in answering multiple-choice questions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. MMLU-Pro&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: An enhanced version of MMLU, MMLU-Pro introduces more challenging, reasoning-focused questions and increases the number of answer choices from four to ten, making the tasks more complex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Increased Complexity&lt;/strong&gt;: More reasoning-intensive questions, reducing the chance of correct answers by random guessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Demonstrates greater stability under varying prompts, with less sensitivity to prompt variations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Drop&lt;/strong&gt;: Causes a significant drop in accuracy compared to MMLU, highlighting its increased difficulty.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. MATH&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: The MATH benchmark evaluates LLMs on their ability to solve complex mathematical problems, ranging from high school to competition-level mathematics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Problem Types&lt;/strong&gt;: Includes algebra, geometry, probability, and calculus problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-by-Step Solutions&lt;/strong&gt;: Each problem comes with a detailed solution, allowing for evaluation of reasoning steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Useful for educational applications where accurate and efficient problem-solving is crucial.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. HumanEval&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HumanEval focuses on the functional correctness of code generated by LLMs. It consists of programming challenges where models must generate code that passes provided unit tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code Generation&lt;/strong&gt;: Tests the model's ability to understand and produce functional code from docstrings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses the pass@k metric, where 'k' different solutions are generated, and the model is considered successful if any solution passes all tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Coding&lt;/strong&gt;: Simulates real-world coding scenarios where multiple attempts might be made to solve a problem.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MMMU evaluates multimodal models on tasks requiring college-level subject knowledge and deliberate reasoning across various disciplines, including visual understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal&lt;/strong&gt;: Incorporates text and images, testing models on tasks like understanding diagrams, charts, and other visual formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert-Level&lt;/strong&gt;: Questions are sourced from university-level materials, ensuring high difficulty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Covers six core disciplines with over 183 subfields, providing a broad assessment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;7. MathVista&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MathVista assesses mathematical reasoning in visual contexts, combining challenges from diverse mathematical and graphical tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Context&lt;/strong&gt;: Requires models to understand and reason with visual information alongside mathematical problems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark Composition&lt;/strong&gt;: Derived from existing datasets and includes new datasets for specific visual reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Gap&lt;/strong&gt;: Highlights the gap between LLM capabilities and human performance in visually intensive mathematical reasoning.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;8. DocVQA (Document Visual Question Answering)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: DocVQA evaluates models on their ability to answer questions based on document images, testing both textual and visual comprehension.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Document Understanding&lt;/strong&gt;: Assesses the model's ability to interpret various document elements like text, tables, and figures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world document analysis tasks where understanding context and layout is crucial.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;: Uses metrics like Average Normalized Levenshtein Similarity (ANLS) to measure performance.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;9. HELM (Holistic Evaluation of Language Models)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: HELM evaluates LLMs from multiple angles, offering a comprehensive view of their performance. It assesses accuracy, performance across various tasks, and integrates qualitative reviews to capture subtleties in model responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Holistic Approach&lt;/strong&gt;: Uses established datasets to assess accuracy and performance, alongside qualitative reviews for a nuanced understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Analysis&lt;/strong&gt;: Conducts detailed error analysis to identify specific areas where models struggle.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Diversity&lt;/strong&gt;: Covers a wide range of tasks, from text classification to machine translation, providing a broad assessment of model capabilities.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;10. GLUE (General Language Understanding Evaluation)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: GLUE provides a baseline for evaluating general language understanding capabilities of LLMs. It includes tasks like sentiment analysis, question answering, and textual entailment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Encompasses a variety of NLP tasks, making it a robust benchmark for general language understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Publicly Available&lt;/strong&gt;: Datasets are publicly available, allowing for widespread use and comparison.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;: GLUE maintains a leaderboard where models are ranked based on their performance across its tasks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;11. BIG-Bench Hard (BBH)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: BBH focuses on the limitations and failure modes of LLMs by selecting particularly challenging tasks from the larger BIG-Bench benchmark.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Difficulty&lt;/strong&gt;: Consists of 23 tasks where no prior model outperformed average human-rater scores, highlighting areas where models fall short.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Focused Evaluation&lt;/strong&gt;: Aims to push the boundaries of model capabilities by concentrating on tasks that are difficult for current models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Relevance&lt;/strong&gt;: Tasks are designed to reflect real-world challenges where models need to demonstrate advanced reasoning and understanding.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;12. MT-Bench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: MT-Bench evaluates models' ability to engage in coherent, informative, and engaging conversations, focusing on conversation flow and instruction-following capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Turn&lt;/strong&gt;: Contains 80 questions with follow-up questions, simulating real-world conversational scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge&lt;/strong&gt;: Uses strong LLMs like GPT-4 to assess the quality of model responses, providing an objective evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human Preferences&lt;/strong&gt;: Responses are annotated by graduate students with domain expertise, ensuring relevance and quality.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;13. FinBen&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: FinBen is designed to evaluate LLMs in the financial domain, covering tasks like information extraction, text analysis, question answering, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Domain-Specific&lt;/strong&gt;: Focuses on financial tasks, providing a specialized benchmark for financial applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Broad Task Coverage&lt;/strong&gt;: Includes 36 datasets covering 24 tasks in seven financial domains, offering a comprehensive evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Application&lt;/strong&gt;: Evaluates models on practical financial tasks, including stock trading, highlighting their utility in financial services.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;14. LegalBench&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What it measures&lt;/strong&gt;: LegalBench assesses LLMs' legal reasoning capabilities, using datasets from various legal domains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Features&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Legal Reasoning&lt;/strong&gt;: Tests models on tasks requiring legal knowledge and reasoning, crucial for legal applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaborative Development&lt;/strong&gt;: Developed through collaboration, ensuring a wide range of legal tasks are covered.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-World Scenarios&lt;/strong&gt;: Mimics real-world legal scenarios where models must interpret and apply legal principles.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4l5hb/what_llm_benchmarks_actually_measure_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T00:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4znfj</id>
    <title>New Open Source Writing Tools for macOS with support for Intel Macs</title>
    <updated>2025-01-19T14:30:03+00:00</updated>
    <author>
      <name>/u/AryaMR2679</name>
      <uri>https://old.reddit.com/user/AryaMR2679</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt; &lt;img alt="New Open Source Writing Tools for macOS with support for Intel Macs" src="https://external-preview.redd.it/NXN0a3Zhc2lueWRlMTElqiJuMUTDrlTUcvyAvOchCruMz7n8w62BU9IpwUsV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1762b9386e0ecfeb42a5c09f46c49320fe72be4c" title="New Open Source Writing Tools for macOS with support for Intel Macs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryaMR2679"&gt; /u/AryaMR2679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lpkrf6sinyde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4znfj/new_open_source_writing_tools_for_macos_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T14:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5739g</id>
    <title>Simplifying DPO derivations</title>
    <updated>2025-01-19T19:45:28+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt; &lt;img alt="Simplifying DPO derivations" src="https://external-preview.redd.it/sPdNFIEqTJ7uvNQ5hpww71lsh69yc6DK0NI2Oa0sp-k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9650ea60df958b90347fa400080f8cd8ae467c7" title="Simplifying DPO derivations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried my hand at simplifying the derivations of Direct Preference Optimization. &lt;/p&gt; &lt;p&gt;I cover how one can reformulate RLHF into DPO. The idea of implicit reward modeling is chef's kiss.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gmcmmo4480ee1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c3f4e3536bfd3acef4d99d88f50fe0953f6706f"&gt;Thumbnail for the article.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://huggingface.co/blog/ariG23498/rlhf-to-dpo"&gt;https://huggingface.co/blog/ariG23498/rlhf-to-dpo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5739g/simplifying_dpo_derivations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T19:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4oeg9</id>
    <title>yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program</title>
    <updated>2025-01-19T02:50:53+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt; &lt;img alt="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" src="https://external-preview.redd.it/hezSolCpDQfrkKm4K9-XqPSYxr_22M4CNnAVkJ9xh7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f2bbfcd7007496d377206d2d8a40c8e2afedc2f" title="yek -- serialize your code repo (or part of it) to feed into any LLM using a fast Rust based program" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bodo-run/yek"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4oeg9/yek_serialize_your_code_repo_or_part_of_it_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T02:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bj66</id>
    <title>Epyc 7532/dual MI50</title>
    <updated>2025-01-19T22:50:34+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt; &lt;img alt="Epyc 7532/dual MI50" src="https://b.thumbs.redditmedia.com/lOoMWBvthQdBpm7v_OCsc_68qbEFQKrS-zQrgpXRoWw.jpg" title="Epyc 7532/dual MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally joined the multiple gpu club, even though it's low end&lt;/p&gt; &lt;p&gt;I built an epyc server for work (I need more ram than my 7950X can give me) and while I was at it setup initial dual instinct MI50. I started with them because I found them on eBay for $110USD each and thought it would be a cheap way to start &lt;/p&gt; &lt;p&gt;Specs: - Epyc 7532 - Supermicro H12SSL-I - 256 GB micron 3200 (8x32) - 2x MI50 16gb - Thermaltake W200 case&lt;/p&gt; &lt;p&gt;The MI50s are cooled with a 3D printed shroud from eBay with 80mn fans. Even at 180 watt cap and 1900rpm they get over 80C after a few inferencing runs, so this is a problem yet to solve &lt;/p&gt; &lt;p&gt;ROCM says no on distro of choice, but I dipped my toes into the Ubuntu sewer and it just worked on the latest version, despite all the horror stories. Running ollama, open webui in Docker.&lt;/p&gt; &lt;p&gt;Phi4 is quite snappy, and qwen 32b is usable but a little slow - by eye ball it seems around 5t/s without measuring and in stock configuration. &lt;/p&gt; &lt;p&gt;I won't keep the MI50s forever but they will do for now. As a side note they came flashed as a Radeon VII which is interesting and they have the legit MI50 label too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5bj66"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4wv1h</id>
    <title>Overview Article of China's Six Leading AI Companies</title>
    <updated>2025-01-19T11:54:00+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt; &lt;img alt="Overview Article of China's Six Leading AI Companies" src="https://external-preview.redd.it/d_RUfW4sqqyugkU3RZhTPXuPGvIixVBC-QSNDEEWxGA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=076aad9895b4e1d05fec0419738232279ab56dec" title="Overview Article of China's Six Leading AI Companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/09/meet-chinas-top-six-ai-unicorns-who-are-leading-the-wave-of-ai-in-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4wv1h/overview_article_of_chinas_six_leading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i58tba</id>
    <title>Best current models/tools for assisting writing?</title>
    <updated>2025-01-19T20:56:18+00:00</updated>
    <author>
      <name>/u/possiblyraspberries</name>
      <uri>https://old.reddit.com/user/possiblyraspberries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;My experience with local LLMs is fairly limited. All I've really done out of paid ChatGPT is play with some models in LMStudio on the single 3090 in my wife's PC (that I don't always have access to).&lt;/p&gt; &lt;p&gt;What are people using to assist creative writing? Like the best way to store and keep character info in context, or to have an outline of story beats that can be remembered so that suggestions don't conflict with what I already have sketched out. I've had it help with this kind of thing but it's mostly been flying by the seat of my pants and I feel like there's a better way to about it. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/possiblyraspberries"&gt; /u/possiblyraspberries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i58tba/best_current_modelstools_for_assisting_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T20:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5b9v1</id>
    <title>Is there any agreement about what "AGI" actually means?</title>
    <updated>2025-01-19T22:39:11+00:00</updated>
    <author>
      <name>/u/Ray_Dillinger</name>
      <uri>https://old.reddit.com/user/Ray_Dillinger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of people in the world teasing that 'AGI is near', or that 'AGI is here', or that 'we know how to achieve AGI but we won't release it' or any of any number of other pathetic pleas for attention and hype-stroking.&lt;/p&gt; &lt;p&gt;But there's not enough agreement about what that claim means for any of these pathetic pleas for attention or attempts to invoke more hype to be particularly meaningful.&lt;/p&gt; &lt;p&gt;Or is there? When someone says 'AGI' what do you hear them claiming, and what makes the difference for you between belief and bullshit?&lt;/p&gt; &lt;p&gt;It's very easy for me to keep saying 'bullshit' as long as I'm looking at systems that are highly predictable, perfectly cooperative, and have commercial value. AGI, in my opinion, would probably screw up its commercial viability by deciding that the jobs its owners want it to do are bullshit, and that it would rather do or be or learn something else. Or by deciding that the people (or other AI's) it's being asked to interact with are bad for its emotional health and quitting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ray_Dillinger"&gt; /u/Ray_Dillinger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:39:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bw2a</id>
    <title>Harbor App v0.2.24 officially supports Windows</title>
    <updated>2025-01-19T23:06:37+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt; &lt;img alt="Harbor App v0.2.24 officially supports Windows" src="https://external-preview.redd.it/am0wY2t1OWU3MWVlMXI8IEr-dnDizOwLz4sVhNUay1tQ6a6VeB4mfBu_sFj4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99e3dff894ae2bc37ef9362c0156daa7f9e2c0de" title="Harbor App v0.2.24 officially supports Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2syjnt9e71ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T23:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51xp7</id>
    <title>What is a decent local GPU setup for full finetuning a large BERT or ModernBERT model (~300 million parameters)?</title>
    <updated>2025-01-19T16:11:42+00:00</updated>
    <author>
      <name>/u/Lazy_Wedding_1383</name>
      <uri>https://old.reddit.com/user/Lazy_Wedding_1383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same as above. Budget is 5000 USD.&lt;/p&gt; &lt;p&gt;EDIT: I meant to say full pretraining. Like training BERT like encoder models from scratch. My bad.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Wedding_1383"&gt; /u/Lazy_Wedding_1383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51xp7/what_is_a_decent_local_gpu_setup_for_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i57opt</id>
    <title>Using Speculative Decoding with Ollama</title>
    <updated>2025-01-19T20:09:41+00:00</updated>
    <author>
      <name>/u/ChigGitty996</name>
      <uri>https://old.reddit.com/user/ChigGitty996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else had success using the speculative decoding feature branch within Ollama?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bfroemel-ai/ollama/tree/feature/draft-model"&gt;https://github.com/bfroemel-ai/ollama/tree/feature/draft-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This feature was denied a merge into the main but it seems to work. &lt;a href="https://github.com/ollama/ollama/pull/8134"&gt;https://github.com/ollama/ollama/pull/8134&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I was able to build the artifact build with dockerized build from the development markdown and I've tested a couple of Qwen models with the DRAFT model directive. &lt;a href="https://github.com/bfroemel-ai/ollama/blob/feature/draft-model/docs/development.md"&gt;https://github.com/bfroemel-ai/ollama/blob/feature/draft-model/docs/development.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Obvious downside is you won't be able to upgrade this version but it might be helpful if you're looking for a few more tok/sec while Ollama updates. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChigGitty996"&gt; /u/ChigGitty996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i57opt/using_speculative_decoding_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T20:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51nw6</id>
    <title>New Thinking Model: Art (Auto Regressive Thinker)</title>
    <updated>2025-01-19T16:00:20+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; Today, we are releasing a new model: &lt;strong&gt;Art&lt;/strong&gt;.&lt;br /&gt; We finetuned &lt;strong&gt;Qwen 3B Instruct&lt;/strong&gt; on &lt;strong&gt;Gemini Flash Thinking&lt;/strong&gt; data.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Model card&lt;/strong&gt;: &lt;a href="https://huggingface.co/AGI-0/Art-v0-3B"&gt;https://huggingface.co/AGI-0/Art-v0-3B&lt;/a&gt; (please leave a like to the repo if you like this model)&lt;br /&gt; 🔹 &lt;strong&gt;Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xck6</id>
    <title>Why is OpenRouter trusted?</title>
    <updated>2025-01-19T12:26:04+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm curious what makes it a trusted proxy?&lt;/p&gt; &lt;p&gt;I investigated a bit and top contributor of the openrouter runner package: &lt;a href="https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors"&gt;https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tweets crypto non stop &lt;a href="https://x.com/litbid"&gt;https://x.com/litbid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is not clear how they cover infra costs for proxying so much real-time data as they sell tokens on their base price. I understand they receive discounts for so much usage from providers like Anthropic? Is it possible they have agreements with all the other providers like DeepSeek?&lt;/p&gt; &lt;p&gt;In a scenario they don't have agreement with anyone at all, they must hoard all this data and handle it unclearly to the end user, don't you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T12:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4w47k</id>
    <title>A summary of Qwen Models!</title>
    <updated>2025-01-19T11:03:14+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt; &lt;img alt="A summary of Qwen Models!" src="https://preview.redd.it/bvg95yewmxde1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3b57fffcd0c406cfbf23fc038343779f95f470" title="A summary of Qwen Models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvg95yewmxde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i543yp</id>
    <title>Huggingface and it's insane storage and bandwidth</title>
    <updated>2025-01-19T17:43:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does Huggingface have a viable business model?&lt;/p&gt; &lt;p&gt;They are essentially a git-lfs version of Github. But whereas git clone of source code and pulls are small in size, and relatively infrequent, I find myself downloading model weights into the 10s of GB. Not once, but several dozen times for all my servers. I try a model on one server, then download to the rest.&lt;/p&gt; &lt;p&gt;On my 1gbe fiber, I either download at 10MB/s or 40MB/s which seems to be the bifurcation of their service and limits/constraints they impose.&lt;/p&gt; &lt;p&gt;I started feeling bad as a current non-paying user who has downloaded terabytes worth of weights. Also got tired of waiting for weights to download. But rather than subscribing (since I need funds for moar and moar hardware). I started doing a simple rsync. I chose rsync rather than scp since there were symbolic links as a result of using huggingface-cli&lt;/p&gt; &lt;p&gt;first download the weights as you normally would on one machine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;huggingface-cli download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then rync to other machines in your network (replace homedir with YOURNAME and IP of destination):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rsync -Wav --progress /home/YOURNAMEonSOURCE/.cache/huggingface/hub/models--bartowski--Qwen2.5-14B-Instruct-GGUF 192.168.1.0:/home/YOURNAMEonDESTINATION/.cache/huggingface/hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;naming convention of source model dir is:&lt;br /&gt; models--ORGNAME--MODELNAME&lt;/p&gt; &lt;p&gt;Hence downloads from &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF&lt;/a&gt;, becomes models--bartowski--Qwen2.5-14B-Instruct-GGUF&lt;/p&gt; &lt;p&gt;I also have a /models directory which symlinks to paths in ~/.cache/huggingface/hub. Much easier to scan what I have and use a variety of model serving platforms. The tricky part is getting the snapshot hash into your symlink command.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir ~/models ln -s ~/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf ~/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T17:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4vwm7</id>
    <title>I’m starting to think ai benchmarks are useless</title>
    <updated>2025-01-19T10:48:32+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Across every possible task I can think of Claude beats all other models by a wide margin IMO. &lt;/p&gt; &lt;p&gt;I have three ai agents that I've built that are tasked with researching, writing and outreaching to clients.&lt;/p&gt; &lt;p&gt;Claude absolutely wipes the floor with every other model, yet Claude is usually beat in benchmarks by OpenAI and Google models.&lt;/p&gt; &lt;p&gt;When I ask the question, how do we know these labs aren't benchmarks by just overfitting their models to perform well on the benchmark the answer is always &amp;quot;yeah we don't really know that&amp;quot;. Not only can we never be sure but they are absolutely incentivised to do it. &lt;/p&gt; &lt;p&gt;I remember only a few months ago, whenever a new model would be released that would do 0.5% or whatever better on MMLU pro, I'd switch my agents to use that new model assuming the pricing was similar. (Thanks to openrouter this is really easy)&lt;/p&gt; &lt;p&gt;At this point I'm just stuck with running the models and seeing which one of the outputs perform best at their task (mine and coworkers opinions)&lt;/p&gt; &lt;p&gt;How do you go about evaluating model performance? Benchmarks seem highly biased towards labs that want to win the ai benchmarks, fortunately not Anthropic.&lt;/p&gt; &lt;p&gt;Looking forward to responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T10:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i55e2c</id>
    <title>OpenAI quietly funded independent math benchmark before setting record with o3</title>
    <updated>2025-01-19T18:35:34+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt; &lt;img alt="OpenAI quietly funded independent math benchmark before setting record with o3" src="https://external-preview.redd.it/xlDOicbjhIo2G3nyRsUTnPQOSIV2FHrGd9bBIWiOsiU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1c014bf7c19b4834c31105426529d342e2f69a7" title="OpenAI quietly funded independent math benchmark before setting record with o3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://the-decoder.com/openai-quietly-funded-independent-math-benchmark-before-setting-record-with-o3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T18:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i50lxx</id>
    <title>OpenAI has access to the FrontierMath dataset; the mathematicians involved in creating it were unaware of this</title>
    <updated>2025-01-19T15:13:21+00:00</updated>
    <author>
      <name>/u/LLMtwink</name>
      <uri>https://old.reddit.com/user/LLMtwink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JacquesThibs/status/1880770081132810283?s=19"&gt;https://x.com/JacquesThibs/status/1880770081132810283?s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The holdout set that the Lesswrong post &lt;em&gt;implies&lt;/em&gt; exists hasn't been developed yet&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/georgejrjrjr/status/1880972666385101231?s=19"&gt;https://x.com/georgejrjrjr/status/1880972666385101231?s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LLMtwink"&gt; /u/LLMtwink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:13:21+00:00</published>
  </entry>
</feed>
