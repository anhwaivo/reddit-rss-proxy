<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-02T08:06:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j1g0rv</id>
    <title>Instruct a non-reasoning model to think using a prompt</title>
    <updated>2025-03-02T02:04:26+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At least pretend to think.. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is the prompt I used:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;First Think deeply for at least five minutes (or longer if needed) to find the best approach.&lt;/p&gt; &lt;p&gt;Document all of your raw thinking, evolving ideas, discarded options, doubts, and reasoning process inside &amp;lt;thinking&amp;gt; tags. This must be pure continuous text with no formatting, line breaks, or symbols — just unstructured internal dialogue.&lt;/p&gt; &lt;p&gt;Write as if talking to yourself, using &amp;quot;I&amp;quot; to explain your thoughts, decisions, changes of mind, and obstacles. Use &amp;quot;but&amp;quot; often to show trade-offs and why you rejected certain paths. Show how your understanding evolved and what assumptions you tested. This is a transparent, step-by-step log of how you reached your answer.&lt;/p&gt; &lt;p&gt;After the &amp;lt;thinking&amp;gt; section, give your final, fully considered, and detailed answer, directly addressing the question.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chatgpt.com/share/67c3bbbb-8788-800a-9dfb-f7ff1f645a0d"&gt;https://chatgpt.com/share/67c3bbbb-8788-800a-9dfb-f7ff1f645a0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chatgpt.com/share/67c3bc93-78c4-800a-89fb-6fddf5b73195"&gt;https://chatgpt.com/share/67c3bc93-78c4-800a-89fb-6fddf5b73195&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1g0rv/instruct_a_nonreasoning_model_to_think_using_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1g0rv/instruct_a_nonreasoning_model_to_think_using_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1g0rv/instruct_a_nonreasoning_model_to_think_using_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T02:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0o8mt</id>
    <title>The first real open source DeepResearch attempt I've seen</title>
    <updated>2025-03-01T01:43:56+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Search-R1&lt;/strong&gt; is a reproduction of &lt;strong&gt;DeepSeek-R1(-Zero)&lt;/strong&gt; methods for &lt;em&gt;training reasoning and searching (tool-call) interleaved LLMs&lt;/em&gt;. Built upon &lt;a href="https://github.com/volcengine/verl"&gt;veRL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Through RL (rule-based outcome reward), the 3B &lt;strong&gt;base&lt;/strong&gt; LLM (both Qwen2.5-3b-base and Llama3.2-3b-base) develops reasoning and search engine calling abilities all on its own.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PeterGriffinJin/Search-R1/tree/main"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1ljni</id>
    <title>ChatGPT o1 pro = o1-preview-2024-09-12 (API)?</title>
    <updated>2025-03-02T07:36:00+00:00</updated>
    <author>
      <name>/u/alw9</name>
      <uri>https://old.reddit.com/user/alw9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to set up a workspace using ChatGPT o1 pro as base, on AnythingLLM. However, I'm not sure which API is linked to ChatGPT o1 pro. All the listings here (&lt;a href="https://platform.openai.com/docs/pricing"&gt;https://platform.openai.com/docs/pricing&lt;/a&gt;) use different names. Can anyone tell me which one is ChatGPT o1 pro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alw9"&gt; /u/alw9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1ljni/chatgpt_o1_pro_o1preview20240912_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1ljni/chatgpt_o1_pro_o1preview20240912_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1ljni/chatgpt_o1_pro_o1preview20240912_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T07:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1cvzg</id>
    <title>Deepseek R1 Distill: 70B iQ2S+imatrix vs 32B iQ4+imatrix</title>
    <updated>2025-03-01T23:27:42+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all. So I have ran Llama 3 70B IQ2S before on my single 3090 completely in VRAM and was quite impressed. Tried 32B Qwen 2.5 and overall, liked it more (mostly due to speed). &lt;/p&gt; &lt;p&gt;I haven't tried the Deepseek distills yet with the above quants, but I was wondering if anyone tried the reasoning distills at low quants? (Specifically 70B). &lt;/p&gt; &lt;p&gt;My primary question is that if reasoning can counter the higher perplexity of the lower Quants? &lt;/p&gt; &lt;p&gt;Also, between the 32B and the 70B non reasoning models, I did like 32B, but like DS V3, tacking reasoning on it can potentially increase output quality a lot on larger models, so 70B might be better than 32B even if the 70B is at iQ2s? &lt;/p&gt; &lt;p&gt;Was wondering if anyone else has compared both. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1cvzg/deepseek_r1_distill_70b_iq2simatrix_vs_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1cvzg/deepseek_r1_distill_70b_iq2simatrix_vs_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1cvzg/deepseek_r1_distill_70b_iq2simatrix_vs_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T23:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1btrr</id>
    <title>Intro to RAG with ollama?</title>
    <updated>2025-03-01T22:38:26+00:00</updated>
    <author>
      <name>/u/la_baguette77</name>
      <uri>https://old.reddit.com/user/la_baguette77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to take a dive into RAG, unfortunately the ollama example requires chromadb which i struggle to install.&lt;/p&gt; &lt;p&gt;Langchain seems not to support ollama (and i find it hard to understand)&lt;/p&gt; &lt;p&gt;FAISS seems a bit overkill, anybody got any good first starting points?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/la_baguette77"&gt; /u/la_baguette77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1btrr/intro_to_rag_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1btrr/intro_to_rag_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1btrr/intro_to_rag_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T22:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1h1j3</id>
    <title>Dual RTX 6000 Ada or swap one for a 5090</title>
    <updated>2025-03-02T02:59:36+00:00</updated>
    <author>
      <name>/u/starboard3751</name>
      <uri>https://old.reddit.com/user/starboard3751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don’t think I’ve seen a similar setup here (to my knowledge) but I’ve been running 2 RTX 6000 Adas, Ryzen 9950X, 128gb RAM. &lt;/p&gt; &lt;p&gt;Yes, not really optimized, PCIe 8x, not running full DDR5 speeds sure, but I’ve got 96gb of VRAM and even if it’s not as efficient as it can be at least I could load big models into VRAM on a prosumer based platform. Also… probably not that cost efficient. &lt;/p&gt; &lt;p&gt;But recently I’ve been running some models that could fit into a 5090 on its own and despite being the same die &lt;em&gt;as the 4090&lt;/em&gt;, the 6000 is slower but at least it’s dual slot. &lt;/p&gt; &lt;p&gt;With the 5090 being dual slot, I’m considering taking the 16gb VRAM hit to have a faster main GPU. I previously ran a 4090 and a 6000, yeah the driver mismatch was annoying, but I’m wondering if that’s gotten any better. Just been missing the extra speed and wondering if anyone has thoughts on this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starboard3751"&gt; /u/starboard3751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1h1j3/dual_rtx_6000_ada_or_swap_one_for_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1h1j3/dual_rtx_6000_ada_or_swap_one_for_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1h1j3/dual_rtx_6000_ada_or_swap_one_for_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T02:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yxm1</id>
    <title>AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well</title>
    <updated>2025-03-01T13:00:07+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt; &lt;img alt="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" src="https://external-preview.redd.it/kjFdjz1K9W0AjIQXXFkEbN2PFu7C3Ga1YSe9tdCduuw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dfacaf6bed989273560dbf7b34460efac41ca6a" title="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-rx-9070-series-gpus-will-feature-support-for-rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0uoht</id>
    <title>Chain of Draft: Thinking Faster by Writing Less</title>
    <updated>2025-03-01T08:10:14+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt; &lt;img alt="Chain of Draft: Thinking Faster by Writing Less" src="https://b.thumbs.redditmedia.com/fwljouG_I7UUcaN7hDPRHRGsfe6zHb3U7bt9TnwQ_OA.jpg" title="Chain of Draft: Thinking Faster by Writing Less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.18600"&gt;https://arxiv.org/abs/2502.18600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CoD System prompt:&lt;/p&gt; &lt;p&gt;Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j0uoht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0r3go</id>
    <title>Day 6: One More Thing, DeepSeek-V3/R1 Inference System Overview</title>
    <updated>2025-03-01T04:19:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md"&gt;https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T04:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0ync3</id>
    <title>TinyR1-32B-Preview: SuperDistillation Achieves Near-R1 Performance with Just 5% of Parameters.</title>
    <updated>2025-03-01T12:43:48+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We applied supervised fine-tuning (SFT) to Deepseek-R1-Distill-Qwen-32B across three target domains—Mathematics, Code, and Science — using the 360-LLaMA-Factory training framework to produce three domain-specific models. We used questions from open-source data as seeds. Meanwhile, responses for mathematics, coding, and science tasks were generated by R1, creating specialized models for each domain. Building on this, we leveraged the Mergekit tool from the Arcee team to combine multiple models, creating Tiny-R1-32B-Preview, which demonstrates strong overall performance.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T12:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yyg1</id>
    <title>AMD Ryzen AI Max+ Pro 395 "Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%</title>
    <updated>2025-03-01T13:01:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt; &lt;img alt="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" src="https://external-preview.redd.it/BkZkseIe-TX7LG_20ukN9CeYcn2i-ty5QOZ4F8-FqfE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d471c1208a7a116839a4c4a45eef1d42f16e7d2c" title="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-benchmarked-in-cpu-mark-outperforms-core-i9-14900hx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1axwy</id>
    <title>5090(32GB vRAM) vs 4090d(48GB vRAM) Did anyone get a 5090? I saw they are out for 2300-2400$ (but not yet in EU)</title>
    <updated>2025-03-01T21:58:59+00:00</updated>
    <author>
      <name>/u/Dry_Parfait2606</name>
      <uri>https://old.reddit.com/user/Dry_Parfait2606</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to see the performance of the 5090... The &amp;lt;2TB/s(1,79TB/s) bandwidth is remarkable!! That's the only rtx that personally would make sense to buy new. For me that's the only deal in town, (the 4090d with 48GB vRAM is next to it)&lt;/p&gt; &lt;p&gt;__________________________________________________________________&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: 5x5090 make no sense. That's MI325 price range&lt;/strong&gt; (as commented below)&lt;/p&gt; &lt;p&gt;__________________________________________________________________&lt;/p&gt; &lt;p&gt;...&lt;strong&gt;5x of 5090 on a cheap Turin/Genoa/Bergamo setup... (fullx16 pcie gen5 bandwidth)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for a 70b16fp &amp;lt;2TB/s, probably 50+t/s, for 12,5-13k&lt;/p&gt; &lt;p&gt;__________________________________________________________________&lt;/p&gt; &lt;p&gt;...&lt;strong&gt;3x of 4090d(48GB) + setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for a 70b16fp &amp;lt;1TB/s, probably 20t/s, for 8,5-9k&lt;/p&gt; &lt;p&gt;&lt;strong&gt;...x4 of 4090d(48GB) + setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for a 70b16fp &amp;lt;1TB/s, probably 20t/s, for 11-11,5k&lt;/p&gt; &lt;p&gt;&lt;strong&gt;...x20 of 4090d(48GB) + 4x4x4x4 bifurcation setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for a 405b16fp &amp;lt;1TB/s, probably 20t/s, for 50-53k&lt;/p&gt; &lt;p&gt;__________________________________________________________________&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: I figured out that there are 3GB instead of 2GB vram chips that could go on a 5090...so 48GB on a 5090 might be possible...&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Parfait2606"&gt; /u/Dry_Parfait2606 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1axwy/509032gb_vram_vs_4090d48gb_vram_did_anyone_get_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1axwy/509032gb_vram_vs_4090d48gb_vram_did_anyone_get_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1axwy/509032gb_vram_vs_4090d48gb_vram_did_anyone_get_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T21:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1893y</id>
    <title>3 Way convo with 2 Sesame AI's and myself</title>
    <updated>2025-03-01T19:58:10+00:00</updated>
    <author>
      <name>/u/shadowdog000</name>
      <uri>https://old.reddit.com/user/shadowdog000</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shadowdog000"&gt; /u/shadowdog000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=R4lkDwDLlh0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1893y/3_way_convo_with_2_sesame_ais_and_myself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1893y/3_way_convo_with_2_sesame_ais_and_myself/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T19:58:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1k2uz</id>
    <title>Tensor Parallel bottlenecks: CPU? Resizable BAR?</title>
    <updated>2025-03-02T05:57:20+00:00</updated>
    <author>
      <name>/u/Zidrewndacht</name>
      <uri>https://old.reddit.com/user/Zidrewndacht</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a pair of RTX 3090 on a chinese Huananzhi X99-T8 with a Xeon E5-2696v3 and 128GB (8x16GB) DDR3 1866. &lt;/p&gt; &lt;p&gt;Each card is installed on a full PCIe 3.0 x16 slot. AIDA64 GPU test gives about 12 GB/s transfer speed from each card to CPU, which is as expected for PCIe 3.0 x16.&lt;/p&gt; &lt;p&gt;However, using TabbyAPI and enabling tensor parallel makes token generation substantially &lt;em&gt;slower&lt;/em&gt; on all my tests with different models and settings. By a &lt;em&gt;long&lt;/em&gt; shot. And both GPUs stay almost idle (about 100W each). Bus load seems to stay as 1%. A single CPU core stays pegged at 3.8GHz with 100% load, during the whole inference time.&lt;/p&gt; &lt;p&gt;For example: prompting &amp;quot;Count from 1 to 100 separated by commas&amp;quot; to Qwen 14B 6.5bpw (FP16 KV cache):&lt;/p&gt; &lt;p&gt;Tensor parallel &lt;strong&gt;on&lt;/strong&gt;:&lt;br /&gt; PP 207.03 T/s, TG: &lt;strong&gt;10.01 T/s&lt;/strong&gt;;&lt;/p&gt; &lt;p&gt;Tensor parallel &lt;strong&gt;off&lt;/strong&gt; (manual &lt;strong&gt;0%/100% split&lt;/strong&gt; so it uses only the 2nd GPU - which has no display attached or anything else running at all):&lt;br /&gt; PP 987.58 T/s, TG: &lt;strong&gt;44.38 T/s&lt;/strong&gt;;&lt;/p&gt; &lt;p&gt;Tensor parallel &lt;strong&gt;off&lt;/strong&gt; (manual &lt;strong&gt;50%/50% split&lt;/strong&gt; so both GPUs have work to do alternately):&lt;br /&gt; PP 167.99 T/s, TG: &lt;strong&gt;36.45 T/s&lt;/strong&gt;;&lt;/p&gt; &lt;p&gt;Those results are &lt;strong&gt;not&lt;/strong&gt; using speculative decoding. &lt;/p&gt; &lt;p&gt;Most results I find online give tensor parallel a performance increase from zero to almost double, but nothing like such a substantial &lt;em&gt;decrease&lt;/em&gt; in performance. Sure, a Haswell Xeon is no Epyc Rome or anything, but... it's the fastest Haswell E5 Xeon, shouldn't have &amp;gt;4x slower single-threaded performance than modern hardware by itself, surely? Typical CPU benchmarks show single-threaded performance about half of modern Raptor Lake stuff, for example.&lt;/p&gt; &lt;p&gt;Other considerations:&lt;/p&gt; &lt;p&gt;The motherboard doesn't have support for Resizable BAR. &lt;strong&gt;xCuri0/ReBarUEFI&lt;/strong&gt; can patch that but I don't have enough evidence that it'd work without issues on this board with a dual-GPU situation (in fact I've once found someone mentioning that adding a second GPU explicitly made it &lt;em&gt;not&lt;/em&gt; work because a single card used all BAR space available or something, preventing booting with a second card installed entirely... so I'm wary).&lt;/p&gt; &lt;p&gt;This is my main desktop, running Windows 11 LTSC. I can't move to Linux or dedicate this hardware to a Linux server at all, so I didn't even test it with Linux. I can't use Hyper-V/WSL as well (as that forces VMware Workstation, which I need, to run at a snail pace over Hyper-V itself).&lt;/p&gt; &lt;p&gt;So, here's the question: &lt;strong&gt;What's the bottleneck for tensor parallel in&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;TabbyAPI/exl2&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;, really?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Has anyone tested this on ancient Haswell Xeons?&lt;/p&gt; &lt;p&gt;Does tensor parallelism &lt;em&gt;require&lt;/em&gt; Resizable BAR (possibly to prevent the CPU having to move the BAR constantly or something)? Is it a known issue?&lt;/p&gt; &lt;p&gt;Or... am I missing something else entirely? What gives?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; sloooow tensor parallel performance on a Haswell CPU, single CPU core bottleneck, no resizable BAR available, Windows. Looking for other experiences, opinions, suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zidrewndacht"&gt; /u/Zidrewndacht &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1k2uz/tensor_parallel_bottlenecks_cpu_resizable_bar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1k2uz/tensor_parallel_bottlenecks_cpu_resizable_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1k2uz/tensor_parallel_bottlenecks_cpu_resizable_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T05:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13hf5</id>
    <title>How are people deploying apps with AI functionality and it not costing them an absolute fortune?</title>
    <updated>2025-03-01T16:35:17+00:00</updated>
    <author>
      <name>/u/joncording12</name>
      <uri>https://old.reddit.com/user/joncording12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all seen lots of web apps coming out which include AI chat functionality. The bit for me I'm most curious about, is a huge amount of them seem to have a free version without chat limits. &lt;/p&gt; &lt;p&gt;I'm building an app at the moment, and while intended for personal use, I'll likely open it up to the world as I think it's pretty cool. I'm mucking about with using an LLM which is going very well. I intend to block this functionality to public users unless they bring-their-own API keys. &lt;/p&gt; &lt;p&gt;In a perfect world, I'd love to have a basic/limited version for free users and then charge a minimal monthly fee which gives them the full version with my app. &lt;/p&gt; &lt;p&gt;But, how are people actually implementing this without it costing an arm and a leg? Are many devs just outright swallowing cost in anticipation of success from a paid offering? &lt;/p&gt; &lt;p&gt;I recently saw &lt;a href="https://www.open-health.me/"&gt;https://www.open-health.me/&lt;/a&gt; on Reddit - and even digging through the source code that is what the developer appears to be doing. I tried asking him but not received a response, but by all appearances it is what people are doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joncording12"&gt; /u/joncording12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13hf5/how_are_people_deploying_apps_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kj9b</id>
    <title>I want to train AI to intimately understand an entire regulations booklet for my work- how do I do that, and cheaply?</title>
    <updated>2025-03-02T06:26:37+00:00</updated>
    <author>
      <name>/u/10c70377</name>
      <uri>https://old.reddit.com/user/10c70377</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says pretty much. I work in engineering and we have an 600 page booklet of regulations we must adhere to. &lt;/p&gt; &lt;p&gt;ChatGPT can kinda answer questions, but it isnt genuinely smart like an engineer in the field would be - &lt;/p&gt; &lt;p&gt;I am wondering, is there a way I can get an AI to understand it and cheaply?&lt;/p&gt; &lt;p&gt;One solution I thought of, was to go through the booklet myself and rewrite it in another document that is more easily text-consumable for an AI, and teach them page-wise. Then I think - well it can't be done in a chat - how do I make it keep the memory of what it's learnt and then call upon it when I need to via API call if it was used in an app?&lt;/p&gt; &lt;p&gt;Thanks for those reading the post so far .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10c70377"&gt; /u/10c70377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T06:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j17d93</id>
    <title>GMK confirms EVO-X2 Mini-PC with Ryzen AI MAX+ PRO 395 "Strix Halo" will launch between Q1/Q2 2025 - VideoCardz.com</title>
    <updated>2025-03-01T19:19:53+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j17d93/gmk_confirms_evox2_minipc_with_ryzen_ai_max_pro/"&gt; &lt;img alt="GMK confirms EVO-X2 Mini-PC with Ryzen AI MAX+ PRO 395 &amp;quot;Strix Halo&amp;quot; will launch between Q1/Q2 2025 - VideoCardz.com" src="https://external-preview.redd.it/lUGT65nSTdLTrmC3Wx_JIlBIlayJpR7LwuFneCRYRuI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4d2065050d74bf11fb8daec38dc6d55bb545adc" title="GMK confirms EVO-X2 Mini-PC with Ryzen AI MAX+ PRO 395 &amp;quot;Strix Halo&amp;quot; will launch between Q1/Q2 2025 - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/gmk-confirms-evo-x2-mini-pc-with-ryzen-ai-max-pro-395-strix-halo-will-launch-between-q1-q2-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j17d93/gmk_confirms_evox2_minipc_with_ryzen_ai_max_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j17d93/gmk_confirms_evox2_minipc_with_ryzen_ai_max_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T19:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j12jh4</id>
    <title>Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!</title>
    <updated>2025-03-01T15:54:30+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt; &lt;img alt="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" src="https://external-preview.redd.it/yySKZcwLWOVfiw-FCxXZZGyMsX-eiuOXklZ8rkauveI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f604a8bea97f9cec32746dee177776a5626460f1" title="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Llama-3.3-R1-70B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0n56h</id>
    <title>Finally, a real-time low-latency voice chat model</title>
    <updated>2025-03-01T00:47:24+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't seen it yet, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tried it fow a few minutes earlier today and another 15 minutes now. I tested and it remembered our chat earlier. It is the first time that I treated AI as a person and felt that I needed to mind my manners and say &amp;quot;thank you&amp;quot; and &amp;quot;good bye&amp;quot; at the end of the conversation.&lt;/p&gt; &lt;p&gt;Honestly, I had more fun chatting with this than some of my ex-girlfriends!&lt;/p&gt; &lt;p&gt;Github here (code not yet dropped):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;``` Model Sizes: We trained three model sizes, delineated by the backbone and decoder sizes:&lt;/p&gt; &lt;p&gt;Tiny: 1B backbone, 100M decoder Small: 3B backbone, 250M decoder Medium: 8B backbone, 300M decoder Each model was trained with a 2048 sequence length (~2 minutes of audio) over five epochs. ```&lt;/p&gt; &lt;p&gt;The model sizes look friendly to local deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j10d5g</id>
    <title>Can you ELI5 why a temp of 0 is bad?</title>
    <updated>2025-03-01T14:14:16+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like common knowledge that &amp;quot;you almost always need temp &amp;gt; 0&amp;quot; but I find this less authoritative than everyone believes. I understand if one is writing creatively, he'd use higher temps to arrive at less boring ideas, but what if the prompts are for STEM topics or just factual information? Wouldn't higher temps force the llm to wonder away from the more likely correct answer, into a maze of more likely wrong answers, and effectively hallucinate more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T14:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1dqou</id>
    <title>Looks like NVIDIA will show off N1X on Gomputex</title>
    <updated>2025-03-02T00:08:35+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to the latest talk, NVIDIA will show off their first AI PC design at Computex (we already knew that it was slated for a release by the end of the year).&lt;/p&gt; &lt;p&gt;We now have early Geekbench results on Windows 11 (at least for 4 low power CPU cores):&lt;/p&gt; &lt;p&gt;1169 in single core 2417 in multi-core&lt;/p&gt; &lt;p&gt;It is likely that it will have higher power ones to.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://beebom.com/nvidias-rumored-n1x-arm-chip-windows-laptops-shows-up-geekbench/"&gt;https://beebom.com/nvidias-rumored-n1x-arm-chip-windows-laptops-shows-up-geekbench/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Hate the fact that you can't edit titles and that auto correct have a tendency to mess things upp. :( ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T00:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j11js6</id>
    <title>I bought 4090D with 48GB VRAM. How to test the performance?</title>
    <updated>2025-03-01T15:10:21+00:00</updated>
    <author>
      <name>/u/slavik-f</name>
      <uri>https://old.reddit.com/user/slavik-f</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paid $3k, shipped from Hong Kong. Received yesterday.&lt;/p&gt; &lt;p&gt;Obviously, the card is modified, and the spec said: &amp;quot;48GB GDDR6 256-bit&amp;quot;. Original 4090/4090D comes with GDDR6X 384-bit.&lt;/p&gt; &lt;p&gt;I installed it to my Dell Precision T7920 (Xeon Gold 5218, 384GB DDR4 RAM, 1400W PSU). I'm running few models with Ollama and it works great so far.&lt;/p&gt; &lt;p&gt;I had RTX 3090 and I even was able to put both GPUs in that system, so now I have 48+24 = 72GB VRAM! When I run load on both GPUs - my 1kW UPS is beeping, showing that I'm using over 100% of it's power (it can do over 100% for few seconds), so looks like I'll need to upgrade it...&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvidia-smi Sat Mar 1 15:00:26 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 560.35.05 Driver Version: 560.35.05 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Off | 00000000:0B:00.0 Off | N/A | | 0% 42C P8 19W / 350W | 4MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 4090 D Off | 00000000:0C:00.0 Off | Off | | 30% 48C P0 50W / 425W | 4MiB / 49140MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when I tried to measure memory bandwidth - I can't find a way to do it. Can someone help me here? How can I measure it?&lt;/p&gt; &lt;p&gt;Also, is there a way to measure Int8 perf (TOPS) ?&lt;/p&gt; &lt;p&gt;Looks like Windows has few more tools to get such data. But I'm on Ubuntu.&lt;/p&gt; &lt;p&gt;Running Ollama with qwen2.5-72b-instruct-q4_K_M (47GB) model with 16k context, on 2 GPUs I'm getting&lt;/p&gt; &lt;p&gt;- 263 t/s for prompt&lt;/p&gt; &lt;p&gt;- 16.6 t/s for response&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 1:&lt;/strong&gt; using &lt;a href="http://ghcr.io/huggingface/gpu-fryer"&gt;ghcr.io/huggingface/gpu-fryer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- RTX 3090: 22 TFLOPS&lt;/p&gt; &lt;p&gt;- RTX 4090D: 49 TFLOPS&lt;/p&gt; &lt;p&gt;I wonder what kind of TFLOPS is it - fp16?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 2&lt;/strong&gt;: using &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md"&gt;llama-bench&lt;/a&gt; (more details in the thread):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 3090&lt;/strong&gt; vs &lt;strong&gt;RTX 4090D&lt;/strong&gt; with qwen2.5-code 32b (18.5GB) model:&lt;/p&gt; &lt;p&gt;- pp512 | 1022.09 vs 2118.70 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 35.28 vs 41.16 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 4090D&lt;/strong&gt; with qwen2.5:72b (47GB) model:&lt;/p&gt; &lt;p&gt;- pp512 | 1001.62 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 18.45 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 3:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4090D vs 4090 for TheBloke/Llama-2-7B-GGUF llama-2-7b.Q4_0.gguf (3.6GB):&lt;/p&gt; &lt;p&gt;- pp512: 9591 vs 14380 t/s&lt;/p&gt; &lt;p&gt;- tg128: 174 vs 187 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slavik-f"&gt; /u/slavik-f &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1idpv</id>
    <title>Is This a Good Deal for $1000? (RTX 3090 24GB, Ryzen 5 5600 X, X570 Aorus Elitei7 + 16GB DDR4)</title>
    <updated>2025-03-02T04:14:32+00:00</updated>
    <author>
      <name>/u/givingupeveryd4y</name>
      <uri>https://old.reddit.com/user/givingupeveryd4y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I found this used gaming PC, I would use it as software development machine (currently stuck on i7-4770k based PC):&lt;/p&gt; &lt;p&gt;- RTX 3090 24Gb Gigabyte Gaming PC&lt;br /&gt; - Ryzen 5 5600x(4.6GHz, 35MB, 65W, AM4)&lt;br /&gt; - Gigabyte X570 Aorus Elite, AMD X570, DDR4, ATX&lt;br /&gt; - Kingston 16GB 3200MHz DDR4&lt;br /&gt; - 750W Corsair RM750i, 80+ Gold&lt;br /&gt; - Thermaltake Commander C31 TG Snow&lt;/p&gt; &lt;p&gt;I'm wondering if it's worth the price ($1k). I would get more ram down the line, and perhaps even extra 3090. I was thinking about running qwen 2.5 32B , Q4 with 32k context for local dev (haven't run anything over 1B atm, looking for recommendations on this as well). WDYT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/givingupeveryd4y"&gt; /u/givingupeveryd4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T04:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0tnsr</id>
    <title>We're still waiting Sam...</title>
    <updated>2025-03-01T06:59:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt; &lt;img alt="We're still waiting Sam..." src="https://preview.redd.it/31jfuybv01me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128f969cd722b072d73b4d77393ee7c0bc1b057b" title="We're still waiting Sam..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/31jfuybv01me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13cwq</id>
    <title>Qwen: “deliver something next week through opensource”</title>
    <updated>2025-03-01T16:29:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt; &lt;img alt="Qwen: “deliver something next week through opensource”" src="https://preview.redd.it/knfs0pgpu3me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=848a461104672e52b7bade6e6a4ea8b55f90ba90" title="Qwen: “deliver something next week through opensource”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Not sure if we can surprise you a lot but we will definitely deliver something next week through opensource.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knfs0pgpu3me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:29:57+00:00</published>
  </entry>
</feed>
